; ModuleID = 'src/P503/generic/fp_generic.c'
source_filename = "src/P503/generic/fp_generic.c"
target datalayout = "e-m:o-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-apple-macosx10.13.0"

@p503x2 = external local_unnamed_addr constant [8 x i64], align 16
@p503x4 = external local_unnamed_addr constant [8 x i64], align 16
@p503 = external local_unnamed_addr constant [8 x i64], align 16
@p503p1 = external local_unnamed_addr constant [8 x i64], align 16

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @mp_sub503_p2(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i74.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i74.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  %arrayidx10.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %sub8.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i74.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i74.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  %arrayidx10.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %sub8.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i74.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i74.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  %arrayidx10.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %sub8.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i74.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i74.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  %arrayidx10.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %sub8.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i74.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i74.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  %arrayidx10.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %sub8.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i74.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i74.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  %arrayidx10.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %sub8.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  %arrayidx10.7 = getelementptr inbounds i64, i64* %c, i64 7
  %16 = load i64, i64* %c, align 8, !tbaa !3
  %17 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 0), align 16, !tbaa !3
  %add21 = add i64 %17, %16
  store i64 %add21, i64* %c, align 8, !tbaa !3
  %xor.i61 = xor i64 %add21, %16
  %xor1.i63 = xor i64 %17, %16
  %or.i64 = or i64 %xor.i61, %xor1.i63
  %xor2.i65 = xor i64 %or.i64, %add21
  %or29 = lshr i64 %xor2.i65, 63
  %18 = load i64, i64* %arrayidx10.1, align 8, !tbaa !3
  %add.1 = add i64 %18, %or29
  %19 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 1), align 8, !tbaa !3
  %add21.1 = add i64 %19, %add.1
  store i64 %add21.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %20 = xor i64 %add.1, -9223372036854775808
  %xor2.i71.1 = and i64 %20, %18
  %xor.i61.1 = xor i64 %add21.1, %add.1
  %xor1.i63.1 = xor i64 %19, %add.1
  %or.i64.1 = or i64 %xor.i61.1, %xor1.i63.1
  %xor2.i65.1 = xor i64 %or.i64.1, %add21.1
  %shr.i7275.1 = or i64 %xor2.i65.1, %xor2.i71.1
  %or29.1 = lshr i64 %shr.i7275.1, 63
  %21 = load i64, i64* %arrayidx10.2, align 8, !tbaa !3
  %add.2 = add i64 %21, %or29.1
  %22 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 2), align 16, !tbaa !3
  %add21.2 = add i64 %22, %add.2
  store i64 %add21.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %23 = xor i64 %add.2, -9223372036854775808
  %xor2.i71.2 = and i64 %23, %21
  %xor.i61.2 = xor i64 %add21.2, %add.2
  %xor1.i63.2 = xor i64 %22, %add.2
  %or.i64.2 = or i64 %xor.i61.2, %xor1.i63.2
  %xor2.i65.2 = xor i64 %or.i64.2, %add21.2
  %shr.i7275.2 = or i64 %xor2.i65.2, %xor2.i71.2
  %or29.2 = lshr i64 %shr.i7275.2, 63
  %24 = load i64, i64* %arrayidx10.3, align 8, !tbaa !3
  %add.3 = add i64 %24, %or29.2
  %25 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 3), align 8, !tbaa !3
  %add21.3 = add i64 %25, %add.3
  store i64 %add21.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %26 = xor i64 %add.3, -9223372036854775808
  %xor2.i71.3 = and i64 %26, %24
  %xor.i61.3 = xor i64 %add21.3, %add.3
  %xor1.i63.3 = xor i64 %25, %add.3
  %or.i64.3 = or i64 %xor.i61.3, %xor1.i63.3
  %xor2.i65.3 = xor i64 %or.i64.3, %add21.3
  %shr.i7275.3 = or i64 %xor2.i65.3, %xor2.i71.3
  %or29.3 = lshr i64 %shr.i7275.3, 63
  %27 = load i64, i64* %arrayidx10.4, align 8, !tbaa !3
  %add.4 = add i64 %27, %or29.3
  %28 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 4), align 16, !tbaa !3
  %add21.4 = add i64 %28, %add.4
  store i64 %add21.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %29 = xor i64 %add.4, -9223372036854775808
  %xor2.i71.4 = and i64 %29, %27
  %xor.i61.4 = xor i64 %add21.4, %add.4
  %xor1.i63.4 = xor i64 %28, %add.4
  %or.i64.4 = or i64 %xor.i61.4, %xor1.i63.4
  %xor2.i65.4 = xor i64 %or.i64.4, %add21.4
  %shr.i7275.4 = or i64 %xor2.i65.4, %xor2.i71.4
  %or29.4 = lshr i64 %shr.i7275.4, 63
  %30 = load i64, i64* %arrayidx10.5, align 8, !tbaa !3
  %add.5 = add i64 %30, %or29.4
  %31 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 5), align 8, !tbaa !3
  %add21.5 = add i64 %31, %add.5
  store i64 %add21.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %32 = xor i64 %add.5, -9223372036854775808
  %xor2.i71.5 = and i64 %32, %30
  %xor.i61.5 = xor i64 %add21.5, %add.5
  %xor1.i63.5 = xor i64 %31, %add.5
  %or.i64.5 = or i64 %xor.i61.5, %xor1.i63.5
  %xor2.i65.5 = xor i64 %or.i64.5, %add21.5
  %shr.i7275.5 = or i64 %xor2.i65.5, %xor2.i71.5
  %or29.5 = lshr i64 %shr.i7275.5, 63
  %add.6 = add i64 %sub8.6, %or29.5
  %33 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 6), align 16, !tbaa !3
  %add21.6 = add i64 %33, %add.6
  store i64 %add21.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %34 = xor i64 %add.6, -9223372036854775808
  %xor2.i71.6 = and i64 %34, %sub8.6
  %xor.i61.6 = xor i64 %add21.6, %add.6
  %xor1.i63.6 = xor i64 %33, %add.6
  %or.i64.6 = or i64 %xor.i61.6, %xor1.i63.6
  %xor2.i65.6 = xor i64 %or.i64.6, %add21.6
  %shr.i7275.6 = or i64 %xor2.i65.6, %xor2.i71.6
  %or29.6 = lshr i64 %shr.i7275.6, 63
  %add.7 = add i64 %sub8.7, %or29.6
  %35 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 7), align 8, !tbaa !3
  %add21.7 = add i64 %35, %add.7
  store i64 %add21.7, i64* %arrayidx10.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @mp_sub503_p4(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i74.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i74.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  %arrayidx10.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %sub8.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i74.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i74.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  %arrayidx10.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %sub8.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i74.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i74.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  %arrayidx10.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %sub8.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i74.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i74.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  %arrayidx10.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %sub8.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i74.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i74.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  %arrayidx10.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %sub8.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i74.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i74.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  %arrayidx10.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %sub8.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  %arrayidx10.7 = getelementptr inbounds i64, i64* %c, i64 7
  %16 = load i64, i64* %c, align 8, !tbaa !3
  %17 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 0), align 16, !tbaa !3
  %add21 = add i64 %17, %16
  store i64 %add21, i64* %c, align 8, !tbaa !3
  %xor.i61 = xor i64 %add21, %16
  %xor1.i63 = xor i64 %17, %16
  %or.i64 = or i64 %xor.i61, %xor1.i63
  %xor2.i65 = xor i64 %or.i64, %add21
  %or29 = lshr i64 %xor2.i65, 63
  %18 = load i64, i64* %arrayidx10.1, align 8, !tbaa !3
  %add.1 = add i64 %18, %or29
  %19 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 1), align 8, !tbaa !3
  %add21.1 = add i64 %19, %add.1
  store i64 %add21.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %20 = xor i64 %add.1, -9223372036854775808
  %xor2.i71.1 = and i64 %20, %18
  %xor.i61.1 = xor i64 %add21.1, %add.1
  %xor1.i63.1 = xor i64 %19, %add.1
  %or.i64.1 = or i64 %xor.i61.1, %xor1.i63.1
  %xor2.i65.1 = xor i64 %or.i64.1, %add21.1
  %shr.i7275.1 = or i64 %xor2.i65.1, %xor2.i71.1
  %or29.1 = lshr i64 %shr.i7275.1, 63
  %21 = load i64, i64* %arrayidx10.2, align 8, !tbaa !3
  %add.2 = add i64 %21, %or29.1
  %22 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 2), align 16, !tbaa !3
  %add21.2 = add i64 %22, %add.2
  store i64 %add21.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %23 = xor i64 %add.2, -9223372036854775808
  %xor2.i71.2 = and i64 %23, %21
  %xor.i61.2 = xor i64 %add21.2, %add.2
  %xor1.i63.2 = xor i64 %22, %add.2
  %or.i64.2 = or i64 %xor.i61.2, %xor1.i63.2
  %xor2.i65.2 = xor i64 %or.i64.2, %add21.2
  %shr.i7275.2 = or i64 %xor2.i65.2, %xor2.i71.2
  %or29.2 = lshr i64 %shr.i7275.2, 63
  %24 = load i64, i64* %arrayidx10.3, align 8, !tbaa !3
  %add.3 = add i64 %24, %or29.2
  %25 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 3), align 8, !tbaa !3
  %add21.3 = add i64 %25, %add.3
  store i64 %add21.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %26 = xor i64 %add.3, -9223372036854775808
  %xor2.i71.3 = and i64 %26, %24
  %xor.i61.3 = xor i64 %add21.3, %add.3
  %xor1.i63.3 = xor i64 %25, %add.3
  %or.i64.3 = or i64 %xor.i61.3, %xor1.i63.3
  %xor2.i65.3 = xor i64 %or.i64.3, %add21.3
  %shr.i7275.3 = or i64 %xor2.i65.3, %xor2.i71.3
  %or29.3 = lshr i64 %shr.i7275.3, 63
  %27 = load i64, i64* %arrayidx10.4, align 8, !tbaa !3
  %add.4 = add i64 %27, %or29.3
  %28 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 4), align 16, !tbaa !3
  %add21.4 = add i64 %28, %add.4
  store i64 %add21.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %29 = xor i64 %add.4, -9223372036854775808
  %xor2.i71.4 = and i64 %29, %27
  %xor.i61.4 = xor i64 %add21.4, %add.4
  %xor1.i63.4 = xor i64 %28, %add.4
  %or.i64.4 = or i64 %xor.i61.4, %xor1.i63.4
  %xor2.i65.4 = xor i64 %or.i64.4, %add21.4
  %shr.i7275.4 = or i64 %xor2.i65.4, %xor2.i71.4
  %or29.4 = lshr i64 %shr.i7275.4, 63
  %30 = load i64, i64* %arrayidx10.5, align 8, !tbaa !3
  %add.5 = add i64 %30, %or29.4
  %31 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 5), align 8, !tbaa !3
  %add21.5 = add i64 %31, %add.5
  store i64 %add21.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %32 = xor i64 %add.5, -9223372036854775808
  %xor2.i71.5 = and i64 %32, %30
  %xor.i61.5 = xor i64 %add21.5, %add.5
  %xor1.i63.5 = xor i64 %31, %add.5
  %or.i64.5 = or i64 %xor.i61.5, %xor1.i63.5
  %xor2.i65.5 = xor i64 %or.i64.5, %add21.5
  %shr.i7275.5 = or i64 %xor2.i65.5, %xor2.i71.5
  %or29.5 = lshr i64 %shr.i7275.5, 63
  %add.6 = add i64 %sub8.6, %or29.5
  %33 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 6), align 16, !tbaa !3
  %add21.6 = add i64 %33, %add.6
  store i64 %add21.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %34 = xor i64 %add.6, -9223372036854775808
  %xor2.i71.6 = and i64 %34, %sub8.6
  %xor.i61.6 = xor i64 %add21.6, %add.6
  %xor1.i63.6 = xor i64 %33, %add.6
  %or.i64.6 = or i64 %xor.i61.6, %xor1.i63.6
  %xor2.i65.6 = xor i64 %or.i64.6, %add21.6
  %shr.i7275.6 = or i64 %xor2.i65.6, %xor2.i71.6
  %or29.6 = lshr i64 %shr.i7275.6, 63
  %add.7 = add i64 %sub8.7, %or29.6
  %35 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x4, i64 0, i64 7), align 8, !tbaa !3
  %add21.7 = add i64 %35, %add.7
  store i64 %add21.7, i64* %arrayidx10.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @fpadd503(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %add3 = add i64 %1, %0
  store i64 %add3, i64* %c, align 8, !tbaa !3
  %xor.i122 = xor i64 %add3, %0
  %xor1.i124 = xor i64 %1, %0
  %or.i125 = or i64 %xor.i122, %xor1.i124
  %xor2.i126 = xor i64 %or.i125, %add3
  %or = lshr i64 %xor2.i126, 63
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %add.1 = add i64 %2, %or
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %add3.1 = add i64 %3, %add.1
  %arrayidx5.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %add3.1, i64* %arrayidx5.1, align 8, !tbaa !3
  %4 = xor i64 %add.1, -9223372036854775808
  %xor2.i.1 = and i64 %4, %2
  %xor.i122.1 = xor i64 %add3.1, %add.1
  %xor1.i124.1 = xor i64 %3, %add.1
  %or.i125.1 = or i64 %xor.i122.1, %xor1.i124.1
  %xor2.i126.1 = xor i64 %or.i125.1, %add3.1
  %shr.i130.1 = or i64 %xor2.i126.1, %xor2.i.1
  %or.1 = lshr i64 %shr.i130.1, 63
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %5 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %add.2 = add i64 %5, %or.1
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %6 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %add3.2 = add i64 %6, %add.2
  %arrayidx5.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %add3.2, i64* %arrayidx5.2, align 8, !tbaa !3
  %7 = xor i64 %add.2, -9223372036854775808
  %xor2.i.2 = and i64 %7, %5
  %xor.i122.2 = xor i64 %add3.2, %add.2
  %xor1.i124.2 = xor i64 %6, %add.2
  %or.i125.2 = or i64 %xor.i122.2, %xor1.i124.2
  %xor2.i126.2 = xor i64 %or.i125.2, %add3.2
  %shr.i130.2 = or i64 %xor2.i126.2, %xor2.i.2
  %or.2 = lshr i64 %shr.i130.2, 63
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %8 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %add.3 = add i64 %8, %or.2
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %9 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %add3.3 = add i64 %9, %add.3
  %arrayidx5.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %add3.3, i64* %arrayidx5.3, align 8, !tbaa !3
  %10 = xor i64 %add.3, -9223372036854775808
  %xor2.i.3 = and i64 %10, %8
  %xor.i122.3 = xor i64 %add3.3, %add.3
  %xor1.i124.3 = xor i64 %9, %add.3
  %or.i125.3 = or i64 %xor.i122.3, %xor1.i124.3
  %xor2.i126.3 = xor i64 %or.i125.3, %add3.3
  %shr.i130.3 = or i64 %xor2.i126.3, %xor2.i.3
  %or.3 = lshr i64 %shr.i130.3, 63
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %11 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %add.4 = add i64 %11, %or.3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %12 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %add3.4 = add i64 %12, %add.4
  %arrayidx5.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %add3.4, i64* %arrayidx5.4, align 8, !tbaa !3
  %13 = xor i64 %add.4, -9223372036854775808
  %xor2.i.4 = and i64 %13, %11
  %xor.i122.4 = xor i64 %add3.4, %add.4
  %xor1.i124.4 = xor i64 %12, %add.4
  %or.i125.4 = or i64 %xor.i122.4, %xor1.i124.4
  %xor2.i126.4 = xor i64 %or.i125.4, %add3.4
  %shr.i130.4 = or i64 %xor2.i126.4, %xor2.i.4
  %or.4 = lshr i64 %shr.i130.4, 63
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %14 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %add.5 = add i64 %14, %or.4
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %15 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %add3.5 = add i64 %15, %add.5
  %arrayidx5.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %add3.5, i64* %arrayidx5.5, align 8, !tbaa !3
  %16 = xor i64 %add.5, -9223372036854775808
  %xor2.i.5 = and i64 %16, %14
  %xor.i122.5 = xor i64 %add3.5, %add.5
  %xor1.i124.5 = xor i64 %15, %add.5
  %or.i125.5 = or i64 %xor.i122.5, %xor1.i124.5
  %xor2.i126.5 = xor i64 %or.i125.5, %add3.5
  %shr.i130.5 = or i64 %xor2.i126.5, %xor2.i.5
  %or.5 = lshr i64 %shr.i130.5, 63
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %17 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %add.6 = add i64 %17, %or.5
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %18 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %add3.6 = add i64 %18, %add.6
  %arrayidx5.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %add3.6, i64* %arrayidx5.6, align 8, !tbaa !3
  %19 = xor i64 %add.6, -9223372036854775808
  %xor2.i.6 = and i64 %19, %17
  %xor.i122.6 = xor i64 %add3.6, %add.6
  %xor1.i124.6 = xor i64 %18, %add.6
  %or.i125.6 = or i64 %xor.i122.6, %xor1.i124.6
  %xor2.i126.6 = xor i64 %or.i125.6, %add3.6
  %shr.i130.6 = or i64 %xor2.i126.6, %xor2.i.6
  %or.6 = lshr i64 %shr.i130.6, 63
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %20 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %add.7 = add i64 %20, %or.6
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %21 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %add3.7 = add i64 %21, %add.7
  %arrayidx5.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %add3.7, i64* %arrayidx5.7, align 8, !tbaa !3
  %22 = load i64, i64* %c, align 8, !tbaa !3
  %23 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 0), align 16, !tbaa !3
  %sub = sub i64 %22, %23
  %xor.i115 = xor i64 %23, %22
  %xor1.i117 = xor i64 %sub, %23
  %or.i118 = or i64 %xor1.i117, %xor.i115
  %xor2.i119 = xor i64 %or.i118, %22
  %shr.i120 = lshr i64 %xor2.i119, 63
  %conv.i121 = trunc i64 %shr.i120 to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %24 = load i64, i64* %arrayidx5.1, align 8, !tbaa !3
  %25 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 1), align 8, !tbaa !3
  %sub.1 = sub i64 %24, %25
  %xor.i115.1 = xor i64 %25, %24
  %xor1.i117.1 = xor i64 %sub.1, %25
  %or.i118.1 = or i64 %xor1.i117.1, %xor.i115.1
  %xor2.i119.1 = xor i64 %or.i118.1, %24
  %shr.i120.1 = lshr i64 %xor2.i119.1, 63
  %conv.i121.1 = trunc i64 %shr.i120.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i114.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i114.1, %conv.i121
  %or25.1 = or i32 %and.1, %conv.i121.1
  %sub27.1 = sub i64 %sub.1, %shr.i120
  store i64 %sub27.1, i64* %arrayidx5.1, align 8, !tbaa !3
  %26 = load i64, i64* %arrayidx5.2, align 8, !tbaa !3
  %27 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 2), align 16, !tbaa !3
  %sub.2 = sub i64 %26, %27
  %xor.i115.2 = xor i64 %27, %26
  %xor1.i117.2 = xor i64 %sub.2, %27
  %or.i118.2 = or i64 %xor1.i117.2, %xor.i115.2
  %xor2.i119.2 = xor i64 %or.i118.2, %26
  %shr.i120.2 = lshr i64 %xor2.i119.2, 63
  %conv.i121.2 = trunc i64 %shr.i120.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i114.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i114.2, %or25.1
  %or25.2 = or i32 %and.2, %conv.i121.2
  %conv26.2 = zext i32 %or25.1 to i64
  %sub27.2 = sub i64 %sub.2, %conv26.2
  store i64 %sub27.2, i64* %arrayidx5.2, align 8, !tbaa !3
  %28 = load i64, i64* %arrayidx5.3, align 8, !tbaa !3
  %29 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 3), align 8, !tbaa !3
  %sub.3 = sub i64 %28, %29
  %xor.i115.3 = xor i64 %29, %28
  %xor1.i117.3 = xor i64 %sub.3, %29
  %or.i118.3 = or i64 %xor1.i117.3, %xor.i115.3
  %xor2.i119.3 = xor i64 %or.i118.3, %28
  %shr.i120.3 = lshr i64 %xor2.i119.3, 63
  %conv.i121.3 = trunc i64 %shr.i120.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i114.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i114.3, %or25.2
  %or25.3 = or i32 %and.3, %conv.i121.3
  %conv26.3 = zext i32 %or25.2 to i64
  %sub27.3 = sub i64 %sub.3, %conv26.3
  store i64 %sub27.3, i64* %arrayidx5.3, align 8, !tbaa !3
  %30 = load i64, i64* %arrayidx5.4, align 8, !tbaa !3
  %31 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 4), align 16, !tbaa !3
  %sub.4 = sub i64 %30, %31
  %xor.i115.4 = xor i64 %31, %30
  %xor1.i117.4 = xor i64 %sub.4, %31
  %or.i118.4 = or i64 %xor1.i117.4, %xor.i115.4
  %xor2.i119.4 = xor i64 %or.i118.4, %30
  %shr.i120.4 = lshr i64 %xor2.i119.4, 63
  %conv.i121.4 = trunc i64 %shr.i120.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i114.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i114.4, %or25.3
  %or25.4 = or i32 %and.4, %conv.i121.4
  %conv26.4 = zext i32 %or25.3 to i64
  %sub27.4 = sub i64 %sub.4, %conv26.4
  store i64 %sub27.4, i64* %arrayidx5.4, align 8, !tbaa !3
  %32 = load i64, i64* %arrayidx5.5, align 8, !tbaa !3
  %33 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 5), align 8, !tbaa !3
  %sub.5 = sub i64 %32, %33
  %xor.i115.5 = xor i64 %33, %32
  %xor1.i117.5 = xor i64 %sub.5, %33
  %or.i118.5 = or i64 %xor1.i117.5, %xor.i115.5
  %xor2.i119.5 = xor i64 %or.i118.5, %32
  %shr.i120.5 = lshr i64 %xor2.i119.5, 63
  %conv.i121.5 = trunc i64 %shr.i120.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i114.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i114.5, %or25.4
  %or25.5 = or i32 %and.5, %conv.i121.5
  %conv26.5 = zext i32 %or25.4 to i64
  %sub27.5 = sub i64 %sub.5, %conv26.5
  store i64 %sub27.5, i64* %arrayidx5.5, align 8, !tbaa !3
  %34 = load i64, i64* %arrayidx5.6, align 8, !tbaa !3
  %35 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 6), align 16, !tbaa !3
  %sub.6 = sub i64 %34, %35
  %xor.i115.6 = xor i64 %35, %34
  %xor1.i117.6 = xor i64 %sub.6, %35
  %or.i118.6 = or i64 %xor1.i117.6, %xor.i115.6
  %xor2.i119.6 = xor i64 %or.i118.6, %34
  %shr.i120.6 = lshr i64 %xor2.i119.6, 63
  %conv.i121.6 = trunc i64 %shr.i120.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i114.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i114.6, %or25.5
  %or25.6 = or i32 %and.6, %conv.i121.6
  %conv26.6 = zext i32 %or25.5 to i64
  %sub27.6 = sub i64 %sub.6, %conv26.6
  store i64 %sub27.6, i64* %arrayidx5.6, align 8, !tbaa !3
  %36 = load i64, i64* %arrayidx5.7, align 8, !tbaa !3
  %37 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 7), align 8, !tbaa !3
  %sub.7 = sub i64 %36, %37
  %xor.i115.7 = xor i64 %37, %36
  %xor1.i117.7 = xor i64 %sub.7, %37
  %or.i118.7 = or i64 %xor1.i117.7, %xor.i115.7
  %xor2.i119.7 = xor i64 %or.i118.7, %36
  %shr.i120.7 = lshr i64 %xor2.i119.7, 63
  %conv.i121.7 = trunc i64 %shr.i120.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i114.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i114.7, %or25.6
  %or25.7 = or i32 %and.7, %conv.i121.7
  %conv26.7 = zext i32 %or25.6 to i64
  %sub27.7 = sub i64 %sub.7, %conv26.7
  store i64 %sub27.7, i64* %arrayidx5.7, align 8, !tbaa !3
  %conv33 = zext i32 %or25.7 to i64
  %sub34 = sub nsw i64 0, %conv33
  %38 = load i64, i64* %c, align 8, !tbaa !3
  %39 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 0), align 16, !tbaa !3
  %and46 = and i64 %39, %sub34
  %add47 = add i64 %and46, %38
  store i64 %add47, i64* %c, align 8, !tbaa !3
  %xor.i102 = xor i64 %add47, %38
  %xor1.i103 = xor i64 %and46, %38
  %or.i104 = or i64 %xor.i102, %xor1.i103
  %xor2.i105 = xor i64 %or.i104, %add47
  %or55 = lshr i64 %xor2.i105, 63
  %40 = load i64, i64* %arrayidx5.1, align 8, !tbaa !3
  %add43.1 = add i64 %40, %or55
  %41 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 1), align 8, !tbaa !3
  %and46.1 = and i64 %41, %sub34
  %add47.1 = add i64 %and46.1, %add43.1
  store i64 %add47.1, i64* %arrayidx5.1, align 8, !tbaa !3
  %42 = xor i64 %add43.1, -9223372036854775808
  %xor2.i111.1 = and i64 %42, %40
  %xor.i102.1 = xor i64 %add47.1, %add43.1
  %xor1.i103.1 = xor i64 %and46.1, %add43.1
  %or.i104.1 = or i64 %xor.i102.1, %xor1.i103.1
  %xor2.i105.1 = xor i64 %or.i104.1, %add47.1
  %shr.i112129.1 = or i64 %xor2.i105.1, %xor2.i111.1
  %or55.1 = lshr i64 %shr.i112129.1, 63
  %43 = load i64, i64* %arrayidx5.2, align 8, !tbaa !3
  %add43.2 = add i64 %43, %or55.1
  %44 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 2), align 16, !tbaa !3
  %and46.2 = and i64 %44, %sub34
  %add47.2 = add i64 %and46.2, %add43.2
  store i64 %add47.2, i64* %arrayidx5.2, align 8, !tbaa !3
  %45 = xor i64 %add43.2, -9223372036854775808
  %xor2.i111.2 = and i64 %45, %43
  %xor.i102.2 = xor i64 %add47.2, %add43.2
  %xor1.i103.2 = xor i64 %and46.2, %add43.2
  %or.i104.2 = or i64 %xor.i102.2, %xor1.i103.2
  %xor2.i105.2 = xor i64 %or.i104.2, %add47.2
  %shr.i112129.2 = or i64 %xor2.i105.2, %xor2.i111.2
  %or55.2 = lshr i64 %shr.i112129.2, 63
  %46 = load i64, i64* %arrayidx5.3, align 8, !tbaa !3
  %add43.3 = add i64 %46, %or55.2
  %47 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 3), align 8, !tbaa !3
  %and46.3 = and i64 %47, %sub34
  %add47.3 = add i64 %and46.3, %add43.3
  store i64 %add47.3, i64* %arrayidx5.3, align 8, !tbaa !3
  %48 = xor i64 %add43.3, -9223372036854775808
  %xor2.i111.3 = and i64 %48, %46
  %xor.i102.3 = xor i64 %add47.3, %add43.3
  %xor1.i103.3 = xor i64 %and46.3, %add43.3
  %or.i104.3 = or i64 %xor.i102.3, %xor1.i103.3
  %xor2.i105.3 = xor i64 %or.i104.3, %add47.3
  %shr.i112129.3 = or i64 %xor2.i105.3, %xor2.i111.3
  %or55.3 = lshr i64 %shr.i112129.3, 63
  %49 = load i64, i64* %arrayidx5.4, align 8, !tbaa !3
  %add43.4 = add i64 %49, %or55.3
  %50 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 4), align 16, !tbaa !3
  %and46.4 = and i64 %50, %sub34
  %add47.4 = add i64 %and46.4, %add43.4
  store i64 %add47.4, i64* %arrayidx5.4, align 8, !tbaa !3
  %51 = xor i64 %add43.4, -9223372036854775808
  %xor2.i111.4 = and i64 %51, %49
  %xor.i102.4 = xor i64 %add47.4, %add43.4
  %xor1.i103.4 = xor i64 %and46.4, %add43.4
  %or.i104.4 = or i64 %xor.i102.4, %xor1.i103.4
  %xor2.i105.4 = xor i64 %or.i104.4, %add47.4
  %shr.i112129.4 = or i64 %xor2.i105.4, %xor2.i111.4
  %or55.4 = lshr i64 %shr.i112129.4, 63
  %52 = load i64, i64* %arrayidx5.5, align 8, !tbaa !3
  %add43.5 = add i64 %52, %or55.4
  %53 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 5), align 8, !tbaa !3
  %and46.5 = and i64 %53, %sub34
  %add47.5 = add i64 %and46.5, %add43.5
  store i64 %add47.5, i64* %arrayidx5.5, align 8, !tbaa !3
  %54 = xor i64 %add43.5, -9223372036854775808
  %xor2.i111.5 = and i64 %54, %52
  %xor.i102.5 = xor i64 %add47.5, %add43.5
  %xor1.i103.5 = xor i64 %and46.5, %add43.5
  %or.i104.5 = or i64 %xor.i102.5, %xor1.i103.5
  %xor2.i105.5 = xor i64 %or.i104.5, %add47.5
  %shr.i112129.5 = or i64 %xor2.i105.5, %xor2.i111.5
  %or55.5 = lshr i64 %shr.i112129.5, 63
  %55 = load i64, i64* %arrayidx5.6, align 8, !tbaa !3
  %add43.6 = add i64 %55, %or55.5
  %56 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 6), align 16, !tbaa !3
  %and46.6 = and i64 %56, %sub34
  %add47.6 = add i64 %and46.6, %add43.6
  store i64 %add47.6, i64* %arrayidx5.6, align 8, !tbaa !3
  %57 = xor i64 %add43.6, -9223372036854775808
  %xor2.i111.6 = and i64 %57, %55
  %xor.i102.6 = xor i64 %add47.6, %add43.6
  %xor1.i103.6 = xor i64 %and46.6, %add43.6
  %or.i104.6 = or i64 %xor.i102.6, %xor1.i103.6
  %xor2.i105.6 = xor i64 %or.i104.6, %add47.6
  %shr.i112129.6 = or i64 %xor2.i105.6, %xor2.i111.6
  %or55.6 = lshr i64 %shr.i112129.6, 63
  %58 = load i64, i64* %arrayidx5.7, align 8, !tbaa !3
  %add43.7 = add i64 %58, %or55.6
  %59 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 7), align 8, !tbaa !3
  %and46.7 = and i64 %59, %sub34
  %add47.7 = add i64 %and46.7, %add43.7
  store i64 %add47.7, i64* %arrayidx5.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @fpsub503(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i79.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i79.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  %arrayidx10.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %sub8.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i79.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i79.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  %arrayidx10.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %sub8.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i79.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i79.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  %arrayidx10.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %sub8.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i79.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i79.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  %arrayidx10.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %sub8.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i79.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i79.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  %arrayidx10.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %sub8.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i79.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i79.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  %arrayidx10.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %sub8.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i79.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i79.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  %arrayidx10.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %sub8.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %conv11 = zext i32 %or.7 to i64
  %sub12 = sub nsw i64 0, %conv11
  %16 = load i64, i64* %c, align 8, !tbaa !3
  %17 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 0), align 16, !tbaa !3
  %and23 = and i64 %17, %sub12
  %add24 = add i64 %and23, %16
  store i64 %add24, i64* %c, align 8, !tbaa !3
  %xor.i66 = xor i64 %add24, %16
  %xor1.i68 = xor i64 %and23, %16
  %or.i69 = or i64 %xor.i66, %xor1.i68
  %xor2.i70 = xor i64 %or.i69, %add24
  %or32 = lshr i64 %xor2.i70, 63
  %18 = load i64, i64* %arrayidx10.1, align 8, !tbaa !3
  %add.1 = add i64 %18, %or32
  %19 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 1), align 8, !tbaa !3
  %and23.1 = and i64 %19, %sub12
  %add24.1 = add i64 %and23.1, %add.1
  store i64 %add24.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %20 = xor i64 %add.1, -9223372036854775808
  %xor2.i76.1 = and i64 %20, %18
  %xor.i66.1 = xor i64 %add24.1, %add.1
  %xor1.i68.1 = xor i64 %and23.1, %add.1
  %or.i69.1 = or i64 %xor.i66.1, %xor1.i68.1
  %xor2.i70.1 = xor i64 %or.i69.1, %add24.1
  %shr.i7780.1 = or i64 %xor2.i70.1, %xor2.i76.1
  %or32.1 = lshr i64 %shr.i7780.1, 63
  %21 = load i64, i64* %arrayidx10.2, align 8, !tbaa !3
  %add.2 = add i64 %21, %or32.1
  %22 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 2), align 16, !tbaa !3
  %and23.2 = and i64 %22, %sub12
  %add24.2 = add i64 %and23.2, %add.2
  store i64 %add24.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %23 = xor i64 %add.2, -9223372036854775808
  %xor2.i76.2 = and i64 %23, %21
  %xor.i66.2 = xor i64 %add24.2, %add.2
  %xor1.i68.2 = xor i64 %and23.2, %add.2
  %or.i69.2 = or i64 %xor.i66.2, %xor1.i68.2
  %xor2.i70.2 = xor i64 %or.i69.2, %add24.2
  %shr.i7780.2 = or i64 %xor2.i70.2, %xor2.i76.2
  %or32.2 = lshr i64 %shr.i7780.2, 63
  %24 = load i64, i64* %arrayidx10.3, align 8, !tbaa !3
  %add.3 = add i64 %24, %or32.2
  %25 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 3), align 8, !tbaa !3
  %and23.3 = and i64 %25, %sub12
  %add24.3 = add i64 %and23.3, %add.3
  store i64 %add24.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %26 = xor i64 %add.3, -9223372036854775808
  %xor2.i76.3 = and i64 %26, %24
  %xor.i66.3 = xor i64 %add24.3, %add.3
  %xor1.i68.3 = xor i64 %and23.3, %add.3
  %or.i69.3 = or i64 %xor.i66.3, %xor1.i68.3
  %xor2.i70.3 = xor i64 %or.i69.3, %add24.3
  %shr.i7780.3 = or i64 %xor2.i70.3, %xor2.i76.3
  %or32.3 = lshr i64 %shr.i7780.3, 63
  %27 = load i64, i64* %arrayidx10.4, align 8, !tbaa !3
  %add.4 = add i64 %27, %or32.3
  %28 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 4), align 16, !tbaa !3
  %and23.4 = and i64 %28, %sub12
  %add24.4 = add i64 %and23.4, %add.4
  store i64 %add24.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %29 = xor i64 %add.4, -9223372036854775808
  %xor2.i76.4 = and i64 %29, %27
  %xor.i66.4 = xor i64 %add24.4, %add.4
  %xor1.i68.4 = xor i64 %and23.4, %add.4
  %or.i69.4 = or i64 %xor.i66.4, %xor1.i68.4
  %xor2.i70.4 = xor i64 %or.i69.4, %add24.4
  %shr.i7780.4 = or i64 %xor2.i70.4, %xor2.i76.4
  %or32.4 = lshr i64 %shr.i7780.4, 63
  %30 = load i64, i64* %arrayidx10.5, align 8, !tbaa !3
  %add.5 = add i64 %30, %or32.4
  %31 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 5), align 8, !tbaa !3
  %and23.5 = and i64 %31, %sub12
  %add24.5 = add i64 %and23.5, %add.5
  store i64 %add24.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %32 = xor i64 %add.5, -9223372036854775808
  %xor2.i76.5 = and i64 %32, %30
  %xor.i66.5 = xor i64 %add24.5, %add.5
  %xor1.i68.5 = xor i64 %and23.5, %add.5
  %or.i69.5 = or i64 %xor.i66.5, %xor1.i68.5
  %xor2.i70.5 = xor i64 %or.i69.5, %add24.5
  %shr.i7780.5 = or i64 %xor2.i70.5, %xor2.i76.5
  %or32.5 = lshr i64 %shr.i7780.5, 63
  %33 = load i64, i64* %arrayidx10.6, align 8, !tbaa !3
  %add.6 = add i64 %33, %or32.5
  %34 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 6), align 16, !tbaa !3
  %and23.6 = and i64 %34, %sub12
  %add24.6 = add i64 %and23.6, %add.6
  store i64 %add24.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %35 = xor i64 %add.6, -9223372036854775808
  %xor2.i76.6 = and i64 %35, %33
  %xor.i66.6 = xor i64 %add24.6, %add.6
  %xor1.i68.6 = xor i64 %and23.6, %add.6
  %or.i69.6 = or i64 %xor.i66.6, %xor1.i68.6
  %xor2.i70.6 = xor i64 %or.i69.6, %add24.6
  %shr.i7780.6 = or i64 %xor2.i70.6, %xor2.i76.6
  %or32.6 = lshr i64 %shr.i7780.6, 63
  %36 = load i64, i64* %arrayidx10.7, align 8, !tbaa !3
  %add.7 = add i64 %36, %or32.6
  %37 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 7), align 8, !tbaa !3
  %and23.7 = and i64 %37, %sub12
  %add24.7 = add i64 %and23.7, %add.7
  store i64 %add24.7, i64* %arrayidx10.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @fpneg503(i64* nocapture %a) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 0), align 16, !tbaa !3
  %1 = load i64, i64* %a, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %a, align 8, !tbaa !3
  %2 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 1), align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %a, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i25.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i25.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  store i64 %sub8.1, i64* %arrayidx2.1, align 8, !tbaa !3
  %4 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 2), align 16, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %a, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i25.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i25.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  store i64 %sub8.2, i64* %arrayidx2.2, align 8, !tbaa !3
  %6 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 3), align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %a, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i25.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i25.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  store i64 %sub8.3, i64* %arrayidx2.3, align 8, !tbaa !3
  %8 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 4), align 16, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %a, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i25.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i25.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  store i64 %sub8.4, i64* %arrayidx2.4, align 8, !tbaa !3
  %10 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 5), align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %a, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i25.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i25.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  store i64 %sub8.5, i64* %arrayidx2.5, align 8, !tbaa !3
  %12 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 6), align 16, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %a, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i25.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i25.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  store i64 %sub8.6, i64* %arrayidx2.6, align 8, !tbaa !3
  %14 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503x2, i64 0, i64 7), align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %a, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  store i64 %sub8.7, i64* %arrayidx2.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define void @fpdiv2_503(i64* nocapture readonly %a, i64* %c) local_unnamed_addr #1 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %and = and i64 %0, 1
  %sub = sub nsw i64 0, %and
  %1 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 0), align 16, !tbaa !3
  %and4 = and i64 %1, %sub
  %add5 = add i64 %and4, %0
  store i64 %add5, i64* %c, align 8, !tbaa !3
  %xor.i27 = xor i64 %add5, %0
  %xor1.i28 = xor i64 %and4, %0
  %or.i29 = or i64 %xor.i27, %xor1.i28
  %xor2.i30 = xor i64 %or.i29, %add5
  %or = lshr i64 %xor2.i30, 63
  %arrayidx1.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx1.1, align 8, !tbaa !3
  %add.1 = add i64 %2, %or
  %3 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 1), align 8, !tbaa !3
  %and4.1 = and i64 %3, %sub
  %add5.1 = add i64 %and4.1, %add.1
  %arrayidx7.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %add5.1, i64* %arrayidx7.1, align 8, !tbaa !3
  %4 = xor i64 %add.1, -9223372036854775808
  %xor2.i.1 = and i64 %4, %2
  %xor.i27.1 = xor i64 %add5.1, %add.1
  %xor1.i28.1 = xor i64 %and4.1, %add.1
  %or.i29.1 = or i64 %xor.i27.1, %xor1.i28.1
  %xor2.i30.1 = xor i64 %or.i29.1, %add5.1
  %shr.i33.1 = or i64 %xor2.i30.1, %xor2.i.1
  %or.1 = lshr i64 %shr.i33.1, 63
  %arrayidx1.2 = getelementptr inbounds i64, i64* %a, i64 2
  %5 = load i64, i64* %arrayidx1.2, align 8, !tbaa !3
  %add.2 = add i64 %5, %or.1
  %6 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 2), align 16, !tbaa !3
  %and4.2 = and i64 %6, %sub
  %add5.2 = add i64 %and4.2, %add.2
  %arrayidx7.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %add5.2, i64* %arrayidx7.2, align 8, !tbaa !3
  %7 = xor i64 %add.2, -9223372036854775808
  %xor2.i.2 = and i64 %7, %5
  %xor.i27.2 = xor i64 %add5.2, %add.2
  %xor1.i28.2 = xor i64 %and4.2, %add.2
  %or.i29.2 = or i64 %xor.i27.2, %xor1.i28.2
  %xor2.i30.2 = xor i64 %or.i29.2, %add5.2
  %shr.i33.2 = or i64 %xor2.i30.2, %xor2.i.2
  %or.2 = lshr i64 %shr.i33.2, 63
  %arrayidx1.3 = getelementptr inbounds i64, i64* %a, i64 3
  %8 = load i64, i64* %arrayidx1.3, align 8, !tbaa !3
  %add.3 = add i64 %8, %or.2
  %9 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 3), align 8, !tbaa !3
  %and4.3 = and i64 %9, %sub
  %add5.3 = add i64 %and4.3, %add.3
  %arrayidx7.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %add5.3, i64* %arrayidx7.3, align 8, !tbaa !3
  %10 = xor i64 %add.3, -9223372036854775808
  %xor2.i.3 = and i64 %10, %8
  %xor.i27.3 = xor i64 %add5.3, %add.3
  %xor1.i28.3 = xor i64 %and4.3, %add.3
  %or.i29.3 = or i64 %xor.i27.3, %xor1.i28.3
  %xor2.i30.3 = xor i64 %or.i29.3, %add5.3
  %shr.i33.3 = or i64 %xor2.i30.3, %xor2.i.3
  %or.3 = lshr i64 %shr.i33.3, 63
  %arrayidx1.4 = getelementptr inbounds i64, i64* %a, i64 4
  %11 = load i64, i64* %arrayidx1.4, align 8, !tbaa !3
  %add.4 = add i64 %11, %or.3
  %12 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 4), align 16, !tbaa !3
  %and4.4 = and i64 %12, %sub
  %add5.4 = add i64 %and4.4, %add.4
  %arrayidx7.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %add5.4, i64* %arrayidx7.4, align 8, !tbaa !3
  %13 = xor i64 %add.4, -9223372036854775808
  %xor2.i.4 = and i64 %13, %11
  %xor.i27.4 = xor i64 %add5.4, %add.4
  %xor1.i28.4 = xor i64 %and4.4, %add.4
  %or.i29.4 = or i64 %xor.i27.4, %xor1.i28.4
  %xor2.i30.4 = xor i64 %or.i29.4, %add5.4
  %shr.i33.4 = or i64 %xor2.i30.4, %xor2.i.4
  %or.4 = lshr i64 %shr.i33.4, 63
  %arrayidx1.5 = getelementptr inbounds i64, i64* %a, i64 5
  %14 = load i64, i64* %arrayidx1.5, align 8, !tbaa !3
  %add.5 = add i64 %14, %or.4
  %15 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 5), align 8, !tbaa !3
  %and4.5 = and i64 %15, %sub
  %add5.5 = add i64 %and4.5, %add.5
  %arrayidx7.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %add5.5, i64* %arrayidx7.5, align 8, !tbaa !3
  %16 = xor i64 %add.5, -9223372036854775808
  %xor2.i.5 = and i64 %16, %14
  %xor.i27.5 = xor i64 %add5.5, %add.5
  %xor1.i28.5 = xor i64 %and4.5, %add.5
  %or.i29.5 = or i64 %xor.i27.5, %xor1.i28.5
  %xor2.i30.5 = xor i64 %or.i29.5, %add5.5
  %shr.i33.5 = or i64 %xor2.i30.5, %xor2.i.5
  %or.5 = lshr i64 %shr.i33.5, 63
  %arrayidx1.6 = getelementptr inbounds i64, i64* %a, i64 6
  %17 = load i64, i64* %arrayidx1.6, align 8, !tbaa !3
  %add.6 = add i64 %17, %or.5
  %18 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 6), align 16, !tbaa !3
  %and4.6 = and i64 %18, %sub
  %add5.6 = add i64 %and4.6, %add.6
  %arrayidx7.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %add5.6, i64* %arrayidx7.6, align 8, !tbaa !3
  %19 = xor i64 %add.6, -9223372036854775808
  %xor2.i.6 = and i64 %19, %17
  %xor.i27.6 = xor i64 %add5.6, %add.6
  %xor1.i28.6 = xor i64 %and4.6, %add.6
  %or.i29.6 = or i64 %xor.i27.6, %xor1.i28.6
  %xor2.i30.6 = xor i64 %or.i29.6, %add5.6
  %shr.i33.6 = or i64 %xor2.i30.6, %xor2.i.6
  %or.6 = lshr i64 %shr.i33.6, 63
  %arrayidx1.7 = getelementptr inbounds i64, i64* %a, i64 7
  %20 = load i64, i64* %arrayidx1.7, align 8, !tbaa !3
  %add.7 = add i64 %20, %or.6
  %21 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 7), align 8, !tbaa !3
  %and4.7 = and i64 %21, %sub
  %add5.7 = add i64 %and4.7, %add.7
  %arrayidx7.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %add5.7, i64* %arrayidx7.7, align 8, !tbaa !3
  tail call void @mp_shiftr1(i64* %c, i32 8) #5
  ret void
}

declare void @mp_shiftr1(i64*, i32) local_unnamed_addr #2

; Function Attrs: norecurse nounwind ssp uwtable
define void @fpcorrection503(i64* nocapture %a) local_unnamed_addr #3 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 0), align 16, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %a, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %3 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 1), align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i79.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i79.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  store i64 %sub8.1, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %5 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 2), align 16, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i79.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i79.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  store i64 %sub8.2, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %7 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 3), align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i79.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i79.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  store i64 %sub8.3, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %9 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 4), align 16, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i79.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i79.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  store i64 %sub8.4, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %11 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 5), align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i79.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i79.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  store i64 %sub8.5, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %13 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 6), align 16, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i79.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i79.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  store i64 %sub8.6, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %15 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 7), align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i79.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i79.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  store i64 %sub8.7, i64* %arrayidx.7, align 8, !tbaa !3
  %conv11 = zext i32 %or.7 to i64
  %sub12 = sub nsw i64 0, %conv11
  %16 = load i64, i64* %a, align 8, !tbaa !3
  %17 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 0), align 16, !tbaa !3
  %and23 = and i64 %17, %sub12
  %add24 = add i64 %and23, %16
  store i64 %add24, i64* %a, align 8, !tbaa !3
  %xor.i66 = xor i64 %add24, %16
  %xor1.i68 = xor i64 %and23, %16
  %or.i69 = or i64 %xor.i66, %xor1.i68
  %xor2.i70 = xor i64 %or.i69, %add24
  %or32 = lshr i64 %xor2.i70, 63
  %18 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %add.1 = add i64 %18, %or32
  %19 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 1), align 8, !tbaa !3
  %and23.1 = and i64 %19, %sub12
  %add24.1 = add i64 %and23.1, %add.1
  store i64 %add24.1, i64* %arrayidx.1, align 8, !tbaa !3
  %20 = xor i64 %add.1, -9223372036854775808
  %xor2.i76.1 = and i64 %20, %18
  %xor.i66.1 = xor i64 %add24.1, %add.1
  %xor1.i68.1 = xor i64 %and23.1, %add.1
  %or.i69.1 = or i64 %xor.i66.1, %xor1.i68.1
  %xor2.i70.1 = xor i64 %or.i69.1, %add24.1
  %shr.i7780.1 = or i64 %xor2.i70.1, %xor2.i76.1
  %or32.1 = lshr i64 %shr.i7780.1, 63
  %21 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %add.2 = add i64 %21, %or32.1
  %22 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 2), align 16, !tbaa !3
  %and23.2 = and i64 %22, %sub12
  %add24.2 = add i64 %and23.2, %add.2
  store i64 %add24.2, i64* %arrayidx.2, align 8, !tbaa !3
  %23 = xor i64 %add.2, -9223372036854775808
  %xor2.i76.2 = and i64 %23, %21
  %xor.i66.2 = xor i64 %add24.2, %add.2
  %xor1.i68.2 = xor i64 %and23.2, %add.2
  %or.i69.2 = or i64 %xor.i66.2, %xor1.i68.2
  %xor2.i70.2 = xor i64 %or.i69.2, %add24.2
  %shr.i7780.2 = or i64 %xor2.i70.2, %xor2.i76.2
  %or32.2 = lshr i64 %shr.i7780.2, 63
  %24 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %add.3 = add i64 %24, %or32.2
  %25 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 3), align 8, !tbaa !3
  %and23.3 = and i64 %25, %sub12
  %add24.3 = add i64 %and23.3, %add.3
  store i64 %add24.3, i64* %arrayidx.3, align 8, !tbaa !3
  %26 = xor i64 %add.3, -9223372036854775808
  %xor2.i76.3 = and i64 %26, %24
  %xor.i66.3 = xor i64 %add24.3, %add.3
  %xor1.i68.3 = xor i64 %and23.3, %add.3
  %or.i69.3 = or i64 %xor.i66.3, %xor1.i68.3
  %xor2.i70.3 = xor i64 %or.i69.3, %add24.3
  %shr.i7780.3 = or i64 %xor2.i70.3, %xor2.i76.3
  %or32.3 = lshr i64 %shr.i7780.3, 63
  %27 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %add.4 = add i64 %27, %or32.3
  %28 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 4), align 16, !tbaa !3
  %and23.4 = and i64 %28, %sub12
  %add24.4 = add i64 %and23.4, %add.4
  store i64 %add24.4, i64* %arrayidx.4, align 8, !tbaa !3
  %29 = xor i64 %add.4, -9223372036854775808
  %xor2.i76.4 = and i64 %29, %27
  %xor.i66.4 = xor i64 %add24.4, %add.4
  %xor1.i68.4 = xor i64 %and23.4, %add.4
  %or.i69.4 = or i64 %xor.i66.4, %xor1.i68.4
  %xor2.i70.4 = xor i64 %or.i69.4, %add24.4
  %shr.i7780.4 = or i64 %xor2.i70.4, %xor2.i76.4
  %or32.4 = lshr i64 %shr.i7780.4, 63
  %30 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %add.5 = add i64 %30, %or32.4
  %31 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 5), align 8, !tbaa !3
  %and23.5 = and i64 %31, %sub12
  %add24.5 = add i64 %and23.5, %add.5
  store i64 %add24.5, i64* %arrayidx.5, align 8, !tbaa !3
  %32 = xor i64 %add.5, -9223372036854775808
  %xor2.i76.5 = and i64 %32, %30
  %xor.i66.5 = xor i64 %add24.5, %add.5
  %xor1.i68.5 = xor i64 %and23.5, %add.5
  %or.i69.5 = or i64 %xor.i66.5, %xor1.i68.5
  %xor2.i70.5 = xor i64 %or.i69.5, %add24.5
  %shr.i7780.5 = or i64 %xor2.i70.5, %xor2.i76.5
  %or32.5 = lshr i64 %shr.i7780.5, 63
  %33 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %add.6 = add i64 %33, %or32.5
  %34 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 6), align 16, !tbaa !3
  %and23.6 = and i64 %34, %sub12
  %add24.6 = add i64 %and23.6, %add.6
  store i64 %add24.6, i64* %arrayidx.6, align 8, !tbaa !3
  %35 = xor i64 %add.6, -9223372036854775808
  %xor2.i76.6 = and i64 %35, %33
  %xor.i66.6 = xor i64 %add24.6, %add.6
  %xor1.i68.6 = xor i64 %and23.6, %add.6
  %or.i69.6 = or i64 %xor.i66.6, %xor1.i68.6
  %xor2.i70.6 = xor i64 %or.i69.6, %add24.6
  %shr.i7780.6 = or i64 %xor2.i70.6, %xor2.i76.6
  %or32.6 = lshr i64 %shr.i7780.6, 63
  %36 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %add.7 = add i64 %36, %or32.6
  %37 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503, i64 0, i64 7), align 8, !tbaa !3
  %and23.7 = and i64 %37, %sub12
  %add24.7 = add i64 %and23.7, %add.7
  store i64 %add24.7, i64* %arrayidx.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: norecurse nounwind ssp uwtable
define void @digit_x_digit(i64 %a, i64 %b, i64* nocapture %c) local_unnamed_addr #3 {
entry:
  %and = and i64 %a, 4294967295
  %shr = lshr i64 %a, 32
  %and1 = and i64 %b, 4294967295
  %shr2 = lshr i64 %b, 32
  %mul = mul nuw i64 %and1, %and
  %mul3 = mul nuw i64 %shr2, %and
  %mul4 = mul nuw i64 %and1, %shr
  %mul5 = mul nuw i64 %shr2, %shr
  %and6 = and i64 %mul, 4294967295
  %shr7 = lshr i64 %mul, 32
  %and8 = and i64 %mul4, 4294967295
  %and9 = and i64 %mul3, 4294967295
  %add = add nuw nsw i64 %shr7, %and8
  %add10 = add nuw nsw i64 %add, %and9
  %shr11 = lshr i64 %add10, 32
  %shl = shl i64 %add10, 32
  %xor68 = or i64 %shl, %and6
  store i64 %xor68, i64* %c, align 8, !tbaa !3
  %shr13 = lshr i64 %mul4, 32
  %shr14 = lshr i64 %mul3, 32
  %and15 = and i64 %mul5, 4294967295
  %add16 = add nuw nsw i64 %shr13, %shr14
  %add17 = add nuw nsw i64 %add16, %and15
  %add18 = add nuw nsw i64 %add17, %shr11
  %and19 = and i64 %add18, 4294967295
  %arrayidx20 = getelementptr inbounds i64, i64* %c, i64 1
  %and21 = and i64 %add18, 30064771072
  %and22 = and i64 %mul5, -4294967296
  %add23 = add i64 %and21, %and22
  %xor2569 = or i64 %add23, %and19
  store i64 %xor2569, i64* %arrayidx20, align 8, !tbaa !3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define void @mp_mul(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c, i32 %nwords) local_unnamed_addr #1 {
entry:
  %cmp220 = icmp eq i32 %nwords, 0
  br i1 %cmp220, label %for.cond25.preheader, label %for.cond1.preheader.preheader

for.cond1.preheader.preheader:                    ; preds = %entry
  %wide.trip.count242 = zext i32 %nwords to i64
  br label %for.cond1.preheader

for.cond1.preheader:                              ; preds = %for.end, %for.cond1.preheader.preheader
  %indvars.iv240 = phi i64 [ 0, %for.cond1.preheader.preheader ], [ %indvars.iv.next241, %for.end ]
  %indvars.iv238 = phi i64 [ 1, %for.cond1.preheader.preheader ], [ %indvars.iv.next239, %for.end ]
  %u.0222 = phi i64 [ 0, %for.cond1.preheader.preheader ], [ %add19, %for.end ]
  %v.0221 = phi i64 [ 0, %for.cond1.preheader.preheader ], [ %add13, %for.end ]
  br label %for.body3

for.cond25.preheader:                             ; preds = %for.end, %entry
  %v.0.lcssa = phi i64 [ 0, %entry ], [ %add13, %for.end ]
  %u.0.lcssa = phi i64 [ 0, %entry ], [ %add19, %for.end ]
  %mul = shl i32 %nwords, 1
  %sub26 = add i32 %mul, -1
  %cmp27210 = icmp ugt i32 %sub26, %nwords
  br i1 %cmp27210, label %for.body29.preheader, label %for.end67

for.body29.preheader:                             ; preds = %for.cond25.preheader
  %0 = zext i32 %nwords to i64
  %wide.trip.count = zext i32 %sub26 to i64
  br label %for.body29

for.body3:                                        ; preds = %for.cond1.preheader, %for.body3
  %indvars.iv232 = phi i64 [ 0, %for.cond1.preheader ], [ %indvars.iv.next233, %for.body3 ]
  %t.1218 = phi i64 [ 0, %for.cond1.preheader ], [ %add19, %for.body3 ]
  %u.1217 = phi i64 [ %u.0222, %for.cond1.preheader ], [ %add13, %for.body3 ]
  %v.1216 = phi i64 [ %v.0221, %for.cond1.preheader ], [ %add8, %for.body3 ]
  %arrayidx = getelementptr inbounds i64, i64* %a, i64 %indvars.iv232
  %1 = load i64, i64* %arrayidx, align 8, !tbaa !3
  %sub = sub nsw i64 %indvars.iv240, %indvars.iv232
  %idxprom4 = and i64 %sub, 4294967295
  %arrayidx5 = getelementptr inbounds i64, i64* %b, i64 %idxprom4
  %2 = load i64, i64* %arrayidx5, align 8, !tbaa !3
  %and.i = and i64 %1, 4294967295
  %shr.i = lshr i64 %1, 32
  %and1.i = and i64 %2, 4294967295
  %shr2.i = lshr i64 %2, 32
  %mul.i = mul nuw i64 %and1.i, %and.i
  %mul3.i = mul nuw i64 %shr2.i, %and.i
  %mul4.i = mul nuw i64 %and1.i, %shr.i
  %mul5.i = mul nuw i64 %shr2.i, %shr.i
  %and6.i = and i64 %mul.i, 4294967295
  %shr7.i = lshr i64 %mul.i, 32
  %and8.i = and i64 %mul4.i, 4294967295
  %and9.i = and i64 %mul3.i, 4294967295
  %add.i = add nuw nsw i64 %shr7.i, %and8.i
  %add10.i = add nuw nsw i64 %add.i, %and9.i
  %shr11.i = lshr i64 %add10.i, 32
  %shl.i = shl i64 %add10.i, 32
  %xor68.i = or i64 %shl.i, %and6.i
  %shr13.i = lshr i64 %mul4.i, 32
  %shr14.i = lshr i64 %mul3.i, 32
  %and15.i = and i64 %mul5.i, 4294967295
  %add16.i = add nuw nsw i64 %shr13.i, %shr14.i
  %add17.i = add nuw nsw i64 %add16.i, %and15.i
  %add18.i = add nuw nsw i64 %add17.i, %shr11.i
  %and19.i = and i64 %add18.i, 4294967295
  %and21.i = and i64 %add18.i, 30064771072
  %and22.i = and i64 %mul5.i, -4294967296
  %add23.i = add i64 %and21.i, %and22.i
  %xor2569.i = or i64 %add23.i, %and19.i
  %add8 = add i64 %xor68.i, %v.1216
  %xor.i188 = xor i64 %add8, %shl.i
  %xor1.i189 = xor i64 %shl.i, %v.1216
  %or.i190 = or i64 %xor.i188, %xor1.i189
  %xor2.i191 = xor i64 %or.i190, %add8
  %shr.i192 = lshr i64 %xor2.i191, 63
  %add12 = add i64 %shr.i192, %xor2569.i
  %add13 = add i64 %add12, %u.1217
  %3 = xor i64 %add12, -9223372036854775808
  %xor2.i185 = and i64 %3, %add23.i
  %xor.i176 = xor i64 %add13, %add12
  %xor1.i177 = xor i64 %add12, %u.1217
  %or.i178 = or i64 %xor.i176, %xor1.i177
  %xor2.i179 = xor i64 %or.i178, %add13
  %shr.i186201 = or i64 %xor2.i179, %xor2.i185
  %or17200 = lshr i64 %shr.i186201, 63
  %add19 = add i64 %or17200, %t.1218
  %indvars.iv.next233 = add nuw nsw i64 %indvars.iv232, 1
  %exitcond237 = icmp eq i64 %indvars.iv.next233, %indvars.iv238
  br i1 %exitcond237, label %for.end, label %for.body3

for.end:                                          ; preds = %for.body3
  %arrayidx21 = getelementptr inbounds i64, i64* %c, i64 %indvars.iv240
  store i64 %add8, i64* %arrayidx21, align 8, !tbaa !3
  %indvars.iv.next241 = add nuw nsw i64 %indvars.iv240, 1
  %indvars.iv.next239 = add nuw nsw i64 %indvars.iv238, 1
  %exitcond243 = icmp eq i64 %indvars.iv.next241, %wide.trip.count242
  br i1 %exitcond243, label %for.cond25.preheader, label %for.cond1.preheader

for.body29:                                       ; preds = %for.end62, %for.body29.preheader
  %indvars.iv228 = phi i64 [ %0, %for.body29.preheader ], [ %indvars.iv.next229, %for.end62 ]
  %indvars.iv = phi i32 [ 1, %for.body29.preheader ], [ %indvars.iv.next, %for.end62 ]
  %u.2212 = phi i64 [ %u.0.lcssa, %for.body29.preheader ], [ %t.3.lcssa, %for.end62 ]
  %v.2211 = phi i64 [ %v.0.lcssa, %for.body29.preheader ], [ %u.3.lcssa, %for.end62 ]
  %4 = trunc i64 %indvars.iv228 to i32
  %5 = sub i32 %4, %nwords
  %j.1202 = add i32 %5, 1
  %cmp33203 = icmp ult i32 %j.1202, %nwords
  br i1 %cmp33203, label %for.body35.preheader, label %for.end62

for.body35.preheader:                             ; preds = %for.body29
  %6 = zext i32 %indvars.iv to i64
  br label %for.body35

for.body35:                                       ; preds = %for.body35, %for.body35.preheader
  %indvars.iv226 = phi i64 [ %6, %for.body35.preheader ], [ %indvars.iv.next227, %for.body35 ]
  %t.3206 = phi i64 [ 0, %for.body35.preheader ], [ %add59, %for.body35 ]
  %u.3205 = phi i64 [ %u.2212, %for.body35.preheader ], [ %add53, %for.body35 ]
  %v.3204 = phi i64 [ %v.2211, %for.body35.preheader ], [ %add45, %for.body35 ]
  %arrayidx37 = getelementptr inbounds i64, i64* %a, i64 %indvars.iv226
  %7 = load i64, i64* %arrayidx37, align 8, !tbaa !3
  %sub38 = sub nsw i64 %indvars.iv228, %indvars.iv226
  %idxprom39 = and i64 %sub38, 4294967295
  %arrayidx40 = getelementptr inbounds i64, i64* %b, i64 %idxprom39
  %8 = load i64, i64* %arrayidx40, align 8, !tbaa !3
  %and.i147 = and i64 %7, 4294967295
  %shr.i148 = lshr i64 %7, 32
  %and1.i149 = and i64 %8, 4294967295
  %shr2.i150 = lshr i64 %8, 32
  %mul.i151 = mul nuw i64 %and1.i149, %and.i147
  %mul3.i152 = mul nuw i64 %shr2.i150, %and.i147
  %mul4.i153 = mul nuw i64 %and1.i149, %shr.i148
  %mul5.i154 = mul nuw i64 %shr2.i150, %shr.i148
  %and6.i155 = and i64 %mul.i151, 4294967295
  %shr7.i156 = lshr i64 %mul.i151, 32
  %and8.i157 = and i64 %mul4.i153, 4294967295
  %and9.i158 = and i64 %mul3.i152, 4294967295
  %add.i159 = add nuw nsw i64 %shr7.i156, %and8.i157
  %add10.i160 = add nuw nsw i64 %add.i159, %and9.i158
  %shr11.i161 = lshr i64 %add10.i160, 32
  %shl.i162 = shl i64 %add10.i160, 32
  %xor68.i163 = or i64 %shl.i162, %and6.i155
  %shr13.i164 = lshr i64 %mul4.i153, 32
  %shr14.i165 = lshr i64 %mul3.i152, 32
  %and15.i166 = and i64 %mul5.i154, 4294967295
  %add16.i167 = add nuw nsw i64 %shr13.i164, %shr14.i165
  %add17.i168 = add nuw nsw i64 %add16.i167, %and15.i166
  %add18.i169 = add nuw nsw i64 %add17.i168, %shr11.i161
  %and19.i170 = and i64 %add18.i169, 4294967295
  %and21.i172 = and i64 %add18.i169, 30064771072
  %and22.i173 = and i64 %mul5.i154, -4294967296
  %add23.i174 = add i64 %and21.i172, %and22.i173
  %xor2569.i175 = or i64 %add23.i174, %and19.i170
  %add45 = add i64 %xor68.i163, %v.3204
  %xor.i141 = xor i64 %add45, %shl.i162
  %xor1.i142 = xor i64 %shl.i162, %v.3204
  %or.i143 = or i64 %xor.i141, %xor1.i142
  %xor2.i144 = xor i64 %or.i143, %add45
  %shr.i145 = lshr i64 %xor2.i144, 63
  %add52 = add i64 %shr.i145, %xor2569.i175
  %add53 = add i64 %add52, %u.3205
  %9 = xor i64 %add52, -9223372036854775808
  %xor2.i138 = and i64 %9, %add23.i174
  %xor.i = xor i64 %add53, %add52
  %xor1.i = xor i64 %add52, %u.3205
  %or.i = or i64 %xor.i, %xor1.i
  %xor2.i = xor i64 %or.i, %add53
  %shr.i139199 = or i64 %xor2.i, %xor2.i138
  %or57198 = lshr i64 %shr.i139199, 63
  %add59 = add i64 %or57198, %t.3206
  %indvars.iv.next227 = add nuw nsw i64 %indvars.iv226, 1
  %lftr.wideiv = trunc i64 %indvars.iv.next227 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nwords
  br i1 %exitcond, label %for.end62, label %for.body35

for.end62:                                        ; preds = %for.body35, %for.body29
  %v.3.lcssa = phi i64 [ %v.2211, %for.body29 ], [ %add45, %for.body35 ]
  %u.3.lcssa = phi i64 [ %u.2212, %for.body29 ], [ %add53, %for.body35 ]
  %t.3.lcssa = phi i64 [ 0, %for.body29 ], [ %add59, %for.body35 ]
  %arrayidx64 = getelementptr inbounds i64, i64* %c, i64 %indvars.iv228
  store i64 %v.3.lcssa, i64* %arrayidx64, align 8, !tbaa !3
  %indvars.iv.next229 = add nuw nsw i64 %indvars.iv228, 1
  %indvars.iv.next = add i32 %indvars.iv, 1
  %exitcond231 = icmp eq i64 %indvars.iv.next229, %wide.trip.count
  br i1 %exitcond231, label %for.end67, label %for.body29

for.end67:                                        ; preds = %for.end62, %for.cond25.preheader
  %v.2.lcssa = phi i64 [ %v.0.lcssa, %for.cond25.preheader ], [ %u.3.lcssa, %for.end62 ]
  %idxprom70.pre-phi = zext i32 %sub26 to i64
  %arrayidx71 = getelementptr inbounds i64, i64* %c, i64 %idxprom70.pre-phi
  store i64 %v.2.lcssa, i64* %arrayidx71, align 8, !tbaa !3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define void @rdc_mont(i64* nocapture readonly %ma, i64* nocapture %mc) local_unnamed_addr #1 {
for.body6.lr.ph.2:
  %mc370 = bitcast i64* %mc to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %mc370, i8 0, i64 64, i1 false)
  %0 = load i64, i64* %ma, align 8, !tbaa !3
  store i64 %0, i64* %mc, align 8, !tbaa !3
  %1 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 1), align 8, !tbaa !3
  %and.i.1 = and i64 %0, 4294967295
  %shr.i.1 = lshr i64 %0, 32
  %and1.i.1 = and i64 %1, 4294967295
  %shr2.i.1 = lshr i64 %1, 32
  %mul.i.1 = mul nuw i64 %and1.i.1, %and.i.1
  %mul3.i.1 = mul nuw i64 %shr2.i.1, %and.i.1
  %mul4.i.1 = mul nuw i64 %and1.i.1, %shr.i.1
  %mul5.i.1 = mul nuw i64 %shr2.i.1, %shr.i.1
  %and6.i.1 = and i64 %mul.i.1, 4294967295
  %shr7.i.1 = lshr i64 %mul.i.1, 32
  %and8.i.1 = and i64 %mul4.i.1, 4294967295
  %and9.i.1 = and i64 %mul3.i.1, 4294967295
  %add.i.1 = add nuw nsw i64 %shr7.i.1, %and8.i.1
  %add10.i.1 = add nuw nsw i64 %add.i.1, %and9.i.1
  %shr11.i.1 = lshr i64 %add10.i.1, 32
  %shl.i.1 = shl i64 %add10.i.1, 32
  %xor68.i.1 = or i64 %shl.i.1, %and6.i.1
  %shr13.i.1 = lshr i64 %mul4.i.1, 32
  %shr14.i.1 = lshr i64 %mul3.i.1, 32
  %and15.i.1 = and i64 %mul5.i.1, 4294967295
  %add16.i.1 = add nuw nsw i64 %shr13.i.1, %shr14.i.1
  %add17.i.1 = add nuw nsw i64 %add16.i.1, %and15.i.1
  %add18.i.1 = add nuw nsw i64 %add17.i.1, %shr11.i.1
  %and19.i.1 = and i64 %add18.i.1, 4294967295
  %and21.i.1 = and i64 %add18.i.1, 30064771072
  %and22.i.1 = and i64 %mul5.i.1, -4294967296
  %add23.i.1 = add i64 %and21.i.1, %and22.i.1
  %xor2569.i.1 = or i64 %add23.i.1, %and19.i.1
  %arrayidx34.1 = getelementptr inbounds i64, i64* %ma, i64 1
  %2 = load i64, i64* %arrayidx34.1, align 8, !tbaa !3
  %add35.1 = add i64 %2, %xor68.i.1
  %xor.i297.1 = xor i64 %add35.1, %shl.i.1
  %xor1.i298.1 = xor i64 %2, %shl.i.1
  %or.i299.1 = or i64 %xor.i297.1, %xor1.i298.1
  %xor2.i300.1 = xor i64 %or.i299.1, %add35.1
  %shr.i301.1 = lshr i64 %xor2.i300.1, 63
  %add41.1 = add i64 %shr.i301.1, %xor2569.i.1
  %arrayidx50.1 = getelementptr inbounds i64, i64* %mc, i64 1
  store i64 %add35.1, i64* %arrayidx50.1, align 8, !tbaa !3
  %3 = xor i64 %add41.1, -9223372036854775808
  %xor2.i294.1 = and i64 %3, %add23.i.1
  %shr.i295.1 = lshr i64 %xor2.i294.1, 63
  %arrayidx34.2 = getelementptr inbounds i64, i64* %ma, i64 2
  %4 = load i64, i64* %arrayidx34.2, align 8, !tbaa !3
  %add35.2 = add i64 %4, %add41.1
  %arrayidx50.2 = getelementptr inbounds i64, i64* %mc, i64 2
  store i64 %add35.2, i64* %arrayidx50.2, align 8, !tbaa !3
  %xor.i297.2 = xor i64 %add35.2, %add41.1
  %xor1.i298.2 = xor i64 %4, %add41.1
  %or.i299.2 = or i64 %xor.i297.2, %xor1.i298.2
  %xor2.i300.2 = xor i64 %or.i299.2, %add35.2
  %shr.i301.2 = lshr i64 %xor2.i300.2, 63
  %add41.2 = add nuw nsw i64 %shr.i301.2, %shr.i295.1
  %5 = load i64, i64* %mc, align 8, !tbaa !3
  %6 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i.3 = and i64 %5, 4294967295
  %shr.i.3 = lshr i64 %5, 32
  %and1.i.3 = and i64 %6, 4294967295
  %shr2.i.3 = lshr i64 %6, 32
  %mul.i.3 = mul nuw i64 %and1.i.3, %and.i.3
  %mul3.i.3 = mul nuw i64 %shr2.i.3, %and.i.3
  %mul4.i.3 = mul nuw i64 %and1.i.3, %shr.i.3
  %mul5.i.3 = mul nuw i64 %shr2.i.3, %shr.i.3
  %and6.i.3 = and i64 %mul.i.3, 4294967295
  %shr7.i.3 = lshr i64 %mul.i.3, 32
  %and8.i.3 = and i64 %mul4.i.3, 4294967295
  %and9.i.3 = and i64 %mul3.i.3, 4294967295
  %add.i.3 = add nuw nsw i64 %shr7.i.3, %and8.i.3
  %add10.i.3 = add nuw nsw i64 %add.i.3, %and9.i.3
  %shr11.i.3 = lshr i64 %add10.i.3, 32
  %shl.i.3 = shl i64 %add10.i.3, 32
  %xor68.i.3 = or i64 %shl.i.3, %and6.i.3
  %shr13.i.3 = lshr i64 %mul4.i.3, 32
  %shr14.i.3 = lshr i64 %mul3.i.3, 32
  %and15.i.3 = and i64 %mul5.i.3, 4294967295
  %add16.i.3 = add nuw nsw i64 %shr13.i.3, %shr14.i.3
  %add17.i.3 = add nuw nsw i64 %add16.i.3, %and15.i.3
  %add18.i.3 = add nuw nsw i64 %add17.i.3, %shr11.i.3
  %and19.i.3 = and i64 %add18.i.3, 4294967295
  %and21.i.3 = and i64 %add18.i.3, 30064771072
  %and22.i.3 = and i64 %mul5.i.3, -4294967296
  %add23.i.3 = add i64 %and21.i.3, %and22.i.3
  %xor2569.i.3 = or i64 %add23.i.3, %and19.i.3
  %add16.3 = add i64 %xor68.i.3, %add41.2
  %7 = xor i64 %add16.3, -9223372036854775808
  %xor2.i318.3 = and i64 %shl.i.3, %7
  %shr.i319.3 = lshr i64 %xor2.i318.3, 63
  %add20.3 = add i64 %shr.i319.3, %xor2569.i.3
  %8 = xor i64 %add20.3, -9223372036854775808
  %xor2.i312.3 = and i64 %8, %add23.i.3
  %or25327.3 = lshr i64 %xor2.i312.3, 63
  %arrayidx34.3 = getelementptr inbounds i64, i64* %ma, i64 3
  %9 = load i64, i64* %arrayidx34.3, align 8, !tbaa !3
  %add35.3 = add i64 %9, %add16.3
  %xor.i297.3 = xor i64 %add35.3, %add16.3
  %xor1.i298.3 = xor i64 %9, %add16.3
  %or.i299.3 = or i64 %xor.i297.3, %xor1.i298.3
  %xor2.i300.3 = xor i64 %or.i299.3, %add35.3
  %shr.i301.3 = lshr i64 %xor2.i300.3, 63
  %add41.3 = add i64 %shr.i301.3, %add20.3
  %arrayidx50.3 = getelementptr inbounds i64, i64* %mc, i64 3
  store i64 %add35.3, i64* %arrayidx50.3, align 8, !tbaa !3
  %10 = xor i64 %add41.3, -9223372036854775808
  %xor2.i294.3 = and i64 %10, %add20.3
  %shr.i295.3 = lshr i64 %xor2.i294.3, 63
  %add48.3 = add nuw nsw i64 %shr.i295.3, %or25327.3
  %11 = load i64, i64* %mc, align 8, !tbaa !3
  %12 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i.4 = and i64 %11, 4294967295
  %shr.i.4 = lshr i64 %11, 32
  %and1.i.4 = and i64 %12, 4294967295
  %shr2.i.4 = lshr i64 %12, 32
  %mul.i.4 = mul nuw i64 %and1.i.4, %and.i.4
  %mul3.i.4 = mul nuw i64 %shr2.i.4, %and.i.4
  %mul4.i.4 = mul nuw i64 %and1.i.4, %shr.i.4
  %mul5.i.4 = mul nuw i64 %shr2.i.4, %shr.i.4
  %and6.i.4 = and i64 %mul.i.4, 4294967295
  %shr7.i.4 = lshr i64 %mul.i.4, 32
  %and8.i.4 = and i64 %mul4.i.4, 4294967295
  %and9.i.4 = and i64 %mul3.i.4, 4294967295
  %add.i.4 = add nuw nsw i64 %shr7.i.4, %and8.i.4
  %add10.i.4 = add nuw nsw i64 %add.i.4, %and9.i.4
  %shr11.i.4 = lshr i64 %add10.i.4, 32
  %shl.i.4 = shl i64 %add10.i.4, 32
  %xor68.i.4 = or i64 %shl.i.4, %and6.i.4
  %shr13.i.4 = lshr i64 %mul4.i.4, 32
  %shr14.i.4 = lshr i64 %mul3.i.4, 32
  %and15.i.4 = and i64 %mul5.i.4, 4294967295
  %add16.i.4 = add nuw nsw i64 %shr13.i.4, %shr14.i.4
  %add17.i.4 = add nuw nsw i64 %add16.i.4, %and15.i.4
  %add18.i.4 = add nuw nsw i64 %add17.i.4, %shr11.i.4
  %and19.i.4 = and i64 %add18.i.4, 4294967295
  %and21.i.4 = and i64 %add18.i.4, 30064771072
  %and22.i.4 = and i64 %mul5.i.4, -4294967296
  %add23.i.4 = add i64 %and21.i.4, %and22.i.4
  %xor2569.i.4 = or i64 %add23.i.4, %and19.i.4
  %add16.4 = add i64 %xor68.i.4, %add41.3
  %xor.i315.4 = xor i64 %add16.4, %shl.i.4
  %xor1.i316.4 = xor i64 %shl.i.4, %add41.3
  %or.i317.4 = or i64 %xor.i315.4, %xor1.i316.4
  %xor2.i318.4 = xor i64 %or.i317.4, %add16.4
  %shr.i319.4 = lshr i64 %xor2.i318.4, 63
  %add20.4 = add i64 %shr.i319.4, %xor2569.i.4
  %add21.4 = add i64 %add20.4, %add48.3
  %13 = xor i64 %add20.4, -9223372036854775808
  %xor2.i312.4 = and i64 %13, %add23.i.4
  %xor.i303.4 = xor i64 %add21.4, %add20.4
  %xor1.i304.4 = xor i64 %add20.4, %add48.3
  %or.i305.4 = or i64 %xor.i303.4, %xor1.i304.4
  %xor2.i306.4 = xor i64 %or.i305.4, %add21.4
  %shr.i313328.4 = or i64 %xor2.i306.4, %xor2.i312.4
  %or25327.4 = lshr i64 %shr.i313328.4, 63
  %arrayidx9.4.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %14 = load i64, i64* %arrayidx9.4.1, align 8, !tbaa !3
  %15 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i.4.1 = and i64 %14, 4294967295
  %shr.i.4.1 = lshr i64 %14, 32
  %and1.i.4.1 = and i64 %15, 4294967295
  %shr2.i.4.1 = lshr i64 %15, 32
  %mul.i.4.1 = mul nuw i64 %and1.i.4.1, %and.i.4.1
  %mul3.i.4.1 = mul nuw i64 %shr2.i.4.1, %and.i.4.1
  %mul4.i.4.1 = mul nuw i64 %and1.i.4.1, %shr.i.4.1
  %mul5.i.4.1 = mul nuw i64 %shr2.i.4.1, %shr.i.4.1
  %and6.i.4.1 = and i64 %mul.i.4.1, 4294967295
  %shr7.i.4.1 = lshr i64 %mul.i.4.1, 32
  %and8.i.4.1 = and i64 %mul4.i.4.1, 4294967295
  %and9.i.4.1 = and i64 %mul3.i.4.1, 4294967295
  %add.i.4.1 = add nuw nsw i64 %shr7.i.4.1, %and8.i.4.1
  %add10.i.4.1 = add nuw nsw i64 %add.i.4.1, %and9.i.4.1
  %shr11.i.4.1 = lshr i64 %add10.i.4.1, 32
  %shl.i.4.1 = shl i64 %add10.i.4.1, 32
  %xor68.i.4.1 = or i64 %shl.i.4.1, %and6.i.4.1
  %shr13.i.4.1 = lshr i64 %mul4.i.4.1, 32
  %shr14.i.4.1 = lshr i64 %mul3.i.4.1, 32
  %and15.i.4.1 = and i64 %mul5.i.4.1, 4294967295
  %add16.i.4.1 = add nuw nsw i64 %shr13.i.4.1, %shr14.i.4.1
  %add17.i.4.1 = add nuw nsw i64 %add16.i.4.1, %and15.i.4.1
  %add18.i.4.1 = add nuw nsw i64 %add17.i.4.1, %shr11.i.4.1
  %and19.i.4.1 = and i64 %add18.i.4.1, 4294967295
  %and21.i.4.1 = and i64 %add18.i.4.1, 30064771072
  %and22.i.4.1 = and i64 %mul5.i.4.1, -4294967296
  %add23.i.4.1 = add i64 %and21.i.4.1, %and22.i.4.1
  %xor2569.i.4.1 = or i64 %add23.i.4.1, %and19.i.4.1
  %add16.4.1 = add i64 %xor68.i.4.1, %add16.4
  %xor.i315.4.1 = xor i64 %add16.4.1, %shl.i.4.1
  %xor1.i316.4.1 = xor i64 %shl.i.4.1, %add16.4
  %or.i317.4.1 = or i64 %xor.i315.4.1, %xor1.i316.4.1
  %xor2.i318.4.1 = xor i64 %or.i317.4.1, %add16.4.1
  %shr.i319.4.1 = lshr i64 %xor2.i318.4.1, 63
  %add20.4.1 = add i64 %shr.i319.4.1, %xor2569.i.4.1
  %add21.4.1 = add i64 %add20.4.1, %add21.4
  %16 = xor i64 %add20.4.1, -9223372036854775808
  %xor2.i312.4.1 = and i64 %16, %add23.i.4.1
  %xor.i303.4.1 = xor i64 %add21.4.1, %add20.4.1
  %xor1.i304.4.1 = xor i64 %add20.4.1, %add21.4
  %or.i305.4.1 = or i64 %xor.i303.4.1, %xor1.i304.4.1
  %xor2.i306.4.1 = xor i64 %or.i305.4.1, %add21.4.1
  %shr.i313328.4.1 = or i64 %xor2.i306.4.1, %xor2.i312.4.1
  %or25327.4.1 = lshr i64 %shr.i313328.4.1, 63
  %add27.4.1 = add nuw nsw i64 %or25327.4.1, %or25327.4
  %arrayidx34.4 = getelementptr inbounds i64, i64* %ma, i64 4
  %17 = load i64, i64* %arrayidx34.4, align 8, !tbaa !3
  %add35.4 = add i64 %17, %add16.4.1
  %xor.i297.4 = xor i64 %add35.4, %add16.4.1
  %xor1.i298.4 = xor i64 %17, %add16.4.1
  %or.i299.4 = or i64 %xor.i297.4, %xor1.i298.4
  %xor2.i300.4 = xor i64 %or.i299.4, %add35.4
  %shr.i301.4 = lshr i64 %xor2.i300.4, 63
  %add41.4 = add i64 %shr.i301.4, %add21.4.1
  %arrayidx50.4 = getelementptr inbounds i64, i64* %mc, i64 4
  store i64 %add35.4, i64* %arrayidx50.4, align 8, !tbaa !3
  %18 = xor i64 %add41.4, -9223372036854775808
  %xor2.i294.4 = and i64 %18, %add21.4.1
  %shr.i295.4 = lshr i64 %xor2.i294.4, 63
  %add48.4 = add nsw i64 %shr.i295.4, %add27.4.1
  %19 = load i64, i64* %mc, align 8, !tbaa !3
  %20 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.5 = and i64 %19, 4294967295
  %shr.i.5 = lshr i64 %19, 32
  %and1.i.5 = and i64 %20, 4294967295
  %shr2.i.5 = lshr i64 %20, 32
  %mul.i.5 = mul nuw i64 %and1.i.5, %and.i.5
  %mul3.i.5 = mul nuw i64 %shr2.i.5, %and.i.5
  %mul4.i.5 = mul nuw i64 %and1.i.5, %shr.i.5
  %mul5.i.5 = mul nuw i64 %shr2.i.5, %shr.i.5
  %and6.i.5 = and i64 %mul.i.5, 4294967295
  %shr7.i.5 = lshr i64 %mul.i.5, 32
  %and8.i.5 = and i64 %mul4.i.5, 4294967295
  %and9.i.5 = and i64 %mul3.i.5, 4294967295
  %add.i.5 = add nuw nsw i64 %shr7.i.5, %and8.i.5
  %add10.i.5 = add nuw nsw i64 %add.i.5, %and9.i.5
  %shr11.i.5 = lshr i64 %add10.i.5, 32
  %shl.i.5 = shl i64 %add10.i.5, 32
  %xor68.i.5 = or i64 %shl.i.5, %and6.i.5
  %shr13.i.5 = lshr i64 %mul4.i.5, 32
  %shr14.i.5 = lshr i64 %mul3.i.5, 32
  %and15.i.5 = and i64 %mul5.i.5, 4294967295
  %add16.i.5 = add nuw nsw i64 %shr13.i.5, %shr14.i.5
  %add17.i.5 = add nuw nsw i64 %add16.i.5, %and15.i.5
  %add18.i.5 = add nuw nsw i64 %add17.i.5, %shr11.i.5
  %and19.i.5 = and i64 %add18.i.5, 4294967295
  %and21.i.5 = and i64 %add18.i.5, 30064771072
  %and22.i.5 = and i64 %mul5.i.5, -4294967296
  %add23.i.5 = add i64 %and21.i.5, %and22.i.5
  %xor2569.i.5 = or i64 %add23.i.5, %and19.i.5
  %add16.5 = add i64 %xor68.i.5, %add41.4
  %xor.i315.5 = xor i64 %add16.5, %shl.i.5
  %xor1.i316.5 = xor i64 %shl.i.5, %add41.4
  %or.i317.5 = or i64 %xor.i315.5, %xor1.i316.5
  %xor2.i318.5 = xor i64 %or.i317.5, %add16.5
  %shr.i319.5 = lshr i64 %xor2.i318.5, 63
  %add20.5 = add i64 %shr.i319.5, %xor2569.i.5
  %add21.5 = add i64 %add20.5, %add48.4
  %21 = xor i64 %add20.5, -9223372036854775808
  %xor2.i312.5 = and i64 %21, %add23.i.5
  %xor.i303.5 = xor i64 %add21.5, %add20.5
  %xor1.i304.5 = xor i64 %add20.5, %add48.4
  %or.i305.5 = or i64 %xor.i303.5, %xor1.i304.5
  %xor2.i306.5 = xor i64 %or.i305.5, %add21.5
  %shr.i313328.5 = or i64 %xor2.i306.5, %xor2.i312.5
  %or25327.5 = lshr i64 %shr.i313328.5, 63
  %arrayidx9.5.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %22 = load i64, i64* %arrayidx9.5.1, align 8, !tbaa !3
  %23 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i.5.1 = and i64 %22, 4294967295
  %shr.i.5.1 = lshr i64 %22, 32
  %and1.i.5.1 = and i64 %23, 4294967295
  %shr2.i.5.1 = lshr i64 %23, 32
  %mul.i.5.1 = mul nuw i64 %and1.i.5.1, %and.i.5.1
  %mul3.i.5.1 = mul nuw i64 %shr2.i.5.1, %and.i.5.1
  %mul4.i.5.1 = mul nuw i64 %and1.i.5.1, %shr.i.5.1
  %mul5.i.5.1 = mul nuw i64 %shr2.i.5.1, %shr.i.5.1
  %and6.i.5.1 = and i64 %mul.i.5.1, 4294967295
  %shr7.i.5.1 = lshr i64 %mul.i.5.1, 32
  %and8.i.5.1 = and i64 %mul4.i.5.1, 4294967295
  %and9.i.5.1 = and i64 %mul3.i.5.1, 4294967295
  %add.i.5.1 = add nuw nsw i64 %shr7.i.5.1, %and8.i.5.1
  %add10.i.5.1 = add nuw nsw i64 %add.i.5.1, %and9.i.5.1
  %shr11.i.5.1 = lshr i64 %add10.i.5.1, 32
  %shl.i.5.1 = shl i64 %add10.i.5.1, 32
  %xor68.i.5.1 = or i64 %shl.i.5.1, %and6.i.5.1
  %shr13.i.5.1 = lshr i64 %mul4.i.5.1, 32
  %shr14.i.5.1 = lshr i64 %mul3.i.5.1, 32
  %and15.i.5.1 = and i64 %mul5.i.5.1, 4294967295
  %add16.i.5.1 = add nuw nsw i64 %shr13.i.5.1, %shr14.i.5.1
  %add17.i.5.1 = add nuw nsw i64 %add16.i.5.1, %and15.i.5.1
  %add18.i.5.1 = add nuw nsw i64 %add17.i.5.1, %shr11.i.5.1
  %and19.i.5.1 = and i64 %add18.i.5.1, 4294967295
  %and21.i.5.1 = and i64 %add18.i.5.1, 30064771072
  %and22.i.5.1 = and i64 %mul5.i.5.1, -4294967296
  %add23.i.5.1 = add i64 %and21.i.5.1, %and22.i.5.1
  %xor2569.i.5.1 = or i64 %add23.i.5.1, %and19.i.5.1
  %add16.5.1 = add i64 %xor68.i.5.1, %add16.5
  %xor.i315.5.1 = xor i64 %add16.5.1, %shl.i.5.1
  %xor1.i316.5.1 = xor i64 %shl.i.5.1, %add16.5
  %or.i317.5.1 = or i64 %xor.i315.5.1, %xor1.i316.5.1
  %xor2.i318.5.1 = xor i64 %or.i317.5.1, %add16.5.1
  %shr.i319.5.1 = lshr i64 %xor2.i318.5.1, 63
  %add20.5.1 = add i64 %shr.i319.5.1, %xor2569.i.5.1
  %add21.5.1 = add i64 %add20.5.1, %add21.5
  %24 = xor i64 %add20.5.1, -9223372036854775808
  %xor2.i312.5.1 = and i64 %24, %add23.i.5.1
  %xor.i303.5.1 = xor i64 %add21.5.1, %add20.5.1
  %xor1.i304.5.1 = xor i64 %add20.5.1, %add21.5
  %or.i305.5.1 = or i64 %xor.i303.5.1, %xor1.i304.5.1
  %xor2.i306.5.1 = xor i64 %or.i305.5.1, %add21.5.1
  %shr.i313328.5.1 = or i64 %xor2.i306.5.1, %xor2.i312.5.1
  %or25327.5.1 = lshr i64 %shr.i313328.5.1, 63
  %add27.5.1 = add nuw nsw i64 %or25327.5.1, %or25327.5
  %arrayidx9.5.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %25 = load i64, i64* %arrayidx9.5.2, align 8, !tbaa !3
  %26 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i.5.2 = and i64 %25, 4294967295
  %shr.i.5.2 = lshr i64 %25, 32
  %and1.i.5.2 = and i64 %26, 4294967295
  %shr2.i.5.2 = lshr i64 %26, 32
  %mul.i.5.2 = mul nuw i64 %and1.i.5.2, %and.i.5.2
  %mul3.i.5.2 = mul nuw i64 %shr2.i.5.2, %and.i.5.2
  %mul4.i.5.2 = mul nuw i64 %and1.i.5.2, %shr.i.5.2
  %mul5.i.5.2 = mul nuw i64 %shr2.i.5.2, %shr.i.5.2
  %and6.i.5.2 = and i64 %mul.i.5.2, 4294967295
  %shr7.i.5.2 = lshr i64 %mul.i.5.2, 32
  %and8.i.5.2 = and i64 %mul4.i.5.2, 4294967295
  %and9.i.5.2 = and i64 %mul3.i.5.2, 4294967295
  %add.i.5.2 = add nuw nsw i64 %shr7.i.5.2, %and8.i.5.2
  %add10.i.5.2 = add nuw nsw i64 %add.i.5.2, %and9.i.5.2
  %shr11.i.5.2 = lshr i64 %add10.i.5.2, 32
  %shl.i.5.2 = shl i64 %add10.i.5.2, 32
  %xor68.i.5.2 = or i64 %shl.i.5.2, %and6.i.5.2
  %shr13.i.5.2 = lshr i64 %mul4.i.5.2, 32
  %shr14.i.5.2 = lshr i64 %mul3.i.5.2, 32
  %and15.i.5.2 = and i64 %mul5.i.5.2, 4294967295
  %add16.i.5.2 = add nuw nsw i64 %shr13.i.5.2, %shr14.i.5.2
  %add17.i.5.2 = add nuw nsw i64 %add16.i.5.2, %and15.i.5.2
  %add18.i.5.2 = add nuw nsw i64 %add17.i.5.2, %shr11.i.5.2
  %and19.i.5.2 = and i64 %add18.i.5.2, 4294967295
  %and21.i.5.2 = and i64 %add18.i.5.2, 30064771072
  %and22.i.5.2 = and i64 %mul5.i.5.2, -4294967296
  %add23.i.5.2 = add i64 %and21.i.5.2, %and22.i.5.2
  %xor2569.i.5.2 = or i64 %add23.i.5.2, %and19.i.5.2
  %add16.5.2 = add i64 %xor68.i.5.2, %add16.5.1
  %xor.i315.5.2 = xor i64 %add16.5.2, %shl.i.5.2
  %xor1.i316.5.2 = xor i64 %shl.i.5.2, %add16.5.1
  %or.i317.5.2 = or i64 %xor.i315.5.2, %xor1.i316.5.2
  %xor2.i318.5.2 = xor i64 %or.i317.5.2, %add16.5.2
  %shr.i319.5.2 = lshr i64 %xor2.i318.5.2, 63
  %add20.5.2 = add i64 %shr.i319.5.2, %xor2569.i.5.2
  %add21.5.2 = add i64 %add20.5.2, %add21.5.1
  %27 = xor i64 %add20.5.2, -9223372036854775808
  %xor2.i312.5.2 = and i64 %27, %add23.i.5.2
  %xor.i303.5.2 = xor i64 %add21.5.2, %add20.5.2
  %xor1.i304.5.2 = xor i64 %add20.5.2, %add21.5.1
  %or.i305.5.2 = or i64 %xor.i303.5.2, %xor1.i304.5.2
  %xor2.i306.5.2 = xor i64 %or.i305.5.2, %add21.5.2
  %shr.i313328.5.2 = or i64 %xor2.i306.5.2, %xor2.i312.5.2
  %or25327.5.2 = lshr i64 %shr.i313328.5.2, 63
  %add27.5.2 = add nsw i64 %or25327.5.2, %add27.5.1
  %arrayidx34.5 = getelementptr inbounds i64, i64* %ma, i64 5
  %28 = load i64, i64* %arrayidx34.5, align 8, !tbaa !3
  %add35.5 = add i64 %28, %add16.5.2
  %xor.i297.5 = xor i64 %add35.5, %add16.5.2
  %xor1.i298.5 = xor i64 %28, %add16.5.2
  %or.i299.5 = or i64 %xor.i297.5, %xor1.i298.5
  %xor2.i300.5 = xor i64 %or.i299.5, %add35.5
  %shr.i301.5 = lshr i64 %xor2.i300.5, 63
  %add41.5 = add i64 %shr.i301.5, %add21.5.2
  %arrayidx50.5 = getelementptr inbounds i64, i64* %mc, i64 5
  store i64 %add35.5, i64* %arrayidx50.5, align 8, !tbaa !3
  %29 = xor i64 %add41.5, -9223372036854775808
  %xor2.i294.5 = and i64 %29, %add21.5.2
  %shr.i295.5 = lshr i64 %xor2.i294.5, 63
  %add48.5 = add nsw i64 %shr.i295.5, %add27.5.2
  %30 = load i64, i64* %mc, align 8, !tbaa !3
  %31 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.6 = and i64 %30, 4294967295
  %shr.i.6 = lshr i64 %30, 32
  %and1.i.6 = and i64 %31, 4294967295
  %shr2.i.6 = lshr i64 %31, 32
  %mul.i.6 = mul nuw i64 %and1.i.6, %and.i.6
  %mul3.i.6 = mul nuw i64 %shr2.i.6, %and.i.6
  %mul4.i.6 = mul nuw i64 %and1.i.6, %shr.i.6
  %mul5.i.6 = mul nuw i64 %shr2.i.6, %shr.i.6
  %and6.i.6 = and i64 %mul.i.6, 4294967295
  %shr7.i.6 = lshr i64 %mul.i.6, 32
  %and8.i.6 = and i64 %mul4.i.6, 4294967295
  %and9.i.6 = and i64 %mul3.i.6, 4294967295
  %add.i.6 = add nuw nsw i64 %shr7.i.6, %and8.i.6
  %add10.i.6 = add nuw nsw i64 %add.i.6, %and9.i.6
  %shr11.i.6 = lshr i64 %add10.i.6, 32
  %shl.i.6 = shl i64 %add10.i.6, 32
  %xor68.i.6 = or i64 %shl.i.6, %and6.i.6
  %shr13.i.6 = lshr i64 %mul4.i.6, 32
  %shr14.i.6 = lshr i64 %mul3.i.6, 32
  %and15.i.6 = and i64 %mul5.i.6, 4294967295
  %add16.i.6 = add nuw nsw i64 %shr13.i.6, %shr14.i.6
  %add17.i.6 = add nuw nsw i64 %add16.i.6, %and15.i.6
  %add18.i.6 = add nuw nsw i64 %add17.i.6, %shr11.i.6
  %and19.i.6 = and i64 %add18.i.6, 4294967295
  %and21.i.6 = and i64 %add18.i.6, 30064771072
  %and22.i.6 = and i64 %mul5.i.6, -4294967296
  %add23.i.6 = add i64 %and21.i.6, %and22.i.6
  %xor2569.i.6 = or i64 %add23.i.6, %and19.i.6
  %add16.6 = add i64 %xor68.i.6, %add41.5
  %xor.i315.6 = xor i64 %add16.6, %shl.i.6
  %xor1.i316.6 = xor i64 %shl.i.6, %add41.5
  %or.i317.6 = or i64 %xor.i315.6, %xor1.i316.6
  %xor2.i318.6 = xor i64 %or.i317.6, %add16.6
  %shr.i319.6 = lshr i64 %xor2.i318.6, 63
  %add20.6 = add i64 %shr.i319.6, %xor2569.i.6
  %add21.6 = add i64 %add20.6, %add48.5
  %32 = xor i64 %add20.6, -9223372036854775808
  %xor2.i312.6 = and i64 %32, %add23.i.6
  %xor.i303.6 = xor i64 %add21.6, %add20.6
  %xor1.i304.6 = xor i64 %add20.6, %add48.5
  %or.i305.6 = or i64 %xor.i303.6, %xor1.i304.6
  %xor2.i306.6 = xor i64 %or.i305.6, %add21.6
  %shr.i313328.6 = or i64 %xor2.i306.6, %xor2.i312.6
  %or25327.6 = lshr i64 %shr.i313328.6, 63
  %arrayidx9.6.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %33 = load i64, i64* %arrayidx9.6.1, align 8, !tbaa !3
  %34 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.6.1 = and i64 %33, 4294967295
  %shr.i.6.1 = lshr i64 %33, 32
  %and1.i.6.1 = and i64 %34, 4294967295
  %shr2.i.6.1 = lshr i64 %34, 32
  %mul.i.6.1 = mul nuw i64 %and1.i.6.1, %and.i.6.1
  %mul3.i.6.1 = mul nuw i64 %shr2.i.6.1, %and.i.6.1
  %mul4.i.6.1 = mul nuw i64 %and1.i.6.1, %shr.i.6.1
  %mul5.i.6.1 = mul nuw i64 %shr2.i.6.1, %shr.i.6.1
  %and6.i.6.1 = and i64 %mul.i.6.1, 4294967295
  %shr7.i.6.1 = lshr i64 %mul.i.6.1, 32
  %and8.i.6.1 = and i64 %mul4.i.6.1, 4294967295
  %and9.i.6.1 = and i64 %mul3.i.6.1, 4294967295
  %add.i.6.1 = add nuw nsw i64 %shr7.i.6.1, %and8.i.6.1
  %add10.i.6.1 = add nuw nsw i64 %add.i.6.1, %and9.i.6.1
  %shr11.i.6.1 = lshr i64 %add10.i.6.1, 32
  %shl.i.6.1 = shl i64 %add10.i.6.1, 32
  %xor68.i.6.1 = or i64 %shl.i.6.1, %and6.i.6.1
  %shr13.i.6.1 = lshr i64 %mul4.i.6.1, 32
  %shr14.i.6.1 = lshr i64 %mul3.i.6.1, 32
  %and15.i.6.1 = and i64 %mul5.i.6.1, 4294967295
  %add16.i.6.1 = add nuw nsw i64 %shr13.i.6.1, %shr14.i.6.1
  %add17.i.6.1 = add nuw nsw i64 %add16.i.6.1, %and15.i.6.1
  %add18.i.6.1 = add nuw nsw i64 %add17.i.6.1, %shr11.i.6.1
  %and19.i.6.1 = and i64 %add18.i.6.1, 4294967295
  %and21.i.6.1 = and i64 %add18.i.6.1, 30064771072
  %and22.i.6.1 = and i64 %mul5.i.6.1, -4294967296
  %add23.i.6.1 = add i64 %and21.i.6.1, %and22.i.6.1
  %xor2569.i.6.1 = or i64 %add23.i.6.1, %and19.i.6.1
  %add16.6.1 = add i64 %xor68.i.6.1, %add16.6
  %xor.i315.6.1 = xor i64 %add16.6.1, %shl.i.6.1
  %xor1.i316.6.1 = xor i64 %shl.i.6.1, %add16.6
  %or.i317.6.1 = or i64 %xor.i315.6.1, %xor1.i316.6.1
  %xor2.i318.6.1 = xor i64 %or.i317.6.1, %add16.6.1
  %shr.i319.6.1 = lshr i64 %xor2.i318.6.1, 63
  %add20.6.1 = add i64 %shr.i319.6.1, %xor2569.i.6.1
  %add21.6.1 = add i64 %add20.6.1, %add21.6
  %35 = xor i64 %add20.6.1, -9223372036854775808
  %xor2.i312.6.1 = and i64 %35, %add23.i.6.1
  %xor.i303.6.1 = xor i64 %add21.6.1, %add20.6.1
  %xor1.i304.6.1 = xor i64 %add20.6.1, %add21.6
  %or.i305.6.1 = or i64 %xor.i303.6.1, %xor1.i304.6.1
  %xor2.i306.6.1 = xor i64 %or.i305.6.1, %add21.6.1
  %shr.i313328.6.1 = or i64 %xor2.i306.6.1, %xor2.i312.6.1
  %or25327.6.1 = lshr i64 %shr.i313328.6.1, 63
  %add27.6.1 = add nuw nsw i64 %or25327.6.1, %or25327.6
  %arrayidx9.6.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %36 = load i64, i64* %arrayidx9.6.2, align 8, !tbaa !3
  %37 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i.6.2 = and i64 %36, 4294967295
  %shr.i.6.2 = lshr i64 %36, 32
  %and1.i.6.2 = and i64 %37, 4294967295
  %shr2.i.6.2 = lshr i64 %37, 32
  %mul.i.6.2 = mul nuw i64 %and1.i.6.2, %and.i.6.2
  %mul3.i.6.2 = mul nuw i64 %shr2.i.6.2, %and.i.6.2
  %mul4.i.6.2 = mul nuw i64 %and1.i.6.2, %shr.i.6.2
  %mul5.i.6.2 = mul nuw i64 %shr2.i.6.2, %shr.i.6.2
  %and6.i.6.2 = and i64 %mul.i.6.2, 4294967295
  %shr7.i.6.2 = lshr i64 %mul.i.6.2, 32
  %and8.i.6.2 = and i64 %mul4.i.6.2, 4294967295
  %and9.i.6.2 = and i64 %mul3.i.6.2, 4294967295
  %add.i.6.2 = add nuw nsw i64 %shr7.i.6.2, %and8.i.6.2
  %add10.i.6.2 = add nuw nsw i64 %add.i.6.2, %and9.i.6.2
  %shr11.i.6.2 = lshr i64 %add10.i.6.2, 32
  %shl.i.6.2 = shl i64 %add10.i.6.2, 32
  %xor68.i.6.2 = or i64 %shl.i.6.2, %and6.i.6.2
  %shr13.i.6.2 = lshr i64 %mul4.i.6.2, 32
  %shr14.i.6.2 = lshr i64 %mul3.i.6.2, 32
  %and15.i.6.2 = and i64 %mul5.i.6.2, 4294967295
  %add16.i.6.2 = add nuw nsw i64 %shr13.i.6.2, %shr14.i.6.2
  %add17.i.6.2 = add nuw nsw i64 %add16.i.6.2, %and15.i.6.2
  %add18.i.6.2 = add nuw nsw i64 %add17.i.6.2, %shr11.i.6.2
  %and19.i.6.2 = and i64 %add18.i.6.2, 4294967295
  %and21.i.6.2 = and i64 %add18.i.6.2, 30064771072
  %and22.i.6.2 = and i64 %mul5.i.6.2, -4294967296
  %add23.i.6.2 = add i64 %and21.i.6.2, %and22.i.6.2
  %xor2569.i.6.2 = or i64 %add23.i.6.2, %and19.i.6.2
  %add16.6.2 = add i64 %xor68.i.6.2, %add16.6.1
  %xor.i315.6.2 = xor i64 %add16.6.2, %shl.i.6.2
  %xor1.i316.6.2 = xor i64 %shl.i.6.2, %add16.6.1
  %or.i317.6.2 = or i64 %xor.i315.6.2, %xor1.i316.6.2
  %xor2.i318.6.2 = xor i64 %or.i317.6.2, %add16.6.2
  %shr.i319.6.2 = lshr i64 %xor2.i318.6.2, 63
  %add20.6.2 = add i64 %shr.i319.6.2, %xor2569.i.6.2
  %add21.6.2 = add i64 %add20.6.2, %add21.6.1
  %38 = xor i64 %add20.6.2, -9223372036854775808
  %xor2.i312.6.2 = and i64 %38, %add23.i.6.2
  %xor.i303.6.2 = xor i64 %add21.6.2, %add20.6.2
  %xor1.i304.6.2 = xor i64 %add20.6.2, %add21.6.1
  %or.i305.6.2 = or i64 %xor.i303.6.2, %xor1.i304.6.2
  %xor2.i306.6.2 = xor i64 %or.i305.6.2, %add21.6.2
  %shr.i313328.6.2 = or i64 %xor2.i306.6.2, %xor2.i312.6.2
  %or25327.6.2 = lshr i64 %shr.i313328.6.2, 63
  %add27.6.2 = add nsw i64 %or25327.6.2, %add27.6.1
  %arrayidx9.6.3 = getelementptr inbounds i64, i64* %mc, i64 3
  %39 = load i64, i64* %arrayidx9.6.3, align 8, !tbaa !3
  %40 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i.6.3 = and i64 %39, 4294967295
  %shr.i.6.3 = lshr i64 %39, 32
  %and1.i.6.3 = and i64 %40, 4294967295
  %shr2.i.6.3 = lshr i64 %40, 32
  %mul.i.6.3 = mul nuw i64 %and1.i.6.3, %and.i.6.3
  %mul3.i.6.3 = mul nuw i64 %shr2.i.6.3, %and.i.6.3
  %mul4.i.6.3 = mul nuw i64 %and1.i.6.3, %shr.i.6.3
  %mul5.i.6.3 = mul nuw i64 %shr2.i.6.3, %shr.i.6.3
  %and6.i.6.3 = and i64 %mul.i.6.3, 4294967295
  %shr7.i.6.3 = lshr i64 %mul.i.6.3, 32
  %and8.i.6.3 = and i64 %mul4.i.6.3, 4294967295
  %and9.i.6.3 = and i64 %mul3.i.6.3, 4294967295
  %add.i.6.3 = add nuw nsw i64 %shr7.i.6.3, %and8.i.6.3
  %add10.i.6.3 = add nuw nsw i64 %add.i.6.3, %and9.i.6.3
  %shr11.i.6.3 = lshr i64 %add10.i.6.3, 32
  %shl.i.6.3 = shl i64 %add10.i.6.3, 32
  %xor68.i.6.3 = or i64 %shl.i.6.3, %and6.i.6.3
  %shr13.i.6.3 = lshr i64 %mul4.i.6.3, 32
  %shr14.i.6.3 = lshr i64 %mul3.i.6.3, 32
  %and15.i.6.3 = and i64 %mul5.i.6.3, 4294967295
  %add16.i.6.3 = add nuw nsw i64 %shr13.i.6.3, %shr14.i.6.3
  %add17.i.6.3 = add nuw nsw i64 %add16.i.6.3, %and15.i.6.3
  %add18.i.6.3 = add nuw nsw i64 %add17.i.6.3, %shr11.i.6.3
  %and19.i.6.3 = and i64 %add18.i.6.3, 4294967295
  %and21.i.6.3 = and i64 %add18.i.6.3, 30064771072
  %and22.i.6.3 = and i64 %mul5.i.6.3, -4294967296
  %add23.i.6.3 = add i64 %and21.i.6.3, %and22.i.6.3
  %xor2569.i.6.3 = or i64 %add23.i.6.3, %and19.i.6.3
  %add16.6.3 = add i64 %xor68.i.6.3, %add16.6.2
  %xor.i315.6.3 = xor i64 %add16.6.3, %shl.i.6.3
  %xor1.i316.6.3 = xor i64 %shl.i.6.3, %add16.6.2
  %or.i317.6.3 = or i64 %xor.i315.6.3, %xor1.i316.6.3
  %xor2.i318.6.3 = xor i64 %or.i317.6.3, %add16.6.3
  %shr.i319.6.3 = lshr i64 %xor2.i318.6.3, 63
  %add20.6.3 = add i64 %shr.i319.6.3, %xor2569.i.6.3
  %add21.6.3 = add i64 %add20.6.3, %add21.6.2
  %41 = xor i64 %add20.6.3, -9223372036854775808
  %xor2.i312.6.3 = and i64 %41, %add23.i.6.3
  %xor.i303.6.3 = xor i64 %add21.6.3, %add20.6.3
  %xor1.i304.6.3 = xor i64 %add20.6.3, %add21.6.2
  %or.i305.6.3 = or i64 %xor.i303.6.3, %xor1.i304.6.3
  %xor2.i306.6.3 = xor i64 %or.i305.6.3, %add21.6.3
  %shr.i313328.6.3 = or i64 %xor2.i306.6.3, %xor2.i312.6.3
  %or25327.6.3 = lshr i64 %shr.i313328.6.3, 63
  %add27.6.3 = add nsw i64 %or25327.6.3, %add27.6.2
  %arrayidx34.6 = getelementptr inbounds i64, i64* %ma, i64 6
  %42 = load i64, i64* %arrayidx34.6, align 8, !tbaa !3
  %add35.6 = add i64 %42, %add16.6.3
  %xor.i297.6 = xor i64 %add35.6, %add16.6.3
  %xor1.i298.6 = xor i64 %42, %add16.6.3
  %or.i299.6 = or i64 %xor.i297.6, %xor1.i298.6
  %xor2.i300.6 = xor i64 %or.i299.6, %add35.6
  %shr.i301.6 = lshr i64 %xor2.i300.6, 63
  %add41.6 = add i64 %shr.i301.6, %add21.6.3
  %arrayidx50.6 = getelementptr inbounds i64, i64* %mc, i64 6
  store i64 %add35.6, i64* %arrayidx50.6, align 8, !tbaa !3
  %43 = xor i64 %add41.6, -9223372036854775808
  %xor2.i294.6 = and i64 %43, %add21.6.3
  %shr.i295.6 = lshr i64 %xor2.i294.6, 63
  %add48.6 = add i64 %shr.i295.6, %add27.6.3
  %44 = load i64, i64* %mc, align 8, !tbaa !3
  %45 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i.7 = and i64 %44, 4294967295
  %shr.i.7 = lshr i64 %44, 32
  %and1.i.7 = and i64 %45, 4294967295
  %shr2.i.7 = lshr i64 %45, 32
  %mul.i.7 = mul nuw i64 %and1.i.7, %and.i.7
  %mul3.i.7 = mul nuw i64 %shr2.i.7, %and.i.7
  %mul4.i.7 = mul nuw i64 %and1.i.7, %shr.i.7
  %mul5.i.7 = mul nuw i64 %shr2.i.7, %shr.i.7
  %and6.i.7 = and i64 %mul.i.7, 4294967295
  %shr7.i.7 = lshr i64 %mul.i.7, 32
  %and8.i.7 = and i64 %mul4.i.7, 4294967295
  %and9.i.7 = and i64 %mul3.i.7, 4294967295
  %add.i.7 = add nuw nsw i64 %shr7.i.7, %and8.i.7
  %add10.i.7 = add nuw nsw i64 %add.i.7, %and9.i.7
  %shr11.i.7 = lshr i64 %add10.i.7, 32
  %shl.i.7 = shl i64 %add10.i.7, 32
  %xor68.i.7 = or i64 %shl.i.7, %and6.i.7
  %shr13.i.7 = lshr i64 %mul4.i.7, 32
  %shr14.i.7 = lshr i64 %mul3.i.7, 32
  %and15.i.7 = and i64 %mul5.i.7, 4294967295
  %add16.i.7 = add nuw nsw i64 %shr13.i.7, %shr14.i.7
  %add17.i.7 = add nuw nsw i64 %add16.i.7, %and15.i.7
  %add18.i.7 = add nuw nsw i64 %add17.i.7, %shr11.i.7
  %and19.i.7 = and i64 %add18.i.7, 4294967295
  %and21.i.7 = and i64 %add18.i.7, 30064771072
  %and22.i.7 = and i64 %mul5.i.7, -4294967296
  %add23.i.7 = add i64 %and21.i.7, %and22.i.7
  %xor2569.i.7 = or i64 %add23.i.7, %and19.i.7
  %add16.7 = add i64 %xor68.i.7, %add41.6
  %xor.i315.7 = xor i64 %add16.7, %shl.i.7
  %xor1.i316.7 = xor i64 %shl.i.7, %add41.6
  %or.i317.7 = or i64 %xor.i315.7, %xor1.i316.7
  %xor2.i318.7 = xor i64 %or.i317.7, %add16.7
  %shr.i319.7 = lshr i64 %xor2.i318.7, 63
  %add20.7 = add i64 %shr.i319.7, %xor2569.i.7
  %add21.7 = add i64 %add20.7, %add48.6
  %46 = xor i64 %add20.7, -9223372036854775808
  %xor2.i312.7 = and i64 %46, %add23.i.7
  %xor.i303.7 = xor i64 %add21.7, %add20.7
  %xor1.i304.7 = xor i64 %add20.7, %add48.6
  %or.i305.7 = or i64 %xor.i303.7, %xor1.i304.7
  %xor2.i306.7 = xor i64 %or.i305.7, %add21.7
  %shr.i313328.7 = or i64 %xor2.i306.7, %xor2.i312.7
  %or25327.7 = lshr i64 %shr.i313328.7, 63
  %arrayidx9.7.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %47 = load i64, i64* %arrayidx9.7.1, align 8, !tbaa !3
  %48 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.7.1 = and i64 %47, 4294967295
  %shr.i.7.1 = lshr i64 %47, 32
  %and1.i.7.1 = and i64 %48, 4294967295
  %shr2.i.7.1 = lshr i64 %48, 32
  %mul.i.7.1 = mul nuw i64 %and1.i.7.1, %and.i.7.1
  %mul3.i.7.1 = mul nuw i64 %shr2.i.7.1, %and.i.7.1
  %mul4.i.7.1 = mul nuw i64 %and1.i.7.1, %shr.i.7.1
  %mul5.i.7.1 = mul nuw i64 %shr2.i.7.1, %shr.i.7.1
  %and6.i.7.1 = and i64 %mul.i.7.1, 4294967295
  %shr7.i.7.1 = lshr i64 %mul.i.7.1, 32
  %and8.i.7.1 = and i64 %mul4.i.7.1, 4294967295
  %and9.i.7.1 = and i64 %mul3.i.7.1, 4294967295
  %add.i.7.1 = add nuw nsw i64 %shr7.i.7.1, %and8.i.7.1
  %add10.i.7.1 = add nuw nsw i64 %add.i.7.1, %and9.i.7.1
  %shr11.i.7.1 = lshr i64 %add10.i.7.1, 32
  %shl.i.7.1 = shl i64 %add10.i.7.1, 32
  %xor68.i.7.1 = or i64 %shl.i.7.1, %and6.i.7.1
  %shr13.i.7.1 = lshr i64 %mul4.i.7.1, 32
  %shr14.i.7.1 = lshr i64 %mul3.i.7.1, 32
  %and15.i.7.1 = and i64 %mul5.i.7.1, 4294967295
  %add16.i.7.1 = add nuw nsw i64 %shr13.i.7.1, %shr14.i.7.1
  %add17.i.7.1 = add nuw nsw i64 %add16.i.7.1, %and15.i.7.1
  %add18.i.7.1 = add nuw nsw i64 %add17.i.7.1, %shr11.i.7.1
  %and19.i.7.1 = and i64 %add18.i.7.1, 4294967295
  %and21.i.7.1 = and i64 %add18.i.7.1, 30064771072
  %and22.i.7.1 = and i64 %mul5.i.7.1, -4294967296
  %add23.i.7.1 = add i64 %and21.i.7.1, %and22.i.7.1
  %xor2569.i.7.1 = or i64 %add23.i.7.1, %and19.i.7.1
  %add16.7.1 = add i64 %xor68.i.7.1, %add16.7
  %xor.i315.7.1 = xor i64 %add16.7.1, %shl.i.7.1
  %xor1.i316.7.1 = xor i64 %shl.i.7.1, %add16.7
  %or.i317.7.1 = or i64 %xor.i315.7.1, %xor1.i316.7.1
  %xor2.i318.7.1 = xor i64 %or.i317.7.1, %add16.7.1
  %shr.i319.7.1 = lshr i64 %xor2.i318.7.1, 63
  %add20.7.1 = add i64 %shr.i319.7.1, %xor2569.i.7.1
  %add21.7.1 = add i64 %add20.7.1, %add21.7
  %49 = xor i64 %add20.7.1, -9223372036854775808
  %xor2.i312.7.1 = and i64 %49, %add23.i.7.1
  %xor.i303.7.1 = xor i64 %add21.7.1, %add20.7.1
  %xor1.i304.7.1 = xor i64 %add20.7.1, %add21.7
  %or.i305.7.1 = or i64 %xor.i303.7.1, %xor1.i304.7.1
  %xor2.i306.7.1 = xor i64 %or.i305.7.1, %add21.7.1
  %shr.i313328.7.1 = or i64 %xor2.i306.7.1, %xor2.i312.7.1
  %or25327.7.1 = lshr i64 %shr.i313328.7.1, 63
  %add27.7.1 = add nuw nsw i64 %or25327.7.1, %or25327.7
  %arrayidx9.7.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %50 = load i64, i64* %arrayidx9.7.2, align 8, !tbaa !3
  %51 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.7.2 = and i64 %50, 4294967295
  %shr.i.7.2 = lshr i64 %50, 32
  %and1.i.7.2 = and i64 %51, 4294967295
  %shr2.i.7.2 = lshr i64 %51, 32
  %mul.i.7.2 = mul nuw i64 %and1.i.7.2, %and.i.7.2
  %mul3.i.7.2 = mul nuw i64 %shr2.i.7.2, %and.i.7.2
  %mul4.i.7.2 = mul nuw i64 %and1.i.7.2, %shr.i.7.2
  %mul5.i.7.2 = mul nuw i64 %shr2.i.7.2, %shr.i.7.2
  %and6.i.7.2 = and i64 %mul.i.7.2, 4294967295
  %shr7.i.7.2 = lshr i64 %mul.i.7.2, 32
  %and8.i.7.2 = and i64 %mul4.i.7.2, 4294967295
  %and9.i.7.2 = and i64 %mul3.i.7.2, 4294967295
  %add.i.7.2 = add nuw nsw i64 %shr7.i.7.2, %and8.i.7.2
  %add10.i.7.2 = add nuw nsw i64 %add.i.7.2, %and9.i.7.2
  %shr11.i.7.2 = lshr i64 %add10.i.7.2, 32
  %shl.i.7.2 = shl i64 %add10.i.7.2, 32
  %xor68.i.7.2 = or i64 %shl.i.7.2, %and6.i.7.2
  %shr13.i.7.2 = lshr i64 %mul4.i.7.2, 32
  %shr14.i.7.2 = lshr i64 %mul3.i.7.2, 32
  %and15.i.7.2 = and i64 %mul5.i.7.2, 4294967295
  %add16.i.7.2 = add nuw nsw i64 %shr13.i.7.2, %shr14.i.7.2
  %add17.i.7.2 = add nuw nsw i64 %add16.i.7.2, %and15.i.7.2
  %add18.i.7.2 = add nuw nsw i64 %add17.i.7.2, %shr11.i.7.2
  %and19.i.7.2 = and i64 %add18.i.7.2, 4294967295
  %and21.i.7.2 = and i64 %add18.i.7.2, 30064771072
  %and22.i.7.2 = and i64 %mul5.i.7.2, -4294967296
  %add23.i.7.2 = add i64 %and21.i.7.2, %and22.i.7.2
  %xor2569.i.7.2 = or i64 %add23.i.7.2, %and19.i.7.2
  %add16.7.2 = add i64 %xor68.i.7.2, %add16.7.1
  %xor.i315.7.2 = xor i64 %add16.7.2, %shl.i.7.2
  %xor1.i316.7.2 = xor i64 %shl.i.7.2, %add16.7.1
  %or.i317.7.2 = or i64 %xor.i315.7.2, %xor1.i316.7.2
  %xor2.i318.7.2 = xor i64 %or.i317.7.2, %add16.7.2
  %shr.i319.7.2 = lshr i64 %xor2.i318.7.2, 63
  %add20.7.2 = add i64 %shr.i319.7.2, %xor2569.i.7.2
  %add21.7.2 = add i64 %add20.7.2, %add21.7.1
  %52 = xor i64 %add20.7.2, -9223372036854775808
  %xor2.i312.7.2 = and i64 %52, %add23.i.7.2
  %xor.i303.7.2 = xor i64 %add21.7.2, %add20.7.2
  %xor1.i304.7.2 = xor i64 %add20.7.2, %add21.7.1
  %or.i305.7.2 = or i64 %xor.i303.7.2, %xor1.i304.7.2
  %xor2.i306.7.2 = xor i64 %or.i305.7.2, %add21.7.2
  %shr.i313328.7.2 = or i64 %xor2.i306.7.2, %xor2.i312.7.2
  %or25327.7.2 = lshr i64 %shr.i313328.7.2, 63
  %add27.7.2 = add nsw i64 %or25327.7.2, %add27.7.1
  %arrayidx9.7.3 = getelementptr inbounds i64, i64* %mc, i64 3
  %53 = load i64, i64* %arrayidx9.7.3, align 8, !tbaa !3
  %54 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i.7.3 = and i64 %53, 4294967295
  %shr.i.7.3 = lshr i64 %53, 32
  %and1.i.7.3 = and i64 %54, 4294967295
  %shr2.i.7.3 = lshr i64 %54, 32
  %mul.i.7.3 = mul nuw i64 %and1.i.7.3, %and.i.7.3
  %mul3.i.7.3 = mul nuw i64 %shr2.i.7.3, %and.i.7.3
  %mul4.i.7.3 = mul nuw i64 %and1.i.7.3, %shr.i.7.3
  %mul5.i.7.3 = mul nuw i64 %shr2.i.7.3, %shr.i.7.3
  %and6.i.7.3 = and i64 %mul.i.7.3, 4294967295
  %shr7.i.7.3 = lshr i64 %mul.i.7.3, 32
  %and8.i.7.3 = and i64 %mul4.i.7.3, 4294967295
  %and9.i.7.3 = and i64 %mul3.i.7.3, 4294967295
  %add.i.7.3 = add nuw nsw i64 %shr7.i.7.3, %and8.i.7.3
  %add10.i.7.3 = add nuw nsw i64 %add.i.7.3, %and9.i.7.3
  %shr11.i.7.3 = lshr i64 %add10.i.7.3, 32
  %shl.i.7.3 = shl i64 %add10.i.7.3, 32
  %xor68.i.7.3 = or i64 %shl.i.7.3, %and6.i.7.3
  %shr13.i.7.3 = lshr i64 %mul4.i.7.3, 32
  %shr14.i.7.3 = lshr i64 %mul3.i.7.3, 32
  %and15.i.7.3 = and i64 %mul5.i.7.3, 4294967295
  %add16.i.7.3 = add nuw nsw i64 %shr13.i.7.3, %shr14.i.7.3
  %add17.i.7.3 = add nuw nsw i64 %add16.i.7.3, %and15.i.7.3
  %add18.i.7.3 = add nuw nsw i64 %add17.i.7.3, %shr11.i.7.3
  %and19.i.7.3 = and i64 %add18.i.7.3, 4294967295
  %and21.i.7.3 = and i64 %add18.i.7.3, 30064771072
  %and22.i.7.3 = and i64 %mul5.i.7.3, -4294967296
  %add23.i.7.3 = add i64 %and21.i.7.3, %and22.i.7.3
  %xor2569.i.7.3 = or i64 %add23.i.7.3, %and19.i.7.3
  %add16.7.3 = add i64 %xor68.i.7.3, %add16.7.2
  %xor.i315.7.3 = xor i64 %add16.7.3, %shl.i.7.3
  %xor1.i316.7.3 = xor i64 %shl.i.7.3, %add16.7.2
  %or.i317.7.3 = or i64 %xor.i315.7.3, %xor1.i316.7.3
  %xor2.i318.7.3 = xor i64 %or.i317.7.3, %add16.7.3
  %shr.i319.7.3 = lshr i64 %xor2.i318.7.3, 63
  %add20.7.3 = add i64 %shr.i319.7.3, %xor2569.i.7.3
  %add21.7.3 = add i64 %add20.7.3, %add21.7.2
  %55 = xor i64 %add20.7.3, -9223372036854775808
  %xor2.i312.7.3 = and i64 %55, %add23.i.7.3
  %xor.i303.7.3 = xor i64 %add21.7.3, %add20.7.3
  %xor1.i304.7.3 = xor i64 %add20.7.3, %add21.7.2
  %or.i305.7.3 = or i64 %xor.i303.7.3, %xor1.i304.7.3
  %xor2.i306.7.3 = xor i64 %or.i305.7.3, %add21.7.3
  %shr.i313328.7.3 = or i64 %xor2.i306.7.3, %xor2.i312.7.3
  %or25327.7.3 = lshr i64 %shr.i313328.7.3, 63
  %add27.7.3 = add nsw i64 %or25327.7.3, %add27.7.2
  %arrayidx9.7.4 = getelementptr inbounds i64, i64* %mc, i64 4
  %56 = load i64, i64* %arrayidx9.7.4, align 8, !tbaa !3
  %57 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i.7.4 = and i64 %56, 4294967295
  %shr.i.7.4 = lshr i64 %56, 32
  %and1.i.7.4 = and i64 %57, 4294967295
  %shr2.i.7.4 = lshr i64 %57, 32
  %mul.i.7.4 = mul nuw i64 %and1.i.7.4, %and.i.7.4
  %mul3.i.7.4 = mul nuw i64 %shr2.i.7.4, %and.i.7.4
  %mul4.i.7.4 = mul nuw i64 %and1.i.7.4, %shr.i.7.4
  %mul5.i.7.4 = mul nuw i64 %shr2.i.7.4, %shr.i.7.4
  %and6.i.7.4 = and i64 %mul.i.7.4, 4294967295
  %shr7.i.7.4 = lshr i64 %mul.i.7.4, 32
  %and8.i.7.4 = and i64 %mul4.i.7.4, 4294967295
  %and9.i.7.4 = and i64 %mul3.i.7.4, 4294967295
  %add.i.7.4 = add nuw nsw i64 %shr7.i.7.4, %and8.i.7.4
  %add10.i.7.4 = add nuw nsw i64 %add.i.7.4, %and9.i.7.4
  %shr11.i.7.4 = lshr i64 %add10.i.7.4, 32
  %shl.i.7.4 = shl i64 %add10.i.7.4, 32
  %xor68.i.7.4 = or i64 %shl.i.7.4, %and6.i.7.4
  %shr13.i.7.4 = lshr i64 %mul4.i.7.4, 32
  %shr14.i.7.4 = lshr i64 %mul3.i.7.4, 32
  %and15.i.7.4 = and i64 %mul5.i.7.4, 4294967295
  %add16.i.7.4 = add nuw nsw i64 %shr13.i.7.4, %shr14.i.7.4
  %add17.i.7.4 = add nuw nsw i64 %add16.i.7.4, %and15.i.7.4
  %add18.i.7.4 = add nuw nsw i64 %add17.i.7.4, %shr11.i.7.4
  %and19.i.7.4 = and i64 %add18.i.7.4, 4294967295
  %and21.i.7.4 = and i64 %add18.i.7.4, 30064771072
  %and22.i.7.4 = and i64 %mul5.i.7.4, -4294967296
  %add23.i.7.4 = add i64 %and21.i.7.4, %and22.i.7.4
  %xor2569.i.7.4 = or i64 %add23.i.7.4, %and19.i.7.4
  %add16.7.4 = add i64 %xor68.i.7.4, %add16.7.3
  %xor.i315.7.4 = xor i64 %add16.7.4, %shl.i.7.4
  %xor1.i316.7.4 = xor i64 %shl.i.7.4, %add16.7.3
  %or.i317.7.4 = or i64 %xor.i315.7.4, %xor1.i316.7.4
  %xor2.i318.7.4 = xor i64 %or.i317.7.4, %add16.7.4
  %shr.i319.7.4 = lshr i64 %xor2.i318.7.4, 63
  %add20.7.4 = add i64 %shr.i319.7.4, %xor2569.i.7.4
  %add21.7.4 = add i64 %add20.7.4, %add21.7.3
  %58 = xor i64 %add20.7.4, -9223372036854775808
  %xor2.i312.7.4 = and i64 %58, %add23.i.7.4
  %xor.i303.7.4 = xor i64 %add21.7.4, %add20.7.4
  %xor1.i304.7.4 = xor i64 %add20.7.4, %add21.7.3
  %or.i305.7.4 = or i64 %xor.i303.7.4, %xor1.i304.7.4
  %xor2.i306.7.4 = xor i64 %or.i305.7.4, %add21.7.4
  %shr.i313328.7.4 = or i64 %xor2.i306.7.4, %xor2.i312.7.4
  %or25327.7.4 = lshr i64 %shr.i313328.7.4, 63
  %add27.7.4 = add i64 %or25327.7.4, %add27.7.3
  %arrayidx34.7 = getelementptr inbounds i64, i64* %ma, i64 7
  %59 = load i64, i64* %arrayidx34.7, align 8, !tbaa !3
  %add35.7 = add i64 %59, %add16.7.4
  %xor.i297.7 = xor i64 %add35.7, %add16.7.4
  %xor1.i298.7 = xor i64 %59, %add16.7.4
  %or.i299.7 = or i64 %xor.i297.7, %xor1.i298.7
  %xor2.i300.7 = xor i64 %or.i299.7, %add35.7
  %shr.i301.7 = lshr i64 %xor2.i300.7, 63
  %add41.7 = add i64 %shr.i301.7, %add21.7.4
  %arrayidx50.7 = getelementptr inbounds i64, i64* %mc, i64 7
  store i64 %add35.7, i64* %arrayidx50.7, align 8, !tbaa !3
  %60 = xor i64 %add41.7, -9223372036854775808
  %xor2.i294.7 = and i64 %60, %add21.7.4
  %shr.i295.7 = lshr i64 %xor2.i294.7, 63
  %add48.7 = add i64 %shr.i295.7, %add27.7.4
  %arrayidx74 = getelementptr inbounds i64, i64* %mc, i64 1
  %61 = load i64, i64* %arrayidx74, align 8, !tbaa !3
  %62 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262 = and i64 %61, 4294967295
  %shr.i263 = lshr i64 %61, 32
  %and1.i264 = and i64 %62, 4294967295
  %shr2.i265 = lshr i64 %62, 32
  %mul.i266 = mul nuw i64 %and1.i264, %and.i262
  %mul3.i267 = mul nuw i64 %shr2.i265, %and.i262
  %mul4.i268 = mul nuw i64 %and1.i264, %shr.i263
  %mul5.i269 = mul nuw i64 %shr2.i265, %shr.i263
  %and6.i270 = and i64 %mul.i266, 4294967295
  %shr7.i271 = lshr i64 %mul.i266, 32
  %and8.i272 = and i64 %mul4.i268, 4294967295
  %and9.i273 = and i64 %mul3.i267, 4294967295
  %add.i274 = add nuw nsw i64 %shr7.i271, %and8.i272
  %add10.i275 = add nuw nsw i64 %add.i274, %and9.i273
  %shr11.i276 = lshr i64 %add10.i275, 32
  %shl.i277 = shl i64 %add10.i275, 32
  %xor68.i278 = or i64 %shl.i277, %and6.i270
  %shr13.i279 = lshr i64 %mul4.i268, 32
  %shr14.i280 = lshr i64 %mul3.i267, 32
  %and15.i281 = and i64 %mul5.i269, 4294967295
  %add16.i282 = add nuw nsw i64 %shr13.i279, %shr14.i280
  %add17.i283 = add nuw nsw i64 %add16.i282, %and15.i281
  %add18.i284 = add nuw nsw i64 %add17.i283, %shr11.i276
  %and19.i285 = and i64 %add18.i284, 4294967295
  %and21.i287 = and i64 %add18.i284, 30064771072
  %and22.i288 = and i64 %mul5.i269, -4294967296
  %add23.i289 = add i64 %and21.i287, %and22.i288
  %xor2569.i290 = or i64 %add23.i289, %and19.i285
  %add82 = add i64 %xor68.i278, %add41.7
  %xor.i256 = xor i64 %add82, %shl.i277
  %xor1.i257 = xor i64 %shl.i277, %add41.7
  %or.i258 = or i64 %xor.i256, %xor1.i257
  %xor2.i259 = xor i64 %or.i258, %add82
  %shr.i260 = lshr i64 %xor2.i259, 63
  %add89 = add i64 %shr.i260, %xor2569.i290
  %add90 = add i64 %add89, %add48.7
  %63 = xor i64 %add89, -9223372036854775808
  %xor2.i253 = and i64 %63, %add23.i289
  %xor.i244 = xor i64 %add90, %add89
  %xor1.i245 = xor i64 %add89, %add48.7
  %or.i246 = or i64 %xor.i244, %xor1.i245
  %xor2.i247 = xor i64 %or.i246, %add90
  %shr.i254326 = or i64 %xor2.i247, %xor2.i253
  %or94325 = lshr i64 %shr.i254326, 63
  %arrayidx74.1372 = getelementptr inbounds i64, i64* %mc, i64 2
  %64 = load i64, i64* %arrayidx74.1372, align 8, !tbaa !3
  %65 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.1373 = and i64 %64, 4294967295
  %shr.i263.1374 = lshr i64 %64, 32
  %and1.i264.1375 = and i64 %65, 4294967295
  %shr2.i265.1376 = lshr i64 %65, 32
  %mul.i266.1377 = mul nuw i64 %and1.i264.1375, %and.i262.1373
  %mul3.i267.1378 = mul nuw i64 %shr2.i265.1376, %and.i262.1373
  %mul4.i268.1379 = mul nuw i64 %and1.i264.1375, %shr.i263.1374
  %mul5.i269.1380 = mul nuw i64 %shr2.i265.1376, %shr.i263.1374
  %and6.i270.1381 = and i64 %mul.i266.1377, 4294967295
  %shr7.i271.1382 = lshr i64 %mul.i266.1377, 32
  %and8.i272.1383 = and i64 %mul4.i268.1379, 4294967295
  %and9.i273.1384 = and i64 %mul3.i267.1378, 4294967295
  %add.i274.1385 = add nuw nsw i64 %shr7.i271.1382, %and8.i272.1383
  %add10.i275.1386 = add nuw nsw i64 %add.i274.1385, %and9.i273.1384
  %shr11.i276.1387 = lshr i64 %add10.i275.1386, 32
  %shl.i277.1388 = shl i64 %add10.i275.1386, 32
  %xor68.i278.1389 = or i64 %shl.i277.1388, %and6.i270.1381
  %shr13.i279.1390 = lshr i64 %mul4.i268.1379, 32
  %shr14.i280.1391 = lshr i64 %mul3.i267.1378, 32
  %and15.i281.1392 = and i64 %mul5.i269.1380, 4294967295
  %add16.i282.1393 = add nuw nsw i64 %shr13.i279.1390, %shr14.i280.1391
  %add17.i283.1394 = add nuw nsw i64 %add16.i282.1393, %and15.i281.1392
  %add18.i284.1395 = add nuw nsw i64 %add17.i283.1394, %shr11.i276.1387
  %and19.i285.1396 = and i64 %add18.i284.1395, 4294967295
  %and21.i287.1397 = and i64 %add18.i284.1395, 30064771072
  %and22.i288.1398 = and i64 %mul5.i269.1380, -4294967296
  %add23.i289.1399 = add i64 %and21.i287.1397, %and22.i288.1398
  %xor2569.i290.1400 = or i64 %add23.i289.1399, %and19.i285.1396
  %add82.1401 = add i64 %xor68.i278.1389, %add82
  %xor.i256.1402 = xor i64 %add82.1401, %shl.i277.1388
  %xor1.i257.1403 = xor i64 %shl.i277.1388, %add82
  %or.i258.1404 = or i64 %xor.i256.1402, %xor1.i257.1403
  %xor2.i259.1405 = xor i64 %or.i258.1404, %add82.1401
  %shr.i260.1406 = lshr i64 %xor2.i259.1405, 63
  %add89.1407 = add i64 %shr.i260.1406, %xor2569.i290.1400
  %add90.1408 = add i64 %add89.1407, %add90
  %66 = xor i64 %add89.1407, -9223372036854775808
  %xor2.i253.1409 = and i64 %66, %add23.i289.1399
  %xor.i244.1410 = xor i64 %add90.1408, %add89.1407
  %xor1.i245.1411 = xor i64 %add89.1407, %add90
  %or.i246.1412 = or i64 %xor.i244.1410, %xor1.i245.1411
  %xor2.i247.1413 = xor i64 %or.i246.1412, %add90.1408
  %shr.i254326.1414 = or i64 %xor2.i247.1413, %xor2.i253.1409
  %or94325.1415 = lshr i64 %shr.i254326.1414, 63
  %add96.1 = add nuw nsw i64 %or94325.1415, %or94325
  %arrayidx74.2422 = getelementptr inbounds i64, i64* %mc, i64 3
  %67 = load i64, i64* %arrayidx74.2422, align 8, !tbaa !3
  %68 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.2423 = and i64 %67, 4294967295
  %shr.i263.2424 = lshr i64 %67, 32
  %and1.i264.2425 = and i64 %68, 4294967295
  %shr2.i265.2426 = lshr i64 %68, 32
  %mul.i266.2427 = mul nuw i64 %and1.i264.2425, %and.i262.2423
  %mul3.i267.2428 = mul nuw i64 %shr2.i265.2426, %and.i262.2423
  %mul4.i268.2429 = mul nuw i64 %and1.i264.2425, %shr.i263.2424
  %mul5.i269.2430 = mul nuw i64 %shr2.i265.2426, %shr.i263.2424
  %and6.i270.2431 = and i64 %mul.i266.2427, 4294967295
  %shr7.i271.2432 = lshr i64 %mul.i266.2427, 32
  %and8.i272.2433 = and i64 %mul4.i268.2429, 4294967295
  %and9.i273.2434 = and i64 %mul3.i267.2428, 4294967295
  %add.i274.2435 = add nuw nsw i64 %shr7.i271.2432, %and8.i272.2433
  %add10.i275.2436 = add nuw nsw i64 %add.i274.2435, %and9.i273.2434
  %shr11.i276.2437 = lshr i64 %add10.i275.2436, 32
  %shl.i277.2438 = shl i64 %add10.i275.2436, 32
  %xor68.i278.2439 = or i64 %shl.i277.2438, %and6.i270.2431
  %shr13.i279.2440 = lshr i64 %mul4.i268.2429, 32
  %shr14.i280.2441 = lshr i64 %mul3.i267.2428, 32
  %and15.i281.2442 = and i64 %mul5.i269.2430, 4294967295
  %add16.i282.2443 = add nuw nsw i64 %shr13.i279.2440, %shr14.i280.2441
  %add17.i283.2444 = add nuw nsw i64 %add16.i282.2443, %and15.i281.2442
  %add18.i284.2445 = add nuw nsw i64 %add17.i283.2444, %shr11.i276.2437
  %and19.i285.2446 = and i64 %add18.i284.2445, 4294967295
  %and21.i287.2447 = and i64 %add18.i284.2445, 30064771072
  %and22.i288.2448 = and i64 %mul5.i269.2430, -4294967296
  %add23.i289.2449 = add i64 %and21.i287.2447, %and22.i288.2448
  %xor2569.i290.2450 = or i64 %add23.i289.2449, %and19.i285.2446
  %add82.2451 = add i64 %xor68.i278.2439, %add82.1401
  %xor.i256.2452 = xor i64 %add82.2451, %shl.i277.2438
  %xor1.i257.2453 = xor i64 %shl.i277.2438, %add82.1401
  %or.i258.2454 = or i64 %xor.i256.2452, %xor1.i257.2453
  %xor2.i259.2455 = xor i64 %or.i258.2454, %add82.2451
  %shr.i260.2456 = lshr i64 %xor2.i259.2455, 63
  %add89.2457 = add i64 %shr.i260.2456, %xor2569.i290.2450
  %add90.2458 = add i64 %add89.2457, %add90.1408
  %69 = xor i64 %add89.2457, -9223372036854775808
  %xor2.i253.2459 = and i64 %69, %add23.i289.2449
  %xor.i244.2460 = xor i64 %add90.2458, %add89.2457
  %xor1.i245.2461 = xor i64 %add89.2457, %add90.1408
  %or.i246.2462 = or i64 %xor.i244.2460, %xor1.i245.2461
  %xor2.i247.2463 = xor i64 %or.i246.2462, %add90.2458
  %shr.i254326.2464 = or i64 %xor2.i247.2463, %xor2.i253.2459
  %or94325.2465 = lshr i64 %shr.i254326.2464, 63
  %add96.2 = add nsw i64 %or94325.2465, %add96.1
  %arrayidx74.3472 = getelementptr inbounds i64, i64* %mc, i64 4
  %70 = load i64, i64* %arrayidx74.3472, align 8, !tbaa !3
  %71 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i262.3473 = and i64 %70, 4294967295
  %shr.i263.3474 = lshr i64 %70, 32
  %and1.i264.3475 = and i64 %71, 4294967295
  %shr2.i265.3476 = lshr i64 %71, 32
  %mul.i266.3477 = mul nuw i64 %and1.i264.3475, %and.i262.3473
  %mul3.i267.3478 = mul nuw i64 %shr2.i265.3476, %and.i262.3473
  %mul4.i268.3479 = mul nuw i64 %and1.i264.3475, %shr.i263.3474
  %mul5.i269.3480 = mul nuw i64 %shr2.i265.3476, %shr.i263.3474
  %and6.i270.3481 = and i64 %mul.i266.3477, 4294967295
  %shr7.i271.3482 = lshr i64 %mul.i266.3477, 32
  %and8.i272.3483 = and i64 %mul4.i268.3479, 4294967295
  %and9.i273.3484 = and i64 %mul3.i267.3478, 4294967295
  %add.i274.3485 = add nuw nsw i64 %shr7.i271.3482, %and8.i272.3483
  %add10.i275.3486 = add nuw nsw i64 %add.i274.3485, %and9.i273.3484
  %shr11.i276.3487 = lshr i64 %add10.i275.3486, 32
  %shl.i277.3488 = shl i64 %add10.i275.3486, 32
  %xor68.i278.3489 = or i64 %shl.i277.3488, %and6.i270.3481
  %shr13.i279.3490 = lshr i64 %mul4.i268.3479, 32
  %shr14.i280.3491 = lshr i64 %mul3.i267.3478, 32
  %and15.i281.3492 = and i64 %mul5.i269.3480, 4294967295
  %add16.i282.3493 = add nuw nsw i64 %shr13.i279.3490, %shr14.i280.3491
  %add17.i283.3494 = add nuw nsw i64 %add16.i282.3493, %and15.i281.3492
  %add18.i284.3495 = add nuw nsw i64 %add17.i283.3494, %shr11.i276.3487
  %and19.i285.3496 = and i64 %add18.i284.3495, 4294967295
  %and21.i287.3497 = and i64 %add18.i284.3495, 30064771072
  %and22.i288.3498 = and i64 %mul5.i269.3480, -4294967296
  %add23.i289.3499 = add i64 %and21.i287.3497, %and22.i288.3498
  %xor2569.i290.3500 = or i64 %add23.i289.3499, %and19.i285.3496
  %add82.3501 = add i64 %xor68.i278.3489, %add82.2451
  %xor.i256.3502 = xor i64 %add82.3501, %shl.i277.3488
  %xor1.i257.3503 = xor i64 %shl.i277.3488, %add82.2451
  %or.i258.3504 = or i64 %xor.i256.3502, %xor1.i257.3503
  %xor2.i259.3505 = xor i64 %or.i258.3504, %add82.3501
  %shr.i260.3506 = lshr i64 %xor2.i259.3505, 63
  %add89.3507 = add i64 %shr.i260.3506, %xor2569.i290.3500
  %add90.3508 = add i64 %add89.3507, %add90.2458
  %72 = xor i64 %add89.3507, -9223372036854775808
  %xor2.i253.3509 = and i64 %72, %add23.i289.3499
  %xor.i244.3510 = xor i64 %add90.3508, %add89.3507
  %xor1.i245.3511 = xor i64 %add89.3507, %add90.2458
  %or.i246.3512 = or i64 %xor.i244.3510, %xor1.i245.3511
  %xor2.i247.3513 = xor i64 %or.i246.3512, %add90.3508
  %shr.i254326.3514 = or i64 %xor2.i247.3513, %xor2.i253.3509
  %or94325.3515 = lshr i64 %shr.i254326.3514, 63
  %add96.3 = add nsw i64 %or94325.3515, %add96.2
  %arrayidx74.4522 = getelementptr inbounds i64, i64* %mc, i64 5
  %73 = load i64, i64* %arrayidx74.4522, align 8, !tbaa !3
  %74 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i262.4523 = and i64 %73, 4294967295
  %shr.i263.4524 = lshr i64 %73, 32
  %and1.i264.4525 = and i64 %74, 4294967295
  %shr2.i265.4526 = lshr i64 %74, 32
  %mul.i266.4527 = mul nuw i64 %and1.i264.4525, %and.i262.4523
  %mul3.i267.4528 = mul nuw i64 %shr2.i265.4526, %and.i262.4523
  %mul4.i268.4529 = mul nuw i64 %and1.i264.4525, %shr.i263.4524
  %mul5.i269.4530 = mul nuw i64 %shr2.i265.4526, %shr.i263.4524
  %and6.i270.4531 = and i64 %mul.i266.4527, 4294967295
  %shr7.i271.4532 = lshr i64 %mul.i266.4527, 32
  %and8.i272.4533 = and i64 %mul4.i268.4529, 4294967295
  %and9.i273.4534 = and i64 %mul3.i267.4528, 4294967295
  %add.i274.4535 = add nuw nsw i64 %shr7.i271.4532, %and8.i272.4533
  %add10.i275.4536 = add nuw nsw i64 %add.i274.4535, %and9.i273.4534
  %shr11.i276.4537 = lshr i64 %add10.i275.4536, 32
  %shl.i277.4538 = shl i64 %add10.i275.4536, 32
  %xor68.i278.4539 = or i64 %shl.i277.4538, %and6.i270.4531
  %shr13.i279.4540 = lshr i64 %mul4.i268.4529, 32
  %shr14.i280.4541 = lshr i64 %mul3.i267.4528, 32
  %and15.i281.4542 = and i64 %mul5.i269.4530, 4294967295
  %add16.i282.4543 = add nuw nsw i64 %shr13.i279.4540, %shr14.i280.4541
  %add17.i283.4544 = add nuw nsw i64 %add16.i282.4543, %and15.i281.4542
  %add18.i284.4545 = add nuw nsw i64 %add17.i283.4544, %shr11.i276.4537
  %and19.i285.4546 = and i64 %add18.i284.4545, 4294967295
  %and21.i287.4547 = and i64 %add18.i284.4545, 30064771072
  %and22.i288.4548 = and i64 %mul5.i269.4530, -4294967296
  %add23.i289.4549 = add i64 %and21.i287.4547, %and22.i288.4548
  %xor2569.i290.4550 = or i64 %add23.i289.4549, %and19.i285.4546
  %add82.4551 = add i64 %xor68.i278.4539, %add82.3501
  %xor.i256.4552 = xor i64 %add82.4551, %shl.i277.4538
  %xor1.i257.4553 = xor i64 %shl.i277.4538, %add82.3501
  %or.i258.4554 = or i64 %xor.i256.4552, %xor1.i257.4553
  %xor2.i259.4555 = xor i64 %or.i258.4554, %add82.4551
  %shr.i260.4556 = lshr i64 %xor2.i259.4555, 63
  %add89.4557 = add i64 %shr.i260.4556, %xor2569.i290.4550
  %add90.4558 = add i64 %add89.4557, %add90.3508
  %75 = xor i64 %add89.4557, -9223372036854775808
  %xor2.i253.4559 = and i64 %75, %add23.i289.4549
  %xor.i244.4560 = xor i64 %add90.4558, %add89.4557
  %xor1.i245.4561 = xor i64 %add89.4557, %add90.3508
  %or.i246.4562 = or i64 %xor.i244.4560, %xor1.i245.4561
  %xor2.i247.4563 = xor i64 %or.i246.4562, %add90.4558
  %shr.i254326.4564 = or i64 %xor2.i247.4563, %xor2.i253.4559
  %or94325.4565 = lshr i64 %shr.i254326.4564, 63
  %add96.4 = add i64 %or94325.4565, %add96.3
  %arrayidx104 = getelementptr inbounds i64, i64* %ma, i64 8
  %76 = load i64, i64* %arrayidx104, align 8, !tbaa !3
  %add105 = add i64 %76, %add82.4551
  %xor.i238 = xor i64 %add105, %add82.4551
  %xor1.i239 = xor i64 %76, %add82.4551
  %or.i240 = or i64 %xor.i238, %xor1.i239
  %xor2.i241 = xor i64 %or.i240, %add105
  %shr.i242 = lshr i64 %xor2.i241, 63
  %add111 = add i64 %shr.i242, %add90.4558
  store i64 %add105, i64* %mc, align 8, !tbaa !3
  %77 = xor i64 %add111, -9223372036854775808
  %xor2.i = and i64 %77, %add90.4558
  %shr.i237 = lshr i64 %xor2.i, 63
  %add118 = add i64 %shr.i237, %add96.4
  %arrayidx74.1 = getelementptr inbounds i64, i64* %mc, i64 2
  %78 = load i64, i64* %arrayidx74.1, align 8, !tbaa !3
  %79 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.1 = and i64 %78, 4294967295
  %shr.i263.1 = lshr i64 %78, 32
  %and1.i264.1 = and i64 %79, 4294967295
  %shr2.i265.1 = lshr i64 %79, 32
  %mul.i266.1 = mul nuw i64 %and1.i264.1, %and.i262.1
  %mul3.i267.1 = mul nuw i64 %shr2.i265.1, %and.i262.1
  %mul4.i268.1 = mul nuw i64 %and1.i264.1, %shr.i263.1
  %mul5.i269.1 = mul nuw i64 %shr2.i265.1, %shr.i263.1
  %and6.i270.1 = and i64 %mul.i266.1, 4294967295
  %shr7.i271.1 = lshr i64 %mul.i266.1, 32
  %and8.i272.1 = and i64 %mul4.i268.1, 4294967295
  %and9.i273.1 = and i64 %mul3.i267.1, 4294967295
  %add.i274.1 = add nuw nsw i64 %shr7.i271.1, %and8.i272.1
  %add10.i275.1 = add nuw nsw i64 %add.i274.1, %and9.i273.1
  %shr11.i276.1 = lshr i64 %add10.i275.1, 32
  %shl.i277.1 = shl i64 %add10.i275.1, 32
  %xor68.i278.1 = or i64 %shl.i277.1, %and6.i270.1
  %shr13.i279.1 = lshr i64 %mul4.i268.1, 32
  %shr14.i280.1 = lshr i64 %mul3.i267.1, 32
  %and15.i281.1 = and i64 %mul5.i269.1, 4294967295
  %add16.i282.1 = add nuw nsw i64 %shr13.i279.1, %shr14.i280.1
  %add17.i283.1 = add nuw nsw i64 %add16.i282.1, %and15.i281.1
  %add18.i284.1 = add nuw nsw i64 %add17.i283.1, %shr11.i276.1
  %and19.i285.1 = and i64 %add18.i284.1, 4294967295
  %and21.i287.1 = and i64 %add18.i284.1, 30064771072
  %and22.i288.1 = and i64 %mul5.i269.1, -4294967296
  %add23.i289.1 = add i64 %and21.i287.1, %and22.i288.1
  %xor2569.i290.1 = or i64 %add23.i289.1, %and19.i285.1
  %add82.1 = add i64 %xor68.i278.1, %add111
  %xor.i256.1 = xor i64 %add82.1, %shl.i277.1
  %xor1.i257.1 = xor i64 %shl.i277.1, %add111
  %or.i258.1 = or i64 %xor.i256.1, %xor1.i257.1
  %xor2.i259.1 = xor i64 %or.i258.1, %add82.1
  %shr.i260.1 = lshr i64 %xor2.i259.1, 63
  %add89.1 = add i64 %shr.i260.1, %xor2569.i290.1
  %add90.1 = add i64 %add89.1, %add118
  %80 = xor i64 %add89.1, -9223372036854775808
  %xor2.i253.1 = and i64 %80, %add23.i289.1
  %xor.i244.1 = xor i64 %add90.1, %add89.1
  %xor1.i245.1 = xor i64 %add89.1, %add118
  %or.i246.1 = or i64 %xor.i244.1, %xor1.i245.1
  %xor2.i247.1 = xor i64 %or.i246.1, %add90.1
  %shr.i254326.1 = or i64 %xor2.i247.1, %xor2.i253.1
  %or94325.1 = lshr i64 %shr.i254326.1, 63
  %arrayidx74.1.1 = getelementptr inbounds i64, i64* %mc, i64 3
  %81 = load i64, i64* %arrayidx74.1.1, align 8, !tbaa !3
  %82 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.1.1 = and i64 %81, 4294967295
  %shr.i263.1.1 = lshr i64 %81, 32
  %and1.i264.1.1 = and i64 %82, 4294967295
  %shr2.i265.1.1 = lshr i64 %82, 32
  %mul.i266.1.1 = mul nuw i64 %and1.i264.1.1, %and.i262.1.1
  %mul3.i267.1.1 = mul nuw i64 %shr2.i265.1.1, %and.i262.1.1
  %mul4.i268.1.1 = mul nuw i64 %and1.i264.1.1, %shr.i263.1.1
  %mul5.i269.1.1 = mul nuw i64 %shr2.i265.1.1, %shr.i263.1.1
  %and6.i270.1.1 = and i64 %mul.i266.1.1, 4294967295
  %shr7.i271.1.1 = lshr i64 %mul.i266.1.1, 32
  %and8.i272.1.1 = and i64 %mul4.i268.1.1, 4294967295
  %and9.i273.1.1 = and i64 %mul3.i267.1.1, 4294967295
  %add.i274.1.1 = add nuw nsw i64 %shr7.i271.1.1, %and8.i272.1.1
  %add10.i275.1.1 = add nuw nsw i64 %add.i274.1.1, %and9.i273.1.1
  %shr11.i276.1.1 = lshr i64 %add10.i275.1.1, 32
  %shl.i277.1.1 = shl i64 %add10.i275.1.1, 32
  %xor68.i278.1.1 = or i64 %shl.i277.1.1, %and6.i270.1.1
  %shr13.i279.1.1 = lshr i64 %mul4.i268.1.1, 32
  %shr14.i280.1.1 = lshr i64 %mul3.i267.1.1, 32
  %and15.i281.1.1 = and i64 %mul5.i269.1.1, 4294967295
  %add16.i282.1.1 = add nuw nsw i64 %shr13.i279.1.1, %shr14.i280.1.1
  %add17.i283.1.1 = add nuw nsw i64 %add16.i282.1.1, %and15.i281.1.1
  %add18.i284.1.1 = add nuw nsw i64 %add17.i283.1.1, %shr11.i276.1.1
  %and19.i285.1.1 = and i64 %add18.i284.1.1, 4294967295
  %and21.i287.1.1 = and i64 %add18.i284.1.1, 30064771072
  %and22.i288.1.1 = and i64 %mul5.i269.1.1, -4294967296
  %add23.i289.1.1 = add i64 %and21.i287.1.1, %and22.i288.1.1
  %xor2569.i290.1.1 = or i64 %add23.i289.1.1, %and19.i285.1.1
  %add82.1.1 = add i64 %xor68.i278.1.1, %add82.1
  %xor.i256.1.1 = xor i64 %add82.1.1, %shl.i277.1.1
  %xor1.i257.1.1 = xor i64 %shl.i277.1.1, %add82.1
  %or.i258.1.1 = or i64 %xor.i256.1.1, %xor1.i257.1.1
  %xor2.i259.1.1 = xor i64 %or.i258.1.1, %add82.1.1
  %shr.i260.1.1 = lshr i64 %xor2.i259.1.1, 63
  %add89.1.1 = add i64 %shr.i260.1.1, %xor2569.i290.1.1
  %add90.1.1 = add i64 %add89.1.1, %add90.1
  %83 = xor i64 %add89.1.1, -9223372036854775808
  %xor2.i253.1.1 = and i64 %83, %add23.i289.1.1
  %xor.i244.1.1 = xor i64 %add90.1.1, %add89.1.1
  %xor1.i245.1.1 = xor i64 %add89.1.1, %add90.1
  %or.i246.1.1 = or i64 %xor.i244.1.1, %xor1.i245.1.1
  %xor2.i247.1.1 = xor i64 %or.i246.1.1, %add90.1.1
  %shr.i254326.1.1 = or i64 %xor2.i247.1.1, %xor2.i253.1.1
  %or94325.1.1 = lshr i64 %shr.i254326.1.1, 63
  %add96.1.1 = add nuw nsw i64 %or94325.1.1, %or94325.1
  %arrayidx74.1.2 = getelementptr inbounds i64, i64* %mc, i64 4
  %84 = load i64, i64* %arrayidx74.1.2, align 8, !tbaa !3
  %85 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.1.2 = and i64 %84, 4294967295
  %shr.i263.1.2 = lshr i64 %84, 32
  %and1.i264.1.2 = and i64 %85, 4294967295
  %shr2.i265.1.2 = lshr i64 %85, 32
  %mul.i266.1.2 = mul nuw i64 %and1.i264.1.2, %and.i262.1.2
  %mul3.i267.1.2 = mul nuw i64 %shr2.i265.1.2, %and.i262.1.2
  %mul4.i268.1.2 = mul nuw i64 %and1.i264.1.2, %shr.i263.1.2
  %mul5.i269.1.2 = mul nuw i64 %shr2.i265.1.2, %shr.i263.1.2
  %and6.i270.1.2 = and i64 %mul.i266.1.2, 4294967295
  %shr7.i271.1.2 = lshr i64 %mul.i266.1.2, 32
  %and8.i272.1.2 = and i64 %mul4.i268.1.2, 4294967295
  %and9.i273.1.2 = and i64 %mul3.i267.1.2, 4294967295
  %add.i274.1.2 = add nuw nsw i64 %shr7.i271.1.2, %and8.i272.1.2
  %add10.i275.1.2 = add nuw nsw i64 %add.i274.1.2, %and9.i273.1.2
  %shr11.i276.1.2 = lshr i64 %add10.i275.1.2, 32
  %shl.i277.1.2 = shl i64 %add10.i275.1.2, 32
  %xor68.i278.1.2 = or i64 %shl.i277.1.2, %and6.i270.1.2
  %shr13.i279.1.2 = lshr i64 %mul4.i268.1.2, 32
  %shr14.i280.1.2 = lshr i64 %mul3.i267.1.2, 32
  %and15.i281.1.2 = and i64 %mul5.i269.1.2, 4294967295
  %add16.i282.1.2 = add nuw nsw i64 %shr13.i279.1.2, %shr14.i280.1.2
  %add17.i283.1.2 = add nuw nsw i64 %add16.i282.1.2, %and15.i281.1.2
  %add18.i284.1.2 = add nuw nsw i64 %add17.i283.1.2, %shr11.i276.1.2
  %and19.i285.1.2 = and i64 %add18.i284.1.2, 4294967295
  %and21.i287.1.2 = and i64 %add18.i284.1.2, 30064771072
  %and22.i288.1.2 = and i64 %mul5.i269.1.2, -4294967296
  %add23.i289.1.2 = add i64 %and21.i287.1.2, %and22.i288.1.2
  %xor2569.i290.1.2 = or i64 %add23.i289.1.2, %and19.i285.1.2
  %add82.1.2 = add i64 %xor68.i278.1.2, %add82.1.1
  %xor.i256.1.2 = xor i64 %add82.1.2, %shl.i277.1.2
  %xor1.i257.1.2 = xor i64 %shl.i277.1.2, %add82.1.1
  %or.i258.1.2 = or i64 %xor.i256.1.2, %xor1.i257.1.2
  %xor2.i259.1.2 = xor i64 %or.i258.1.2, %add82.1.2
  %shr.i260.1.2 = lshr i64 %xor2.i259.1.2, 63
  %add89.1.2 = add i64 %shr.i260.1.2, %xor2569.i290.1.2
  %add90.1.2 = add i64 %add89.1.2, %add90.1.1
  %86 = xor i64 %add89.1.2, -9223372036854775808
  %xor2.i253.1.2 = and i64 %86, %add23.i289.1.2
  %xor.i244.1.2 = xor i64 %add90.1.2, %add89.1.2
  %xor1.i245.1.2 = xor i64 %add89.1.2, %add90.1.1
  %or.i246.1.2 = or i64 %xor.i244.1.2, %xor1.i245.1.2
  %xor2.i247.1.2 = xor i64 %or.i246.1.2, %add90.1.2
  %shr.i254326.1.2 = or i64 %xor2.i247.1.2, %xor2.i253.1.2
  %or94325.1.2 = lshr i64 %shr.i254326.1.2, 63
  %add96.1.2 = add nsw i64 %or94325.1.2, %add96.1.1
  %arrayidx74.1.3 = getelementptr inbounds i64, i64* %mc, i64 5
  %87 = load i64, i64* %arrayidx74.1.3, align 8, !tbaa !3
  %88 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i262.1.3 = and i64 %87, 4294967295
  %shr.i263.1.3 = lshr i64 %87, 32
  %and1.i264.1.3 = and i64 %88, 4294967295
  %shr2.i265.1.3 = lshr i64 %88, 32
  %mul.i266.1.3 = mul nuw i64 %and1.i264.1.3, %and.i262.1.3
  %mul3.i267.1.3 = mul nuw i64 %shr2.i265.1.3, %and.i262.1.3
  %mul4.i268.1.3 = mul nuw i64 %and1.i264.1.3, %shr.i263.1.3
  %mul5.i269.1.3 = mul nuw i64 %shr2.i265.1.3, %shr.i263.1.3
  %and6.i270.1.3 = and i64 %mul.i266.1.3, 4294967295
  %shr7.i271.1.3 = lshr i64 %mul.i266.1.3, 32
  %and8.i272.1.3 = and i64 %mul4.i268.1.3, 4294967295
  %and9.i273.1.3 = and i64 %mul3.i267.1.3, 4294967295
  %add.i274.1.3 = add nuw nsw i64 %shr7.i271.1.3, %and8.i272.1.3
  %add10.i275.1.3 = add nuw nsw i64 %add.i274.1.3, %and9.i273.1.3
  %shr11.i276.1.3 = lshr i64 %add10.i275.1.3, 32
  %shl.i277.1.3 = shl i64 %add10.i275.1.3, 32
  %xor68.i278.1.3 = or i64 %shl.i277.1.3, %and6.i270.1.3
  %shr13.i279.1.3 = lshr i64 %mul4.i268.1.3, 32
  %shr14.i280.1.3 = lshr i64 %mul3.i267.1.3, 32
  %and15.i281.1.3 = and i64 %mul5.i269.1.3, 4294967295
  %add16.i282.1.3 = add nuw nsw i64 %shr13.i279.1.3, %shr14.i280.1.3
  %add17.i283.1.3 = add nuw nsw i64 %add16.i282.1.3, %and15.i281.1.3
  %add18.i284.1.3 = add nuw nsw i64 %add17.i283.1.3, %shr11.i276.1.3
  %and19.i285.1.3 = and i64 %add18.i284.1.3, 4294967295
  %and21.i287.1.3 = and i64 %add18.i284.1.3, 30064771072
  %and22.i288.1.3 = and i64 %mul5.i269.1.3, -4294967296
  %add23.i289.1.3 = add i64 %and21.i287.1.3, %and22.i288.1.3
  %xor2569.i290.1.3 = or i64 %add23.i289.1.3, %and19.i285.1.3
  %add82.1.3 = add i64 %xor68.i278.1.3, %add82.1.2
  %xor.i256.1.3 = xor i64 %add82.1.3, %shl.i277.1.3
  %xor1.i257.1.3 = xor i64 %shl.i277.1.3, %add82.1.2
  %or.i258.1.3 = or i64 %xor.i256.1.3, %xor1.i257.1.3
  %xor2.i259.1.3 = xor i64 %or.i258.1.3, %add82.1.3
  %shr.i260.1.3 = lshr i64 %xor2.i259.1.3, 63
  %add89.1.3 = add i64 %shr.i260.1.3, %xor2569.i290.1.3
  %add90.1.3 = add i64 %add89.1.3, %add90.1.2
  %89 = xor i64 %add89.1.3, -9223372036854775808
  %xor2.i253.1.3 = and i64 %89, %add23.i289.1.3
  %xor.i244.1.3 = xor i64 %add90.1.3, %add89.1.3
  %xor1.i245.1.3 = xor i64 %add89.1.3, %add90.1.2
  %or.i246.1.3 = or i64 %xor.i244.1.3, %xor1.i245.1.3
  %xor2.i247.1.3 = xor i64 %or.i246.1.3, %add90.1.3
  %shr.i254326.1.3 = or i64 %xor2.i247.1.3, %xor2.i253.1.3
  %or94325.1.3 = lshr i64 %shr.i254326.1.3, 63
  %add96.1.3 = add nsw i64 %or94325.1.3, %add96.1.2
  %arrayidx74.1.4 = getelementptr inbounds i64, i64* %mc, i64 6
  %90 = load i64, i64* %arrayidx74.1.4, align 8, !tbaa !3
  %91 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i262.1.4 = and i64 %90, 4294967295
  %shr.i263.1.4 = lshr i64 %90, 32
  %and1.i264.1.4 = and i64 %91, 4294967295
  %shr2.i265.1.4 = lshr i64 %91, 32
  %mul.i266.1.4 = mul nuw i64 %and1.i264.1.4, %and.i262.1.4
  %mul3.i267.1.4 = mul nuw i64 %shr2.i265.1.4, %and.i262.1.4
  %mul4.i268.1.4 = mul nuw i64 %and1.i264.1.4, %shr.i263.1.4
  %mul5.i269.1.4 = mul nuw i64 %shr2.i265.1.4, %shr.i263.1.4
  %and6.i270.1.4 = and i64 %mul.i266.1.4, 4294967295
  %shr7.i271.1.4 = lshr i64 %mul.i266.1.4, 32
  %and8.i272.1.4 = and i64 %mul4.i268.1.4, 4294967295
  %and9.i273.1.4 = and i64 %mul3.i267.1.4, 4294967295
  %add.i274.1.4 = add nuw nsw i64 %shr7.i271.1.4, %and8.i272.1.4
  %add10.i275.1.4 = add nuw nsw i64 %add.i274.1.4, %and9.i273.1.4
  %shr11.i276.1.4 = lshr i64 %add10.i275.1.4, 32
  %shl.i277.1.4 = shl i64 %add10.i275.1.4, 32
  %xor68.i278.1.4 = or i64 %shl.i277.1.4, %and6.i270.1.4
  %shr13.i279.1.4 = lshr i64 %mul4.i268.1.4, 32
  %shr14.i280.1.4 = lshr i64 %mul3.i267.1.4, 32
  %and15.i281.1.4 = and i64 %mul5.i269.1.4, 4294967295
  %add16.i282.1.4 = add nuw nsw i64 %shr13.i279.1.4, %shr14.i280.1.4
  %add17.i283.1.4 = add nuw nsw i64 %add16.i282.1.4, %and15.i281.1.4
  %add18.i284.1.4 = add nuw nsw i64 %add17.i283.1.4, %shr11.i276.1.4
  %and19.i285.1.4 = and i64 %add18.i284.1.4, 4294967295
  %and21.i287.1.4 = and i64 %add18.i284.1.4, 30064771072
  %and22.i288.1.4 = and i64 %mul5.i269.1.4, -4294967296
  %add23.i289.1.4 = add i64 %and21.i287.1.4, %and22.i288.1.4
  %xor2569.i290.1.4 = or i64 %add23.i289.1.4, %and19.i285.1.4
  %add82.1.4 = add i64 %xor68.i278.1.4, %add82.1.3
  %xor.i256.1.4 = xor i64 %add82.1.4, %shl.i277.1.4
  %xor1.i257.1.4 = xor i64 %shl.i277.1.4, %add82.1.3
  %or.i258.1.4 = or i64 %xor.i256.1.4, %xor1.i257.1.4
  %xor2.i259.1.4 = xor i64 %or.i258.1.4, %add82.1.4
  %shr.i260.1.4 = lshr i64 %xor2.i259.1.4, 63
  %add89.1.4 = add i64 %shr.i260.1.4, %xor2569.i290.1.4
  %add90.1.4 = add i64 %add89.1.4, %add90.1.3
  %92 = xor i64 %add89.1.4, -9223372036854775808
  %xor2.i253.1.4 = and i64 %92, %add23.i289.1.4
  %xor.i244.1.4 = xor i64 %add90.1.4, %add89.1.4
  %xor1.i245.1.4 = xor i64 %add89.1.4, %add90.1.3
  %or.i246.1.4 = or i64 %xor.i244.1.4, %xor1.i245.1.4
  %xor2.i247.1.4 = xor i64 %or.i246.1.4, %add90.1.4
  %shr.i254326.1.4 = or i64 %xor2.i247.1.4, %xor2.i253.1.4
  %or94325.1.4 = lshr i64 %shr.i254326.1.4, 63
  %add96.1.4 = add i64 %or94325.1.4, %add96.1.3
  %arrayidx104.1 = getelementptr inbounds i64, i64* %ma, i64 9
  %93 = load i64, i64* %arrayidx104.1, align 8, !tbaa !3
  %add105.1 = add i64 %93, %add82.1.4
  %xor.i238.1 = xor i64 %add105.1, %add82.1.4
  %xor1.i239.1 = xor i64 %93, %add82.1.4
  %or.i240.1 = or i64 %xor.i238.1, %xor1.i239.1
  %xor2.i241.1 = xor i64 %or.i240.1, %add105.1
  %shr.i242.1 = lshr i64 %xor2.i241.1, 63
  %add111.1 = add i64 %shr.i242.1, %add90.1.4
  store i64 %add105.1, i64* %arrayidx50.1, align 8, !tbaa !3
  %94 = xor i64 %add111.1, -9223372036854775808
  %xor2.i.1 = and i64 %94, %add90.1.4
  %shr.i237.1 = lshr i64 %xor2.i.1, 63
  %add118.1 = add i64 %shr.i237.1, %add96.1.4
  %arrayidx74.2 = getelementptr inbounds i64, i64* %mc, i64 3
  %95 = load i64, i64* %arrayidx74.2, align 8, !tbaa !3
  %96 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.2 = and i64 %95, 4294967295
  %shr.i263.2 = lshr i64 %95, 32
  %and1.i264.2 = and i64 %96, 4294967295
  %shr2.i265.2 = lshr i64 %96, 32
  %mul.i266.2 = mul nuw i64 %and1.i264.2, %and.i262.2
  %mul3.i267.2 = mul nuw i64 %shr2.i265.2, %and.i262.2
  %mul4.i268.2 = mul nuw i64 %and1.i264.2, %shr.i263.2
  %mul5.i269.2 = mul nuw i64 %shr2.i265.2, %shr.i263.2
  %and6.i270.2 = and i64 %mul.i266.2, 4294967295
  %shr7.i271.2 = lshr i64 %mul.i266.2, 32
  %and8.i272.2 = and i64 %mul4.i268.2, 4294967295
  %and9.i273.2 = and i64 %mul3.i267.2, 4294967295
  %add.i274.2 = add nuw nsw i64 %shr7.i271.2, %and8.i272.2
  %add10.i275.2 = add nuw nsw i64 %add.i274.2, %and9.i273.2
  %shr11.i276.2 = lshr i64 %add10.i275.2, 32
  %shl.i277.2 = shl i64 %add10.i275.2, 32
  %xor68.i278.2 = or i64 %shl.i277.2, %and6.i270.2
  %shr13.i279.2 = lshr i64 %mul4.i268.2, 32
  %shr14.i280.2 = lshr i64 %mul3.i267.2, 32
  %and15.i281.2 = and i64 %mul5.i269.2, 4294967295
  %add16.i282.2 = add nuw nsw i64 %shr13.i279.2, %shr14.i280.2
  %add17.i283.2 = add nuw nsw i64 %add16.i282.2, %and15.i281.2
  %add18.i284.2 = add nuw nsw i64 %add17.i283.2, %shr11.i276.2
  %and19.i285.2 = and i64 %add18.i284.2, 4294967295
  %and21.i287.2 = and i64 %add18.i284.2, 30064771072
  %and22.i288.2 = and i64 %mul5.i269.2, -4294967296
  %add23.i289.2 = add i64 %and21.i287.2, %and22.i288.2
  %xor2569.i290.2 = or i64 %add23.i289.2, %and19.i285.2
  %add82.2 = add i64 %xor68.i278.2, %add111.1
  %xor.i256.2 = xor i64 %add82.2, %shl.i277.2
  %xor1.i257.2 = xor i64 %shl.i277.2, %add111.1
  %or.i258.2 = or i64 %xor.i256.2, %xor1.i257.2
  %xor2.i259.2 = xor i64 %or.i258.2, %add82.2
  %shr.i260.2 = lshr i64 %xor2.i259.2, 63
  %add89.2 = add i64 %shr.i260.2, %xor2569.i290.2
  %add90.2 = add i64 %add89.2, %add118.1
  %97 = xor i64 %add89.2, -9223372036854775808
  %xor2.i253.2 = and i64 %97, %add23.i289.2
  %xor.i244.2 = xor i64 %add90.2, %add89.2
  %xor1.i245.2 = xor i64 %add89.2, %add118.1
  %or.i246.2 = or i64 %xor.i244.2, %xor1.i245.2
  %xor2.i247.2 = xor i64 %or.i246.2, %add90.2
  %shr.i254326.2 = or i64 %xor2.i247.2, %xor2.i253.2
  %or94325.2 = lshr i64 %shr.i254326.2, 63
  %arrayidx74.2.1 = getelementptr inbounds i64, i64* %mc, i64 4
  %98 = load i64, i64* %arrayidx74.2.1, align 8, !tbaa !3
  %99 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.2.1 = and i64 %98, 4294967295
  %shr.i263.2.1 = lshr i64 %98, 32
  %and1.i264.2.1 = and i64 %99, 4294967295
  %shr2.i265.2.1 = lshr i64 %99, 32
  %mul.i266.2.1 = mul nuw i64 %and1.i264.2.1, %and.i262.2.1
  %mul3.i267.2.1 = mul nuw i64 %shr2.i265.2.1, %and.i262.2.1
  %mul4.i268.2.1 = mul nuw i64 %and1.i264.2.1, %shr.i263.2.1
  %mul5.i269.2.1 = mul nuw i64 %shr2.i265.2.1, %shr.i263.2.1
  %and6.i270.2.1 = and i64 %mul.i266.2.1, 4294967295
  %shr7.i271.2.1 = lshr i64 %mul.i266.2.1, 32
  %and8.i272.2.1 = and i64 %mul4.i268.2.1, 4294967295
  %and9.i273.2.1 = and i64 %mul3.i267.2.1, 4294967295
  %add.i274.2.1 = add nuw nsw i64 %shr7.i271.2.1, %and8.i272.2.1
  %add10.i275.2.1 = add nuw nsw i64 %add.i274.2.1, %and9.i273.2.1
  %shr11.i276.2.1 = lshr i64 %add10.i275.2.1, 32
  %shl.i277.2.1 = shl i64 %add10.i275.2.1, 32
  %xor68.i278.2.1 = or i64 %shl.i277.2.1, %and6.i270.2.1
  %shr13.i279.2.1 = lshr i64 %mul4.i268.2.1, 32
  %shr14.i280.2.1 = lshr i64 %mul3.i267.2.1, 32
  %and15.i281.2.1 = and i64 %mul5.i269.2.1, 4294967295
  %add16.i282.2.1 = add nuw nsw i64 %shr13.i279.2.1, %shr14.i280.2.1
  %add17.i283.2.1 = add nuw nsw i64 %add16.i282.2.1, %and15.i281.2.1
  %add18.i284.2.1 = add nuw nsw i64 %add17.i283.2.1, %shr11.i276.2.1
  %and19.i285.2.1 = and i64 %add18.i284.2.1, 4294967295
  %and21.i287.2.1 = and i64 %add18.i284.2.1, 30064771072
  %and22.i288.2.1 = and i64 %mul5.i269.2.1, -4294967296
  %add23.i289.2.1 = add i64 %and21.i287.2.1, %and22.i288.2.1
  %xor2569.i290.2.1 = or i64 %add23.i289.2.1, %and19.i285.2.1
  %add82.2.1 = add i64 %xor68.i278.2.1, %add82.2
  %xor.i256.2.1 = xor i64 %add82.2.1, %shl.i277.2.1
  %xor1.i257.2.1 = xor i64 %shl.i277.2.1, %add82.2
  %or.i258.2.1 = or i64 %xor.i256.2.1, %xor1.i257.2.1
  %xor2.i259.2.1 = xor i64 %or.i258.2.1, %add82.2.1
  %shr.i260.2.1 = lshr i64 %xor2.i259.2.1, 63
  %add89.2.1 = add i64 %shr.i260.2.1, %xor2569.i290.2.1
  %add90.2.1 = add i64 %add89.2.1, %add90.2
  %100 = xor i64 %add89.2.1, -9223372036854775808
  %xor2.i253.2.1 = and i64 %100, %add23.i289.2.1
  %xor.i244.2.1 = xor i64 %add90.2.1, %add89.2.1
  %xor1.i245.2.1 = xor i64 %add89.2.1, %add90.2
  %or.i246.2.1 = or i64 %xor.i244.2.1, %xor1.i245.2.1
  %xor2.i247.2.1 = xor i64 %or.i246.2.1, %add90.2.1
  %shr.i254326.2.1 = or i64 %xor2.i247.2.1, %xor2.i253.2.1
  %or94325.2.1 = lshr i64 %shr.i254326.2.1, 63
  %add96.2.1 = add nuw nsw i64 %or94325.2.1, %or94325.2
  %arrayidx74.2.2 = getelementptr inbounds i64, i64* %mc, i64 5
  %101 = load i64, i64* %arrayidx74.2.2, align 8, !tbaa !3
  %102 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.2.2 = and i64 %101, 4294967295
  %shr.i263.2.2 = lshr i64 %101, 32
  %and1.i264.2.2 = and i64 %102, 4294967295
  %shr2.i265.2.2 = lshr i64 %102, 32
  %mul.i266.2.2 = mul nuw i64 %and1.i264.2.2, %and.i262.2.2
  %mul3.i267.2.2 = mul nuw i64 %shr2.i265.2.2, %and.i262.2.2
  %mul4.i268.2.2 = mul nuw i64 %and1.i264.2.2, %shr.i263.2.2
  %mul5.i269.2.2 = mul nuw i64 %shr2.i265.2.2, %shr.i263.2.2
  %and6.i270.2.2 = and i64 %mul.i266.2.2, 4294967295
  %shr7.i271.2.2 = lshr i64 %mul.i266.2.2, 32
  %and8.i272.2.2 = and i64 %mul4.i268.2.2, 4294967295
  %and9.i273.2.2 = and i64 %mul3.i267.2.2, 4294967295
  %add.i274.2.2 = add nuw nsw i64 %shr7.i271.2.2, %and8.i272.2.2
  %add10.i275.2.2 = add nuw nsw i64 %add.i274.2.2, %and9.i273.2.2
  %shr11.i276.2.2 = lshr i64 %add10.i275.2.2, 32
  %shl.i277.2.2 = shl i64 %add10.i275.2.2, 32
  %xor68.i278.2.2 = or i64 %shl.i277.2.2, %and6.i270.2.2
  %shr13.i279.2.2 = lshr i64 %mul4.i268.2.2, 32
  %shr14.i280.2.2 = lshr i64 %mul3.i267.2.2, 32
  %and15.i281.2.2 = and i64 %mul5.i269.2.2, 4294967295
  %add16.i282.2.2 = add nuw nsw i64 %shr13.i279.2.2, %shr14.i280.2.2
  %add17.i283.2.2 = add nuw nsw i64 %add16.i282.2.2, %and15.i281.2.2
  %add18.i284.2.2 = add nuw nsw i64 %add17.i283.2.2, %shr11.i276.2.2
  %and19.i285.2.2 = and i64 %add18.i284.2.2, 4294967295
  %and21.i287.2.2 = and i64 %add18.i284.2.2, 30064771072
  %and22.i288.2.2 = and i64 %mul5.i269.2.2, -4294967296
  %add23.i289.2.2 = add i64 %and21.i287.2.2, %and22.i288.2.2
  %xor2569.i290.2.2 = or i64 %add23.i289.2.2, %and19.i285.2.2
  %add82.2.2 = add i64 %xor68.i278.2.2, %add82.2.1
  %xor.i256.2.2 = xor i64 %add82.2.2, %shl.i277.2.2
  %xor1.i257.2.2 = xor i64 %shl.i277.2.2, %add82.2.1
  %or.i258.2.2 = or i64 %xor.i256.2.2, %xor1.i257.2.2
  %xor2.i259.2.2 = xor i64 %or.i258.2.2, %add82.2.2
  %shr.i260.2.2 = lshr i64 %xor2.i259.2.2, 63
  %add89.2.2 = add i64 %shr.i260.2.2, %xor2569.i290.2.2
  %add90.2.2 = add i64 %add89.2.2, %add90.2.1
  %103 = xor i64 %add89.2.2, -9223372036854775808
  %xor2.i253.2.2 = and i64 %103, %add23.i289.2.2
  %xor.i244.2.2 = xor i64 %add90.2.2, %add89.2.2
  %xor1.i245.2.2 = xor i64 %add89.2.2, %add90.2.1
  %or.i246.2.2 = or i64 %xor.i244.2.2, %xor1.i245.2.2
  %xor2.i247.2.2 = xor i64 %or.i246.2.2, %add90.2.2
  %shr.i254326.2.2 = or i64 %xor2.i247.2.2, %xor2.i253.2.2
  %or94325.2.2 = lshr i64 %shr.i254326.2.2, 63
  %add96.2.2 = add nsw i64 %or94325.2.2, %add96.2.1
  %arrayidx74.2.3 = getelementptr inbounds i64, i64* %mc, i64 6
  %104 = load i64, i64* %arrayidx74.2.3, align 8, !tbaa !3
  %105 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i262.2.3 = and i64 %104, 4294967295
  %shr.i263.2.3 = lshr i64 %104, 32
  %and1.i264.2.3 = and i64 %105, 4294967295
  %shr2.i265.2.3 = lshr i64 %105, 32
  %mul.i266.2.3 = mul nuw i64 %and1.i264.2.3, %and.i262.2.3
  %mul3.i267.2.3 = mul nuw i64 %shr2.i265.2.3, %and.i262.2.3
  %mul4.i268.2.3 = mul nuw i64 %and1.i264.2.3, %shr.i263.2.3
  %mul5.i269.2.3 = mul nuw i64 %shr2.i265.2.3, %shr.i263.2.3
  %and6.i270.2.3 = and i64 %mul.i266.2.3, 4294967295
  %shr7.i271.2.3 = lshr i64 %mul.i266.2.3, 32
  %and8.i272.2.3 = and i64 %mul4.i268.2.3, 4294967295
  %and9.i273.2.3 = and i64 %mul3.i267.2.3, 4294967295
  %add.i274.2.3 = add nuw nsw i64 %shr7.i271.2.3, %and8.i272.2.3
  %add10.i275.2.3 = add nuw nsw i64 %add.i274.2.3, %and9.i273.2.3
  %shr11.i276.2.3 = lshr i64 %add10.i275.2.3, 32
  %shl.i277.2.3 = shl i64 %add10.i275.2.3, 32
  %xor68.i278.2.3 = or i64 %shl.i277.2.3, %and6.i270.2.3
  %shr13.i279.2.3 = lshr i64 %mul4.i268.2.3, 32
  %shr14.i280.2.3 = lshr i64 %mul3.i267.2.3, 32
  %and15.i281.2.3 = and i64 %mul5.i269.2.3, 4294967295
  %add16.i282.2.3 = add nuw nsw i64 %shr13.i279.2.3, %shr14.i280.2.3
  %add17.i283.2.3 = add nuw nsw i64 %add16.i282.2.3, %and15.i281.2.3
  %add18.i284.2.3 = add nuw nsw i64 %add17.i283.2.3, %shr11.i276.2.3
  %and19.i285.2.3 = and i64 %add18.i284.2.3, 4294967295
  %and21.i287.2.3 = and i64 %add18.i284.2.3, 30064771072
  %and22.i288.2.3 = and i64 %mul5.i269.2.3, -4294967296
  %add23.i289.2.3 = add i64 %and21.i287.2.3, %and22.i288.2.3
  %xor2569.i290.2.3 = or i64 %add23.i289.2.3, %and19.i285.2.3
  %add82.2.3 = add i64 %xor68.i278.2.3, %add82.2.2
  %xor.i256.2.3 = xor i64 %add82.2.3, %shl.i277.2.3
  %xor1.i257.2.3 = xor i64 %shl.i277.2.3, %add82.2.2
  %or.i258.2.3 = or i64 %xor.i256.2.3, %xor1.i257.2.3
  %xor2.i259.2.3 = xor i64 %or.i258.2.3, %add82.2.3
  %shr.i260.2.3 = lshr i64 %xor2.i259.2.3, 63
  %add89.2.3 = add i64 %shr.i260.2.3, %xor2569.i290.2.3
  %add90.2.3 = add i64 %add89.2.3, %add90.2.2
  %106 = xor i64 %add89.2.3, -9223372036854775808
  %xor2.i253.2.3 = and i64 %106, %add23.i289.2.3
  %xor.i244.2.3 = xor i64 %add90.2.3, %add89.2.3
  %xor1.i245.2.3 = xor i64 %add89.2.3, %add90.2.2
  %or.i246.2.3 = or i64 %xor.i244.2.3, %xor1.i245.2.3
  %xor2.i247.2.3 = xor i64 %or.i246.2.3, %add90.2.3
  %shr.i254326.2.3 = or i64 %xor2.i247.2.3, %xor2.i253.2.3
  %or94325.2.3 = lshr i64 %shr.i254326.2.3, 63
  %add96.2.3 = add nsw i64 %or94325.2.3, %add96.2.2
  %arrayidx74.2.4 = getelementptr inbounds i64, i64* %mc, i64 7
  %107 = load i64, i64* %arrayidx74.2.4, align 8, !tbaa !3
  %108 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i262.2.4 = and i64 %107, 4294967295
  %shr.i263.2.4 = lshr i64 %107, 32
  %and1.i264.2.4 = and i64 %108, 4294967295
  %shr2.i265.2.4 = lshr i64 %108, 32
  %mul.i266.2.4 = mul nuw i64 %and1.i264.2.4, %and.i262.2.4
  %mul3.i267.2.4 = mul nuw i64 %shr2.i265.2.4, %and.i262.2.4
  %mul4.i268.2.4 = mul nuw i64 %and1.i264.2.4, %shr.i263.2.4
  %mul5.i269.2.4 = mul nuw i64 %shr2.i265.2.4, %shr.i263.2.4
  %and6.i270.2.4 = and i64 %mul.i266.2.4, 4294967295
  %shr7.i271.2.4 = lshr i64 %mul.i266.2.4, 32
  %and8.i272.2.4 = and i64 %mul4.i268.2.4, 4294967295
  %and9.i273.2.4 = and i64 %mul3.i267.2.4, 4294967295
  %add.i274.2.4 = add nuw nsw i64 %shr7.i271.2.4, %and8.i272.2.4
  %add10.i275.2.4 = add nuw nsw i64 %add.i274.2.4, %and9.i273.2.4
  %shr11.i276.2.4 = lshr i64 %add10.i275.2.4, 32
  %shl.i277.2.4 = shl i64 %add10.i275.2.4, 32
  %xor68.i278.2.4 = or i64 %shl.i277.2.4, %and6.i270.2.4
  %shr13.i279.2.4 = lshr i64 %mul4.i268.2.4, 32
  %shr14.i280.2.4 = lshr i64 %mul3.i267.2.4, 32
  %and15.i281.2.4 = and i64 %mul5.i269.2.4, 4294967295
  %add16.i282.2.4 = add nuw nsw i64 %shr13.i279.2.4, %shr14.i280.2.4
  %add17.i283.2.4 = add nuw nsw i64 %add16.i282.2.4, %and15.i281.2.4
  %add18.i284.2.4 = add nuw nsw i64 %add17.i283.2.4, %shr11.i276.2.4
  %and19.i285.2.4 = and i64 %add18.i284.2.4, 4294967295
  %and21.i287.2.4 = and i64 %add18.i284.2.4, 30064771072
  %and22.i288.2.4 = and i64 %mul5.i269.2.4, -4294967296
  %add23.i289.2.4 = add i64 %and21.i287.2.4, %and22.i288.2.4
  %xor2569.i290.2.4 = or i64 %add23.i289.2.4, %and19.i285.2.4
  %add82.2.4 = add i64 %xor68.i278.2.4, %add82.2.3
  %xor.i256.2.4 = xor i64 %add82.2.4, %shl.i277.2.4
  %xor1.i257.2.4 = xor i64 %shl.i277.2.4, %add82.2.3
  %or.i258.2.4 = or i64 %xor.i256.2.4, %xor1.i257.2.4
  %xor2.i259.2.4 = xor i64 %or.i258.2.4, %add82.2.4
  %shr.i260.2.4 = lshr i64 %xor2.i259.2.4, 63
  %add89.2.4 = add i64 %shr.i260.2.4, %xor2569.i290.2.4
  %add90.2.4 = add i64 %add89.2.4, %add90.2.3
  %109 = xor i64 %add89.2.4, -9223372036854775808
  %xor2.i253.2.4 = and i64 %109, %add23.i289.2.4
  %xor.i244.2.4 = xor i64 %add90.2.4, %add89.2.4
  %xor1.i245.2.4 = xor i64 %add89.2.4, %add90.2.3
  %or.i246.2.4 = or i64 %xor.i244.2.4, %xor1.i245.2.4
  %xor2.i247.2.4 = xor i64 %or.i246.2.4, %add90.2.4
  %shr.i254326.2.4 = or i64 %xor2.i247.2.4, %xor2.i253.2.4
  %or94325.2.4 = lshr i64 %shr.i254326.2.4, 63
  %add96.2.4 = add i64 %or94325.2.4, %add96.2.3
  %arrayidx104.2 = getelementptr inbounds i64, i64* %ma, i64 10
  %110 = load i64, i64* %arrayidx104.2, align 8, !tbaa !3
  %add105.2 = add i64 %110, %add82.2.4
  %xor.i238.2 = xor i64 %add105.2, %add82.2.4
  %xor1.i239.2 = xor i64 %110, %add82.2.4
  %or.i240.2 = or i64 %xor.i238.2, %xor1.i239.2
  %xor2.i241.2 = xor i64 %or.i240.2, %add105.2
  %shr.i242.2 = lshr i64 %xor2.i241.2, 63
  %add111.2 = add i64 %shr.i242.2, %add90.2.4
  store i64 %add105.2, i64* %arrayidx50.2, align 8, !tbaa !3
  %111 = xor i64 %add111.2, -9223372036854775808
  %xor2.i.2 = and i64 %111, %add90.2.4
  %shr.i237.2 = lshr i64 %xor2.i.2, 63
  %add118.2 = add i64 %shr.i237.2, %add96.2.4
  %arrayidx74.3 = getelementptr inbounds i64, i64* %mc, i64 4
  %112 = load i64, i64* %arrayidx74.3, align 8, !tbaa !3
  %113 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.3 = and i64 %112, 4294967295
  %shr.i263.3 = lshr i64 %112, 32
  %and1.i264.3 = and i64 %113, 4294967295
  %shr2.i265.3 = lshr i64 %113, 32
  %mul.i266.3 = mul nuw i64 %and1.i264.3, %and.i262.3
  %mul3.i267.3 = mul nuw i64 %shr2.i265.3, %and.i262.3
  %mul4.i268.3 = mul nuw i64 %and1.i264.3, %shr.i263.3
  %mul5.i269.3 = mul nuw i64 %shr2.i265.3, %shr.i263.3
  %and6.i270.3 = and i64 %mul.i266.3, 4294967295
  %shr7.i271.3 = lshr i64 %mul.i266.3, 32
  %and8.i272.3 = and i64 %mul4.i268.3, 4294967295
  %and9.i273.3 = and i64 %mul3.i267.3, 4294967295
  %add.i274.3 = add nuw nsw i64 %shr7.i271.3, %and8.i272.3
  %add10.i275.3 = add nuw nsw i64 %add.i274.3, %and9.i273.3
  %shr11.i276.3 = lshr i64 %add10.i275.3, 32
  %shl.i277.3 = shl i64 %add10.i275.3, 32
  %xor68.i278.3 = or i64 %shl.i277.3, %and6.i270.3
  %shr13.i279.3 = lshr i64 %mul4.i268.3, 32
  %shr14.i280.3 = lshr i64 %mul3.i267.3, 32
  %and15.i281.3 = and i64 %mul5.i269.3, 4294967295
  %add16.i282.3 = add nuw nsw i64 %shr13.i279.3, %shr14.i280.3
  %add17.i283.3 = add nuw nsw i64 %add16.i282.3, %and15.i281.3
  %add18.i284.3 = add nuw nsw i64 %add17.i283.3, %shr11.i276.3
  %and19.i285.3 = and i64 %add18.i284.3, 4294967295
  %and21.i287.3 = and i64 %add18.i284.3, 30064771072
  %and22.i288.3 = and i64 %mul5.i269.3, -4294967296
  %add23.i289.3 = add i64 %and21.i287.3, %and22.i288.3
  %xor2569.i290.3 = or i64 %add23.i289.3, %and19.i285.3
  %add82.3 = add i64 %xor68.i278.3, %add111.2
  %xor.i256.3 = xor i64 %add82.3, %shl.i277.3
  %xor1.i257.3 = xor i64 %shl.i277.3, %add111.2
  %or.i258.3 = or i64 %xor.i256.3, %xor1.i257.3
  %xor2.i259.3 = xor i64 %or.i258.3, %add82.3
  %shr.i260.3 = lshr i64 %xor2.i259.3, 63
  %add89.3 = add i64 %shr.i260.3, %xor2569.i290.3
  %add90.3 = add i64 %add89.3, %add118.2
  %114 = xor i64 %add89.3, -9223372036854775808
  %xor2.i253.3 = and i64 %114, %add23.i289.3
  %xor.i244.3 = xor i64 %add90.3, %add89.3
  %xor1.i245.3 = xor i64 %add89.3, %add118.2
  %or.i246.3 = or i64 %xor.i244.3, %xor1.i245.3
  %xor2.i247.3 = xor i64 %or.i246.3, %add90.3
  %shr.i254326.3 = or i64 %xor2.i247.3, %xor2.i253.3
  %or94325.3 = lshr i64 %shr.i254326.3, 63
  %arrayidx74.3.1 = getelementptr inbounds i64, i64* %mc, i64 5
  %115 = load i64, i64* %arrayidx74.3.1, align 8, !tbaa !3
  %116 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.3.1 = and i64 %115, 4294967295
  %shr.i263.3.1 = lshr i64 %115, 32
  %and1.i264.3.1 = and i64 %116, 4294967295
  %shr2.i265.3.1 = lshr i64 %116, 32
  %mul.i266.3.1 = mul nuw i64 %and1.i264.3.1, %and.i262.3.1
  %mul3.i267.3.1 = mul nuw i64 %shr2.i265.3.1, %and.i262.3.1
  %mul4.i268.3.1 = mul nuw i64 %and1.i264.3.1, %shr.i263.3.1
  %mul5.i269.3.1 = mul nuw i64 %shr2.i265.3.1, %shr.i263.3.1
  %and6.i270.3.1 = and i64 %mul.i266.3.1, 4294967295
  %shr7.i271.3.1 = lshr i64 %mul.i266.3.1, 32
  %and8.i272.3.1 = and i64 %mul4.i268.3.1, 4294967295
  %and9.i273.3.1 = and i64 %mul3.i267.3.1, 4294967295
  %add.i274.3.1 = add nuw nsw i64 %shr7.i271.3.1, %and8.i272.3.1
  %add10.i275.3.1 = add nuw nsw i64 %add.i274.3.1, %and9.i273.3.1
  %shr11.i276.3.1 = lshr i64 %add10.i275.3.1, 32
  %shl.i277.3.1 = shl i64 %add10.i275.3.1, 32
  %xor68.i278.3.1 = or i64 %shl.i277.3.1, %and6.i270.3.1
  %shr13.i279.3.1 = lshr i64 %mul4.i268.3.1, 32
  %shr14.i280.3.1 = lshr i64 %mul3.i267.3.1, 32
  %and15.i281.3.1 = and i64 %mul5.i269.3.1, 4294967295
  %add16.i282.3.1 = add nuw nsw i64 %shr13.i279.3.1, %shr14.i280.3.1
  %add17.i283.3.1 = add nuw nsw i64 %add16.i282.3.1, %and15.i281.3.1
  %add18.i284.3.1 = add nuw nsw i64 %add17.i283.3.1, %shr11.i276.3.1
  %and19.i285.3.1 = and i64 %add18.i284.3.1, 4294967295
  %and21.i287.3.1 = and i64 %add18.i284.3.1, 30064771072
  %and22.i288.3.1 = and i64 %mul5.i269.3.1, -4294967296
  %add23.i289.3.1 = add i64 %and21.i287.3.1, %and22.i288.3.1
  %xor2569.i290.3.1 = or i64 %add23.i289.3.1, %and19.i285.3.1
  %add82.3.1 = add i64 %xor68.i278.3.1, %add82.3
  %xor.i256.3.1 = xor i64 %add82.3.1, %shl.i277.3.1
  %xor1.i257.3.1 = xor i64 %shl.i277.3.1, %add82.3
  %or.i258.3.1 = or i64 %xor.i256.3.1, %xor1.i257.3.1
  %xor2.i259.3.1 = xor i64 %or.i258.3.1, %add82.3.1
  %shr.i260.3.1 = lshr i64 %xor2.i259.3.1, 63
  %add89.3.1 = add i64 %shr.i260.3.1, %xor2569.i290.3.1
  %add90.3.1 = add i64 %add89.3.1, %add90.3
  %117 = xor i64 %add89.3.1, -9223372036854775808
  %xor2.i253.3.1 = and i64 %117, %add23.i289.3.1
  %xor.i244.3.1 = xor i64 %add90.3.1, %add89.3.1
  %xor1.i245.3.1 = xor i64 %add89.3.1, %add90.3
  %or.i246.3.1 = or i64 %xor.i244.3.1, %xor1.i245.3.1
  %xor2.i247.3.1 = xor i64 %or.i246.3.1, %add90.3.1
  %shr.i254326.3.1 = or i64 %xor2.i247.3.1, %xor2.i253.3.1
  %or94325.3.1 = lshr i64 %shr.i254326.3.1, 63
  %add96.3.1 = add nuw nsw i64 %or94325.3.1, %or94325.3
  %arrayidx74.3.2 = getelementptr inbounds i64, i64* %mc, i64 6
  %118 = load i64, i64* %arrayidx74.3.2, align 8, !tbaa !3
  %119 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.3.2 = and i64 %118, 4294967295
  %shr.i263.3.2 = lshr i64 %118, 32
  %and1.i264.3.2 = and i64 %119, 4294967295
  %shr2.i265.3.2 = lshr i64 %119, 32
  %mul.i266.3.2 = mul nuw i64 %and1.i264.3.2, %and.i262.3.2
  %mul3.i267.3.2 = mul nuw i64 %shr2.i265.3.2, %and.i262.3.2
  %mul4.i268.3.2 = mul nuw i64 %and1.i264.3.2, %shr.i263.3.2
  %mul5.i269.3.2 = mul nuw i64 %shr2.i265.3.2, %shr.i263.3.2
  %and6.i270.3.2 = and i64 %mul.i266.3.2, 4294967295
  %shr7.i271.3.2 = lshr i64 %mul.i266.3.2, 32
  %and8.i272.3.2 = and i64 %mul4.i268.3.2, 4294967295
  %and9.i273.3.2 = and i64 %mul3.i267.3.2, 4294967295
  %add.i274.3.2 = add nuw nsw i64 %shr7.i271.3.2, %and8.i272.3.2
  %add10.i275.3.2 = add nuw nsw i64 %add.i274.3.2, %and9.i273.3.2
  %shr11.i276.3.2 = lshr i64 %add10.i275.3.2, 32
  %shl.i277.3.2 = shl i64 %add10.i275.3.2, 32
  %xor68.i278.3.2 = or i64 %shl.i277.3.2, %and6.i270.3.2
  %shr13.i279.3.2 = lshr i64 %mul4.i268.3.2, 32
  %shr14.i280.3.2 = lshr i64 %mul3.i267.3.2, 32
  %and15.i281.3.2 = and i64 %mul5.i269.3.2, 4294967295
  %add16.i282.3.2 = add nuw nsw i64 %shr13.i279.3.2, %shr14.i280.3.2
  %add17.i283.3.2 = add nuw nsw i64 %add16.i282.3.2, %and15.i281.3.2
  %add18.i284.3.2 = add nuw nsw i64 %add17.i283.3.2, %shr11.i276.3.2
  %and19.i285.3.2 = and i64 %add18.i284.3.2, 4294967295
  %and21.i287.3.2 = and i64 %add18.i284.3.2, 30064771072
  %and22.i288.3.2 = and i64 %mul5.i269.3.2, -4294967296
  %add23.i289.3.2 = add i64 %and21.i287.3.2, %and22.i288.3.2
  %xor2569.i290.3.2 = or i64 %add23.i289.3.2, %and19.i285.3.2
  %add82.3.2 = add i64 %xor68.i278.3.2, %add82.3.1
  %xor.i256.3.2 = xor i64 %add82.3.2, %shl.i277.3.2
  %xor1.i257.3.2 = xor i64 %shl.i277.3.2, %add82.3.1
  %or.i258.3.2 = or i64 %xor.i256.3.2, %xor1.i257.3.2
  %xor2.i259.3.2 = xor i64 %or.i258.3.2, %add82.3.2
  %shr.i260.3.2 = lshr i64 %xor2.i259.3.2, 63
  %add89.3.2 = add i64 %shr.i260.3.2, %xor2569.i290.3.2
  %add90.3.2 = add i64 %add89.3.2, %add90.3.1
  %120 = xor i64 %add89.3.2, -9223372036854775808
  %xor2.i253.3.2 = and i64 %120, %add23.i289.3.2
  %xor.i244.3.2 = xor i64 %add90.3.2, %add89.3.2
  %xor1.i245.3.2 = xor i64 %add89.3.2, %add90.3.1
  %or.i246.3.2 = or i64 %xor.i244.3.2, %xor1.i245.3.2
  %xor2.i247.3.2 = xor i64 %or.i246.3.2, %add90.3.2
  %shr.i254326.3.2 = or i64 %xor2.i247.3.2, %xor2.i253.3.2
  %or94325.3.2 = lshr i64 %shr.i254326.3.2, 63
  %add96.3.2 = add nsw i64 %or94325.3.2, %add96.3.1
  %arrayidx74.3.3 = getelementptr inbounds i64, i64* %mc, i64 7
  %121 = load i64, i64* %arrayidx74.3.3, align 8, !tbaa !3
  %122 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 4), align 16, !tbaa !3
  %and.i262.3.3 = and i64 %121, 4294967295
  %shr.i263.3.3 = lshr i64 %121, 32
  %and1.i264.3.3 = and i64 %122, 4294967295
  %shr2.i265.3.3 = lshr i64 %122, 32
  %mul.i266.3.3 = mul nuw i64 %and1.i264.3.3, %and.i262.3.3
  %mul3.i267.3.3 = mul nuw i64 %shr2.i265.3.3, %and.i262.3.3
  %mul4.i268.3.3 = mul nuw i64 %and1.i264.3.3, %shr.i263.3.3
  %mul5.i269.3.3 = mul nuw i64 %shr2.i265.3.3, %shr.i263.3.3
  %and6.i270.3.3 = and i64 %mul.i266.3.3, 4294967295
  %shr7.i271.3.3 = lshr i64 %mul.i266.3.3, 32
  %and8.i272.3.3 = and i64 %mul4.i268.3.3, 4294967295
  %and9.i273.3.3 = and i64 %mul3.i267.3.3, 4294967295
  %add.i274.3.3 = add nuw nsw i64 %shr7.i271.3.3, %and8.i272.3.3
  %add10.i275.3.3 = add nuw nsw i64 %add.i274.3.3, %and9.i273.3.3
  %shr11.i276.3.3 = lshr i64 %add10.i275.3.3, 32
  %shl.i277.3.3 = shl i64 %add10.i275.3.3, 32
  %xor68.i278.3.3 = or i64 %shl.i277.3.3, %and6.i270.3.3
  %shr13.i279.3.3 = lshr i64 %mul4.i268.3.3, 32
  %shr14.i280.3.3 = lshr i64 %mul3.i267.3.3, 32
  %and15.i281.3.3 = and i64 %mul5.i269.3.3, 4294967295
  %add16.i282.3.3 = add nuw nsw i64 %shr13.i279.3.3, %shr14.i280.3.3
  %add17.i283.3.3 = add nuw nsw i64 %add16.i282.3.3, %and15.i281.3.3
  %add18.i284.3.3 = add nuw nsw i64 %add17.i283.3.3, %shr11.i276.3.3
  %and19.i285.3.3 = and i64 %add18.i284.3.3, 4294967295
  %and21.i287.3.3 = and i64 %add18.i284.3.3, 30064771072
  %and22.i288.3.3 = and i64 %mul5.i269.3.3, -4294967296
  %add23.i289.3.3 = add i64 %and21.i287.3.3, %and22.i288.3.3
  %xor2569.i290.3.3 = or i64 %add23.i289.3.3, %and19.i285.3.3
  %add82.3.3 = add i64 %xor68.i278.3.3, %add82.3.2
  %xor.i256.3.3 = xor i64 %add82.3.3, %shl.i277.3.3
  %xor1.i257.3.3 = xor i64 %shl.i277.3.3, %add82.3.2
  %or.i258.3.3 = or i64 %xor.i256.3.3, %xor1.i257.3.3
  %xor2.i259.3.3 = xor i64 %or.i258.3.3, %add82.3.3
  %shr.i260.3.3 = lshr i64 %xor2.i259.3.3, 63
  %add89.3.3 = add i64 %shr.i260.3.3, %xor2569.i290.3.3
  %add90.3.3 = add i64 %add89.3.3, %add90.3.2
  %123 = xor i64 %add89.3.3, -9223372036854775808
  %xor2.i253.3.3 = and i64 %123, %add23.i289.3.3
  %xor.i244.3.3 = xor i64 %add90.3.3, %add89.3.3
  %xor1.i245.3.3 = xor i64 %add89.3.3, %add90.3.2
  %or.i246.3.3 = or i64 %xor.i244.3.3, %xor1.i245.3.3
  %xor2.i247.3.3 = xor i64 %or.i246.3.3, %add90.3.3
  %shr.i254326.3.3 = or i64 %xor2.i247.3.3, %xor2.i253.3.3
  %or94325.3.3 = lshr i64 %shr.i254326.3.3, 63
  %add96.3.3 = add nsw i64 %or94325.3.3, %add96.3.2
  %arrayidx104.3 = getelementptr inbounds i64, i64* %ma, i64 11
  %124 = load i64, i64* %arrayidx104.3, align 8, !tbaa !3
  %add105.3 = add i64 %124, %add82.3.3
  %xor.i238.3 = xor i64 %add105.3, %add82.3.3
  %xor1.i239.3 = xor i64 %124, %add82.3.3
  %or.i240.3 = or i64 %xor.i238.3, %xor1.i239.3
  %xor2.i241.3 = xor i64 %or.i240.3, %add105.3
  %shr.i242.3 = lshr i64 %xor2.i241.3, 63
  %add111.3 = add i64 %shr.i242.3, %add90.3.3
  store i64 %add105.3, i64* %arrayidx50.3, align 8, !tbaa !3
  %125 = xor i64 %add111.3, -9223372036854775808
  %xor2.i.3 = and i64 %125, %add90.3.3
  %shr.i237.3 = lshr i64 %xor2.i.3, 63
  %add118.3 = add i64 %shr.i237.3, %add96.3.3
  %arrayidx74.4 = getelementptr inbounds i64, i64* %mc, i64 5
  %126 = load i64, i64* %arrayidx74.4, align 8, !tbaa !3
  %127 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.4 = and i64 %126, 4294967295
  %shr.i263.4 = lshr i64 %126, 32
  %and1.i264.4 = and i64 %127, 4294967295
  %shr2.i265.4 = lshr i64 %127, 32
  %mul.i266.4 = mul nuw i64 %and1.i264.4, %and.i262.4
  %mul3.i267.4 = mul nuw i64 %shr2.i265.4, %and.i262.4
  %mul4.i268.4 = mul nuw i64 %and1.i264.4, %shr.i263.4
  %mul5.i269.4 = mul nuw i64 %shr2.i265.4, %shr.i263.4
  %and6.i270.4 = and i64 %mul.i266.4, 4294967295
  %shr7.i271.4 = lshr i64 %mul.i266.4, 32
  %and8.i272.4 = and i64 %mul4.i268.4, 4294967295
  %and9.i273.4 = and i64 %mul3.i267.4, 4294967295
  %add.i274.4 = add nuw nsw i64 %shr7.i271.4, %and8.i272.4
  %add10.i275.4 = add nuw nsw i64 %add.i274.4, %and9.i273.4
  %shr11.i276.4 = lshr i64 %add10.i275.4, 32
  %shl.i277.4 = shl i64 %add10.i275.4, 32
  %xor68.i278.4 = or i64 %shl.i277.4, %and6.i270.4
  %shr13.i279.4 = lshr i64 %mul4.i268.4, 32
  %shr14.i280.4 = lshr i64 %mul3.i267.4, 32
  %and15.i281.4 = and i64 %mul5.i269.4, 4294967295
  %add16.i282.4 = add nuw nsw i64 %shr13.i279.4, %shr14.i280.4
  %add17.i283.4 = add nuw nsw i64 %add16.i282.4, %and15.i281.4
  %add18.i284.4 = add nuw nsw i64 %add17.i283.4, %shr11.i276.4
  %and19.i285.4 = and i64 %add18.i284.4, 4294967295
  %and21.i287.4 = and i64 %add18.i284.4, 30064771072
  %and22.i288.4 = and i64 %mul5.i269.4, -4294967296
  %add23.i289.4 = add i64 %and21.i287.4, %and22.i288.4
  %xor2569.i290.4 = or i64 %add23.i289.4, %and19.i285.4
  %add82.4 = add i64 %xor68.i278.4, %add111.3
  %xor.i256.4 = xor i64 %add82.4, %shl.i277.4
  %xor1.i257.4 = xor i64 %shl.i277.4, %add111.3
  %or.i258.4 = or i64 %xor.i256.4, %xor1.i257.4
  %xor2.i259.4 = xor i64 %or.i258.4, %add82.4
  %shr.i260.4 = lshr i64 %xor2.i259.4, 63
  %add89.4 = add i64 %shr.i260.4, %xor2569.i290.4
  %add90.4 = add i64 %add89.4, %add118.3
  %128 = xor i64 %add89.4, -9223372036854775808
  %xor2.i253.4 = and i64 %128, %add23.i289.4
  %xor.i244.4 = xor i64 %add90.4, %add89.4
  %xor1.i245.4 = xor i64 %add89.4, %add118.3
  %or.i246.4 = or i64 %xor.i244.4, %xor1.i245.4
  %xor2.i247.4 = xor i64 %or.i246.4, %add90.4
  %shr.i254326.4 = or i64 %xor2.i247.4, %xor2.i253.4
  %or94325.4 = lshr i64 %shr.i254326.4, 63
  %arrayidx74.4.1 = getelementptr inbounds i64, i64* %mc, i64 6
  %129 = load i64, i64* %arrayidx74.4.1, align 8, !tbaa !3
  %130 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.4.1 = and i64 %129, 4294967295
  %shr.i263.4.1 = lshr i64 %129, 32
  %and1.i264.4.1 = and i64 %130, 4294967295
  %shr2.i265.4.1 = lshr i64 %130, 32
  %mul.i266.4.1 = mul nuw i64 %and1.i264.4.1, %and.i262.4.1
  %mul3.i267.4.1 = mul nuw i64 %shr2.i265.4.1, %and.i262.4.1
  %mul4.i268.4.1 = mul nuw i64 %and1.i264.4.1, %shr.i263.4.1
  %mul5.i269.4.1 = mul nuw i64 %shr2.i265.4.1, %shr.i263.4.1
  %and6.i270.4.1 = and i64 %mul.i266.4.1, 4294967295
  %shr7.i271.4.1 = lshr i64 %mul.i266.4.1, 32
  %and8.i272.4.1 = and i64 %mul4.i268.4.1, 4294967295
  %and9.i273.4.1 = and i64 %mul3.i267.4.1, 4294967295
  %add.i274.4.1 = add nuw nsw i64 %shr7.i271.4.1, %and8.i272.4.1
  %add10.i275.4.1 = add nuw nsw i64 %add.i274.4.1, %and9.i273.4.1
  %shr11.i276.4.1 = lshr i64 %add10.i275.4.1, 32
  %shl.i277.4.1 = shl i64 %add10.i275.4.1, 32
  %xor68.i278.4.1 = or i64 %shl.i277.4.1, %and6.i270.4.1
  %shr13.i279.4.1 = lshr i64 %mul4.i268.4.1, 32
  %shr14.i280.4.1 = lshr i64 %mul3.i267.4.1, 32
  %and15.i281.4.1 = and i64 %mul5.i269.4.1, 4294967295
  %add16.i282.4.1 = add nuw nsw i64 %shr13.i279.4.1, %shr14.i280.4.1
  %add17.i283.4.1 = add nuw nsw i64 %add16.i282.4.1, %and15.i281.4.1
  %add18.i284.4.1 = add nuw nsw i64 %add17.i283.4.1, %shr11.i276.4.1
  %and19.i285.4.1 = and i64 %add18.i284.4.1, 4294967295
  %and21.i287.4.1 = and i64 %add18.i284.4.1, 30064771072
  %and22.i288.4.1 = and i64 %mul5.i269.4.1, -4294967296
  %add23.i289.4.1 = add i64 %and21.i287.4.1, %and22.i288.4.1
  %xor2569.i290.4.1 = or i64 %add23.i289.4.1, %and19.i285.4.1
  %add82.4.1 = add i64 %xor68.i278.4.1, %add82.4
  %xor.i256.4.1 = xor i64 %add82.4.1, %shl.i277.4.1
  %xor1.i257.4.1 = xor i64 %shl.i277.4.1, %add82.4
  %or.i258.4.1 = or i64 %xor.i256.4.1, %xor1.i257.4.1
  %xor2.i259.4.1 = xor i64 %or.i258.4.1, %add82.4.1
  %shr.i260.4.1 = lshr i64 %xor2.i259.4.1, 63
  %add89.4.1 = add i64 %shr.i260.4.1, %xor2569.i290.4.1
  %add90.4.1 = add i64 %add89.4.1, %add90.4
  %131 = xor i64 %add89.4.1, -9223372036854775808
  %xor2.i253.4.1 = and i64 %131, %add23.i289.4.1
  %xor.i244.4.1 = xor i64 %add90.4.1, %add89.4.1
  %xor1.i245.4.1 = xor i64 %add89.4.1, %add90.4
  %or.i246.4.1 = or i64 %xor.i244.4.1, %xor1.i245.4.1
  %xor2.i247.4.1 = xor i64 %or.i246.4.1, %add90.4.1
  %shr.i254326.4.1 = or i64 %xor2.i247.4.1, %xor2.i253.4.1
  %or94325.4.1 = lshr i64 %shr.i254326.4.1, 63
  %add96.4.1 = add nuw nsw i64 %or94325.4.1, %or94325.4
  %arrayidx74.4.2 = getelementptr inbounds i64, i64* %mc, i64 7
  %132 = load i64, i64* %arrayidx74.4.2, align 8, !tbaa !3
  %133 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.4.2 = and i64 %132, 4294967295
  %shr.i263.4.2 = lshr i64 %132, 32
  %and1.i264.4.2 = and i64 %133, 4294967295
  %shr2.i265.4.2 = lshr i64 %133, 32
  %mul.i266.4.2 = mul nuw i64 %and1.i264.4.2, %and.i262.4.2
  %mul3.i267.4.2 = mul nuw i64 %shr2.i265.4.2, %and.i262.4.2
  %mul4.i268.4.2 = mul nuw i64 %and1.i264.4.2, %shr.i263.4.2
  %mul5.i269.4.2 = mul nuw i64 %shr2.i265.4.2, %shr.i263.4.2
  %and6.i270.4.2 = and i64 %mul.i266.4.2, 4294967295
  %shr7.i271.4.2 = lshr i64 %mul.i266.4.2, 32
  %and8.i272.4.2 = and i64 %mul4.i268.4.2, 4294967295
  %and9.i273.4.2 = and i64 %mul3.i267.4.2, 4294967295
  %add.i274.4.2 = add nuw nsw i64 %shr7.i271.4.2, %and8.i272.4.2
  %add10.i275.4.2 = add nuw nsw i64 %add.i274.4.2, %and9.i273.4.2
  %shr11.i276.4.2 = lshr i64 %add10.i275.4.2, 32
  %shl.i277.4.2 = shl i64 %add10.i275.4.2, 32
  %xor68.i278.4.2 = or i64 %shl.i277.4.2, %and6.i270.4.2
  %shr13.i279.4.2 = lshr i64 %mul4.i268.4.2, 32
  %shr14.i280.4.2 = lshr i64 %mul3.i267.4.2, 32
  %and15.i281.4.2 = and i64 %mul5.i269.4.2, 4294967295
  %add16.i282.4.2 = add nuw nsw i64 %shr13.i279.4.2, %shr14.i280.4.2
  %add17.i283.4.2 = add nuw nsw i64 %add16.i282.4.2, %and15.i281.4.2
  %add18.i284.4.2 = add nuw nsw i64 %add17.i283.4.2, %shr11.i276.4.2
  %and19.i285.4.2 = and i64 %add18.i284.4.2, 4294967295
  %and21.i287.4.2 = and i64 %add18.i284.4.2, 30064771072
  %and22.i288.4.2 = and i64 %mul5.i269.4.2, -4294967296
  %add23.i289.4.2 = add i64 %and21.i287.4.2, %and22.i288.4.2
  %xor2569.i290.4.2 = or i64 %add23.i289.4.2, %and19.i285.4.2
  %add82.4.2 = add i64 %xor68.i278.4.2, %add82.4.1
  %xor.i256.4.2 = xor i64 %add82.4.2, %shl.i277.4.2
  %xor1.i257.4.2 = xor i64 %shl.i277.4.2, %add82.4.1
  %or.i258.4.2 = or i64 %xor.i256.4.2, %xor1.i257.4.2
  %xor2.i259.4.2 = xor i64 %or.i258.4.2, %add82.4.2
  %shr.i260.4.2 = lshr i64 %xor2.i259.4.2, 63
  %add89.4.2 = add i64 %shr.i260.4.2, %xor2569.i290.4.2
  %add90.4.2 = add i64 %add89.4.2, %add90.4.1
  %134 = xor i64 %add89.4.2, -9223372036854775808
  %xor2.i253.4.2 = and i64 %134, %add23.i289.4.2
  %xor.i244.4.2 = xor i64 %add90.4.2, %add89.4.2
  %xor1.i245.4.2 = xor i64 %add89.4.2, %add90.4.1
  %or.i246.4.2 = or i64 %xor.i244.4.2, %xor1.i245.4.2
  %xor2.i247.4.2 = xor i64 %or.i246.4.2, %add90.4.2
  %shr.i254326.4.2 = or i64 %xor2.i247.4.2, %xor2.i253.4.2
  %or94325.4.2 = lshr i64 %shr.i254326.4.2, 63
  %add96.4.2 = add nsw i64 %or94325.4.2, %add96.4.1
  %arrayidx104.4 = getelementptr inbounds i64, i64* %ma, i64 12
  %135 = load i64, i64* %arrayidx104.4, align 8, !tbaa !3
  %add105.4 = add i64 %135, %add82.4.2
  %xor.i238.4 = xor i64 %add105.4, %add82.4.2
  %xor1.i239.4 = xor i64 %135, %add82.4.2
  %or.i240.4 = or i64 %xor.i238.4, %xor1.i239.4
  %xor2.i241.4 = xor i64 %or.i240.4, %add105.4
  %shr.i242.4 = lshr i64 %xor2.i241.4, 63
  %add111.4 = add i64 %shr.i242.4, %add90.4.2
  store i64 %add105.4, i64* %arrayidx50.4, align 8, !tbaa !3
  %136 = xor i64 %add111.4, -9223372036854775808
  %xor2.i.4 = and i64 %136, %add90.4.2
  %shr.i237.4 = lshr i64 %xor2.i.4, 63
  %add118.4 = add nsw i64 %shr.i237.4, %add96.4.2
  %arrayidx74.5 = getelementptr inbounds i64, i64* %mc, i64 6
  %137 = load i64, i64* %arrayidx74.5, align 8, !tbaa !3
  %138 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.5 = and i64 %137, 4294967295
  %shr.i263.5 = lshr i64 %137, 32
  %and1.i264.5 = and i64 %138, 4294967295
  %shr2.i265.5 = lshr i64 %138, 32
  %mul.i266.5 = mul nuw i64 %and1.i264.5, %and.i262.5
  %mul3.i267.5 = mul nuw i64 %shr2.i265.5, %and.i262.5
  %mul4.i268.5 = mul nuw i64 %and1.i264.5, %shr.i263.5
  %mul5.i269.5 = mul nuw i64 %shr2.i265.5, %shr.i263.5
  %and6.i270.5 = and i64 %mul.i266.5, 4294967295
  %shr7.i271.5 = lshr i64 %mul.i266.5, 32
  %and8.i272.5 = and i64 %mul4.i268.5, 4294967295
  %and9.i273.5 = and i64 %mul3.i267.5, 4294967295
  %add.i274.5 = add nuw nsw i64 %shr7.i271.5, %and8.i272.5
  %add10.i275.5 = add nuw nsw i64 %add.i274.5, %and9.i273.5
  %shr11.i276.5 = lshr i64 %add10.i275.5, 32
  %shl.i277.5 = shl i64 %add10.i275.5, 32
  %xor68.i278.5 = or i64 %shl.i277.5, %and6.i270.5
  %shr13.i279.5 = lshr i64 %mul4.i268.5, 32
  %shr14.i280.5 = lshr i64 %mul3.i267.5, 32
  %and15.i281.5 = and i64 %mul5.i269.5, 4294967295
  %add16.i282.5 = add nuw nsw i64 %shr13.i279.5, %shr14.i280.5
  %add17.i283.5 = add nuw nsw i64 %add16.i282.5, %and15.i281.5
  %add18.i284.5 = add nuw nsw i64 %add17.i283.5, %shr11.i276.5
  %and19.i285.5 = and i64 %add18.i284.5, 4294967295
  %and21.i287.5 = and i64 %add18.i284.5, 30064771072
  %and22.i288.5 = and i64 %mul5.i269.5, -4294967296
  %add23.i289.5 = add i64 %and21.i287.5, %and22.i288.5
  %xor2569.i290.5 = or i64 %add23.i289.5, %and19.i285.5
  %add82.5 = add i64 %xor68.i278.5, %add111.4
  %xor.i256.5 = xor i64 %add82.5, %shl.i277.5
  %xor1.i257.5 = xor i64 %shl.i277.5, %add111.4
  %or.i258.5 = or i64 %xor.i256.5, %xor1.i257.5
  %xor2.i259.5 = xor i64 %or.i258.5, %add82.5
  %shr.i260.5 = lshr i64 %xor2.i259.5, 63
  %add89.5 = add i64 %shr.i260.5, %xor2569.i290.5
  %add90.5 = add i64 %add89.5, %add118.4
  %139 = xor i64 %add89.5, -9223372036854775808
  %xor2.i253.5 = and i64 %139, %add23.i289.5
  %xor.i244.5 = xor i64 %add90.5, %add89.5
  %xor1.i245.5 = xor i64 %add89.5, %add118.4
  %or.i246.5 = or i64 %xor.i244.5, %xor1.i245.5
  %xor2.i247.5 = xor i64 %or.i246.5, %add90.5
  %shr.i254326.5 = or i64 %xor2.i247.5, %xor2.i253.5
  %or94325.5 = lshr i64 %shr.i254326.5, 63
  %arrayidx74.5.1 = getelementptr inbounds i64, i64* %mc, i64 7
  %140 = load i64, i64* %arrayidx74.5.1, align 8, !tbaa !3
  %141 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.5.1 = and i64 %140, 4294967295
  %shr.i263.5.1 = lshr i64 %140, 32
  %and1.i264.5.1 = and i64 %141, 4294967295
  %shr2.i265.5.1 = lshr i64 %141, 32
  %mul.i266.5.1 = mul nuw i64 %and1.i264.5.1, %and.i262.5.1
  %mul3.i267.5.1 = mul nuw i64 %shr2.i265.5.1, %and.i262.5.1
  %mul4.i268.5.1 = mul nuw i64 %and1.i264.5.1, %shr.i263.5.1
  %mul5.i269.5.1 = mul nuw i64 %shr2.i265.5.1, %shr.i263.5.1
  %and6.i270.5.1 = and i64 %mul.i266.5.1, 4294967295
  %shr7.i271.5.1 = lshr i64 %mul.i266.5.1, 32
  %and8.i272.5.1 = and i64 %mul4.i268.5.1, 4294967295
  %and9.i273.5.1 = and i64 %mul3.i267.5.1, 4294967295
  %add.i274.5.1 = add nuw nsw i64 %shr7.i271.5.1, %and8.i272.5.1
  %add10.i275.5.1 = add nuw nsw i64 %add.i274.5.1, %and9.i273.5.1
  %shr11.i276.5.1 = lshr i64 %add10.i275.5.1, 32
  %shl.i277.5.1 = shl i64 %add10.i275.5.1, 32
  %xor68.i278.5.1 = or i64 %shl.i277.5.1, %and6.i270.5.1
  %shr13.i279.5.1 = lshr i64 %mul4.i268.5.1, 32
  %shr14.i280.5.1 = lshr i64 %mul3.i267.5.1, 32
  %and15.i281.5.1 = and i64 %mul5.i269.5.1, 4294967295
  %add16.i282.5.1 = add nuw nsw i64 %shr13.i279.5.1, %shr14.i280.5.1
  %add17.i283.5.1 = add nuw nsw i64 %add16.i282.5.1, %and15.i281.5.1
  %add18.i284.5.1 = add nuw nsw i64 %add17.i283.5.1, %shr11.i276.5.1
  %and19.i285.5.1 = and i64 %add18.i284.5.1, 4294967295
  %and21.i287.5.1 = and i64 %add18.i284.5.1, 30064771072
  %and22.i288.5.1 = and i64 %mul5.i269.5.1, -4294967296
  %add23.i289.5.1 = add i64 %and21.i287.5.1, %and22.i288.5.1
  %xor2569.i290.5.1 = or i64 %add23.i289.5.1, %and19.i285.5.1
  %add82.5.1 = add i64 %xor68.i278.5.1, %add82.5
  %xor.i256.5.1 = xor i64 %add82.5.1, %shl.i277.5.1
  %xor1.i257.5.1 = xor i64 %shl.i277.5.1, %add82.5
  %or.i258.5.1 = or i64 %xor.i256.5.1, %xor1.i257.5.1
  %xor2.i259.5.1 = xor i64 %or.i258.5.1, %add82.5.1
  %shr.i260.5.1 = lshr i64 %xor2.i259.5.1, 63
  %add89.5.1 = add i64 %shr.i260.5.1, %xor2569.i290.5.1
  %add90.5.1 = add i64 %add89.5.1, %add90.5
  %142 = xor i64 %add89.5.1, -9223372036854775808
  %xor2.i253.5.1 = and i64 %142, %add23.i289.5.1
  %xor.i244.5.1 = xor i64 %add90.5.1, %add89.5.1
  %xor1.i245.5.1 = xor i64 %add89.5.1, %add90.5
  %or.i246.5.1 = or i64 %xor.i244.5.1, %xor1.i245.5.1
  %xor2.i247.5.1 = xor i64 %or.i246.5.1, %add90.5.1
  %shr.i254326.5.1 = or i64 %xor2.i247.5.1, %xor2.i253.5.1
  %or94325.5.1 = lshr i64 %shr.i254326.5.1, 63
  %add96.5.1 = add nuw nsw i64 %or94325.5.1, %or94325.5
  %arrayidx104.5 = getelementptr inbounds i64, i64* %ma, i64 13
  %143 = load i64, i64* %arrayidx104.5, align 8, !tbaa !3
  %add105.5 = add i64 %143, %add82.5.1
  %xor.i238.5 = xor i64 %add105.5, %add82.5.1
  %xor1.i239.5 = xor i64 %143, %add82.5.1
  %or.i240.5 = or i64 %xor.i238.5, %xor1.i239.5
  %xor2.i241.5 = xor i64 %or.i240.5, %add105.5
  %shr.i242.5 = lshr i64 %xor2.i241.5, 63
  %add111.5 = add i64 %shr.i242.5, %add90.5.1
  store i64 %add105.5, i64* %arrayidx50.5, align 8, !tbaa !3
  %144 = xor i64 %add111.5, -9223372036854775808
  %xor2.i.5 = and i64 %144, %add90.5.1
  %shr.i237.5 = lshr i64 %xor2.i.5, 63
  %add118.5 = add nsw i64 %shr.i237.5, %add96.5.1
  %arrayidx74.6 = getelementptr inbounds i64, i64* %mc, i64 7
  %145 = load i64, i64* %arrayidx74.6, align 8, !tbaa !3
  %146 = load i64, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @p503p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.6 = and i64 %145, 4294967295
  %shr.i263.6 = lshr i64 %145, 32
  %and1.i264.6 = and i64 %146, 4294967295
  %shr2.i265.6 = lshr i64 %146, 32
  %mul.i266.6 = mul nuw i64 %and1.i264.6, %and.i262.6
  %mul3.i267.6 = mul nuw i64 %shr2.i265.6, %and.i262.6
  %mul4.i268.6 = mul nuw i64 %and1.i264.6, %shr.i263.6
  %mul5.i269.6 = mul nuw i64 %shr2.i265.6, %shr.i263.6
  %and6.i270.6 = and i64 %mul.i266.6, 4294967295
  %shr7.i271.6 = lshr i64 %mul.i266.6, 32
  %and8.i272.6 = and i64 %mul4.i268.6, 4294967295
  %and9.i273.6 = and i64 %mul3.i267.6, 4294967295
  %add.i274.6 = add nuw nsw i64 %shr7.i271.6, %and8.i272.6
  %add10.i275.6 = add nuw nsw i64 %add.i274.6, %and9.i273.6
  %shr11.i276.6 = lshr i64 %add10.i275.6, 32
  %shl.i277.6 = shl i64 %add10.i275.6, 32
  %xor68.i278.6 = or i64 %shl.i277.6, %and6.i270.6
  %shr13.i279.6 = lshr i64 %mul4.i268.6, 32
  %shr14.i280.6 = lshr i64 %mul3.i267.6, 32
  %and15.i281.6 = and i64 %mul5.i269.6, 4294967295
  %add16.i282.6 = add nuw nsw i64 %shr13.i279.6, %shr14.i280.6
  %add17.i283.6 = add nuw nsw i64 %add16.i282.6, %and15.i281.6
  %add18.i284.6 = add nuw nsw i64 %add17.i283.6, %shr11.i276.6
  %and19.i285.6 = and i64 %add18.i284.6, 4294967295
  %and21.i287.6 = and i64 %add18.i284.6, 30064771072
  %and22.i288.6 = and i64 %mul5.i269.6, -4294967296
  %add23.i289.6 = add i64 %and21.i287.6, %and22.i288.6
  %xor2569.i290.6 = or i64 %add23.i289.6, %and19.i285.6
  %add82.6 = add i64 %xor68.i278.6, %add111.5
  %xor.i256.6 = xor i64 %add82.6, %shl.i277.6
  %xor1.i257.6 = xor i64 %shl.i277.6, %add111.5
  %or.i258.6 = or i64 %xor.i256.6, %xor1.i257.6
  %xor2.i259.6 = xor i64 %or.i258.6, %add82.6
  %shr.i260.6 = lshr i64 %xor2.i259.6, 63
  %add89.6 = add i64 %shr.i260.6, %xor2569.i290.6
  %add90.6 = add i64 %add89.6, %add118.5
  %arrayidx104.6 = getelementptr inbounds i64, i64* %ma, i64 14
  %147 = load i64, i64* %arrayidx104.6, align 8, !tbaa !3
  %add105.6 = add i64 %147, %add82.6
  %xor.i238.6 = xor i64 %add105.6, %add82.6
  %xor1.i239.6 = xor i64 %147, %add82.6
  %or.i240.6 = or i64 %xor.i238.6, %xor1.i239.6
  %xor2.i241.6 = xor i64 %or.i240.6, %add105.6
  %shr.i242.6 = lshr i64 %xor2.i241.6, 63
  %add111.6 = add i64 %shr.i242.6, %add90.6
  store i64 %add105.6, i64* %arrayidx50.6, align 8, !tbaa !3
  %arrayidx127 = getelementptr inbounds i64, i64* %ma, i64 15
  %148 = load i64, i64* %arrayidx127, align 8, !tbaa !3
  %add128 = add i64 %148, %add111.6
  store i64 %add128, i64* %arrayidx50.7, align 8, !tbaa !3
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1) #4

attributes #0 = { inlinehint norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { argmemonly nounwind }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{!"clang version 7.0.0 (tags/RELEASE_700/final)"}
!3 = !{!4, !4, i64 0}
!4 = !{!"long long", !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C/C++ TBAA"}

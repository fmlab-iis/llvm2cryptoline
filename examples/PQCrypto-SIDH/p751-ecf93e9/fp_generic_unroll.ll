; ModuleID = 'src/P751/generic/fp_generic.c'
source_filename = "src/P751/generic/fp_generic.c"
target datalayout = "e-m:o-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-apple-macosx10.13.0"

@p751x2 = external local_unnamed_addr constant [12 x i64], align 16
@p751x4 = external local_unnamed_addr constant [12 x i64], align 16
@p751 = external local_unnamed_addr constant [12 x i64], align 16
@p751p1 = external local_unnamed_addr constant [12 x i64], align 16

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @mp_sub751_p2(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i74.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i74.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  %arrayidx10.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %sub8.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i74.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i74.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  %arrayidx10.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %sub8.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i74.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i74.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  %arrayidx10.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %sub8.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i74.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i74.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  %arrayidx10.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %sub8.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i74.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i74.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  %arrayidx10.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %sub8.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i74.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i74.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  %arrayidx10.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %sub8.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i74.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i74.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  %arrayidx10.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %sub8.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %arrayidx.8 = getelementptr inbounds i64, i64* %a, i64 8
  %16 = load i64, i64* %arrayidx.8, align 8, !tbaa !3
  %arrayidx2.8 = getelementptr inbounds i64, i64* %b, i64 8
  %17 = load i64, i64* %arrayidx2.8, align 8, !tbaa !3
  %sub.8 = sub i64 %16, %17
  %xor.i.8 = xor i64 %17, %16
  %xor1.i.8 = xor i64 %sub.8, %17
  %or.i.8 = or i64 %xor1.i.8, %xor.i.8
  %xor2.i.8 = xor i64 %or.i.8, %16
  %shr.i.8 = lshr i64 %xor2.i.8, 63
  %conv.i.8 = trunc i64 %shr.i.8 to i32
  %sub.i.i.8 = sub i64 0, %sub.8
  %or.i.i.8 = or i64 %sub.8, %sub.i.i.8
  %shr.i.i.8 = lshr i64 %or.i.i.8, 63
  %conv.i.i.8 = trunc i64 %shr.i.i.8 to i32
  %xor.i74.8 = xor i32 %conv.i.i.8, 1
  %and.8 = and i32 %xor.i74.8, %or.7
  %or.8 = or i32 %and.8, %conv.i.8
  %conv.8 = zext i32 %or.7 to i64
  %sub8.8 = sub i64 %sub.8, %conv.8
  %arrayidx10.8 = getelementptr inbounds i64, i64* %c, i64 8
  store i64 %sub8.8, i64* %arrayidx10.8, align 8, !tbaa !3
  %arrayidx.9 = getelementptr inbounds i64, i64* %a, i64 9
  %18 = load i64, i64* %arrayidx.9, align 8, !tbaa !3
  %arrayidx2.9 = getelementptr inbounds i64, i64* %b, i64 9
  %19 = load i64, i64* %arrayidx2.9, align 8, !tbaa !3
  %sub.9 = sub i64 %18, %19
  %xor.i.9 = xor i64 %19, %18
  %xor1.i.9 = xor i64 %sub.9, %19
  %or.i.9 = or i64 %xor1.i.9, %xor.i.9
  %xor2.i.9 = xor i64 %or.i.9, %18
  %shr.i.9 = lshr i64 %xor2.i.9, 63
  %conv.i.9 = trunc i64 %shr.i.9 to i32
  %sub.i.i.9 = sub i64 0, %sub.9
  %or.i.i.9 = or i64 %sub.9, %sub.i.i.9
  %shr.i.i.9 = lshr i64 %or.i.i.9, 63
  %conv.i.i.9 = trunc i64 %shr.i.i.9 to i32
  %xor.i74.9 = xor i32 %conv.i.i.9, 1
  %and.9 = and i32 %xor.i74.9, %or.8
  %or.9 = or i32 %and.9, %conv.i.9
  %conv.9 = zext i32 %or.8 to i64
  %sub8.9 = sub i64 %sub.9, %conv.9
  %arrayidx10.9 = getelementptr inbounds i64, i64* %c, i64 9
  store i64 %sub8.9, i64* %arrayidx10.9, align 8, !tbaa !3
  %arrayidx.10 = getelementptr inbounds i64, i64* %a, i64 10
  %20 = load i64, i64* %arrayidx.10, align 8, !tbaa !3
  %arrayidx2.10 = getelementptr inbounds i64, i64* %b, i64 10
  %21 = load i64, i64* %arrayidx2.10, align 8, !tbaa !3
  %sub.10 = sub i64 %20, %21
  %xor.i.10 = xor i64 %21, %20
  %xor1.i.10 = xor i64 %sub.10, %21
  %or.i.10 = or i64 %xor1.i.10, %xor.i.10
  %xor2.i.10 = xor i64 %or.i.10, %20
  %shr.i.10 = lshr i64 %xor2.i.10, 63
  %conv.i.10 = trunc i64 %shr.i.10 to i32
  %sub.i.i.10 = sub i64 0, %sub.10
  %or.i.i.10 = or i64 %sub.10, %sub.i.i.10
  %shr.i.i.10 = lshr i64 %or.i.i.10, 63
  %conv.i.i.10 = trunc i64 %shr.i.i.10 to i32
  %xor.i74.10 = xor i32 %conv.i.i.10, 1
  %and.10 = and i32 %xor.i74.10, %or.9
  %or.10 = or i32 %and.10, %conv.i.10
  %conv.10 = zext i32 %or.9 to i64
  %sub8.10 = sub i64 %sub.10, %conv.10
  %arrayidx10.10 = getelementptr inbounds i64, i64* %c, i64 10
  store i64 %sub8.10, i64* %arrayidx10.10, align 8, !tbaa !3
  %arrayidx.11 = getelementptr inbounds i64, i64* %a, i64 11
  %22 = load i64, i64* %arrayidx.11, align 8, !tbaa !3
  %arrayidx2.11 = getelementptr inbounds i64, i64* %b, i64 11
  %23 = load i64, i64* %arrayidx2.11, align 8, !tbaa !3
  %sub.11 = sub i64 %22, %23
  %conv.11 = zext i32 %or.10 to i64
  %sub8.11 = sub i64 %sub.11, %conv.11
  %arrayidx10.11 = getelementptr inbounds i64, i64* %c, i64 11
  store i64 %sub8.11, i64* %arrayidx10.11, align 8, !tbaa !3
  %24 = load i64, i64* %c, align 8, !tbaa !3
  %25 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 0), align 16, !tbaa !3
  %add21 = add i64 %25, %24
  store i64 %add21, i64* %c, align 8, !tbaa !3
  %xor.i61 = xor i64 %add21, %24
  %xor1.i63 = xor i64 %25, %24
  %or.i64 = or i64 %xor.i61, %xor1.i63
  %xor2.i65 = xor i64 %or.i64, %add21
  %or29 = lshr i64 %xor2.i65, 63
  %26 = load i64, i64* %arrayidx10.1, align 8, !tbaa !3
  %add.1 = add i64 %26, %or29
  %27 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 1), align 8, !tbaa !3
  %add21.1 = add i64 %27, %add.1
  store i64 %add21.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %28 = xor i64 %add.1, -9223372036854775808
  %xor2.i71.1 = and i64 %28, %26
  %xor.i61.1 = xor i64 %add21.1, %add.1
  %xor1.i63.1 = xor i64 %27, %add.1
  %or.i64.1 = or i64 %xor.i61.1, %xor1.i63.1
  %xor2.i65.1 = xor i64 %or.i64.1, %add21.1
  %shr.i7275.1 = or i64 %xor2.i65.1, %xor2.i71.1
  %or29.1 = lshr i64 %shr.i7275.1, 63
  %29 = load i64, i64* %arrayidx10.2, align 8, !tbaa !3
  %add.2 = add i64 %29, %or29.1
  %30 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 2), align 16, !tbaa !3
  %add21.2 = add i64 %30, %add.2
  store i64 %add21.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %31 = xor i64 %add.2, -9223372036854775808
  %xor2.i71.2 = and i64 %31, %29
  %xor.i61.2 = xor i64 %add21.2, %add.2
  %xor1.i63.2 = xor i64 %30, %add.2
  %or.i64.2 = or i64 %xor.i61.2, %xor1.i63.2
  %xor2.i65.2 = xor i64 %or.i64.2, %add21.2
  %shr.i7275.2 = or i64 %xor2.i65.2, %xor2.i71.2
  %or29.2 = lshr i64 %shr.i7275.2, 63
  %32 = load i64, i64* %arrayidx10.3, align 8, !tbaa !3
  %add.3 = add i64 %32, %or29.2
  %33 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 3), align 8, !tbaa !3
  %add21.3 = add i64 %33, %add.3
  store i64 %add21.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %34 = xor i64 %add.3, -9223372036854775808
  %xor2.i71.3 = and i64 %34, %32
  %xor.i61.3 = xor i64 %add21.3, %add.3
  %xor1.i63.3 = xor i64 %33, %add.3
  %or.i64.3 = or i64 %xor.i61.3, %xor1.i63.3
  %xor2.i65.3 = xor i64 %or.i64.3, %add21.3
  %shr.i7275.3 = or i64 %xor2.i65.3, %xor2.i71.3
  %or29.3 = lshr i64 %shr.i7275.3, 63
  %35 = load i64, i64* %arrayidx10.4, align 8, !tbaa !3
  %add.4 = add i64 %35, %or29.3
  %36 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 4), align 16, !tbaa !3
  %add21.4 = add i64 %36, %add.4
  store i64 %add21.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %37 = xor i64 %add.4, -9223372036854775808
  %xor2.i71.4 = and i64 %37, %35
  %xor.i61.4 = xor i64 %add21.4, %add.4
  %xor1.i63.4 = xor i64 %36, %add.4
  %or.i64.4 = or i64 %xor.i61.4, %xor1.i63.4
  %xor2.i65.4 = xor i64 %or.i64.4, %add21.4
  %shr.i7275.4 = or i64 %xor2.i65.4, %xor2.i71.4
  %or29.4 = lshr i64 %shr.i7275.4, 63
  %38 = load i64, i64* %arrayidx10.5, align 8, !tbaa !3
  %add.5 = add i64 %38, %or29.4
  %39 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 5), align 8, !tbaa !3
  %add21.5 = add i64 %39, %add.5
  store i64 %add21.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %40 = xor i64 %add.5, -9223372036854775808
  %xor2.i71.5 = and i64 %40, %38
  %xor.i61.5 = xor i64 %add21.5, %add.5
  %xor1.i63.5 = xor i64 %39, %add.5
  %or.i64.5 = or i64 %xor.i61.5, %xor1.i63.5
  %xor2.i65.5 = xor i64 %or.i64.5, %add21.5
  %shr.i7275.5 = or i64 %xor2.i65.5, %xor2.i71.5
  %or29.5 = lshr i64 %shr.i7275.5, 63
  %41 = load i64, i64* %arrayidx10.6, align 8, !tbaa !3
  %add.6 = add i64 %41, %or29.5
  %42 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 6), align 16, !tbaa !3
  %add21.6 = add i64 %42, %add.6
  store i64 %add21.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %43 = xor i64 %add.6, -9223372036854775808
  %xor2.i71.6 = and i64 %43, %41
  %xor.i61.6 = xor i64 %add21.6, %add.6
  %xor1.i63.6 = xor i64 %42, %add.6
  %or.i64.6 = or i64 %xor.i61.6, %xor1.i63.6
  %xor2.i65.6 = xor i64 %or.i64.6, %add21.6
  %shr.i7275.6 = or i64 %xor2.i65.6, %xor2.i71.6
  %or29.6 = lshr i64 %shr.i7275.6, 63
  %44 = load i64, i64* %arrayidx10.7, align 8, !tbaa !3
  %add.7 = add i64 %44, %or29.6
  %45 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 7), align 8, !tbaa !3
  %add21.7 = add i64 %45, %add.7
  store i64 %add21.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %46 = xor i64 %add.7, -9223372036854775808
  %xor2.i71.7 = and i64 %46, %44
  %xor.i61.7 = xor i64 %add21.7, %add.7
  %xor1.i63.7 = xor i64 %45, %add.7
  %or.i64.7 = or i64 %xor.i61.7, %xor1.i63.7
  %xor2.i65.7 = xor i64 %or.i64.7, %add21.7
  %shr.i7275.7 = or i64 %xor2.i65.7, %xor2.i71.7
  %or29.7 = lshr i64 %shr.i7275.7, 63
  %47 = load i64, i64* %arrayidx10.8, align 8, !tbaa !3
  %add.8 = add i64 %47, %or29.7
  %48 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 8), align 16, !tbaa !3
  %add21.8 = add i64 %48, %add.8
  store i64 %add21.8, i64* %arrayidx10.8, align 8, !tbaa !3
  %49 = xor i64 %add.8, -9223372036854775808
  %xor2.i71.8 = and i64 %49, %47
  %xor.i61.8 = xor i64 %add21.8, %add.8
  %xor1.i63.8 = xor i64 %48, %add.8
  %or.i64.8 = or i64 %xor.i61.8, %xor1.i63.8
  %xor2.i65.8 = xor i64 %or.i64.8, %add21.8
  %shr.i7275.8 = or i64 %xor2.i65.8, %xor2.i71.8
  %or29.8 = lshr i64 %shr.i7275.8, 63
  %50 = load i64, i64* %arrayidx10.9, align 8, !tbaa !3
  %add.9 = add i64 %50, %or29.8
  %51 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 9), align 8, !tbaa !3
  %add21.9 = add i64 %51, %add.9
  store i64 %add21.9, i64* %arrayidx10.9, align 8, !tbaa !3
  %52 = xor i64 %add.9, -9223372036854775808
  %xor2.i71.9 = and i64 %52, %50
  %xor.i61.9 = xor i64 %add21.9, %add.9
  %xor1.i63.9 = xor i64 %51, %add.9
  %or.i64.9 = or i64 %xor.i61.9, %xor1.i63.9
  %xor2.i65.9 = xor i64 %or.i64.9, %add21.9
  %shr.i7275.9 = or i64 %xor2.i65.9, %xor2.i71.9
  %or29.9 = lshr i64 %shr.i7275.9, 63
  %53 = load i64, i64* %arrayidx10.10, align 8, !tbaa !3
  %add.10 = add i64 %53, %or29.9
  %54 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 10), align 16, !tbaa !3
  %add21.10 = add i64 %54, %add.10
  store i64 %add21.10, i64* %arrayidx10.10, align 8, !tbaa !3
  %55 = xor i64 %add.10, -9223372036854775808
  %xor2.i71.10 = and i64 %55, %53
  %xor.i61.10 = xor i64 %add21.10, %add.10
  %xor1.i63.10 = xor i64 %54, %add.10
  %or.i64.10 = or i64 %xor.i61.10, %xor1.i63.10
  %xor2.i65.10 = xor i64 %or.i64.10, %add21.10
  %shr.i7275.10 = or i64 %xor2.i65.10, %xor2.i71.10
  %or29.10 = lshr i64 %shr.i7275.10, 63
  %56 = load i64, i64* %arrayidx10.11, align 8, !tbaa !3
  %add.11 = add i64 %56, %or29.10
  %57 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 11), align 8, !tbaa !3
  %add21.11 = add i64 %57, %add.11
  store i64 %add21.11, i64* %arrayidx10.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @mp_sub751_p4(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i74.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i74.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  %arrayidx10.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %sub8.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i74.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i74.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  %arrayidx10.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %sub8.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i74.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i74.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  %arrayidx10.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %sub8.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i74.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i74.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  %arrayidx10.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %sub8.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i74.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i74.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  %arrayidx10.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %sub8.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i74.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i74.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  %arrayidx10.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %sub8.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i74.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i74.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  %arrayidx10.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %sub8.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %arrayidx.8 = getelementptr inbounds i64, i64* %a, i64 8
  %16 = load i64, i64* %arrayidx.8, align 8, !tbaa !3
  %arrayidx2.8 = getelementptr inbounds i64, i64* %b, i64 8
  %17 = load i64, i64* %arrayidx2.8, align 8, !tbaa !3
  %sub.8 = sub i64 %16, %17
  %xor.i.8 = xor i64 %17, %16
  %xor1.i.8 = xor i64 %sub.8, %17
  %or.i.8 = or i64 %xor1.i.8, %xor.i.8
  %xor2.i.8 = xor i64 %or.i.8, %16
  %shr.i.8 = lshr i64 %xor2.i.8, 63
  %conv.i.8 = trunc i64 %shr.i.8 to i32
  %sub.i.i.8 = sub i64 0, %sub.8
  %or.i.i.8 = or i64 %sub.8, %sub.i.i.8
  %shr.i.i.8 = lshr i64 %or.i.i.8, 63
  %conv.i.i.8 = trunc i64 %shr.i.i.8 to i32
  %xor.i74.8 = xor i32 %conv.i.i.8, 1
  %and.8 = and i32 %xor.i74.8, %or.7
  %or.8 = or i32 %and.8, %conv.i.8
  %conv.8 = zext i32 %or.7 to i64
  %sub8.8 = sub i64 %sub.8, %conv.8
  %arrayidx10.8 = getelementptr inbounds i64, i64* %c, i64 8
  store i64 %sub8.8, i64* %arrayidx10.8, align 8, !tbaa !3
  %arrayidx.9 = getelementptr inbounds i64, i64* %a, i64 9
  %18 = load i64, i64* %arrayidx.9, align 8, !tbaa !3
  %arrayidx2.9 = getelementptr inbounds i64, i64* %b, i64 9
  %19 = load i64, i64* %arrayidx2.9, align 8, !tbaa !3
  %sub.9 = sub i64 %18, %19
  %xor.i.9 = xor i64 %19, %18
  %xor1.i.9 = xor i64 %sub.9, %19
  %or.i.9 = or i64 %xor1.i.9, %xor.i.9
  %xor2.i.9 = xor i64 %or.i.9, %18
  %shr.i.9 = lshr i64 %xor2.i.9, 63
  %conv.i.9 = trunc i64 %shr.i.9 to i32
  %sub.i.i.9 = sub i64 0, %sub.9
  %or.i.i.9 = or i64 %sub.9, %sub.i.i.9
  %shr.i.i.9 = lshr i64 %or.i.i.9, 63
  %conv.i.i.9 = trunc i64 %shr.i.i.9 to i32
  %xor.i74.9 = xor i32 %conv.i.i.9, 1
  %and.9 = and i32 %xor.i74.9, %or.8
  %or.9 = or i32 %and.9, %conv.i.9
  %conv.9 = zext i32 %or.8 to i64
  %sub8.9 = sub i64 %sub.9, %conv.9
  %arrayidx10.9 = getelementptr inbounds i64, i64* %c, i64 9
  store i64 %sub8.9, i64* %arrayidx10.9, align 8, !tbaa !3
  %arrayidx.10 = getelementptr inbounds i64, i64* %a, i64 10
  %20 = load i64, i64* %arrayidx.10, align 8, !tbaa !3
  %arrayidx2.10 = getelementptr inbounds i64, i64* %b, i64 10
  %21 = load i64, i64* %arrayidx2.10, align 8, !tbaa !3
  %sub.10 = sub i64 %20, %21
  %xor.i.10 = xor i64 %21, %20
  %xor1.i.10 = xor i64 %sub.10, %21
  %or.i.10 = or i64 %xor1.i.10, %xor.i.10
  %xor2.i.10 = xor i64 %or.i.10, %20
  %shr.i.10 = lshr i64 %xor2.i.10, 63
  %conv.i.10 = trunc i64 %shr.i.10 to i32
  %sub.i.i.10 = sub i64 0, %sub.10
  %or.i.i.10 = or i64 %sub.10, %sub.i.i.10
  %shr.i.i.10 = lshr i64 %or.i.i.10, 63
  %conv.i.i.10 = trunc i64 %shr.i.i.10 to i32
  %xor.i74.10 = xor i32 %conv.i.i.10, 1
  %and.10 = and i32 %xor.i74.10, %or.9
  %or.10 = or i32 %and.10, %conv.i.10
  %conv.10 = zext i32 %or.9 to i64
  %sub8.10 = sub i64 %sub.10, %conv.10
  %arrayidx10.10 = getelementptr inbounds i64, i64* %c, i64 10
  store i64 %sub8.10, i64* %arrayidx10.10, align 8, !tbaa !3
  %arrayidx.11 = getelementptr inbounds i64, i64* %a, i64 11
  %22 = load i64, i64* %arrayidx.11, align 8, !tbaa !3
  %arrayidx2.11 = getelementptr inbounds i64, i64* %b, i64 11
  %23 = load i64, i64* %arrayidx2.11, align 8, !tbaa !3
  %sub.11 = sub i64 %22, %23
  %conv.11 = zext i32 %or.10 to i64
  %sub8.11 = sub i64 %sub.11, %conv.11
  %arrayidx10.11 = getelementptr inbounds i64, i64* %c, i64 11
  store i64 %sub8.11, i64* %arrayidx10.11, align 8, !tbaa !3
  %24 = load i64, i64* %c, align 8, !tbaa !3
  %25 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 0), align 16, !tbaa !3
  %add21 = add i64 %25, %24
  store i64 %add21, i64* %c, align 8, !tbaa !3
  %xor.i61 = xor i64 %add21, %24
  %xor1.i63 = xor i64 %25, %24
  %or.i64 = or i64 %xor.i61, %xor1.i63
  %xor2.i65 = xor i64 %or.i64, %add21
  %or29 = lshr i64 %xor2.i65, 63
  %26 = load i64, i64* %arrayidx10.1, align 8, !tbaa !3
  %add.1 = add i64 %26, %or29
  %27 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 1), align 8, !tbaa !3
  %add21.1 = add i64 %27, %add.1
  store i64 %add21.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %28 = xor i64 %add.1, -9223372036854775808
  %xor2.i71.1 = and i64 %28, %26
  %xor.i61.1 = xor i64 %add21.1, %add.1
  %xor1.i63.1 = xor i64 %27, %add.1
  %or.i64.1 = or i64 %xor.i61.1, %xor1.i63.1
  %xor2.i65.1 = xor i64 %or.i64.1, %add21.1
  %shr.i7275.1 = or i64 %xor2.i65.1, %xor2.i71.1
  %or29.1 = lshr i64 %shr.i7275.1, 63
  %29 = load i64, i64* %arrayidx10.2, align 8, !tbaa !3
  %add.2 = add i64 %29, %or29.1
  %30 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 2), align 16, !tbaa !3
  %add21.2 = add i64 %30, %add.2
  store i64 %add21.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %31 = xor i64 %add.2, -9223372036854775808
  %xor2.i71.2 = and i64 %31, %29
  %xor.i61.2 = xor i64 %add21.2, %add.2
  %xor1.i63.2 = xor i64 %30, %add.2
  %or.i64.2 = or i64 %xor.i61.2, %xor1.i63.2
  %xor2.i65.2 = xor i64 %or.i64.2, %add21.2
  %shr.i7275.2 = or i64 %xor2.i65.2, %xor2.i71.2
  %or29.2 = lshr i64 %shr.i7275.2, 63
  %32 = load i64, i64* %arrayidx10.3, align 8, !tbaa !3
  %add.3 = add i64 %32, %or29.2
  %33 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 3), align 8, !tbaa !3
  %add21.3 = add i64 %33, %add.3
  store i64 %add21.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %34 = xor i64 %add.3, -9223372036854775808
  %xor2.i71.3 = and i64 %34, %32
  %xor.i61.3 = xor i64 %add21.3, %add.3
  %xor1.i63.3 = xor i64 %33, %add.3
  %or.i64.3 = or i64 %xor.i61.3, %xor1.i63.3
  %xor2.i65.3 = xor i64 %or.i64.3, %add21.3
  %shr.i7275.3 = or i64 %xor2.i65.3, %xor2.i71.3
  %or29.3 = lshr i64 %shr.i7275.3, 63
  %35 = load i64, i64* %arrayidx10.4, align 8, !tbaa !3
  %add.4 = add i64 %35, %or29.3
  %36 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 4), align 16, !tbaa !3
  %add21.4 = add i64 %36, %add.4
  store i64 %add21.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %37 = xor i64 %add.4, -9223372036854775808
  %xor2.i71.4 = and i64 %37, %35
  %xor.i61.4 = xor i64 %add21.4, %add.4
  %xor1.i63.4 = xor i64 %36, %add.4
  %or.i64.4 = or i64 %xor.i61.4, %xor1.i63.4
  %xor2.i65.4 = xor i64 %or.i64.4, %add21.4
  %shr.i7275.4 = or i64 %xor2.i65.4, %xor2.i71.4
  %or29.4 = lshr i64 %shr.i7275.4, 63
  %38 = load i64, i64* %arrayidx10.5, align 8, !tbaa !3
  %add.5 = add i64 %38, %or29.4
  %39 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 5), align 8, !tbaa !3
  %add21.5 = add i64 %39, %add.5
  store i64 %add21.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %40 = xor i64 %add.5, -9223372036854775808
  %xor2.i71.5 = and i64 %40, %38
  %xor.i61.5 = xor i64 %add21.5, %add.5
  %xor1.i63.5 = xor i64 %39, %add.5
  %or.i64.5 = or i64 %xor.i61.5, %xor1.i63.5
  %xor2.i65.5 = xor i64 %or.i64.5, %add21.5
  %shr.i7275.5 = or i64 %xor2.i65.5, %xor2.i71.5
  %or29.5 = lshr i64 %shr.i7275.5, 63
  %41 = load i64, i64* %arrayidx10.6, align 8, !tbaa !3
  %add.6 = add i64 %41, %or29.5
  %42 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 6), align 16, !tbaa !3
  %add21.6 = add i64 %42, %add.6
  store i64 %add21.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %43 = xor i64 %add.6, -9223372036854775808
  %xor2.i71.6 = and i64 %43, %41
  %xor.i61.6 = xor i64 %add21.6, %add.6
  %xor1.i63.6 = xor i64 %42, %add.6
  %or.i64.6 = or i64 %xor.i61.6, %xor1.i63.6
  %xor2.i65.6 = xor i64 %or.i64.6, %add21.6
  %shr.i7275.6 = or i64 %xor2.i65.6, %xor2.i71.6
  %or29.6 = lshr i64 %shr.i7275.6, 63
  %44 = load i64, i64* %arrayidx10.7, align 8, !tbaa !3
  %add.7 = add i64 %44, %or29.6
  %45 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 7), align 8, !tbaa !3
  %add21.7 = add i64 %45, %add.7
  store i64 %add21.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %46 = xor i64 %add.7, -9223372036854775808
  %xor2.i71.7 = and i64 %46, %44
  %xor.i61.7 = xor i64 %add21.7, %add.7
  %xor1.i63.7 = xor i64 %45, %add.7
  %or.i64.7 = or i64 %xor.i61.7, %xor1.i63.7
  %xor2.i65.7 = xor i64 %or.i64.7, %add21.7
  %shr.i7275.7 = or i64 %xor2.i65.7, %xor2.i71.7
  %or29.7 = lshr i64 %shr.i7275.7, 63
  %47 = load i64, i64* %arrayidx10.8, align 8, !tbaa !3
  %add.8 = add i64 %47, %or29.7
  %48 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 8), align 16, !tbaa !3
  %add21.8 = add i64 %48, %add.8
  store i64 %add21.8, i64* %arrayidx10.8, align 8, !tbaa !3
  %49 = xor i64 %add.8, -9223372036854775808
  %xor2.i71.8 = and i64 %49, %47
  %xor.i61.8 = xor i64 %add21.8, %add.8
  %xor1.i63.8 = xor i64 %48, %add.8
  %or.i64.8 = or i64 %xor.i61.8, %xor1.i63.8
  %xor2.i65.8 = xor i64 %or.i64.8, %add21.8
  %shr.i7275.8 = or i64 %xor2.i65.8, %xor2.i71.8
  %or29.8 = lshr i64 %shr.i7275.8, 63
  %50 = load i64, i64* %arrayidx10.9, align 8, !tbaa !3
  %add.9 = add i64 %50, %or29.8
  %51 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 9), align 8, !tbaa !3
  %add21.9 = add i64 %51, %add.9
  store i64 %add21.9, i64* %arrayidx10.9, align 8, !tbaa !3
  %52 = xor i64 %add.9, -9223372036854775808
  %xor2.i71.9 = and i64 %52, %50
  %xor.i61.9 = xor i64 %add21.9, %add.9
  %xor1.i63.9 = xor i64 %51, %add.9
  %or.i64.9 = or i64 %xor.i61.9, %xor1.i63.9
  %xor2.i65.9 = xor i64 %or.i64.9, %add21.9
  %shr.i7275.9 = or i64 %xor2.i65.9, %xor2.i71.9
  %or29.9 = lshr i64 %shr.i7275.9, 63
  %53 = load i64, i64* %arrayidx10.10, align 8, !tbaa !3
  %add.10 = add i64 %53, %or29.9
  %54 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 10), align 16, !tbaa !3
  %add21.10 = add i64 %54, %add.10
  store i64 %add21.10, i64* %arrayidx10.10, align 8, !tbaa !3
  %55 = xor i64 %add.10, -9223372036854775808
  %xor2.i71.10 = and i64 %55, %53
  %xor.i61.10 = xor i64 %add21.10, %add.10
  %xor1.i63.10 = xor i64 %54, %add.10
  %or.i64.10 = or i64 %xor.i61.10, %xor1.i63.10
  %xor2.i65.10 = xor i64 %or.i64.10, %add21.10
  %shr.i7275.10 = or i64 %xor2.i65.10, %xor2.i71.10
  %or29.10 = lshr i64 %shr.i7275.10, 63
  %56 = load i64, i64* %arrayidx10.11, align 8, !tbaa !3
  %add.11 = add i64 %56, %or29.10
  %57 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x4, i64 0, i64 11), align 8, !tbaa !3
  %add21.11 = add i64 %57, %add.11
  store i64 %add21.11, i64* %arrayidx10.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @fpadd751(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %add3 = add i64 %1, %0
  store i64 %add3, i64* %c, align 8, !tbaa !3
  %xor.i122 = xor i64 %add3, %0
  %xor1.i124 = xor i64 %1, %0
  %or.i125 = or i64 %xor.i122, %xor1.i124
  %xor2.i126 = xor i64 %or.i125, %add3
  %or = lshr i64 %xor2.i126, 63
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %add.1 = add i64 %2, %or
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %add3.1 = add i64 %3, %add.1
  %arrayidx5.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %add3.1, i64* %arrayidx5.1, align 8, !tbaa !3
  %4 = xor i64 %add.1, -9223372036854775808
  %xor2.i.1 = and i64 %4, %2
  %xor.i122.1 = xor i64 %add3.1, %add.1
  %xor1.i124.1 = xor i64 %3, %add.1
  %or.i125.1 = or i64 %xor.i122.1, %xor1.i124.1
  %xor2.i126.1 = xor i64 %or.i125.1, %add3.1
  %shr.i130.1 = or i64 %xor2.i126.1, %xor2.i.1
  %or.1 = lshr i64 %shr.i130.1, 63
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %5 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %add.2 = add i64 %5, %or.1
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %6 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %add3.2 = add i64 %6, %add.2
  %arrayidx5.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %add3.2, i64* %arrayidx5.2, align 8, !tbaa !3
  %7 = xor i64 %add.2, -9223372036854775808
  %xor2.i.2 = and i64 %7, %5
  %xor.i122.2 = xor i64 %add3.2, %add.2
  %xor1.i124.2 = xor i64 %6, %add.2
  %or.i125.2 = or i64 %xor.i122.2, %xor1.i124.2
  %xor2.i126.2 = xor i64 %or.i125.2, %add3.2
  %shr.i130.2 = or i64 %xor2.i126.2, %xor2.i.2
  %or.2 = lshr i64 %shr.i130.2, 63
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %8 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %add.3 = add i64 %8, %or.2
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %9 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %add3.3 = add i64 %9, %add.3
  %arrayidx5.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %add3.3, i64* %arrayidx5.3, align 8, !tbaa !3
  %10 = xor i64 %add.3, -9223372036854775808
  %xor2.i.3 = and i64 %10, %8
  %xor.i122.3 = xor i64 %add3.3, %add.3
  %xor1.i124.3 = xor i64 %9, %add.3
  %or.i125.3 = or i64 %xor.i122.3, %xor1.i124.3
  %xor2.i126.3 = xor i64 %or.i125.3, %add3.3
  %shr.i130.3 = or i64 %xor2.i126.3, %xor2.i.3
  %or.3 = lshr i64 %shr.i130.3, 63
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %11 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %add.4 = add i64 %11, %or.3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %12 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %add3.4 = add i64 %12, %add.4
  %arrayidx5.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %add3.4, i64* %arrayidx5.4, align 8, !tbaa !3
  %13 = xor i64 %add.4, -9223372036854775808
  %xor2.i.4 = and i64 %13, %11
  %xor.i122.4 = xor i64 %add3.4, %add.4
  %xor1.i124.4 = xor i64 %12, %add.4
  %or.i125.4 = or i64 %xor.i122.4, %xor1.i124.4
  %xor2.i126.4 = xor i64 %or.i125.4, %add3.4
  %shr.i130.4 = or i64 %xor2.i126.4, %xor2.i.4
  %or.4 = lshr i64 %shr.i130.4, 63
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %14 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %add.5 = add i64 %14, %or.4
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %15 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %add3.5 = add i64 %15, %add.5
  %arrayidx5.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %add3.5, i64* %arrayidx5.5, align 8, !tbaa !3
  %16 = xor i64 %add.5, -9223372036854775808
  %xor2.i.5 = and i64 %16, %14
  %xor.i122.5 = xor i64 %add3.5, %add.5
  %xor1.i124.5 = xor i64 %15, %add.5
  %or.i125.5 = or i64 %xor.i122.5, %xor1.i124.5
  %xor2.i126.5 = xor i64 %or.i125.5, %add3.5
  %shr.i130.5 = or i64 %xor2.i126.5, %xor2.i.5
  %or.5 = lshr i64 %shr.i130.5, 63
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %17 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %add.6 = add i64 %17, %or.5
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %18 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %add3.6 = add i64 %18, %add.6
  %arrayidx5.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %add3.6, i64* %arrayidx5.6, align 8, !tbaa !3
  %19 = xor i64 %add.6, -9223372036854775808
  %xor2.i.6 = and i64 %19, %17
  %xor.i122.6 = xor i64 %add3.6, %add.6
  %xor1.i124.6 = xor i64 %18, %add.6
  %or.i125.6 = or i64 %xor.i122.6, %xor1.i124.6
  %xor2.i126.6 = xor i64 %or.i125.6, %add3.6
  %shr.i130.6 = or i64 %xor2.i126.6, %xor2.i.6
  %or.6 = lshr i64 %shr.i130.6, 63
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %20 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %add.7 = add i64 %20, %or.6
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %21 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %add3.7 = add i64 %21, %add.7
  %arrayidx5.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %add3.7, i64* %arrayidx5.7, align 8, !tbaa !3
  %22 = xor i64 %add.7, -9223372036854775808
  %xor2.i.7 = and i64 %22, %20
  %xor.i122.7 = xor i64 %add3.7, %add.7
  %xor1.i124.7 = xor i64 %21, %add.7
  %or.i125.7 = or i64 %xor.i122.7, %xor1.i124.7
  %xor2.i126.7 = xor i64 %or.i125.7, %add3.7
  %shr.i130.7 = or i64 %xor2.i126.7, %xor2.i.7
  %or.7 = lshr i64 %shr.i130.7, 63
  %arrayidx.8 = getelementptr inbounds i64, i64* %a, i64 8
  %23 = load i64, i64* %arrayidx.8, align 8, !tbaa !3
  %add.8 = add i64 %23, %or.7
  %arrayidx2.8 = getelementptr inbounds i64, i64* %b, i64 8
  %24 = load i64, i64* %arrayidx2.8, align 8, !tbaa !3
  %add3.8 = add i64 %24, %add.8
  %arrayidx5.8 = getelementptr inbounds i64, i64* %c, i64 8
  store i64 %add3.8, i64* %arrayidx5.8, align 8, !tbaa !3
  %25 = xor i64 %add.8, -9223372036854775808
  %xor2.i.8 = and i64 %25, %23
  %xor.i122.8 = xor i64 %add3.8, %add.8
  %xor1.i124.8 = xor i64 %24, %add.8
  %or.i125.8 = or i64 %xor.i122.8, %xor1.i124.8
  %xor2.i126.8 = xor i64 %or.i125.8, %add3.8
  %shr.i130.8 = or i64 %xor2.i126.8, %xor2.i.8
  %or.8 = lshr i64 %shr.i130.8, 63
  %arrayidx.9 = getelementptr inbounds i64, i64* %a, i64 9
  %26 = load i64, i64* %arrayidx.9, align 8, !tbaa !3
  %add.9 = add i64 %26, %or.8
  %arrayidx2.9 = getelementptr inbounds i64, i64* %b, i64 9
  %27 = load i64, i64* %arrayidx2.9, align 8, !tbaa !3
  %add3.9 = add i64 %27, %add.9
  %arrayidx5.9 = getelementptr inbounds i64, i64* %c, i64 9
  store i64 %add3.9, i64* %arrayidx5.9, align 8, !tbaa !3
  %28 = xor i64 %add.9, -9223372036854775808
  %xor2.i.9 = and i64 %28, %26
  %xor.i122.9 = xor i64 %add3.9, %add.9
  %xor1.i124.9 = xor i64 %27, %add.9
  %or.i125.9 = or i64 %xor.i122.9, %xor1.i124.9
  %xor2.i126.9 = xor i64 %or.i125.9, %add3.9
  %shr.i130.9 = or i64 %xor2.i126.9, %xor2.i.9
  %or.9 = lshr i64 %shr.i130.9, 63
  %arrayidx.10 = getelementptr inbounds i64, i64* %a, i64 10
  %29 = load i64, i64* %arrayidx.10, align 8, !tbaa !3
  %add.10 = add i64 %29, %or.9
  %arrayidx2.10 = getelementptr inbounds i64, i64* %b, i64 10
  %30 = load i64, i64* %arrayidx2.10, align 8, !tbaa !3
  %add3.10 = add i64 %30, %add.10
  %arrayidx5.10 = getelementptr inbounds i64, i64* %c, i64 10
  store i64 %add3.10, i64* %arrayidx5.10, align 8, !tbaa !3
  %31 = xor i64 %add.10, -9223372036854775808
  %xor2.i.10 = and i64 %31, %29
  %xor.i122.10 = xor i64 %add3.10, %add.10
  %xor1.i124.10 = xor i64 %30, %add.10
  %or.i125.10 = or i64 %xor.i122.10, %xor1.i124.10
  %xor2.i126.10 = xor i64 %or.i125.10, %add3.10
  %shr.i130.10 = or i64 %xor2.i126.10, %xor2.i.10
  %or.10 = lshr i64 %shr.i130.10, 63
  %arrayidx.11 = getelementptr inbounds i64, i64* %a, i64 11
  %32 = load i64, i64* %arrayidx.11, align 8, !tbaa !3
  %add.11 = add i64 %32, %or.10
  %arrayidx2.11 = getelementptr inbounds i64, i64* %b, i64 11
  %33 = load i64, i64* %arrayidx2.11, align 8, !tbaa !3
  %add3.11 = add i64 %33, %add.11
  %arrayidx5.11 = getelementptr inbounds i64, i64* %c, i64 11
  store i64 %add3.11, i64* %arrayidx5.11, align 8, !tbaa !3
  %34 = load i64, i64* %c, align 8, !tbaa !3
  %35 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 0), align 16, !tbaa !3
  %sub = sub i64 %34, %35
  %xor.i115 = xor i64 %35, %34
  %xor1.i117 = xor i64 %sub, %35
  %or.i118 = or i64 %xor1.i117, %xor.i115
  %xor2.i119 = xor i64 %or.i118, %34
  %shr.i120 = lshr i64 %xor2.i119, 63
  %conv.i121 = trunc i64 %shr.i120 to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %36 = load i64, i64* %arrayidx5.1, align 8, !tbaa !3
  %37 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 1), align 8, !tbaa !3
  %sub.1 = sub i64 %36, %37
  %xor.i115.1 = xor i64 %37, %36
  %xor1.i117.1 = xor i64 %sub.1, %37
  %or.i118.1 = or i64 %xor1.i117.1, %xor.i115.1
  %xor2.i119.1 = xor i64 %or.i118.1, %36
  %shr.i120.1 = lshr i64 %xor2.i119.1, 63
  %conv.i121.1 = trunc i64 %shr.i120.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i114.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i114.1, %conv.i121
  %or25.1 = or i32 %and.1, %conv.i121.1
  %sub27.1 = sub i64 %sub.1, %shr.i120
  store i64 %sub27.1, i64* %arrayidx5.1, align 8, !tbaa !3
  %38 = load i64, i64* %arrayidx5.2, align 8, !tbaa !3
  %39 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 2), align 16, !tbaa !3
  %sub.2 = sub i64 %38, %39
  %xor.i115.2 = xor i64 %39, %38
  %xor1.i117.2 = xor i64 %sub.2, %39
  %or.i118.2 = or i64 %xor1.i117.2, %xor.i115.2
  %xor2.i119.2 = xor i64 %or.i118.2, %38
  %shr.i120.2 = lshr i64 %xor2.i119.2, 63
  %conv.i121.2 = trunc i64 %shr.i120.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i114.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i114.2, %or25.1
  %or25.2 = or i32 %and.2, %conv.i121.2
  %conv26.2 = zext i32 %or25.1 to i64
  %sub27.2 = sub i64 %sub.2, %conv26.2
  store i64 %sub27.2, i64* %arrayidx5.2, align 8, !tbaa !3
  %40 = load i64, i64* %arrayidx5.3, align 8, !tbaa !3
  %41 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 3), align 8, !tbaa !3
  %sub.3 = sub i64 %40, %41
  %xor.i115.3 = xor i64 %41, %40
  %xor1.i117.3 = xor i64 %sub.3, %41
  %or.i118.3 = or i64 %xor1.i117.3, %xor.i115.3
  %xor2.i119.3 = xor i64 %or.i118.3, %40
  %shr.i120.3 = lshr i64 %xor2.i119.3, 63
  %conv.i121.3 = trunc i64 %shr.i120.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i114.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i114.3, %or25.2
  %or25.3 = or i32 %and.3, %conv.i121.3
  %conv26.3 = zext i32 %or25.2 to i64
  %sub27.3 = sub i64 %sub.3, %conv26.3
  store i64 %sub27.3, i64* %arrayidx5.3, align 8, !tbaa !3
  %42 = load i64, i64* %arrayidx5.4, align 8, !tbaa !3
  %43 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 4), align 16, !tbaa !3
  %sub.4 = sub i64 %42, %43
  %xor.i115.4 = xor i64 %43, %42
  %xor1.i117.4 = xor i64 %sub.4, %43
  %or.i118.4 = or i64 %xor1.i117.4, %xor.i115.4
  %xor2.i119.4 = xor i64 %or.i118.4, %42
  %shr.i120.4 = lshr i64 %xor2.i119.4, 63
  %conv.i121.4 = trunc i64 %shr.i120.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i114.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i114.4, %or25.3
  %or25.4 = or i32 %and.4, %conv.i121.4
  %conv26.4 = zext i32 %or25.3 to i64
  %sub27.4 = sub i64 %sub.4, %conv26.4
  store i64 %sub27.4, i64* %arrayidx5.4, align 8, !tbaa !3
  %44 = load i64, i64* %arrayidx5.5, align 8, !tbaa !3
  %45 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 5), align 8, !tbaa !3
  %sub.5 = sub i64 %44, %45
  %xor.i115.5 = xor i64 %45, %44
  %xor1.i117.5 = xor i64 %sub.5, %45
  %or.i118.5 = or i64 %xor1.i117.5, %xor.i115.5
  %xor2.i119.5 = xor i64 %or.i118.5, %44
  %shr.i120.5 = lshr i64 %xor2.i119.5, 63
  %conv.i121.5 = trunc i64 %shr.i120.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i114.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i114.5, %or25.4
  %or25.5 = or i32 %and.5, %conv.i121.5
  %conv26.5 = zext i32 %or25.4 to i64
  %sub27.5 = sub i64 %sub.5, %conv26.5
  store i64 %sub27.5, i64* %arrayidx5.5, align 8, !tbaa !3
  %46 = load i64, i64* %arrayidx5.6, align 8, !tbaa !3
  %47 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 6), align 16, !tbaa !3
  %sub.6 = sub i64 %46, %47
  %xor.i115.6 = xor i64 %47, %46
  %xor1.i117.6 = xor i64 %sub.6, %47
  %or.i118.6 = or i64 %xor1.i117.6, %xor.i115.6
  %xor2.i119.6 = xor i64 %or.i118.6, %46
  %shr.i120.6 = lshr i64 %xor2.i119.6, 63
  %conv.i121.6 = trunc i64 %shr.i120.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i114.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i114.6, %or25.5
  %or25.6 = or i32 %and.6, %conv.i121.6
  %conv26.6 = zext i32 %or25.5 to i64
  %sub27.6 = sub i64 %sub.6, %conv26.6
  store i64 %sub27.6, i64* %arrayidx5.6, align 8, !tbaa !3
  %48 = load i64, i64* %arrayidx5.7, align 8, !tbaa !3
  %49 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 7), align 8, !tbaa !3
  %sub.7 = sub i64 %48, %49
  %xor.i115.7 = xor i64 %49, %48
  %xor1.i117.7 = xor i64 %sub.7, %49
  %or.i118.7 = or i64 %xor1.i117.7, %xor.i115.7
  %xor2.i119.7 = xor i64 %or.i118.7, %48
  %shr.i120.7 = lshr i64 %xor2.i119.7, 63
  %conv.i121.7 = trunc i64 %shr.i120.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i114.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i114.7, %or25.6
  %or25.7 = or i32 %and.7, %conv.i121.7
  %conv26.7 = zext i32 %or25.6 to i64
  %sub27.7 = sub i64 %sub.7, %conv26.7
  store i64 %sub27.7, i64* %arrayidx5.7, align 8, !tbaa !3
  %50 = load i64, i64* %arrayidx5.8, align 8, !tbaa !3
  %51 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 8), align 16, !tbaa !3
  %sub.8 = sub i64 %50, %51
  %xor.i115.8 = xor i64 %51, %50
  %xor1.i117.8 = xor i64 %sub.8, %51
  %or.i118.8 = or i64 %xor1.i117.8, %xor.i115.8
  %xor2.i119.8 = xor i64 %or.i118.8, %50
  %shr.i120.8 = lshr i64 %xor2.i119.8, 63
  %conv.i121.8 = trunc i64 %shr.i120.8 to i32
  %sub.i.i.8 = sub i64 0, %sub.8
  %or.i.i.8 = or i64 %sub.8, %sub.i.i.8
  %shr.i.i.8 = lshr i64 %or.i.i.8, 63
  %conv.i.i.8 = trunc i64 %shr.i.i.8 to i32
  %xor.i114.8 = xor i32 %conv.i.i.8, 1
  %and.8 = and i32 %xor.i114.8, %or25.7
  %or25.8 = or i32 %and.8, %conv.i121.8
  %conv26.8 = zext i32 %or25.7 to i64
  %sub27.8 = sub i64 %sub.8, %conv26.8
  store i64 %sub27.8, i64* %arrayidx5.8, align 8, !tbaa !3
  %52 = load i64, i64* %arrayidx5.9, align 8, !tbaa !3
  %53 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 9), align 8, !tbaa !3
  %sub.9 = sub i64 %52, %53
  %xor.i115.9 = xor i64 %53, %52
  %xor1.i117.9 = xor i64 %sub.9, %53
  %or.i118.9 = or i64 %xor1.i117.9, %xor.i115.9
  %xor2.i119.9 = xor i64 %or.i118.9, %52
  %shr.i120.9 = lshr i64 %xor2.i119.9, 63
  %conv.i121.9 = trunc i64 %shr.i120.9 to i32
  %sub.i.i.9 = sub i64 0, %sub.9
  %or.i.i.9 = or i64 %sub.9, %sub.i.i.9
  %shr.i.i.9 = lshr i64 %or.i.i.9, 63
  %conv.i.i.9 = trunc i64 %shr.i.i.9 to i32
  %xor.i114.9 = xor i32 %conv.i.i.9, 1
  %and.9 = and i32 %xor.i114.9, %or25.8
  %or25.9 = or i32 %and.9, %conv.i121.9
  %conv26.9 = zext i32 %or25.8 to i64
  %sub27.9 = sub i64 %sub.9, %conv26.9
  store i64 %sub27.9, i64* %arrayidx5.9, align 8, !tbaa !3
  %54 = load i64, i64* %arrayidx5.10, align 8, !tbaa !3
  %55 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 10), align 16, !tbaa !3
  %sub.10 = sub i64 %54, %55
  %xor.i115.10 = xor i64 %55, %54
  %xor1.i117.10 = xor i64 %sub.10, %55
  %or.i118.10 = or i64 %xor1.i117.10, %xor.i115.10
  %xor2.i119.10 = xor i64 %or.i118.10, %54
  %shr.i120.10 = lshr i64 %xor2.i119.10, 63
  %conv.i121.10 = trunc i64 %shr.i120.10 to i32
  %sub.i.i.10 = sub i64 0, %sub.10
  %or.i.i.10 = or i64 %sub.10, %sub.i.i.10
  %shr.i.i.10 = lshr i64 %or.i.i.10, 63
  %conv.i.i.10 = trunc i64 %shr.i.i.10 to i32
  %xor.i114.10 = xor i32 %conv.i.i.10, 1
  %and.10 = and i32 %xor.i114.10, %or25.9
  %or25.10 = or i32 %and.10, %conv.i121.10
  %conv26.10 = zext i32 %or25.9 to i64
  %sub27.10 = sub i64 %sub.10, %conv26.10
  store i64 %sub27.10, i64* %arrayidx5.10, align 8, !tbaa !3
  %56 = load i64, i64* %arrayidx5.11, align 8, !tbaa !3
  %57 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 11), align 8, !tbaa !3
  %sub.11 = sub i64 %56, %57
  %xor.i115.11 = xor i64 %57, %56
  %xor1.i117.11 = xor i64 %sub.11, %57
  %or.i118.11 = or i64 %xor1.i117.11, %xor.i115.11
  %xor2.i119.11 = xor i64 %or.i118.11, %56
  %shr.i120.11 = lshr i64 %xor2.i119.11, 63
  %conv.i121.11 = trunc i64 %shr.i120.11 to i32
  %sub.i.i.11 = sub i64 0, %sub.11
  %or.i.i.11 = or i64 %sub.11, %sub.i.i.11
  %shr.i.i.11 = lshr i64 %or.i.i.11, 63
  %conv.i.i.11 = trunc i64 %shr.i.i.11 to i32
  %xor.i114.11 = xor i32 %conv.i.i.11, 1
  %and.11 = and i32 %xor.i114.11, %or25.10
  %or25.11 = or i32 %and.11, %conv.i121.11
  %conv26.11 = zext i32 %or25.10 to i64
  %sub27.11 = sub i64 %sub.11, %conv26.11
  store i64 %sub27.11, i64* %arrayidx5.11, align 8, !tbaa !3
  %conv33 = zext i32 %or25.11 to i64
  %sub34 = sub nsw i64 0, %conv33
  %58 = load i64, i64* %c, align 8, !tbaa !3
  %59 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 0), align 16, !tbaa !3
  %and46 = and i64 %59, %sub34
  %add47 = add i64 %and46, %58
  store i64 %add47, i64* %c, align 8, !tbaa !3
  %xor.i102 = xor i64 %add47, %58
  %xor1.i103 = xor i64 %and46, %58
  %or.i104 = or i64 %xor.i102, %xor1.i103
  %xor2.i105 = xor i64 %or.i104, %add47
  %or55 = lshr i64 %xor2.i105, 63
  %60 = load i64, i64* %arrayidx5.1, align 8, !tbaa !3
  %add43.1 = add i64 %60, %or55
  %61 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 1), align 8, !tbaa !3
  %and46.1 = and i64 %61, %sub34
  %add47.1 = add i64 %and46.1, %add43.1
  store i64 %add47.1, i64* %arrayidx5.1, align 8, !tbaa !3
  %62 = xor i64 %add43.1, -9223372036854775808
  %xor2.i111.1 = and i64 %62, %60
  %xor.i102.1 = xor i64 %add47.1, %add43.1
  %xor1.i103.1 = xor i64 %and46.1, %add43.1
  %or.i104.1 = or i64 %xor.i102.1, %xor1.i103.1
  %xor2.i105.1 = xor i64 %or.i104.1, %add47.1
  %shr.i112129.1 = or i64 %xor2.i105.1, %xor2.i111.1
  %or55.1 = lshr i64 %shr.i112129.1, 63
  %63 = load i64, i64* %arrayidx5.2, align 8, !tbaa !3
  %add43.2 = add i64 %63, %or55.1
  %64 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 2), align 16, !tbaa !3
  %and46.2 = and i64 %64, %sub34
  %add47.2 = add i64 %and46.2, %add43.2
  store i64 %add47.2, i64* %arrayidx5.2, align 8, !tbaa !3
  %65 = xor i64 %add43.2, -9223372036854775808
  %xor2.i111.2 = and i64 %65, %63
  %xor.i102.2 = xor i64 %add47.2, %add43.2
  %xor1.i103.2 = xor i64 %and46.2, %add43.2
  %or.i104.2 = or i64 %xor.i102.2, %xor1.i103.2
  %xor2.i105.2 = xor i64 %or.i104.2, %add47.2
  %shr.i112129.2 = or i64 %xor2.i105.2, %xor2.i111.2
  %or55.2 = lshr i64 %shr.i112129.2, 63
  %66 = load i64, i64* %arrayidx5.3, align 8, !tbaa !3
  %add43.3 = add i64 %66, %or55.2
  %67 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 3), align 8, !tbaa !3
  %and46.3 = and i64 %67, %sub34
  %add47.3 = add i64 %and46.3, %add43.3
  store i64 %add47.3, i64* %arrayidx5.3, align 8, !tbaa !3
  %68 = xor i64 %add43.3, -9223372036854775808
  %xor2.i111.3 = and i64 %68, %66
  %xor.i102.3 = xor i64 %add47.3, %add43.3
  %xor1.i103.3 = xor i64 %and46.3, %add43.3
  %or.i104.3 = or i64 %xor.i102.3, %xor1.i103.3
  %xor2.i105.3 = xor i64 %or.i104.3, %add47.3
  %shr.i112129.3 = or i64 %xor2.i105.3, %xor2.i111.3
  %or55.3 = lshr i64 %shr.i112129.3, 63
  %69 = load i64, i64* %arrayidx5.4, align 8, !tbaa !3
  %add43.4 = add i64 %69, %or55.3
  %70 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 4), align 16, !tbaa !3
  %and46.4 = and i64 %70, %sub34
  %add47.4 = add i64 %and46.4, %add43.4
  store i64 %add47.4, i64* %arrayidx5.4, align 8, !tbaa !3
  %71 = xor i64 %add43.4, -9223372036854775808
  %xor2.i111.4 = and i64 %71, %69
  %xor.i102.4 = xor i64 %add47.4, %add43.4
  %xor1.i103.4 = xor i64 %and46.4, %add43.4
  %or.i104.4 = or i64 %xor.i102.4, %xor1.i103.4
  %xor2.i105.4 = xor i64 %or.i104.4, %add47.4
  %shr.i112129.4 = or i64 %xor2.i105.4, %xor2.i111.4
  %or55.4 = lshr i64 %shr.i112129.4, 63
  %72 = load i64, i64* %arrayidx5.5, align 8, !tbaa !3
  %add43.5 = add i64 %72, %or55.4
  %73 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 5), align 8, !tbaa !3
  %and46.5 = and i64 %73, %sub34
  %add47.5 = add i64 %and46.5, %add43.5
  store i64 %add47.5, i64* %arrayidx5.5, align 8, !tbaa !3
  %74 = xor i64 %add43.5, -9223372036854775808
  %xor2.i111.5 = and i64 %74, %72
  %xor.i102.5 = xor i64 %add47.5, %add43.5
  %xor1.i103.5 = xor i64 %and46.5, %add43.5
  %or.i104.5 = or i64 %xor.i102.5, %xor1.i103.5
  %xor2.i105.5 = xor i64 %or.i104.5, %add47.5
  %shr.i112129.5 = or i64 %xor2.i105.5, %xor2.i111.5
  %or55.5 = lshr i64 %shr.i112129.5, 63
  %75 = load i64, i64* %arrayidx5.6, align 8, !tbaa !3
  %add43.6 = add i64 %75, %or55.5
  %76 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 6), align 16, !tbaa !3
  %and46.6 = and i64 %76, %sub34
  %add47.6 = add i64 %and46.6, %add43.6
  store i64 %add47.6, i64* %arrayidx5.6, align 8, !tbaa !3
  %77 = xor i64 %add43.6, -9223372036854775808
  %xor2.i111.6 = and i64 %77, %75
  %xor.i102.6 = xor i64 %add47.6, %add43.6
  %xor1.i103.6 = xor i64 %and46.6, %add43.6
  %or.i104.6 = or i64 %xor.i102.6, %xor1.i103.6
  %xor2.i105.6 = xor i64 %or.i104.6, %add47.6
  %shr.i112129.6 = or i64 %xor2.i105.6, %xor2.i111.6
  %or55.6 = lshr i64 %shr.i112129.6, 63
  %78 = load i64, i64* %arrayidx5.7, align 8, !tbaa !3
  %add43.7 = add i64 %78, %or55.6
  %79 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 7), align 8, !tbaa !3
  %and46.7 = and i64 %79, %sub34
  %add47.7 = add i64 %and46.7, %add43.7
  store i64 %add47.7, i64* %arrayidx5.7, align 8, !tbaa !3
  %80 = xor i64 %add43.7, -9223372036854775808
  %xor2.i111.7 = and i64 %80, %78
  %xor.i102.7 = xor i64 %add47.7, %add43.7
  %xor1.i103.7 = xor i64 %and46.7, %add43.7
  %or.i104.7 = or i64 %xor.i102.7, %xor1.i103.7
  %xor2.i105.7 = xor i64 %or.i104.7, %add47.7
  %shr.i112129.7 = or i64 %xor2.i105.7, %xor2.i111.7
  %or55.7 = lshr i64 %shr.i112129.7, 63
  %81 = load i64, i64* %arrayidx5.8, align 8, !tbaa !3
  %add43.8 = add i64 %81, %or55.7
  %82 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 8), align 16, !tbaa !3
  %and46.8 = and i64 %82, %sub34
  %add47.8 = add i64 %and46.8, %add43.8
  store i64 %add47.8, i64* %arrayidx5.8, align 8, !tbaa !3
  %83 = xor i64 %add43.8, -9223372036854775808
  %xor2.i111.8 = and i64 %83, %81
  %xor.i102.8 = xor i64 %add47.8, %add43.8
  %xor1.i103.8 = xor i64 %and46.8, %add43.8
  %or.i104.8 = or i64 %xor.i102.8, %xor1.i103.8
  %xor2.i105.8 = xor i64 %or.i104.8, %add47.8
  %shr.i112129.8 = or i64 %xor2.i105.8, %xor2.i111.8
  %or55.8 = lshr i64 %shr.i112129.8, 63
  %84 = load i64, i64* %arrayidx5.9, align 8, !tbaa !3
  %add43.9 = add i64 %84, %or55.8
  %85 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 9), align 8, !tbaa !3
  %and46.9 = and i64 %85, %sub34
  %add47.9 = add i64 %and46.9, %add43.9
  store i64 %add47.9, i64* %arrayidx5.9, align 8, !tbaa !3
  %86 = xor i64 %add43.9, -9223372036854775808
  %xor2.i111.9 = and i64 %86, %84
  %xor.i102.9 = xor i64 %add47.9, %add43.9
  %xor1.i103.9 = xor i64 %and46.9, %add43.9
  %or.i104.9 = or i64 %xor.i102.9, %xor1.i103.9
  %xor2.i105.9 = xor i64 %or.i104.9, %add47.9
  %shr.i112129.9 = or i64 %xor2.i105.9, %xor2.i111.9
  %or55.9 = lshr i64 %shr.i112129.9, 63
  %87 = load i64, i64* %arrayidx5.10, align 8, !tbaa !3
  %add43.10 = add i64 %87, %or55.9
  %88 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 10), align 16, !tbaa !3
  %and46.10 = and i64 %88, %sub34
  %add47.10 = add i64 %and46.10, %add43.10
  store i64 %add47.10, i64* %arrayidx5.10, align 8, !tbaa !3
  %89 = xor i64 %add43.10, -9223372036854775808
  %xor2.i111.10 = and i64 %89, %87
  %xor.i102.10 = xor i64 %add47.10, %add43.10
  %xor1.i103.10 = xor i64 %and46.10, %add43.10
  %or.i104.10 = or i64 %xor.i102.10, %xor1.i103.10
  %xor2.i105.10 = xor i64 %or.i104.10, %add47.10
  %shr.i112129.10 = or i64 %xor2.i105.10, %xor2.i111.10
  %or55.10 = lshr i64 %shr.i112129.10, 63
  %90 = load i64, i64* %arrayidx5.11, align 8, !tbaa !3
  %add43.11 = add i64 %90, %or55.10
  %91 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 11), align 8, !tbaa !3
  %and46.11 = and i64 %91, %sub34
  %add47.11 = add i64 %and46.11, %add43.11
  store i64 %add47.11, i64* %arrayidx5.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @fpsub751(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* %b, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %c, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %b, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i79.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i79.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  %arrayidx10.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %sub8.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %b, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i79.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i79.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  %arrayidx10.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %sub8.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %b, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i79.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i79.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  %arrayidx10.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %sub8.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %b, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i79.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i79.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  %arrayidx10.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %sub8.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %b, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i79.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i79.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  %arrayidx10.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %sub8.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %b, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i79.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i79.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  %arrayidx10.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %sub8.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %b, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i79.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i79.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  %arrayidx10.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %sub8.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %arrayidx.8 = getelementptr inbounds i64, i64* %a, i64 8
  %16 = load i64, i64* %arrayidx.8, align 8, !tbaa !3
  %arrayidx2.8 = getelementptr inbounds i64, i64* %b, i64 8
  %17 = load i64, i64* %arrayidx2.8, align 8, !tbaa !3
  %sub.8 = sub i64 %16, %17
  %xor.i.8 = xor i64 %17, %16
  %xor1.i.8 = xor i64 %sub.8, %17
  %or.i.8 = or i64 %xor1.i.8, %xor.i.8
  %xor2.i.8 = xor i64 %or.i.8, %16
  %shr.i.8 = lshr i64 %xor2.i.8, 63
  %conv.i.8 = trunc i64 %shr.i.8 to i32
  %sub.i.i.8 = sub i64 0, %sub.8
  %or.i.i.8 = or i64 %sub.8, %sub.i.i.8
  %shr.i.i.8 = lshr i64 %or.i.i.8, 63
  %conv.i.i.8 = trunc i64 %shr.i.i.8 to i32
  %xor.i79.8 = xor i32 %conv.i.i.8, 1
  %and.8 = and i32 %xor.i79.8, %or.7
  %or.8 = or i32 %and.8, %conv.i.8
  %conv.8 = zext i32 %or.7 to i64
  %sub8.8 = sub i64 %sub.8, %conv.8
  %arrayidx10.8 = getelementptr inbounds i64, i64* %c, i64 8
  store i64 %sub8.8, i64* %arrayidx10.8, align 8, !tbaa !3
  %arrayidx.9 = getelementptr inbounds i64, i64* %a, i64 9
  %18 = load i64, i64* %arrayidx.9, align 8, !tbaa !3
  %arrayidx2.9 = getelementptr inbounds i64, i64* %b, i64 9
  %19 = load i64, i64* %arrayidx2.9, align 8, !tbaa !3
  %sub.9 = sub i64 %18, %19
  %xor.i.9 = xor i64 %19, %18
  %xor1.i.9 = xor i64 %sub.9, %19
  %or.i.9 = or i64 %xor1.i.9, %xor.i.9
  %xor2.i.9 = xor i64 %or.i.9, %18
  %shr.i.9 = lshr i64 %xor2.i.9, 63
  %conv.i.9 = trunc i64 %shr.i.9 to i32
  %sub.i.i.9 = sub i64 0, %sub.9
  %or.i.i.9 = or i64 %sub.9, %sub.i.i.9
  %shr.i.i.9 = lshr i64 %or.i.i.9, 63
  %conv.i.i.9 = trunc i64 %shr.i.i.9 to i32
  %xor.i79.9 = xor i32 %conv.i.i.9, 1
  %and.9 = and i32 %xor.i79.9, %or.8
  %or.9 = or i32 %and.9, %conv.i.9
  %conv.9 = zext i32 %or.8 to i64
  %sub8.9 = sub i64 %sub.9, %conv.9
  %arrayidx10.9 = getelementptr inbounds i64, i64* %c, i64 9
  store i64 %sub8.9, i64* %arrayidx10.9, align 8, !tbaa !3
  %arrayidx.10 = getelementptr inbounds i64, i64* %a, i64 10
  %20 = load i64, i64* %arrayidx.10, align 8, !tbaa !3
  %arrayidx2.10 = getelementptr inbounds i64, i64* %b, i64 10
  %21 = load i64, i64* %arrayidx2.10, align 8, !tbaa !3
  %sub.10 = sub i64 %20, %21
  %xor.i.10 = xor i64 %21, %20
  %xor1.i.10 = xor i64 %sub.10, %21
  %or.i.10 = or i64 %xor1.i.10, %xor.i.10
  %xor2.i.10 = xor i64 %or.i.10, %20
  %shr.i.10 = lshr i64 %xor2.i.10, 63
  %conv.i.10 = trunc i64 %shr.i.10 to i32
  %sub.i.i.10 = sub i64 0, %sub.10
  %or.i.i.10 = or i64 %sub.10, %sub.i.i.10
  %shr.i.i.10 = lshr i64 %or.i.i.10, 63
  %conv.i.i.10 = trunc i64 %shr.i.i.10 to i32
  %xor.i79.10 = xor i32 %conv.i.i.10, 1
  %and.10 = and i32 %xor.i79.10, %or.9
  %or.10 = or i32 %and.10, %conv.i.10
  %conv.10 = zext i32 %or.9 to i64
  %sub8.10 = sub i64 %sub.10, %conv.10
  %arrayidx10.10 = getelementptr inbounds i64, i64* %c, i64 10
  store i64 %sub8.10, i64* %arrayidx10.10, align 8, !tbaa !3
  %arrayidx.11 = getelementptr inbounds i64, i64* %a, i64 11
  %22 = load i64, i64* %arrayidx.11, align 8, !tbaa !3
  %arrayidx2.11 = getelementptr inbounds i64, i64* %b, i64 11
  %23 = load i64, i64* %arrayidx2.11, align 8, !tbaa !3
  %sub.11 = sub i64 %22, %23
  %xor.i.11 = xor i64 %23, %22
  %xor1.i.11 = xor i64 %sub.11, %23
  %or.i.11 = or i64 %xor1.i.11, %xor.i.11
  %xor2.i.11 = xor i64 %or.i.11, %22
  %shr.i.11 = lshr i64 %xor2.i.11, 63
  %conv.i.11 = trunc i64 %shr.i.11 to i32
  %sub.i.i.11 = sub i64 0, %sub.11
  %or.i.i.11 = or i64 %sub.11, %sub.i.i.11
  %shr.i.i.11 = lshr i64 %or.i.i.11, 63
  %conv.i.i.11 = trunc i64 %shr.i.i.11 to i32
  %xor.i79.11 = xor i32 %conv.i.i.11, 1
  %and.11 = and i32 %xor.i79.11, %or.10
  %or.11 = or i32 %and.11, %conv.i.11
  %conv.11 = zext i32 %or.10 to i64
  %sub8.11 = sub i64 %sub.11, %conv.11
  %arrayidx10.11 = getelementptr inbounds i64, i64* %c, i64 11
  store i64 %sub8.11, i64* %arrayidx10.11, align 8, !tbaa !3
  %conv11 = zext i32 %or.11 to i64
  %sub12 = sub nsw i64 0, %conv11
  %24 = load i64, i64* %c, align 8, !tbaa !3
  %25 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 0), align 16, !tbaa !3
  %and23 = and i64 %25, %sub12
  %add24 = add i64 %and23, %24
  store i64 %add24, i64* %c, align 8, !tbaa !3
  %xor.i66 = xor i64 %add24, %24
  %xor1.i68 = xor i64 %and23, %24
  %or.i69 = or i64 %xor.i66, %xor1.i68
  %xor2.i70 = xor i64 %or.i69, %add24
  %or32 = lshr i64 %xor2.i70, 63
  %26 = load i64, i64* %arrayidx10.1, align 8, !tbaa !3
  %add.1 = add i64 %26, %or32
  %27 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 1), align 8, !tbaa !3
  %and23.1 = and i64 %27, %sub12
  %add24.1 = add i64 %and23.1, %add.1
  store i64 %add24.1, i64* %arrayidx10.1, align 8, !tbaa !3
  %28 = xor i64 %add.1, -9223372036854775808
  %xor2.i76.1 = and i64 %28, %26
  %xor.i66.1 = xor i64 %add24.1, %add.1
  %xor1.i68.1 = xor i64 %and23.1, %add.1
  %or.i69.1 = or i64 %xor.i66.1, %xor1.i68.1
  %xor2.i70.1 = xor i64 %or.i69.1, %add24.1
  %shr.i7780.1 = or i64 %xor2.i70.1, %xor2.i76.1
  %or32.1 = lshr i64 %shr.i7780.1, 63
  %29 = load i64, i64* %arrayidx10.2, align 8, !tbaa !3
  %add.2 = add i64 %29, %or32.1
  %30 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 2), align 16, !tbaa !3
  %and23.2 = and i64 %30, %sub12
  %add24.2 = add i64 %and23.2, %add.2
  store i64 %add24.2, i64* %arrayidx10.2, align 8, !tbaa !3
  %31 = xor i64 %add.2, -9223372036854775808
  %xor2.i76.2 = and i64 %31, %29
  %xor.i66.2 = xor i64 %add24.2, %add.2
  %xor1.i68.2 = xor i64 %and23.2, %add.2
  %or.i69.2 = or i64 %xor.i66.2, %xor1.i68.2
  %xor2.i70.2 = xor i64 %or.i69.2, %add24.2
  %shr.i7780.2 = or i64 %xor2.i70.2, %xor2.i76.2
  %or32.2 = lshr i64 %shr.i7780.2, 63
  %32 = load i64, i64* %arrayidx10.3, align 8, !tbaa !3
  %add.3 = add i64 %32, %or32.2
  %33 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 3), align 8, !tbaa !3
  %and23.3 = and i64 %33, %sub12
  %add24.3 = add i64 %and23.3, %add.3
  store i64 %add24.3, i64* %arrayidx10.3, align 8, !tbaa !3
  %34 = xor i64 %add.3, -9223372036854775808
  %xor2.i76.3 = and i64 %34, %32
  %xor.i66.3 = xor i64 %add24.3, %add.3
  %xor1.i68.3 = xor i64 %and23.3, %add.3
  %or.i69.3 = or i64 %xor.i66.3, %xor1.i68.3
  %xor2.i70.3 = xor i64 %or.i69.3, %add24.3
  %shr.i7780.3 = or i64 %xor2.i70.3, %xor2.i76.3
  %or32.3 = lshr i64 %shr.i7780.3, 63
  %35 = load i64, i64* %arrayidx10.4, align 8, !tbaa !3
  %add.4 = add i64 %35, %or32.3
  %36 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 4), align 16, !tbaa !3
  %and23.4 = and i64 %36, %sub12
  %add24.4 = add i64 %and23.4, %add.4
  store i64 %add24.4, i64* %arrayidx10.4, align 8, !tbaa !3
  %37 = xor i64 %add.4, -9223372036854775808
  %xor2.i76.4 = and i64 %37, %35
  %xor.i66.4 = xor i64 %add24.4, %add.4
  %xor1.i68.4 = xor i64 %and23.4, %add.4
  %or.i69.4 = or i64 %xor.i66.4, %xor1.i68.4
  %xor2.i70.4 = xor i64 %or.i69.4, %add24.4
  %shr.i7780.4 = or i64 %xor2.i70.4, %xor2.i76.4
  %or32.4 = lshr i64 %shr.i7780.4, 63
  %38 = load i64, i64* %arrayidx10.5, align 8, !tbaa !3
  %add.5 = add i64 %38, %or32.4
  %39 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 5), align 8, !tbaa !3
  %and23.5 = and i64 %39, %sub12
  %add24.5 = add i64 %and23.5, %add.5
  store i64 %add24.5, i64* %arrayidx10.5, align 8, !tbaa !3
  %40 = xor i64 %add.5, -9223372036854775808
  %xor2.i76.5 = and i64 %40, %38
  %xor.i66.5 = xor i64 %add24.5, %add.5
  %xor1.i68.5 = xor i64 %and23.5, %add.5
  %or.i69.5 = or i64 %xor.i66.5, %xor1.i68.5
  %xor2.i70.5 = xor i64 %or.i69.5, %add24.5
  %shr.i7780.5 = or i64 %xor2.i70.5, %xor2.i76.5
  %or32.5 = lshr i64 %shr.i7780.5, 63
  %41 = load i64, i64* %arrayidx10.6, align 8, !tbaa !3
  %add.6 = add i64 %41, %or32.5
  %42 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 6), align 16, !tbaa !3
  %and23.6 = and i64 %42, %sub12
  %add24.6 = add i64 %and23.6, %add.6
  store i64 %add24.6, i64* %arrayidx10.6, align 8, !tbaa !3
  %43 = xor i64 %add.6, -9223372036854775808
  %xor2.i76.6 = and i64 %43, %41
  %xor.i66.6 = xor i64 %add24.6, %add.6
  %xor1.i68.6 = xor i64 %and23.6, %add.6
  %or.i69.6 = or i64 %xor.i66.6, %xor1.i68.6
  %xor2.i70.6 = xor i64 %or.i69.6, %add24.6
  %shr.i7780.6 = or i64 %xor2.i70.6, %xor2.i76.6
  %or32.6 = lshr i64 %shr.i7780.6, 63
  %44 = load i64, i64* %arrayidx10.7, align 8, !tbaa !3
  %add.7 = add i64 %44, %or32.6
  %45 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 7), align 8, !tbaa !3
  %and23.7 = and i64 %45, %sub12
  %add24.7 = add i64 %and23.7, %add.7
  store i64 %add24.7, i64* %arrayidx10.7, align 8, !tbaa !3
  %46 = xor i64 %add.7, -9223372036854775808
  %xor2.i76.7 = and i64 %46, %44
  %xor.i66.7 = xor i64 %add24.7, %add.7
  %xor1.i68.7 = xor i64 %and23.7, %add.7
  %or.i69.7 = or i64 %xor.i66.7, %xor1.i68.7
  %xor2.i70.7 = xor i64 %or.i69.7, %add24.7
  %shr.i7780.7 = or i64 %xor2.i70.7, %xor2.i76.7
  %or32.7 = lshr i64 %shr.i7780.7, 63
  %47 = load i64, i64* %arrayidx10.8, align 8, !tbaa !3
  %add.8 = add i64 %47, %or32.7
  %48 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 8), align 16, !tbaa !3
  %and23.8 = and i64 %48, %sub12
  %add24.8 = add i64 %and23.8, %add.8
  store i64 %add24.8, i64* %arrayidx10.8, align 8, !tbaa !3
  %49 = xor i64 %add.8, -9223372036854775808
  %xor2.i76.8 = and i64 %49, %47
  %xor.i66.8 = xor i64 %add24.8, %add.8
  %xor1.i68.8 = xor i64 %and23.8, %add.8
  %or.i69.8 = or i64 %xor.i66.8, %xor1.i68.8
  %xor2.i70.8 = xor i64 %or.i69.8, %add24.8
  %shr.i7780.8 = or i64 %xor2.i70.8, %xor2.i76.8
  %or32.8 = lshr i64 %shr.i7780.8, 63
  %50 = load i64, i64* %arrayidx10.9, align 8, !tbaa !3
  %add.9 = add i64 %50, %or32.8
  %51 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 9), align 8, !tbaa !3
  %and23.9 = and i64 %51, %sub12
  %add24.9 = add i64 %and23.9, %add.9
  store i64 %add24.9, i64* %arrayidx10.9, align 8, !tbaa !3
  %52 = xor i64 %add.9, -9223372036854775808
  %xor2.i76.9 = and i64 %52, %50
  %xor.i66.9 = xor i64 %add24.9, %add.9
  %xor1.i68.9 = xor i64 %and23.9, %add.9
  %or.i69.9 = or i64 %xor.i66.9, %xor1.i68.9
  %xor2.i70.9 = xor i64 %or.i69.9, %add24.9
  %shr.i7780.9 = or i64 %xor2.i70.9, %xor2.i76.9
  %or32.9 = lshr i64 %shr.i7780.9, 63
  %53 = load i64, i64* %arrayidx10.10, align 8, !tbaa !3
  %add.10 = add i64 %53, %or32.9
  %54 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 10), align 16, !tbaa !3
  %and23.10 = and i64 %54, %sub12
  %add24.10 = add i64 %and23.10, %add.10
  store i64 %add24.10, i64* %arrayidx10.10, align 8, !tbaa !3
  %55 = xor i64 %add.10, -9223372036854775808
  %xor2.i76.10 = and i64 %55, %53
  %xor.i66.10 = xor i64 %add24.10, %add.10
  %xor1.i68.10 = xor i64 %and23.10, %add.10
  %or.i69.10 = or i64 %xor.i66.10, %xor1.i68.10
  %xor2.i70.10 = xor i64 %or.i69.10, %add24.10
  %shr.i7780.10 = or i64 %xor2.i70.10, %xor2.i76.10
  %or32.10 = lshr i64 %shr.i7780.10, 63
  %56 = load i64, i64* %arrayidx10.11, align 8, !tbaa !3
  %add.11 = add i64 %56, %or32.10
  %57 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 11), align 8, !tbaa !3
  %and23.11 = and i64 %57, %sub12
  %add24.11 = add i64 %and23.11, %add.11
  store i64 %add24.11, i64* %arrayidx10.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: inlinehint norecurse nounwind ssp uwtable
define void @fpneg751(i64* nocapture %a) local_unnamed_addr #0 {
entry:
  %0 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 0), align 16, !tbaa !3
  %1 = load i64, i64* %a, align 8, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %a, align 8, !tbaa !3
  %2 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 1), align 8, !tbaa !3
  %arrayidx2.1 = getelementptr inbounds i64, i64* %a, i64 1
  %3 = load i64, i64* %arrayidx2.1, align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i25.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i25.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  store i64 %sub8.1, i64* %arrayidx2.1, align 8, !tbaa !3
  %4 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 2), align 16, !tbaa !3
  %arrayidx2.2 = getelementptr inbounds i64, i64* %a, i64 2
  %5 = load i64, i64* %arrayidx2.2, align 8, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i25.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i25.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  store i64 %sub8.2, i64* %arrayidx2.2, align 8, !tbaa !3
  %6 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 3), align 8, !tbaa !3
  %arrayidx2.3 = getelementptr inbounds i64, i64* %a, i64 3
  %7 = load i64, i64* %arrayidx2.3, align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i25.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i25.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  store i64 %sub8.3, i64* %arrayidx2.3, align 8, !tbaa !3
  %8 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 4), align 16, !tbaa !3
  %arrayidx2.4 = getelementptr inbounds i64, i64* %a, i64 4
  %9 = load i64, i64* %arrayidx2.4, align 8, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i25.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i25.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  store i64 %sub8.4, i64* %arrayidx2.4, align 8, !tbaa !3
  %10 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 5), align 8, !tbaa !3
  %arrayidx2.5 = getelementptr inbounds i64, i64* %a, i64 5
  %11 = load i64, i64* %arrayidx2.5, align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i25.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i25.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  store i64 %sub8.5, i64* %arrayidx2.5, align 8, !tbaa !3
  %12 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 6), align 16, !tbaa !3
  %arrayidx2.6 = getelementptr inbounds i64, i64* %a, i64 6
  %13 = load i64, i64* %arrayidx2.6, align 8, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i25.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i25.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  store i64 %sub8.6, i64* %arrayidx2.6, align 8, !tbaa !3
  %14 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 7), align 8, !tbaa !3
  %arrayidx2.7 = getelementptr inbounds i64, i64* %a, i64 7
  %15 = load i64, i64* %arrayidx2.7, align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i25.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i25.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  store i64 %sub8.7, i64* %arrayidx2.7, align 8, !tbaa !3
  %16 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 8), align 16, !tbaa !3
  %arrayidx2.8 = getelementptr inbounds i64, i64* %a, i64 8
  %17 = load i64, i64* %arrayidx2.8, align 8, !tbaa !3
  %sub.8 = sub i64 %16, %17
  %xor.i.8 = xor i64 %17, %16
  %xor1.i.8 = xor i64 %sub.8, %17
  %or.i.8 = or i64 %xor1.i.8, %xor.i.8
  %xor2.i.8 = xor i64 %or.i.8, %16
  %shr.i.8 = lshr i64 %xor2.i.8, 63
  %conv.i.8 = trunc i64 %shr.i.8 to i32
  %sub.i.i.8 = sub i64 0, %sub.8
  %or.i.i.8 = or i64 %sub.8, %sub.i.i.8
  %shr.i.i.8 = lshr i64 %or.i.i.8, 63
  %conv.i.i.8 = trunc i64 %shr.i.i.8 to i32
  %xor.i25.8 = xor i32 %conv.i.i.8, 1
  %and.8 = and i32 %xor.i25.8, %or.7
  %or.8 = or i32 %and.8, %conv.i.8
  %conv.8 = zext i32 %or.7 to i64
  %sub8.8 = sub i64 %sub.8, %conv.8
  store i64 %sub8.8, i64* %arrayidx2.8, align 8, !tbaa !3
  %18 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 9), align 8, !tbaa !3
  %arrayidx2.9 = getelementptr inbounds i64, i64* %a, i64 9
  %19 = load i64, i64* %arrayidx2.9, align 8, !tbaa !3
  %sub.9 = sub i64 %18, %19
  %xor.i.9 = xor i64 %19, %18
  %xor1.i.9 = xor i64 %sub.9, %19
  %or.i.9 = or i64 %xor1.i.9, %xor.i.9
  %xor2.i.9 = xor i64 %or.i.9, %18
  %shr.i.9 = lshr i64 %xor2.i.9, 63
  %conv.i.9 = trunc i64 %shr.i.9 to i32
  %sub.i.i.9 = sub i64 0, %sub.9
  %or.i.i.9 = or i64 %sub.9, %sub.i.i.9
  %shr.i.i.9 = lshr i64 %or.i.i.9, 63
  %conv.i.i.9 = trunc i64 %shr.i.i.9 to i32
  %xor.i25.9 = xor i32 %conv.i.i.9, 1
  %and.9 = and i32 %xor.i25.9, %or.8
  %or.9 = or i32 %and.9, %conv.i.9
  %conv.9 = zext i32 %or.8 to i64
  %sub8.9 = sub i64 %sub.9, %conv.9
  store i64 %sub8.9, i64* %arrayidx2.9, align 8, !tbaa !3
  %20 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 10), align 16, !tbaa !3
  %arrayidx2.10 = getelementptr inbounds i64, i64* %a, i64 10
  %21 = load i64, i64* %arrayidx2.10, align 8, !tbaa !3
  %sub.10 = sub i64 %20, %21
  %xor.i.10 = xor i64 %21, %20
  %xor1.i.10 = xor i64 %sub.10, %21
  %or.i.10 = or i64 %xor1.i.10, %xor.i.10
  %xor2.i.10 = xor i64 %or.i.10, %20
  %shr.i.10 = lshr i64 %xor2.i.10, 63
  %conv.i.10 = trunc i64 %shr.i.10 to i32
  %sub.i.i.10 = sub i64 0, %sub.10
  %or.i.i.10 = or i64 %sub.10, %sub.i.i.10
  %shr.i.i.10 = lshr i64 %or.i.i.10, 63
  %conv.i.i.10 = trunc i64 %shr.i.i.10 to i32
  %xor.i25.10 = xor i32 %conv.i.i.10, 1
  %and.10 = and i32 %xor.i25.10, %or.9
  %or.10 = or i32 %and.10, %conv.i.10
  %conv.10 = zext i32 %or.9 to i64
  %sub8.10 = sub i64 %sub.10, %conv.10
  store i64 %sub8.10, i64* %arrayidx2.10, align 8, !tbaa !3
  %22 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751x2, i64 0, i64 11), align 8, !tbaa !3
  %arrayidx2.11 = getelementptr inbounds i64, i64* %a, i64 11
  %23 = load i64, i64* %arrayidx2.11, align 8, !tbaa !3
  %sub.11 = sub i64 %22, %23
  %conv.11 = zext i32 %or.10 to i64
  %sub8.11 = sub i64 %sub.11, %conv.11
  store i64 %sub8.11, i64* %arrayidx2.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define void @fpdiv2_751(i64* nocapture readonly %a, i64* %c) local_unnamed_addr #1 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %and = and i64 %0, 1
  %sub = sub nsw i64 0, %and
  %1 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 0), align 16, !tbaa !3
  %and4 = and i64 %1, %sub
  %add5 = add i64 %and4, %0
  store i64 %add5, i64* %c, align 8, !tbaa !3
  %xor.i27 = xor i64 %add5, %0
  %xor1.i28 = xor i64 %and4, %0
  %or.i29 = or i64 %xor.i27, %xor1.i28
  %xor2.i30 = xor i64 %or.i29, %add5
  %or = lshr i64 %xor2.i30, 63
  %arrayidx1.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx1.1, align 8, !tbaa !3
  %add.1 = add i64 %2, %or
  %3 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 1), align 8, !tbaa !3
  %and4.1 = and i64 %3, %sub
  %add5.1 = add i64 %and4.1, %add.1
  %arrayidx7.1 = getelementptr inbounds i64, i64* %c, i64 1
  store i64 %add5.1, i64* %arrayidx7.1, align 8, !tbaa !3
  %4 = xor i64 %add.1, -9223372036854775808
  %xor2.i.1 = and i64 %4, %2
  %xor.i27.1 = xor i64 %add5.1, %add.1
  %xor1.i28.1 = xor i64 %and4.1, %add.1
  %or.i29.1 = or i64 %xor.i27.1, %xor1.i28.1
  %xor2.i30.1 = xor i64 %or.i29.1, %add5.1
  %shr.i33.1 = or i64 %xor2.i30.1, %xor2.i.1
  %or.1 = lshr i64 %shr.i33.1, 63
  %arrayidx1.2 = getelementptr inbounds i64, i64* %a, i64 2
  %5 = load i64, i64* %arrayidx1.2, align 8, !tbaa !3
  %add.2 = add i64 %5, %or.1
  %6 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 2), align 16, !tbaa !3
  %and4.2 = and i64 %6, %sub
  %add5.2 = add i64 %and4.2, %add.2
  %arrayidx7.2 = getelementptr inbounds i64, i64* %c, i64 2
  store i64 %add5.2, i64* %arrayidx7.2, align 8, !tbaa !3
  %7 = xor i64 %add.2, -9223372036854775808
  %xor2.i.2 = and i64 %7, %5
  %xor.i27.2 = xor i64 %add5.2, %add.2
  %xor1.i28.2 = xor i64 %and4.2, %add.2
  %or.i29.2 = or i64 %xor.i27.2, %xor1.i28.2
  %xor2.i30.2 = xor i64 %or.i29.2, %add5.2
  %shr.i33.2 = or i64 %xor2.i30.2, %xor2.i.2
  %or.2 = lshr i64 %shr.i33.2, 63
  %arrayidx1.3 = getelementptr inbounds i64, i64* %a, i64 3
  %8 = load i64, i64* %arrayidx1.3, align 8, !tbaa !3
  %add.3 = add i64 %8, %or.2
  %9 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 3), align 8, !tbaa !3
  %and4.3 = and i64 %9, %sub
  %add5.3 = add i64 %and4.3, %add.3
  %arrayidx7.3 = getelementptr inbounds i64, i64* %c, i64 3
  store i64 %add5.3, i64* %arrayidx7.3, align 8, !tbaa !3
  %10 = xor i64 %add.3, -9223372036854775808
  %xor2.i.3 = and i64 %10, %8
  %xor.i27.3 = xor i64 %add5.3, %add.3
  %xor1.i28.3 = xor i64 %and4.3, %add.3
  %or.i29.3 = or i64 %xor.i27.3, %xor1.i28.3
  %xor2.i30.3 = xor i64 %or.i29.3, %add5.3
  %shr.i33.3 = or i64 %xor2.i30.3, %xor2.i.3
  %or.3 = lshr i64 %shr.i33.3, 63
  %arrayidx1.4 = getelementptr inbounds i64, i64* %a, i64 4
  %11 = load i64, i64* %arrayidx1.4, align 8, !tbaa !3
  %add.4 = add i64 %11, %or.3
  %12 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 4), align 16, !tbaa !3
  %and4.4 = and i64 %12, %sub
  %add5.4 = add i64 %and4.4, %add.4
  %arrayidx7.4 = getelementptr inbounds i64, i64* %c, i64 4
  store i64 %add5.4, i64* %arrayidx7.4, align 8, !tbaa !3
  %13 = xor i64 %add.4, -9223372036854775808
  %xor2.i.4 = and i64 %13, %11
  %xor.i27.4 = xor i64 %add5.4, %add.4
  %xor1.i28.4 = xor i64 %and4.4, %add.4
  %or.i29.4 = or i64 %xor.i27.4, %xor1.i28.4
  %xor2.i30.4 = xor i64 %or.i29.4, %add5.4
  %shr.i33.4 = or i64 %xor2.i30.4, %xor2.i.4
  %or.4 = lshr i64 %shr.i33.4, 63
  %arrayidx1.5 = getelementptr inbounds i64, i64* %a, i64 5
  %14 = load i64, i64* %arrayidx1.5, align 8, !tbaa !3
  %add.5 = add i64 %14, %or.4
  %15 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 5), align 8, !tbaa !3
  %and4.5 = and i64 %15, %sub
  %add5.5 = add i64 %and4.5, %add.5
  %arrayidx7.5 = getelementptr inbounds i64, i64* %c, i64 5
  store i64 %add5.5, i64* %arrayidx7.5, align 8, !tbaa !3
  %16 = xor i64 %add.5, -9223372036854775808
  %xor2.i.5 = and i64 %16, %14
  %xor.i27.5 = xor i64 %add5.5, %add.5
  %xor1.i28.5 = xor i64 %and4.5, %add.5
  %or.i29.5 = or i64 %xor.i27.5, %xor1.i28.5
  %xor2.i30.5 = xor i64 %or.i29.5, %add5.5
  %shr.i33.5 = or i64 %xor2.i30.5, %xor2.i.5
  %or.5 = lshr i64 %shr.i33.5, 63
  %arrayidx1.6 = getelementptr inbounds i64, i64* %a, i64 6
  %17 = load i64, i64* %arrayidx1.6, align 8, !tbaa !3
  %add.6 = add i64 %17, %or.5
  %18 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 6), align 16, !tbaa !3
  %and4.6 = and i64 %18, %sub
  %add5.6 = add i64 %and4.6, %add.6
  %arrayidx7.6 = getelementptr inbounds i64, i64* %c, i64 6
  store i64 %add5.6, i64* %arrayidx7.6, align 8, !tbaa !3
  %19 = xor i64 %add.6, -9223372036854775808
  %xor2.i.6 = and i64 %19, %17
  %xor.i27.6 = xor i64 %add5.6, %add.6
  %xor1.i28.6 = xor i64 %and4.6, %add.6
  %or.i29.6 = or i64 %xor.i27.6, %xor1.i28.6
  %xor2.i30.6 = xor i64 %or.i29.6, %add5.6
  %shr.i33.6 = or i64 %xor2.i30.6, %xor2.i.6
  %or.6 = lshr i64 %shr.i33.6, 63
  %arrayidx1.7 = getelementptr inbounds i64, i64* %a, i64 7
  %20 = load i64, i64* %arrayidx1.7, align 8, !tbaa !3
  %add.7 = add i64 %20, %or.6
  %21 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 7), align 8, !tbaa !3
  %and4.7 = and i64 %21, %sub
  %add5.7 = add i64 %and4.7, %add.7
  %arrayidx7.7 = getelementptr inbounds i64, i64* %c, i64 7
  store i64 %add5.7, i64* %arrayidx7.7, align 8, !tbaa !3
  %22 = xor i64 %add.7, -9223372036854775808
  %xor2.i.7 = and i64 %22, %20
  %xor.i27.7 = xor i64 %add5.7, %add.7
  %xor1.i28.7 = xor i64 %and4.7, %add.7
  %or.i29.7 = or i64 %xor.i27.7, %xor1.i28.7
  %xor2.i30.7 = xor i64 %or.i29.7, %add5.7
  %shr.i33.7 = or i64 %xor2.i30.7, %xor2.i.7
  %or.7 = lshr i64 %shr.i33.7, 63
  %arrayidx1.8 = getelementptr inbounds i64, i64* %a, i64 8
  %23 = load i64, i64* %arrayidx1.8, align 8, !tbaa !3
  %add.8 = add i64 %23, %or.7
  %24 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 8), align 16, !tbaa !3
  %and4.8 = and i64 %24, %sub
  %add5.8 = add i64 %and4.8, %add.8
  %arrayidx7.8 = getelementptr inbounds i64, i64* %c, i64 8
  store i64 %add5.8, i64* %arrayidx7.8, align 8, !tbaa !3
  %25 = xor i64 %add.8, -9223372036854775808
  %xor2.i.8 = and i64 %25, %23
  %xor.i27.8 = xor i64 %add5.8, %add.8
  %xor1.i28.8 = xor i64 %and4.8, %add.8
  %or.i29.8 = or i64 %xor.i27.8, %xor1.i28.8
  %xor2.i30.8 = xor i64 %or.i29.8, %add5.8
  %shr.i33.8 = or i64 %xor2.i30.8, %xor2.i.8
  %or.8 = lshr i64 %shr.i33.8, 63
  %arrayidx1.9 = getelementptr inbounds i64, i64* %a, i64 9
  %26 = load i64, i64* %arrayidx1.9, align 8, !tbaa !3
  %add.9 = add i64 %26, %or.8
  %27 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 9), align 8, !tbaa !3
  %and4.9 = and i64 %27, %sub
  %add5.9 = add i64 %and4.9, %add.9
  %arrayidx7.9 = getelementptr inbounds i64, i64* %c, i64 9
  store i64 %add5.9, i64* %arrayidx7.9, align 8, !tbaa !3
  %28 = xor i64 %add.9, -9223372036854775808
  %xor2.i.9 = and i64 %28, %26
  %xor.i27.9 = xor i64 %add5.9, %add.9
  %xor1.i28.9 = xor i64 %and4.9, %add.9
  %or.i29.9 = or i64 %xor.i27.9, %xor1.i28.9
  %xor2.i30.9 = xor i64 %or.i29.9, %add5.9
  %shr.i33.9 = or i64 %xor2.i30.9, %xor2.i.9
  %or.9 = lshr i64 %shr.i33.9, 63
  %arrayidx1.10 = getelementptr inbounds i64, i64* %a, i64 10
  %29 = load i64, i64* %arrayidx1.10, align 8, !tbaa !3
  %add.10 = add i64 %29, %or.9
  %30 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 10), align 16, !tbaa !3
  %and4.10 = and i64 %30, %sub
  %add5.10 = add i64 %and4.10, %add.10
  %arrayidx7.10 = getelementptr inbounds i64, i64* %c, i64 10
  store i64 %add5.10, i64* %arrayidx7.10, align 8, !tbaa !3
  %31 = xor i64 %add.10, -9223372036854775808
  %xor2.i.10 = and i64 %31, %29
  %xor.i27.10 = xor i64 %add5.10, %add.10
  %xor1.i28.10 = xor i64 %and4.10, %add.10
  %or.i29.10 = or i64 %xor.i27.10, %xor1.i28.10
  %xor2.i30.10 = xor i64 %or.i29.10, %add5.10
  %shr.i33.10 = or i64 %xor2.i30.10, %xor2.i.10
  %or.10 = lshr i64 %shr.i33.10, 63
  %arrayidx1.11 = getelementptr inbounds i64, i64* %a, i64 11
  %32 = load i64, i64* %arrayidx1.11, align 8, !tbaa !3
  %add.11 = add i64 %32, %or.10
  %33 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 11), align 8, !tbaa !3
  %and4.11 = and i64 %33, %sub
  %add5.11 = add i64 %and4.11, %add.11
  %arrayidx7.11 = getelementptr inbounds i64, i64* %c, i64 11
  store i64 %add5.11, i64* %arrayidx7.11, align 8, !tbaa !3
  tail call void @mp_shiftr1(i64* %c, i32 12) #5
  ret void
}

declare void @mp_shiftr1(i64*, i32) local_unnamed_addr #2

; Function Attrs: norecurse nounwind ssp uwtable
define void @fpcorrection751(i64* nocapture %a) local_unnamed_addr #3 {
entry:
  %0 = load i64, i64* %a, align 8, !tbaa !3
  %1 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 0), align 16, !tbaa !3
  %sub = sub i64 %0, %1
  %xor.i = xor i64 %1, %0
  %xor1.i = xor i64 %sub, %1
  %or.i = or i64 %xor1.i, %xor.i
  %xor2.i = xor i64 %or.i, %0
  %shr.i = lshr i64 %xor2.i, 63
  %conv.i = trunc i64 %shr.i to i32
  store i64 %sub, i64* %a, align 8, !tbaa !3
  %arrayidx.1 = getelementptr inbounds i64, i64* %a, i64 1
  %2 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %3 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 1), align 8, !tbaa !3
  %sub.1 = sub i64 %2, %3
  %xor.i.1 = xor i64 %3, %2
  %xor1.i.1 = xor i64 %sub.1, %3
  %or.i.1 = or i64 %xor1.i.1, %xor.i.1
  %xor2.i.1 = xor i64 %or.i.1, %2
  %shr.i.1 = lshr i64 %xor2.i.1, 63
  %conv.i.1 = trunc i64 %shr.i.1 to i32
  %sub.i.i.1 = sub i64 0, %sub.1
  %or.i.i.1 = or i64 %sub.1, %sub.i.i.1
  %shr.i.i.1 = lshr i64 %or.i.i.1, 63
  %conv.i.i.1 = trunc i64 %shr.i.i.1 to i32
  %xor.i79.1 = xor i32 %conv.i.i.1, 1
  %and.1 = and i32 %xor.i79.1, %conv.i
  %or.1 = or i32 %and.1, %conv.i.1
  %sub8.1 = sub i64 %sub.1, %shr.i
  store i64 %sub8.1, i64* %arrayidx.1, align 8, !tbaa !3
  %arrayidx.2 = getelementptr inbounds i64, i64* %a, i64 2
  %4 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %5 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 2), align 16, !tbaa !3
  %sub.2 = sub i64 %4, %5
  %xor.i.2 = xor i64 %5, %4
  %xor1.i.2 = xor i64 %sub.2, %5
  %or.i.2 = or i64 %xor1.i.2, %xor.i.2
  %xor2.i.2 = xor i64 %or.i.2, %4
  %shr.i.2 = lshr i64 %xor2.i.2, 63
  %conv.i.2 = trunc i64 %shr.i.2 to i32
  %sub.i.i.2 = sub i64 0, %sub.2
  %or.i.i.2 = or i64 %sub.2, %sub.i.i.2
  %shr.i.i.2 = lshr i64 %or.i.i.2, 63
  %conv.i.i.2 = trunc i64 %shr.i.i.2 to i32
  %xor.i79.2 = xor i32 %conv.i.i.2, 1
  %and.2 = and i32 %xor.i79.2, %or.1
  %or.2 = or i32 %and.2, %conv.i.2
  %conv.2 = zext i32 %or.1 to i64
  %sub8.2 = sub i64 %sub.2, %conv.2
  store i64 %sub8.2, i64* %arrayidx.2, align 8, !tbaa !3
  %arrayidx.3 = getelementptr inbounds i64, i64* %a, i64 3
  %6 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %7 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 3), align 8, !tbaa !3
  %sub.3 = sub i64 %6, %7
  %xor.i.3 = xor i64 %7, %6
  %xor1.i.3 = xor i64 %sub.3, %7
  %or.i.3 = or i64 %xor1.i.3, %xor.i.3
  %xor2.i.3 = xor i64 %or.i.3, %6
  %shr.i.3 = lshr i64 %xor2.i.3, 63
  %conv.i.3 = trunc i64 %shr.i.3 to i32
  %sub.i.i.3 = sub i64 0, %sub.3
  %or.i.i.3 = or i64 %sub.3, %sub.i.i.3
  %shr.i.i.3 = lshr i64 %or.i.i.3, 63
  %conv.i.i.3 = trunc i64 %shr.i.i.3 to i32
  %xor.i79.3 = xor i32 %conv.i.i.3, 1
  %and.3 = and i32 %xor.i79.3, %or.2
  %or.3 = or i32 %and.3, %conv.i.3
  %conv.3 = zext i32 %or.2 to i64
  %sub8.3 = sub i64 %sub.3, %conv.3
  store i64 %sub8.3, i64* %arrayidx.3, align 8, !tbaa !3
  %arrayidx.4 = getelementptr inbounds i64, i64* %a, i64 4
  %8 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %9 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 4), align 16, !tbaa !3
  %sub.4 = sub i64 %8, %9
  %xor.i.4 = xor i64 %9, %8
  %xor1.i.4 = xor i64 %sub.4, %9
  %or.i.4 = or i64 %xor1.i.4, %xor.i.4
  %xor2.i.4 = xor i64 %or.i.4, %8
  %shr.i.4 = lshr i64 %xor2.i.4, 63
  %conv.i.4 = trunc i64 %shr.i.4 to i32
  %sub.i.i.4 = sub i64 0, %sub.4
  %or.i.i.4 = or i64 %sub.4, %sub.i.i.4
  %shr.i.i.4 = lshr i64 %or.i.i.4, 63
  %conv.i.i.4 = trunc i64 %shr.i.i.4 to i32
  %xor.i79.4 = xor i32 %conv.i.i.4, 1
  %and.4 = and i32 %xor.i79.4, %or.3
  %or.4 = or i32 %and.4, %conv.i.4
  %conv.4 = zext i32 %or.3 to i64
  %sub8.4 = sub i64 %sub.4, %conv.4
  store i64 %sub8.4, i64* %arrayidx.4, align 8, !tbaa !3
  %arrayidx.5 = getelementptr inbounds i64, i64* %a, i64 5
  %10 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %11 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 5), align 8, !tbaa !3
  %sub.5 = sub i64 %10, %11
  %xor.i.5 = xor i64 %11, %10
  %xor1.i.5 = xor i64 %sub.5, %11
  %or.i.5 = or i64 %xor1.i.5, %xor.i.5
  %xor2.i.5 = xor i64 %or.i.5, %10
  %shr.i.5 = lshr i64 %xor2.i.5, 63
  %conv.i.5 = trunc i64 %shr.i.5 to i32
  %sub.i.i.5 = sub i64 0, %sub.5
  %or.i.i.5 = or i64 %sub.5, %sub.i.i.5
  %shr.i.i.5 = lshr i64 %or.i.i.5, 63
  %conv.i.i.5 = trunc i64 %shr.i.i.5 to i32
  %xor.i79.5 = xor i32 %conv.i.i.5, 1
  %and.5 = and i32 %xor.i79.5, %or.4
  %or.5 = or i32 %and.5, %conv.i.5
  %conv.5 = zext i32 %or.4 to i64
  %sub8.5 = sub i64 %sub.5, %conv.5
  store i64 %sub8.5, i64* %arrayidx.5, align 8, !tbaa !3
  %arrayidx.6 = getelementptr inbounds i64, i64* %a, i64 6
  %12 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %13 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 6), align 16, !tbaa !3
  %sub.6 = sub i64 %12, %13
  %xor.i.6 = xor i64 %13, %12
  %xor1.i.6 = xor i64 %sub.6, %13
  %or.i.6 = or i64 %xor1.i.6, %xor.i.6
  %xor2.i.6 = xor i64 %or.i.6, %12
  %shr.i.6 = lshr i64 %xor2.i.6, 63
  %conv.i.6 = trunc i64 %shr.i.6 to i32
  %sub.i.i.6 = sub i64 0, %sub.6
  %or.i.i.6 = or i64 %sub.6, %sub.i.i.6
  %shr.i.i.6 = lshr i64 %or.i.i.6, 63
  %conv.i.i.6 = trunc i64 %shr.i.i.6 to i32
  %xor.i79.6 = xor i32 %conv.i.i.6, 1
  %and.6 = and i32 %xor.i79.6, %or.5
  %or.6 = or i32 %and.6, %conv.i.6
  %conv.6 = zext i32 %or.5 to i64
  %sub8.6 = sub i64 %sub.6, %conv.6
  store i64 %sub8.6, i64* %arrayidx.6, align 8, !tbaa !3
  %arrayidx.7 = getelementptr inbounds i64, i64* %a, i64 7
  %14 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %15 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 7), align 8, !tbaa !3
  %sub.7 = sub i64 %14, %15
  %xor.i.7 = xor i64 %15, %14
  %xor1.i.7 = xor i64 %sub.7, %15
  %or.i.7 = or i64 %xor1.i.7, %xor.i.7
  %xor2.i.7 = xor i64 %or.i.7, %14
  %shr.i.7 = lshr i64 %xor2.i.7, 63
  %conv.i.7 = trunc i64 %shr.i.7 to i32
  %sub.i.i.7 = sub i64 0, %sub.7
  %or.i.i.7 = or i64 %sub.7, %sub.i.i.7
  %shr.i.i.7 = lshr i64 %or.i.i.7, 63
  %conv.i.i.7 = trunc i64 %shr.i.i.7 to i32
  %xor.i79.7 = xor i32 %conv.i.i.7, 1
  %and.7 = and i32 %xor.i79.7, %or.6
  %or.7 = or i32 %and.7, %conv.i.7
  %conv.7 = zext i32 %or.6 to i64
  %sub8.7 = sub i64 %sub.7, %conv.7
  store i64 %sub8.7, i64* %arrayidx.7, align 8, !tbaa !3
  %arrayidx.8 = getelementptr inbounds i64, i64* %a, i64 8
  %16 = load i64, i64* %arrayidx.8, align 8, !tbaa !3
  %17 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 8), align 16, !tbaa !3
  %sub.8 = sub i64 %16, %17
  %xor.i.8 = xor i64 %17, %16
  %xor1.i.8 = xor i64 %sub.8, %17
  %or.i.8 = or i64 %xor1.i.8, %xor.i.8
  %xor2.i.8 = xor i64 %or.i.8, %16
  %shr.i.8 = lshr i64 %xor2.i.8, 63
  %conv.i.8 = trunc i64 %shr.i.8 to i32
  %sub.i.i.8 = sub i64 0, %sub.8
  %or.i.i.8 = or i64 %sub.8, %sub.i.i.8
  %shr.i.i.8 = lshr i64 %or.i.i.8, 63
  %conv.i.i.8 = trunc i64 %shr.i.i.8 to i32
  %xor.i79.8 = xor i32 %conv.i.i.8, 1
  %and.8 = and i32 %xor.i79.8, %or.7
  %or.8 = or i32 %and.8, %conv.i.8
  %conv.8 = zext i32 %or.7 to i64
  %sub8.8 = sub i64 %sub.8, %conv.8
  store i64 %sub8.8, i64* %arrayidx.8, align 8, !tbaa !3
  %arrayidx.9 = getelementptr inbounds i64, i64* %a, i64 9
  %18 = load i64, i64* %arrayidx.9, align 8, !tbaa !3
  %19 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 9), align 8, !tbaa !3
  %sub.9 = sub i64 %18, %19
  %xor.i.9 = xor i64 %19, %18
  %xor1.i.9 = xor i64 %sub.9, %19
  %or.i.9 = or i64 %xor1.i.9, %xor.i.9
  %xor2.i.9 = xor i64 %or.i.9, %18
  %shr.i.9 = lshr i64 %xor2.i.9, 63
  %conv.i.9 = trunc i64 %shr.i.9 to i32
  %sub.i.i.9 = sub i64 0, %sub.9
  %or.i.i.9 = or i64 %sub.9, %sub.i.i.9
  %shr.i.i.9 = lshr i64 %or.i.i.9, 63
  %conv.i.i.9 = trunc i64 %shr.i.i.9 to i32
  %xor.i79.9 = xor i32 %conv.i.i.9, 1
  %and.9 = and i32 %xor.i79.9, %or.8
  %or.9 = or i32 %and.9, %conv.i.9
  %conv.9 = zext i32 %or.8 to i64
  %sub8.9 = sub i64 %sub.9, %conv.9
  store i64 %sub8.9, i64* %arrayidx.9, align 8, !tbaa !3
  %arrayidx.10 = getelementptr inbounds i64, i64* %a, i64 10
  %20 = load i64, i64* %arrayidx.10, align 8, !tbaa !3
  %21 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 10), align 16, !tbaa !3
  %sub.10 = sub i64 %20, %21
  %xor.i.10 = xor i64 %21, %20
  %xor1.i.10 = xor i64 %sub.10, %21
  %or.i.10 = or i64 %xor1.i.10, %xor.i.10
  %xor2.i.10 = xor i64 %or.i.10, %20
  %shr.i.10 = lshr i64 %xor2.i.10, 63
  %conv.i.10 = trunc i64 %shr.i.10 to i32
  %sub.i.i.10 = sub i64 0, %sub.10
  %or.i.i.10 = or i64 %sub.10, %sub.i.i.10
  %shr.i.i.10 = lshr i64 %or.i.i.10, 63
  %conv.i.i.10 = trunc i64 %shr.i.i.10 to i32
  %xor.i79.10 = xor i32 %conv.i.i.10, 1
  %and.10 = and i32 %xor.i79.10, %or.9
  %or.10 = or i32 %and.10, %conv.i.10
  %conv.10 = zext i32 %or.9 to i64
  %sub8.10 = sub i64 %sub.10, %conv.10
  store i64 %sub8.10, i64* %arrayidx.10, align 8, !tbaa !3
  %arrayidx.11 = getelementptr inbounds i64, i64* %a, i64 11
  %22 = load i64, i64* %arrayidx.11, align 8, !tbaa !3
  %23 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 11), align 8, !tbaa !3
  %sub.11 = sub i64 %22, %23
  %xor.i.11 = xor i64 %23, %22
  %xor1.i.11 = xor i64 %sub.11, %23
  %or.i.11 = or i64 %xor1.i.11, %xor.i.11
  %xor2.i.11 = xor i64 %or.i.11, %22
  %shr.i.11 = lshr i64 %xor2.i.11, 63
  %conv.i.11 = trunc i64 %shr.i.11 to i32
  %sub.i.i.11 = sub i64 0, %sub.11
  %or.i.i.11 = or i64 %sub.11, %sub.i.i.11
  %shr.i.i.11 = lshr i64 %or.i.i.11, 63
  %conv.i.i.11 = trunc i64 %shr.i.i.11 to i32
  %xor.i79.11 = xor i32 %conv.i.i.11, 1
  %and.11 = and i32 %xor.i79.11, %or.10
  %or.11 = or i32 %and.11, %conv.i.11
  %conv.11 = zext i32 %or.10 to i64
  %sub8.11 = sub i64 %sub.11, %conv.11
  store i64 %sub8.11, i64* %arrayidx.11, align 8, !tbaa !3
  %conv11 = zext i32 %or.11 to i64
  %sub12 = sub nsw i64 0, %conv11
  %24 = load i64, i64* %a, align 8, !tbaa !3
  %25 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 0), align 16, !tbaa !3
  %and23 = and i64 %25, %sub12
  %add24 = add i64 %and23, %24
  store i64 %add24, i64* %a, align 8, !tbaa !3
  %xor.i66 = xor i64 %add24, %24
  %xor1.i68 = xor i64 %and23, %24
  %or.i69 = or i64 %xor.i66, %xor1.i68
  %xor2.i70 = xor i64 %or.i69, %add24
  %or32 = lshr i64 %xor2.i70, 63
  %26 = load i64, i64* %arrayidx.1, align 8, !tbaa !3
  %add.1 = add i64 %26, %or32
  %27 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 1), align 8, !tbaa !3
  %and23.1 = and i64 %27, %sub12
  %add24.1 = add i64 %and23.1, %add.1
  store i64 %add24.1, i64* %arrayidx.1, align 8, !tbaa !3
  %28 = xor i64 %add.1, -9223372036854775808
  %xor2.i76.1 = and i64 %28, %26
  %xor.i66.1 = xor i64 %add24.1, %add.1
  %xor1.i68.1 = xor i64 %and23.1, %add.1
  %or.i69.1 = or i64 %xor.i66.1, %xor1.i68.1
  %xor2.i70.1 = xor i64 %or.i69.1, %add24.1
  %shr.i7780.1 = or i64 %xor2.i70.1, %xor2.i76.1
  %or32.1 = lshr i64 %shr.i7780.1, 63
  %29 = load i64, i64* %arrayidx.2, align 8, !tbaa !3
  %add.2 = add i64 %29, %or32.1
  %30 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 2), align 16, !tbaa !3
  %and23.2 = and i64 %30, %sub12
  %add24.2 = add i64 %and23.2, %add.2
  store i64 %add24.2, i64* %arrayidx.2, align 8, !tbaa !3
  %31 = xor i64 %add.2, -9223372036854775808
  %xor2.i76.2 = and i64 %31, %29
  %xor.i66.2 = xor i64 %add24.2, %add.2
  %xor1.i68.2 = xor i64 %and23.2, %add.2
  %or.i69.2 = or i64 %xor.i66.2, %xor1.i68.2
  %xor2.i70.2 = xor i64 %or.i69.2, %add24.2
  %shr.i7780.2 = or i64 %xor2.i70.2, %xor2.i76.2
  %or32.2 = lshr i64 %shr.i7780.2, 63
  %32 = load i64, i64* %arrayidx.3, align 8, !tbaa !3
  %add.3 = add i64 %32, %or32.2
  %33 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 3), align 8, !tbaa !3
  %and23.3 = and i64 %33, %sub12
  %add24.3 = add i64 %and23.3, %add.3
  store i64 %add24.3, i64* %arrayidx.3, align 8, !tbaa !3
  %34 = xor i64 %add.3, -9223372036854775808
  %xor2.i76.3 = and i64 %34, %32
  %xor.i66.3 = xor i64 %add24.3, %add.3
  %xor1.i68.3 = xor i64 %and23.3, %add.3
  %or.i69.3 = or i64 %xor.i66.3, %xor1.i68.3
  %xor2.i70.3 = xor i64 %or.i69.3, %add24.3
  %shr.i7780.3 = or i64 %xor2.i70.3, %xor2.i76.3
  %or32.3 = lshr i64 %shr.i7780.3, 63
  %35 = load i64, i64* %arrayidx.4, align 8, !tbaa !3
  %add.4 = add i64 %35, %or32.3
  %36 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 4), align 16, !tbaa !3
  %and23.4 = and i64 %36, %sub12
  %add24.4 = add i64 %and23.4, %add.4
  store i64 %add24.4, i64* %arrayidx.4, align 8, !tbaa !3
  %37 = xor i64 %add.4, -9223372036854775808
  %xor2.i76.4 = and i64 %37, %35
  %xor.i66.4 = xor i64 %add24.4, %add.4
  %xor1.i68.4 = xor i64 %and23.4, %add.4
  %or.i69.4 = or i64 %xor.i66.4, %xor1.i68.4
  %xor2.i70.4 = xor i64 %or.i69.4, %add24.4
  %shr.i7780.4 = or i64 %xor2.i70.4, %xor2.i76.4
  %or32.4 = lshr i64 %shr.i7780.4, 63
  %38 = load i64, i64* %arrayidx.5, align 8, !tbaa !3
  %add.5 = add i64 %38, %or32.4
  %39 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 5), align 8, !tbaa !3
  %and23.5 = and i64 %39, %sub12
  %add24.5 = add i64 %and23.5, %add.5
  store i64 %add24.5, i64* %arrayidx.5, align 8, !tbaa !3
  %40 = xor i64 %add.5, -9223372036854775808
  %xor2.i76.5 = and i64 %40, %38
  %xor.i66.5 = xor i64 %add24.5, %add.5
  %xor1.i68.5 = xor i64 %and23.5, %add.5
  %or.i69.5 = or i64 %xor.i66.5, %xor1.i68.5
  %xor2.i70.5 = xor i64 %or.i69.5, %add24.5
  %shr.i7780.5 = or i64 %xor2.i70.5, %xor2.i76.5
  %or32.5 = lshr i64 %shr.i7780.5, 63
  %41 = load i64, i64* %arrayidx.6, align 8, !tbaa !3
  %add.6 = add i64 %41, %or32.5
  %42 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 6), align 16, !tbaa !3
  %and23.6 = and i64 %42, %sub12
  %add24.6 = add i64 %and23.6, %add.6
  store i64 %add24.6, i64* %arrayidx.6, align 8, !tbaa !3
  %43 = xor i64 %add.6, -9223372036854775808
  %xor2.i76.6 = and i64 %43, %41
  %xor.i66.6 = xor i64 %add24.6, %add.6
  %xor1.i68.6 = xor i64 %and23.6, %add.6
  %or.i69.6 = or i64 %xor.i66.6, %xor1.i68.6
  %xor2.i70.6 = xor i64 %or.i69.6, %add24.6
  %shr.i7780.6 = or i64 %xor2.i70.6, %xor2.i76.6
  %or32.6 = lshr i64 %shr.i7780.6, 63
  %44 = load i64, i64* %arrayidx.7, align 8, !tbaa !3
  %add.7 = add i64 %44, %or32.6
  %45 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 7), align 8, !tbaa !3
  %and23.7 = and i64 %45, %sub12
  %add24.7 = add i64 %and23.7, %add.7
  store i64 %add24.7, i64* %arrayidx.7, align 8, !tbaa !3
  %46 = xor i64 %add.7, -9223372036854775808
  %xor2.i76.7 = and i64 %46, %44
  %xor.i66.7 = xor i64 %add24.7, %add.7
  %xor1.i68.7 = xor i64 %and23.7, %add.7
  %or.i69.7 = or i64 %xor.i66.7, %xor1.i68.7
  %xor2.i70.7 = xor i64 %or.i69.7, %add24.7
  %shr.i7780.7 = or i64 %xor2.i70.7, %xor2.i76.7
  %or32.7 = lshr i64 %shr.i7780.7, 63
  %47 = load i64, i64* %arrayidx.8, align 8, !tbaa !3
  %add.8 = add i64 %47, %or32.7
  %48 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 8), align 16, !tbaa !3
  %and23.8 = and i64 %48, %sub12
  %add24.8 = add i64 %and23.8, %add.8
  store i64 %add24.8, i64* %arrayidx.8, align 8, !tbaa !3
  %49 = xor i64 %add.8, -9223372036854775808
  %xor2.i76.8 = and i64 %49, %47
  %xor.i66.8 = xor i64 %add24.8, %add.8
  %xor1.i68.8 = xor i64 %and23.8, %add.8
  %or.i69.8 = or i64 %xor.i66.8, %xor1.i68.8
  %xor2.i70.8 = xor i64 %or.i69.8, %add24.8
  %shr.i7780.8 = or i64 %xor2.i70.8, %xor2.i76.8
  %or32.8 = lshr i64 %shr.i7780.8, 63
  %50 = load i64, i64* %arrayidx.9, align 8, !tbaa !3
  %add.9 = add i64 %50, %or32.8
  %51 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 9), align 8, !tbaa !3
  %and23.9 = and i64 %51, %sub12
  %add24.9 = add i64 %and23.9, %add.9
  store i64 %add24.9, i64* %arrayidx.9, align 8, !tbaa !3
  %52 = xor i64 %add.9, -9223372036854775808
  %xor2.i76.9 = and i64 %52, %50
  %xor.i66.9 = xor i64 %add24.9, %add.9
  %xor1.i68.9 = xor i64 %and23.9, %add.9
  %or.i69.9 = or i64 %xor.i66.9, %xor1.i68.9
  %xor2.i70.9 = xor i64 %or.i69.9, %add24.9
  %shr.i7780.9 = or i64 %xor2.i70.9, %xor2.i76.9
  %or32.9 = lshr i64 %shr.i7780.9, 63
  %53 = load i64, i64* %arrayidx.10, align 8, !tbaa !3
  %add.10 = add i64 %53, %or32.9
  %54 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 10), align 16, !tbaa !3
  %and23.10 = and i64 %54, %sub12
  %add24.10 = add i64 %and23.10, %add.10
  store i64 %add24.10, i64* %arrayidx.10, align 8, !tbaa !3
  %55 = xor i64 %add.10, -9223372036854775808
  %xor2.i76.10 = and i64 %55, %53
  %xor.i66.10 = xor i64 %add24.10, %add.10
  %xor1.i68.10 = xor i64 %and23.10, %add.10
  %or.i69.10 = or i64 %xor.i66.10, %xor1.i68.10
  %xor2.i70.10 = xor i64 %or.i69.10, %add24.10
  %shr.i7780.10 = or i64 %xor2.i70.10, %xor2.i76.10
  %or32.10 = lshr i64 %shr.i7780.10, 63
  %56 = load i64, i64* %arrayidx.11, align 8, !tbaa !3
  %add.11 = add i64 %56, %or32.10
  %57 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751, i64 0, i64 11), align 8, !tbaa !3
  %and23.11 = and i64 %57, %sub12
  %add24.11 = add i64 %and23.11, %add.11
  store i64 %add24.11, i64* %arrayidx.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: norecurse nounwind ssp uwtable
define void @digit_x_digit(i64 %a, i64 %b, i64* nocapture %c) local_unnamed_addr #3 {
entry:
  %and = and i64 %a, 4294967295
  %shr = lshr i64 %a, 32
  %and1 = and i64 %b, 4294967295
  %shr2 = lshr i64 %b, 32
  %mul = mul nuw i64 %and1, %and
  %mul3 = mul nuw i64 %shr2, %and
  %mul4 = mul nuw i64 %and1, %shr
  %mul5 = mul nuw i64 %shr2, %shr
  %and6 = and i64 %mul, 4294967295
  %shr7 = lshr i64 %mul, 32
  %and8 = and i64 %mul4, 4294967295
  %and9 = and i64 %mul3, 4294967295
  %add = add nuw nsw i64 %shr7, %and8
  %add10 = add nuw nsw i64 %add, %and9
  %shr11 = lshr i64 %add10, 32
  %shl = shl i64 %add10, 32
  %xor68 = or i64 %shl, %and6
  store i64 %xor68, i64* %c, align 8, !tbaa !3
  %shr13 = lshr i64 %mul4, 32
  %shr14 = lshr i64 %mul3, 32
  %and15 = and i64 %mul5, 4294967295
  %add16 = add nuw nsw i64 %shr13, %shr14
  %add17 = add nuw nsw i64 %add16, %and15
  %add18 = add nuw nsw i64 %add17, %shr11
  %and19 = and i64 %add18, 4294967295
  %arrayidx20 = getelementptr inbounds i64, i64* %c, i64 1
  %and21 = and i64 %add18, 30064771072
  %and22 = and i64 %mul5, -4294967296
  %add23 = add i64 %and21, %and22
  %xor2569 = or i64 %add23, %and19
  store i64 %xor2569, i64* %arrayidx20, align 8, !tbaa !3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define void @mp_mul(i64* nocapture readonly %a, i64* nocapture readonly %b, i64* nocapture %c, i32 %nwords) local_unnamed_addr #1 {
entry:
  %cmp220 = icmp eq i32 %nwords, 0
  br i1 %cmp220, label %for.cond25.preheader, label %for.cond1.preheader.preheader

for.cond1.preheader.preheader:                    ; preds = %entry
  %wide.trip.count242 = zext i32 %nwords to i64
  br label %for.cond1.preheader

for.cond1.preheader:                              ; preds = %for.end, %for.cond1.preheader.preheader
  %indvars.iv240 = phi i64 [ 0, %for.cond1.preheader.preheader ], [ %indvars.iv.next241, %for.end ]
  %indvars.iv238 = phi i64 [ 1, %for.cond1.preheader.preheader ], [ %indvars.iv.next239, %for.end ]
  %u.0222 = phi i64 [ 0, %for.cond1.preheader.preheader ], [ %add19, %for.end ]
  %v.0221 = phi i64 [ 0, %for.cond1.preheader.preheader ], [ %add13, %for.end ]
  br label %for.body3

for.cond25.preheader:                             ; preds = %for.end, %entry
  %v.0.lcssa = phi i64 [ 0, %entry ], [ %add13, %for.end ]
  %u.0.lcssa = phi i64 [ 0, %entry ], [ %add19, %for.end ]
  %mul = shl i32 %nwords, 1
  %sub26 = add i32 %mul, -1
  %cmp27210 = icmp ugt i32 %sub26, %nwords
  br i1 %cmp27210, label %for.body29.preheader, label %for.end67

for.body29.preheader:                             ; preds = %for.cond25.preheader
  %0 = zext i32 %nwords to i64
  %wide.trip.count = zext i32 %sub26 to i64
  br label %for.body29

for.body3:                                        ; preds = %for.cond1.preheader, %for.body3
  %indvars.iv232 = phi i64 [ 0, %for.cond1.preheader ], [ %indvars.iv.next233, %for.body3 ]
  %t.1218 = phi i64 [ 0, %for.cond1.preheader ], [ %add19, %for.body3 ]
  %u.1217 = phi i64 [ %u.0222, %for.cond1.preheader ], [ %add13, %for.body3 ]
  %v.1216 = phi i64 [ %v.0221, %for.cond1.preheader ], [ %add8, %for.body3 ]
  %arrayidx = getelementptr inbounds i64, i64* %a, i64 %indvars.iv232
  %1 = load i64, i64* %arrayidx, align 8, !tbaa !3
  %sub = sub nsw i64 %indvars.iv240, %indvars.iv232
  %idxprom4 = and i64 %sub, 4294967295
  %arrayidx5 = getelementptr inbounds i64, i64* %b, i64 %idxprom4
  %2 = load i64, i64* %arrayidx5, align 8, !tbaa !3
  %and.i = and i64 %1, 4294967295
  %shr.i = lshr i64 %1, 32
  %and1.i = and i64 %2, 4294967295
  %shr2.i = lshr i64 %2, 32
  %mul.i = mul nuw i64 %and1.i, %and.i
  %mul3.i = mul nuw i64 %shr2.i, %and.i
  %mul4.i = mul nuw i64 %and1.i, %shr.i
  %mul5.i = mul nuw i64 %shr2.i, %shr.i
  %and6.i = and i64 %mul.i, 4294967295
  %shr7.i = lshr i64 %mul.i, 32
  %and8.i = and i64 %mul4.i, 4294967295
  %and9.i = and i64 %mul3.i, 4294967295
  %add.i = add nuw nsw i64 %shr7.i, %and8.i
  %add10.i = add nuw nsw i64 %add.i, %and9.i
  %shr11.i = lshr i64 %add10.i, 32
  %shl.i = shl i64 %add10.i, 32
  %xor68.i = or i64 %shl.i, %and6.i
  %shr13.i = lshr i64 %mul4.i, 32
  %shr14.i = lshr i64 %mul3.i, 32
  %and15.i = and i64 %mul5.i, 4294967295
  %add16.i = add nuw nsw i64 %shr13.i, %shr14.i
  %add17.i = add nuw nsw i64 %add16.i, %and15.i
  %add18.i = add nuw nsw i64 %add17.i, %shr11.i
  %and19.i = and i64 %add18.i, 4294967295
  %and21.i = and i64 %add18.i, 30064771072
  %and22.i = and i64 %mul5.i, -4294967296
  %add23.i = add i64 %and21.i, %and22.i
  %xor2569.i = or i64 %add23.i, %and19.i
  %add8 = add i64 %xor68.i, %v.1216
  %xor.i188 = xor i64 %add8, %shl.i
  %xor1.i189 = xor i64 %shl.i, %v.1216
  %or.i190 = or i64 %xor.i188, %xor1.i189
  %xor2.i191 = xor i64 %or.i190, %add8
  %shr.i192 = lshr i64 %xor2.i191, 63
  %add12 = add i64 %shr.i192, %xor2569.i
  %add13 = add i64 %add12, %u.1217
  %3 = xor i64 %add12, -9223372036854775808
  %xor2.i185 = and i64 %3, %add23.i
  %xor.i176 = xor i64 %add13, %add12
  %xor1.i177 = xor i64 %add12, %u.1217
  %or.i178 = or i64 %xor.i176, %xor1.i177
  %xor2.i179 = xor i64 %or.i178, %add13
  %shr.i186201 = or i64 %xor2.i179, %xor2.i185
  %or17200 = lshr i64 %shr.i186201, 63
  %add19 = add i64 %or17200, %t.1218
  %indvars.iv.next233 = add nuw nsw i64 %indvars.iv232, 1
  %exitcond237 = icmp eq i64 %indvars.iv.next233, %indvars.iv238
  br i1 %exitcond237, label %for.end, label %for.body3

for.end:                                          ; preds = %for.body3
  %arrayidx21 = getelementptr inbounds i64, i64* %c, i64 %indvars.iv240
  store i64 %add8, i64* %arrayidx21, align 8, !tbaa !3
  %indvars.iv.next241 = add nuw nsw i64 %indvars.iv240, 1
  %indvars.iv.next239 = add nuw nsw i64 %indvars.iv238, 1
  %exitcond243 = icmp eq i64 %indvars.iv.next241, %wide.trip.count242
  br i1 %exitcond243, label %for.cond25.preheader, label %for.cond1.preheader

for.body29:                                       ; preds = %for.end62, %for.body29.preheader
  %indvars.iv228 = phi i64 [ %0, %for.body29.preheader ], [ %indvars.iv.next229, %for.end62 ]
  %indvars.iv = phi i32 [ 1, %for.body29.preheader ], [ %indvars.iv.next, %for.end62 ]
  %u.2212 = phi i64 [ %u.0.lcssa, %for.body29.preheader ], [ %t.3.lcssa, %for.end62 ]
  %v.2211 = phi i64 [ %v.0.lcssa, %for.body29.preheader ], [ %u.3.lcssa, %for.end62 ]
  %4 = trunc i64 %indvars.iv228 to i32
  %5 = sub i32 %4, %nwords
  %j.1202 = add i32 %5, 1
  %cmp33203 = icmp ult i32 %j.1202, %nwords
  br i1 %cmp33203, label %for.body35.preheader, label %for.end62

for.body35.preheader:                             ; preds = %for.body29
  %6 = zext i32 %indvars.iv to i64
  br label %for.body35

for.body35:                                       ; preds = %for.body35, %for.body35.preheader
  %indvars.iv226 = phi i64 [ %6, %for.body35.preheader ], [ %indvars.iv.next227, %for.body35 ]
  %t.3206 = phi i64 [ 0, %for.body35.preheader ], [ %add59, %for.body35 ]
  %u.3205 = phi i64 [ %u.2212, %for.body35.preheader ], [ %add53, %for.body35 ]
  %v.3204 = phi i64 [ %v.2211, %for.body35.preheader ], [ %add45, %for.body35 ]
  %arrayidx37 = getelementptr inbounds i64, i64* %a, i64 %indvars.iv226
  %7 = load i64, i64* %arrayidx37, align 8, !tbaa !3
  %sub38 = sub nsw i64 %indvars.iv228, %indvars.iv226
  %idxprom39 = and i64 %sub38, 4294967295
  %arrayidx40 = getelementptr inbounds i64, i64* %b, i64 %idxprom39
  %8 = load i64, i64* %arrayidx40, align 8, !tbaa !3
  %and.i147 = and i64 %7, 4294967295
  %shr.i148 = lshr i64 %7, 32
  %and1.i149 = and i64 %8, 4294967295
  %shr2.i150 = lshr i64 %8, 32
  %mul.i151 = mul nuw i64 %and1.i149, %and.i147
  %mul3.i152 = mul nuw i64 %shr2.i150, %and.i147
  %mul4.i153 = mul nuw i64 %and1.i149, %shr.i148
  %mul5.i154 = mul nuw i64 %shr2.i150, %shr.i148
  %and6.i155 = and i64 %mul.i151, 4294967295
  %shr7.i156 = lshr i64 %mul.i151, 32
  %and8.i157 = and i64 %mul4.i153, 4294967295
  %and9.i158 = and i64 %mul3.i152, 4294967295
  %add.i159 = add nuw nsw i64 %shr7.i156, %and8.i157
  %add10.i160 = add nuw nsw i64 %add.i159, %and9.i158
  %shr11.i161 = lshr i64 %add10.i160, 32
  %shl.i162 = shl i64 %add10.i160, 32
  %xor68.i163 = or i64 %shl.i162, %and6.i155
  %shr13.i164 = lshr i64 %mul4.i153, 32
  %shr14.i165 = lshr i64 %mul3.i152, 32
  %and15.i166 = and i64 %mul5.i154, 4294967295
  %add16.i167 = add nuw nsw i64 %shr13.i164, %shr14.i165
  %add17.i168 = add nuw nsw i64 %add16.i167, %and15.i166
  %add18.i169 = add nuw nsw i64 %add17.i168, %shr11.i161
  %and19.i170 = and i64 %add18.i169, 4294967295
  %and21.i172 = and i64 %add18.i169, 30064771072
  %and22.i173 = and i64 %mul5.i154, -4294967296
  %add23.i174 = add i64 %and21.i172, %and22.i173
  %xor2569.i175 = or i64 %add23.i174, %and19.i170
  %add45 = add i64 %xor68.i163, %v.3204
  %xor.i141 = xor i64 %add45, %shl.i162
  %xor1.i142 = xor i64 %shl.i162, %v.3204
  %or.i143 = or i64 %xor.i141, %xor1.i142
  %xor2.i144 = xor i64 %or.i143, %add45
  %shr.i145 = lshr i64 %xor2.i144, 63
  %add52 = add i64 %shr.i145, %xor2569.i175
  %add53 = add i64 %add52, %u.3205
  %9 = xor i64 %add52, -9223372036854775808
  %xor2.i138 = and i64 %9, %add23.i174
  %xor.i = xor i64 %add53, %add52
  %xor1.i = xor i64 %add52, %u.3205
  %or.i = or i64 %xor.i, %xor1.i
  %xor2.i = xor i64 %or.i, %add53
  %shr.i139199 = or i64 %xor2.i, %xor2.i138
  %or57198 = lshr i64 %shr.i139199, 63
  %add59 = add i64 %or57198, %t.3206
  %indvars.iv.next227 = add nuw nsw i64 %indvars.iv226, 1
  %lftr.wideiv = trunc i64 %indvars.iv.next227 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nwords
  br i1 %exitcond, label %for.end62, label %for.body35

for.end62:                                        ; preds = %for.body35, %for.body29
  %v.3.lcssa = phi i64 [ %v.2211, %for.body29 ], [ %add45, %for.body35 ]
  %u.3.lcssa = phi i64 [ %u.2212, %for.body29 ], [ %add53, %for.body35 ]
  %t.3.lcssa = phi i64 [ 0, %for.body29 ], [ %add59, %for.body35 ]
  %arrayidx64 = getelementptr inbounds i64, i64* %c, i64 %indvars.iv228
  store i64 %v.3.lcssa, i64* %arrayidx64, align 8, !tbaa !3
  %indvars.iv.next229 = add nuw nsw i64 %indvars.iv228, 1
  %indvars.iv.next = add i32 %indvars.iv, 1
  %exitcond231 = icmp eq i64 %indvars.iv.next229, %wide.trip.count
  br i1 %exitcond231, label %for.end67, label %for.body29

for.end67:                                        ; preds = %for.end62, %for.cond25.preheader
  %v.2.lcssa = phi i64 [ %v.0.lcssa, %for.cond25.preheader ], [ %u.3.lcssa, %for.end62 ]
  %idxprom70.pre-phi = zext i32 %sub26 to i64
  %arrayidx71 = getelementptr inbounds i64, i64* %c, i64 %idxprom70.pre-phi
  store i64 %v.2.lcssa, i64* %arrayidx71, align 8, !tbaa !3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define void @rdc_mont(i64* nocapture readonly %ma, i64* nocapture %mc) local_unnamed_addr #1 {
for.body6.lr.ph.2:
  %mc370 = bitcast i64* %mc to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %mc370, i8 0, i64 96, i1 false)
  %0 = load i64, i64* %ma, align 8, !tbaa !3
  store i64 %0, i64* %mc, align 8, !tbaa !3
  %1 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 1), align 8, !tbaa !3
  %and.i.1 = and i64 %0, 4294967295
  %shr.i.1 = lshr i64 %0, 32
  %and1.i.1 = and i64 %1, 4294967295
  %shr2.i.1 = lshr i64 %1, 32
  %mul.i.1 = mul nuw i64 %and1.i.1, %and.i.1
  %mul3.i.1 = mul nuw i64 %shr2.i.1, %and.i.1
  %mul4.i.1 = mul nuw i64 %and1.i.1, %shr.i.1
  %mul5.i.1 = mul nuw i64 %shr2.i.1, %shr.i.1
  %and6.i.1 = and i64 %mul.i.1, 4294967295
  %shr7.i.1 = lshr i64 %mul.i.1, 32
  %and8.i.1 = and i64 %mul4.i.1, 4294967295
  %and9.i.1 = and i64 %mul3.i.1, 4294967295
  %add.i.1 = add nuw nsw i64 %shr7.i.1, %and8.i.1
  %add10.i.1 = add nuw nsw i64 %add.i.1, %and9.i.1
  %shr11.i.1 = lshr i64 %add10.i.1, 32
  %shl.i.1 = shl i64 %add10.i.1, 32
  %xor68.i.1 = or i64 %shl.i.1, %and6.i.1
  %shr13.i.1 = lshr i64 %mul4.i.1, 32
  %shr14.i.1 = lshr i64 %mul3.i.1, 32
  %and15.i.1 = and i64 %mul5.i.1, 4294967295
  %add16.i.1 = add nuw nsw i64 %shr13.i.1, %shr14.i.1
  %add17.i.1 = add nuw nsw i64 %add16.i.1, %and15.i.1
  %add18.i.1 = add nuw nsw i64 %add17.i.1, %shr11.i.1
  %and19.i.1 = and i64 %add18.i.1, 4294967295
  %and21.i.1 = and i64 %add18.i.1, 30064771072
  %and22.i.1 = and i64 %mul5.i.1, -4294967296
  %add23.i.1 = add i64 %and21.i.1, %and22.i.1
  %xor2569.i.1 = or i64 %add23.i.1, %and19.i.1
  %arrayidx34.1 = getelementptr inbounds i64, i64* %ma, i64 1
  %2 = load i64, i64* %arrayidx34.1, align 8, !tbaa !3
  %add35.1 = add i64 %2, %xor68.i.1
  %xor.i297.1 = xor i64 %add35.1, %shl.i.1
  %xor1.i298.1 = xor i64 %2, %shl.i.1
  %or.i299.1 = or i64 %xor.i297.1, %xor1.i298.1
  %xor2.i300.1 = xor i64 %or.i299.1, %add35.1
  %shr.i301.1 = lshr i64 %xor2.i300.1, 63
  %add41.1 = add i64 %shr.i301.1, %xor2569.i.1
  %arrayidx50.1 = getelementptr inbounds i64, i64* %mc, i64 1
  store i64 %add35.1, i64* %arrayidx50.1, align 8, !tbaa !3
  %3 = xor i64 %add41.1, -9223372036854775808
  %xor2.i294.1 = and i64 %3, %add23.i.1
  %shr.i295.1 = lshr i64 %xor2.i294.1, 63
  %4 = load i64, i64* %mc, align 8, !tbaa !3
  %5 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 2), align 16, !tbaa !3
  %and.i.2 = and i64 %4, 4294967295
  %shr.i.2 = lshr i64 %4, 32
  %and1.i.2 = and i64 %5, 4294967295
  %shr2.i.2 = lshr i64 %5, 32
  %mul.i.2 = mul nuw i64 %and1.i.2, %and.i.2
  %mul3.i.2 = mul nuw i64 %shr2.i.2, %and.i.2
  %mul4.i.2 = mul nuw i64 %and1.i.2, %shr.i.2
  %mul5.i.2 = mul nuw i64 %shr2.i.2, %shr.i.2
  %and6.i.2 = and i64 %mul.i.2, 4294967295
  %shr7.i.2 = lshr i64 %mul.i.2, 32
  %and8.i.2 = and i64 %mul4.i.2, 4294967295
  %and9.i.2 = and i64 %mul3.i.2, 4294967295
  %add.i.2 = add nuw nsw i64 %shr7.i.2, %and8.i.2
  %add10.i.2 = add nuw nsw i64 %add.i.2, %and9.i.2
  %shr11.i.2 = lshr i64 %add10.i.2, 32
  %shl.i.2 = shl i64 %add10.i.2, 32
  %xor68.i.2 = or i64 %shl.i.2, %and6.i.2
  %shr13.i.2 = lshr i64 %mul4.i.2, 32
  %shr14.i.2 = lshr i64 %mul3.i.2, 32
  %and15.i.2 = and i64 %mul5.i.2, 4294967295
  %add16.i.2 = add nuw nsw i64 %shr13.i.2, %shr14.i.2
  %add17.i.2 = add nuw nsw i64 %add16.i.2, %and15.i.2
  %add18.i.2 = add nuw nsw i64 %add17.i.2, %shr11.i.2
  %and19.i.2 = and i64 %add18.i.2, 4294967295
  %and21.i.2 = and i64 %add18.i.2, 30064771072
  %and22.i.2 = and i64 %mul5.i.2, -4294967296
  %add23.i.2 = add i64 %and21.i.2, %and22.i.2
  %xor2569.i.2 = or i64 %add23.i.2, %and19.i.2
  %add16.2 = add i64 %xor68.i.2, %add41.1
  %xor.i315.2 = xor i64 %add16.2, %shl.i.2
  %xor1.i316.2 = xor i64 %shl.i.2, %add41.1
  %or.i317.2 = or i64 %xor.i315.2, %xor1.i316.2
  %xor2.i318.2 = xor i64 %or.i317.2, %add16.2
  %shr.i319.2 = lshr i64 %xor2.i318.2, 63
  %add20.2 = add i64 %shr.i319.2, %xor2569.i.2
  %add21.2 = add i64 %add20.2, %shr.i295.1
  %6 = xor i64 %add20.2, -9223372036854775808
  %xor2.i312.2 = and i64 %6, %add23.i.2
  %7 = xor i64 %add21.2, -9223372036854775808
  %xor2.i306.2 = and i64 %add20.2, %7
  %shr.i313328.2 = or i64 %xor2.i306.2, %xor2.i312.2
  %or25327.2 = lshr i64 %shr.i313328.2, 63
  %arrayidx9.2.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %8 = load i64, i64* %arrayidx9.2.1, align 8, !tbaa !3
  %9 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 1), align 8, !tbaa !3
  %and.i.2.1 = and i64 %8, 4294967295
  %shr.i.2.1 = lshr i64 %8, 32
  %and1.i.2.1 = and i64 %9, 4294967295
  %shr2.i.2.1 = lshr i64 %9, 32
  %mul.i.2.1 = mul nuw i64 %and1.i.2.1, %and.i.2.1
  %mul3.i.2.1 = mul nuw i64 %shr2.i.2.1, %and.i.2.1
  %mul4.i.2.1 = mul nuw i64 %and1.i.2.1, %shr.i.2.1
  %mul5.i.2.1 = mul nuw i64 %shr2.i.2.1, %shr.i.2.1
  %and6.i.2.1 = and i64 %mul.i.2.1, 4294967295
  %shr7.i.2.1 = lshr i64 %mul.i.2.1, 32
  %and8.i.2.1 = and i64 %mul4.i.2.1, 4294967295
  %and9.i.2.1 = and i64 %mul3.i.2.1, 4294967295
  %add.i.2.1 = add nuw nsw i64 %shr7.i.2.1, %and8.i.2.1
  %add10.i.2.1 = add nuw nsw i64 %add.i.2.1, %and9.i.2.1
  %shr11.i.2.1 = lshr i64 %add10.i.2.1, 32
  %shl.i.2.1 = shl i64 %add10.i.2.1, 32
  %xor68.i.2.1 = or i64 %shl.i.2.1, %and6.i.2.1
  %shr13.i.2.1 = lshr i64 %mul4.i.2.1, 32
  %shr14.i.2.1 = lshr i64 %mul3.i.2.1, 32
  %and15.i.2.1 = and i64 %mul5.i.2.1, 4294967295
  %add16.i.2.1 = add nuw nsw i64 %shr13.i.2.1, %shr14.i.2.1
  %add17.i.2.1 = add nuw nsw i64 %add16.i.2.1, %and15.i.2.1
  %add18.i.2.1 = add nuw nsw i64 %add17.i.2.1, %shr11.i.2.1
  %and19.i.2.1 = and i64 %add18.i.2.1, 4294967295
  %and21.i.2.1 = and i64 %add18.i.2.1, 30064771072
  %and22.i.2.1 = and i64 %mul5.i.2.1, -4294967296
  %add23.i.2.1 = add i64 %and21.i.2.1, %and22.i.2.1
  %xor2569.i.2.1 = or i64 %add23.i.2.1, %and19.i.2.1
  %add16.2.1 = add i64 %xor68.i.2.1, %add16.2
  %xor.i315.2.1 = xor i64 %add16.2.1, %shl.i.2.1
  %xor1.i316.2.1 = xor i64 %shl.i.2.1, %add16.2
  %or.i317.2.1 = or i64 %xor.i315.2.1, %xor1.i316.2.1
  %xor2.i318.2.1 = xor i64 %or.i317.2.1, %add16.2.1
  %shr.i319.2.1 = lshr i64 %xor2.i318.2.1, 63
  %add20.2.1 = add i64 %shr.i319.2.1, %xor2569.i.2.1
  %add21.2.1 = add i64 %add20.2.1, %add21.2
  %10 = xor i64 %add20.2.1, -9223372036854775808
  %xor2.i312.2.1 = and i64 %10, %add23.i.2.1
  %xor.i303.2.1 = xor i64 %add21.2.1, %add20.2.1
  %xor1.i304.2.1 = xor i64 %add20.2.1, %add21.2
  %or.i305.2.1 = or i64 %xor.i303.2.1, %xor1.i304.2.1
  %xor2.i306.2.1 = xor i64 %or.i305.2.1, %add21.2.1
  %shr.i313328.2.1 = or i64 %xor2.i306.2.1, %xor2.i312.2.1
  %or25327.2.1 = lshr i64 %shr.i313328.2.1, 63
  %add27.2.1 = add nuw nsw i64 %or25327.2.1, %or25327.2
  %arrayidx34.2 = getelementptr inbounds i64, i64* %ma, i64 2
  %11 = load i64, i64* %arrayidx34.2, align 8, !tbaa !3
  %add35.2 = add i64 %11, %add16.2.1
  %xor.i297.2 = xor i64 %add35.2, %add16.2.1
  %xor1.i298.2 = xor i64 %11, %add16.2.1
  %or.i299.2 = or i64 %xor.i297.2, %xor1.i298.2
  %xor2.i300.2 = xor i64 %or.i299.2, %add35.2
  %shr.i301.2 = lshr i64 %xor2.i300.2, 63
  %add41.2 = add i64 %shr.i301.2, %add21.2.1
  %arrayidx50.2 = getelementptr inbounds i64, i64* %mc, i64 2
  store i64 %add35.2, i64* %arrayidx50.2, align 8, !tbaa !3
  %12 = xor i64 %add41.2, -9223372036854775808
  %xor2.i294.2 = and i64 %12, %add21.2.1
  %shr.i295.2 = lshr i64 %xor2.i294.2, 63
  %add48.2 = add nsw i64 %shr.i295.2, %add27.2.1
  %13 = load i64, i64* %mc, align 8, !tbaa !3
  %14 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 3), align 8, !tbaa !3
  %and.i.3 = and i64 %13, 4294967295
  %shr.i.3 = lshr i64 %13, 32
  %and1.i.3 = and i64 %14, 4294967295
  %shr2.i.3 = lshr i64 %14, 32
  %mul.i.3 = mul nuw i64 %and1.i.3, %and.i.3
  %mul3.i.3 = mul nuw i64 %shr2.i.3, %and.i.3
  %mul4.i.3 = mul nuw i64 %and1.i.3, %shr.i.3
  %mul5.i.3 = mul nuw i64 %shr2.i.3, %shr.i.3
  %and6.i.3 = and i64 %mul.i.3, 4294967295
  %shr7.i.3 = lshr i64 %mul.i.3, 32
  %and8.i.3 = and i64 %mul4.i.3, 4294967295
  %and9.i.3 = and i64 %mul3.i.3, 4294967295
  %add.i.3 = add nuw nsw i64 %shr7.i.3, %and8.i.3
  %add10.i.3 = add nuw nsw i64 %add.i.3, %and9.i.3
  %shr11.i.3 = lshr i64 %add10.i.3, 32
  %shl.i.3 = shl i64 %add10.i.3, 32
  %xor68.i.3 = or i64 %shl.i.3, %and6.i.3
  %shr13.i.3 = lshr i64 %mul4.i.3, 32
  %shr14.i.3 = lshr i64 %mul3.i.3, 32
  %and15.i.3 = and i64 %mul5.i.3, 4294967295
  %add16.i.3 = add nuw nsw i64 %shr13.i.3, %shr14.i.3
  %add17.i.3 = add nuw nsw i64 %add16.i.3, %and15.i.3
  %add18.i.3 = add nuw nsw i64 %add17.i.3, %shr11.i.3
  %and19.i.3 = and i64 %add18.i.3, 4294967295
  %and21.i.3 = and i64 %add18.i.3, 30064771072
  %and22.i.3 = and i64 %mul5.i.3, -4294967296
  %add23.i.3 = add i64 %and21.i.3, %and22.i.3
  %xor2569.i.3 = or i64 %add23.i.3, %and19.i.3
  %add16.3 = add i64 %xor68.i.3, %add41.2
  %xor.i315.3 = xor i64 %add16.3, %shl.i.3
  %xor1.i316.3 = xor i64 %shl.i.3, %add41.2
  %or.i317.3 = or i64 %xor.i315.3, %xor1.i316.3
  %xor2.i318.3 = xor i64 %or.i317.3, %add16.3
  %shr.i319.3 = lshr i64 %xor2.i318.3, 63
  %add20.3 = add i64 %shr.i319.3, %xor2569.i.3
  %add21.3 = add i64 %add20.3, %add48.2
  %15 = xor i64 %add20.3, -9223372036854775808
  %xor2.i312.3 = and i64 %15, %add23.i.3
  %xor.i303.3 = xor i64 %add21.3, %add20.3
  %xor1.i304.3 = xor i64 %add20.3, %add48.2
  %or.i305.3 = or i64 %xor.i303.3, %xor1.i304.3
  %xor2.i306.3 = xor i64 %or.i305.3, %add21.3
  %shr.i313328.3 = or i64 %xor2.i306.3, %xor2.i312.3
  %or25327.3 = lshr i64 %shr.i313328.3, 63
  %arrayidx9.3.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %16 = load i64, i64* %arrayidx9.3.1, align 8, !tbaa !3
  %17 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 2), align 16, !tbaa !3
  %and.i.3.1 = and i64 %16, 4294967295
  %shr.i.3.1 = lshr i64 %16, 32
  %and1.i.3.1 = and i64 %17, 4294967295
  %shr2.i.3.1 = lshr i64 %17, 32
  %mul.i.3.1 = mul nuw i64 %and1.i.3.1, %and.i.3.1
  %mul3.i.3.1 = mul nuw i64 %shr2.i.3.1, %and.i.3.1
  %mul4.i.3.1 = mul nuw i64 %and1.i.3.1, %shr.i.3.1
  %mul5.i.3.1 = mul nuw i64 %shr2.i.3.1, %shr.i.3.1
  %and6.i.3.1 = and i64 %mul.i.3.1, 4294967295
  %shr7.i.3.1 = lshr i64 %mul.i.3.1, 32
  %and8.i.3.1 = and i64 %mul4.i.3.1, 4294967295
  %and9.i.3.1 = and i64 %mul3.i.3.1, 4294967295
  %add.i.3.1 = add nuw nsw i64 %shr7.i.3.1, %and8.i.3.1
  %add10.i.3.1 = add nuw nsw i64 %add.i.3.1, %and9.i.3.1
  %shr11.i.3.1 = lshr i64 %add10.i.3.1, 32
  %shl.i.3.1 = shl i64 %add10.i.3.1, 32
  %xor68.i.3.1 = or i64 %shl.i.3.1, %and6.i.3.1
  %shr13.i.3.1 = lshr i64 %mul4.i.3.1, 32
  %shr14.i.3.1 = lshr i64 %mul3.i.3.1, 32
  %and15.i.3.1 = and i64 %mul5.i.3.1, 4294967295
  %add16.i.3.1 = add nuw nsw i64 %shr13.i.3.1, %shr14.i.3.1
  %add17.i.3.1 = add nuw nsw i64 %add16.i.3.1, %and15.i.3.1
  %add18.i.3.1 = add nuw nsw i64 %add17.i.3.1, %shr11.i.3.1
  %and19.i.3.1 = and i64 %add18.i.3.1, 4294967295
  %and21.i.3.1 = and i64 %add18.i.3.1, 30064771072
  %and22.i.3.1 = and i64 %mul5.i.3.1, -4294967296
  %add23.i.3.1 = add i64 %and21.i.3.1, %and22.i.3.1
  %xor2569.i.3.1 = or i64 %add23.i.3.1, %and19.i.3.1
  %add16.3.1 = add i64 %xor68.i.3.1, %add16.3
  %xor.i315.3.1 = xor i64 %add16.3.1, %shl.i.3.1
  %xor1.i316.3.1 = xor i64 %shl.i.3.1, %add16.3
  %or.i317.3.1 = or i64 %xor.i315.3.1, %xor1.i316.3.1
  %xor2.i318.3.1 = xor i64 %or.i317.3.1, %add16.3.1
  %shr.i319.3.1 = lshr i64 %xor2.i318.3.1, 63
  %add20.3.1 = add i64 %shr.i319.3.1, %xor2569.i.3.1
  %add21.3.1 = add i64 %add20.3.1, %add21.3
  %18 = xor i64 %add20.3.1, -9223372036854775808
  %xor2.i312.3.1 = and i64 %18, %add23.i.3.1
  %xor.i303.3.1 = xor i64 %add21.3.1, %add20.3.1
  %xor1.i304.3.1 = xor i64 %add20.3.1, %add21.3
  %or.i305.3.1 = or i64 %xor.i303.3.1, %xor1.i304.3.1
  %xor2.i306.3.1 = xor i64 %or.i305.3.1, %add21.3.1
  %shr.i313328.3.1 = or i64 %xor2.i306.3.1, %xor2.i312.3.1
  %or25327.3.1 = lshr i64 %shr.i313328.3.1, 63
  %add27.3.1 = add nuw nsw i64 %or25327.3.1, %or25327.3
  %arrayidx9.3.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %19 = load i64, i64* %arrayidx9.3.2, align 8, !tbaa !3
  %20 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 1), align 8, !tbaa !3
  %and.i.3.2 = and i64 %19, 4294967295
  %shr.i.3.2 = lshr i64 %19, 32
  %and1.i.3.2 = and i64 %20, 4294967295
  %shr2.i.3.2 = lshr i64 %20, 32
  %mul.i.3.2 = mul nuw i64 %and1.i.3.2, %and.i.3.2
  %mul3.i.3.2 = mul nuw i64 %shr2.i.3.2, %and.i.3.2
  %mul4.i.3.2 = mul nuw i64 %and1.i.3.2, %shr.i.3.2
  %mul5.i.3.2 = mul nuw i64 %shr2.i.3.2, %shr.i.3.2
  %and6.i.3.2 = and i64 %mul.i.3.2, 4294967295
  %shr7.i.3.2 = lshr i64 %mul.i.3.2, 32
  %and8.i.3.2 = and i64 %mul4.i.3.2, 4294967295
  %and9.i.3.2 = and i64 %mul3.i.3.2, 4294967295
  %add.i.3.2 = add nuw nsw i64 %shr7.i.3.2, %and8.i.3.2
  %add10.i.3.2 = add nuw nsw i64 %add.i.3.2, %and9.i.3.2
  %shr11.i.3.2 = lshr i64 %add10.i.3.2, 32
  %shl.i.3.2 = shl i64 %add10.i.3.2, 32
  %xor68.i.3.2 = or i64 %shl.i.3.2, %and6.i.3.2
  %shr13.i.3.2 = lshr i64 %mul4.i.3.2, 32
  %shr14.i.3.2 = lshr i64 %mul3.i.3.2, 32
  %and15.i.3.2 = and i64 %mul5.i.3.2, 4294967295
  %add16.i.3.2 = add nuw nsw i64 %shr13.i.3.2, %shr14.i.3.2
  %add17.i.3.2 = add nuw nsw i64 %add16.i.3.2, %and15.i.3.2
  %add18.i.3.2 = add nuw nsw i64 %add17.i.3.2, %shr11.i.3.2
  %and19.i.3.2 = and i64 %add18.i.3.2, 4294967295
  %and21.i.3.2 = and i64 %add18.i.3.2, 30064771072
  %and22.i.3.2 = and i64 %mul5.i.3.2, -4294967296
  %add23.i.3.2 = add i64 %and21.i.3.2, %and22.i.3.2
  %xor2569.i.3.2 = or i64 %add23.i.3.2, %and19.i.3.2
  %add16.3.2 = add i64 %xor68.i.3.2, %add16.3.1
  %xor.i315.3.2 = xor i64 %add16.3.2, %shl.i.3.2
  %xor1.i316.3.2 = xor i64 %shl.i.3.2, %add16.3.1
  %or.i317.3.2 = or i64 %xor.i315.3.2, %xor1.i316.3.2
  %xor2.i318.3.2 = xor i64 %or.i317.3.2, %add16.3.2
  %shr.i319.3.2 = lshr i64 %xor2.i318.3.2, 63
  %add20.3.2 = add i64 %shr.i319.3.2, %xor2569.i.3.2
  %add21.3.2 = add i64 %add20.3.2, %add21.3.1
  %21 = xor i64 %add20.3.2, -9223372036854775808
  %xor2.i312.3.2 = and i64 %21, %add23.i.3.2
  %xor.i303.3.2 = xor i64 %add21.3.2, %add20.3.2
  %xor1.i304.3.2 = xor i64 %add20.3.2, %add21.3.1
  %or.i305.3.2 = or i64 %xor.i303.3.2, %xor1.i304.3.2
  %xor2.i306.3.2 = xor i64 %or.i305.3.2, %add21.3.2
  %shr.i313328.3.2 = or i64 %xor2.i306.3.2, %xor2.i312.3.2
  %or25327.3.2 = lshr i64 %shr.i313328.3.2, 63
  %add27.3.2 = add nsw i64 %or25327.3.2, %add27.3.1
  %arrayidx34.3 = getelementptr inbounds i64, i64* %ma, i64 3
  %22 = load i64, i64* %arrayidx34.3, align 8, !tbaa !3
  %add35.3 = add i64 %22, %add16.3.2
  %xor.i297.3 = xor i64 %add35.3, %add16.3.2
  %xor1.i298.3 = xor i64 %22, %add16.3.2
  %or.i299.3 = or i64 %xor.i297.3, %xor1.i298.3
  %xor2.i300.3 = xor i64 %or.i299.3, %add35.3
  %shr.i301.3 = lshr i64 %xor2.i300.3, 63
  %add41.3 = add i64 %shr.i301.3, %add21.3.2
  %23 = xor i64 %add41.3, -9223372036854775808
  %xor2.i294.3 = and i64 %23, %add21.3.2
  %shr.i295.3 = lshr i64 %xor2.i294.3, 63
  %add48.3 = add nsw i64 %shr.i295.3, %add27.3.2
  %arrayidx50.3 = getelementptr inbounds i64, i64* %mc, i64 3
  store i64 %add35.3, i64* %arrayidx50.3, align 8, !tbaa !3
  %arrayidx34.4 = getelementptr inbounds i64, i64* %ma, i64 4
  %24 = load i64, i64* %arrayidx34.4, align 8, !tbaa !3
  %add35.4 = add i64 %24, %add41.3
  %xor.i297.4 = xor i64 %add35.4, %add41.3
  %xor1.i298.4 = xor i64 %24, %add41.3
  %or.i299.4 = or i64 %xor.i297.4, %xor1.i298.4
  %xor2.i300.4 = xor i64 %or.i299.4, %add35.4
  %shr.i301.4 = lshr i64 %xor2.i300.4, 63
  %add41.4 = add i64 %shr.i301.4, %add48.3
  %arrayidx50.4 = getelementptr inbounds i64, i64* %mc, i64 4
  store i64 %add35.4, i64* %arrayidx50.4, align 8, !tbaa !3
  %25 = xor i64 %add41.4, -9223372036854775808
  %xor2.i294.4 = and i64 %25, %add48.3
  %shr.i295.4 = lshr i64 %xor2.i294.4, 63
  %26 = load i64, i64* %mc, align 8, !tbaa !3
  %27 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.5 = and i64 %26, 4294967295
  %shr.i.5 = lshr i64 %26, 32
  %and1.i.5 = and i64 %27, 4294967295
  %shr2.i.5 = lshr i64 %27, 32
  %mul.i.5 = mul nuw i64 %and1.i.5, %and.i.5
  %mul3.i.5 = mul nuw i64 %shr2.i.5, %and.i.5
  %mul4.i.5 = mul nuw i64 %and1.i.5, %shr.i.5
  %mul5.i.5 = mul nuw i64 %shr2.i.5, %shr.i.5
  %and6.i.5 = and i64 %mul.i.5, 4294967295
  %shr7.i.5 = lshr i64 %mul.i.5, 32
  %and8.i.5 = and i64 %mul4.i.5, 4294967295
  %and9.i.5 = and i64 %mul3.i.5, 4294967295
  %add.i.5 = add nuw nsw i64 %shr7.i.5, %and8.i.5
  %add10.i.5 = add nuw nsw i64 %add.i.5, %and9.i.5
  %shr11.i.5 = lshr i64 %add10.i.5, 32
  %shl.i.5 = shl i64 %add10.i.5, 32
  %xor68.i.5 = or i64 %shl.i.5, %and6.i.5
  %shr13.i.5 = lshr i64 %mul4.i.5, 32
  %shr14.i.5 = lshr i64 %mul3.i.5, 32
  %and15.i.5 = and i64 %mul5.i.5, 4294967295
  %add16.i.5 = add nuw nsw i64 %shr13.i.5, %shr14.i.5
  %add17.i.5 = add nuw nsw i64 %add16.i.5, %and15.i.5
  %add18.i.5 = add nuw nsw i64 %add17.i.5, %shr11.i.5
  %and19.i.5 = and i64 %add18.i.5, 4294967295
  %and21.i.5 = and i64 %add18.i.5, 30064771072
  %and22.i.5 = and i64 %mul5.i.5, -4294967296
  %add23.i.5 = add i64 %and21.i.5, %and22.i.5
  %xor2569.i.5 = or i64 %add23.i.5, %and19.i.5
  %add16.5 = add i64 %xor68.i.5, %add41.4
  %xor.i315.5 = xor i64 %add16.5, %shl.i.5
  %xor1.i316.5 = xor i64 %shl.i.5, %add41.4
  %or.i317.5 = or i64 %xor.i315.5, %xor1.i316.5
  %xor2.i318.5 = xor i64 %or.i317.5, %add16.5
  %shr.i319.5 = lshr i64 %xor2.i318.5, 63
  %add20.5 = add i64 %shr.i319.5, %xor2569.i.5
  %add21.5 = add i64 %add20.5, %shr.i295.4
  %28 = xor i64 %add20.5, -9223372036854775808
  %xor2.i312.5 = and i64 %28, %add23.i.5
  %29 = xor i64 %add21.5, -9223372036854775808
  %xor2.i306.5 = and i64 %add20.5, %29
  %shr.i313328.5 = or i64 %xor2.i306.5, %xor2.i312.5
  %or25327.5 = lshr i64 %shr.i313328.5, 63
  %arrayidx34.5 = getelementptr inbounds i64, i64* %ma, i64 5
  %30 = load i64, i64* %arrayidx34.5, align 8, !tbaa !3
  %add35.5 = add i64 %30, %add16.5
  %xor.i297.5 = xor i64 %add35.5, %add16.5
  %xor1.i298.5 = xor i64 %30, %add16.5
  %or.i299.5 = or i64 %xor.i297.5, %xor1.i298.5
  %xor2.i300.5 = xor i64 %or.i299.5, %add35.5
  %shr.i301.5 = lshr i64 %xor2.i300.5, 63
  %add41.5 = add i64 %shr.i301.5, %add21.5
  %arrayidx50.5 = getelementptr inbounds i64, i64* %mc, i64 5
  store i64 %add35.5, i64* %arrayidx50.5, align 8, !tbaa !3
  %31 = xor i64 %add41.5, -9223372036854775808
  %xor2.i294.5 = and i64 %31, %add21.5
  %shr.i295.5 = lshr i64 %xor2.i294.5, 63
  %add48.5 = add nuw nsw i64 %shr.i295.5, %or25327.5
  %32 = load i64, i64* %mc, align 8, !tbaa !3
  %33 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.6 = and i64 %32, 4294967295
  %shr.i.6 = lshr i64 %32, 32
  %and1.i.6 = and i64 %33, 4294967295
  %shr2.i.6 = lshr i64 %33, 32
  %mul.i.6 = mul nuw i64 %and1.i.6, %and.i.6
  %mul3.i.6 = mul nuw i64 %shr2.i.6, %and.i.6
  %mul4.i.6 = mul nuw i64 %and1.i.6, %shr.i.6
  %mul5.i.6 = mul nuw i64 %shr2.i.6, %shr.i.6
  %and6.i.6 = and i64 %mul.i.6, 4294967295
  %shr7.i.6 = lshr i64 %mul.i.6, 32
  %and8.i.6 = and i64 %mul4.i.6, 4294967295
  %and9.i.6 = and i64 %mul3.i.6, 4294967295
  %add.i.6 = add nuw nsw i64 %shr7.i.6, %and8.i.6
  %add10.i.6 = add nuw nsw i64 %add.i.6, %and9.i.6
  %shr11.i.6 = lshr i64 %add10.i.6, 32
  %shl.i.6 = shl i64 %add10.i.6, 32
  %xor68.i.6 = or i64 %shl.i.6, %and6.i.6
  %shr13.i.6 = lshr i64 %mul4.i.6, 32
  %shr14.i.6 = lshr i64 %mul3.i.6, 32
  %and15.i.6 = and i64 %mul5.i.6, 4294967295
  %add16.i.6 = add nuw nsw i64 %shr13.i.6, %shr14.i.6
  %add17.i.6 = add nuw nsw i64 %add16.i.6, %and15.i.6
  %add18.i.6 = add nuw nsw i64 %add17.i.6, %shr11.i.6
  %and19.i.6 = and i64 %add18.i.6, 4294967295
  %and21.i.6 = and i64 %add18.i.6, 30064771072
  %and22.i.6 = and i64 %mul5.i.6, -4294967296
  %add23.i.6 = add i64 %and21.i.6, %and22.i.6
  %xor2569.i.6 = or i64 %add23.i.6, %and19.i.6
  %add16.6 = add i64 %xor68.i.6, %add41.5
  %xor.i315.6 = xor i64 %add16.6, %shl.i.6
  %xor1.i316.6 = xor i64 %shl.i.6, %add41.5
  %or.i317.6 = or i64 %xor.i315.6, %xor1.i316.6
  %xor2.i318.6 = xor i64 %or.i317.6, %add16.6
  %shr.i319.6 = lshr i64 %xor2.i318.6, 63
  %add20.6 = add i64 %shr.i319.6, %xor2569.i.6
  %add21.6 = add i64 %add20.6, %add48.5
  %34 = xor i64 %add20.6, -9223372036854775808
  %xor2.i312.6 = and i64 %34, %add23.i.6
  %xor.i303.6 = xor i64 %add21.6, %add20.6
  %xor1.i304.6 = xor i64 %add20.6, %add48.5
  %or.i305.6 = or i64 %xor.i303.6, %xor1.i304.6
  %xor2.i306.6 = xor i64 %or.i305.6, %add21.6
  %shr.i313328.6 = or i64 %xor2.i306.6, %xor2.i312.6
  %or25327.6 = lshr i64 %shr.i313328.6, 63
  %arrayidx9.6.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %35 = load i64, i64* %arrayidx9.6.1, align 8, !tbaa !3
  %36 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.6.1 = and i64 %35, 4294967295
  %shr.i.6.1 = lshr i64 %35, 32
  %and1.i.6.1 = and i64 %36, 4294967295
  %shr2.i.6.1 = lshr i64 %36, 32
  %mul.i.6.1 = mul nuw i64 %and1.i.6.1, %and.i.6.1
  %mul3.i.6.1 = mul nuw i64 %shr2.i.6.1, %and.i.6.1
  %mul4.i.6.1 = mul nuw i64 %and1.i.6.1, %shr.i.6.1
  %mul5.i.6.1 = mul nuw i64 %shr2.i.6.1, %shr.i.6.1
  %and6.i.6.1 = and i64 %mul.i.6.1, 4294967295
  %shr7.i.6.1 = lshr i64 %mul.i.6.1, 32
  %and8.i.6.1 = and i64 %mul4.i.6.1, 4294967295
  %and9.i.6.1 = and i64 %mul3.i.6.1, 4294967295
  %add.i.6.1 = add nuw nsw i64 %shr7.i.6.1, %and8.i.6.1
  %add10.i.6.1 = add nuw nsw i64 %add.i.6.1, %and9.i.6.1
  %shr11.i.6.1 = lshr i64 %add10.i.6.1, 32
  %shl.i.6.1 = shl i64 %add10.i.6.1, 32
  %xor68.i.6.1 = or i64 %shl.i.6.1, %and6.i.6.1
  %shr13.i.6.1 = lshr i64 %mul4.i.6.1, 32
  %shr14.i.6.1 = lshr i64 %mul3.i.6.1, 32
  %and15.i.6.1 = and i64 %mul5.i.6.1, 4294967295
  %add16.i.6.1 = add nuw nsw i64 %shr13.i.6.1, %shr14.i.6.1
  %add17.i.6.1 = add nuw nsw i64 %add16.i.6.1, %and15.i.6.1
  %add18.i.6.1 = add nuw nsw i64 %add17.i.6.1, %shr11.i.6.1
  %and19.i.6.1 = and i64 %add18.i.6.1, 4294967295
  %and21.i.6.1 = and i64 %add18.i.6.1, 30064771072
  %and22.i.6.1 = and i64 %mul5.i.6.1, -4294967296
  %add23.i.6.1 = add i64 %and21.i.6.1, %and22.i.6.1
  %xor2569.i.6.1 = or i64 %add23.i.6.1, %and19.i.6.1
  %add16.6.1 = add i64 %xor68.i.6.1, %add16.6
  %xor.i315.6.1 = xor i64 %add16.6.1, %shl.i.6.1
  %xor1.i316.6.1 = xor i64 %shl.i.6.1, %add16.6
  %or.i317.6.1 = or i64 %xor.i315.6.1, %xor1.i316.6.1
  %xor2.i318.6.1 = xor i64 %or.i317.6.1, %add16.6.1
  %shr.i319.6.1 = lshr i64 %xor2.i318.6.1, 63
  %add20.6.1 = add i64 %shr.i319.6.1, %xor2569.i.6.1
  %add21.6.1 = add i64 %add20.6.1, %add21.6
  %37 = xor i64 %add20.6.1, -9223372036854775808
  %xor2.i312.6.1 = and i64 %37, %add23.i.6.1
  %xor.i303.6.1 = xor i64 %add21.6.1, %add20.6.1
  %xor1.i304.6.1 = xor i64 %add20.6.1, %add21.6
  %or.i305.6.1 = or i64 %xor.i303.6.1, %xor1.i304.6.1
  %xor2.i306.6.1 = xor i64 %or.i305.6.1, %add21.6.1
  %shr.i313328.6.1 = or i64 %xor2.i306.6.1, %xor2.i312.6.1
  %or25327.6.1 = lshr i64 %shr.i313328.6.1, 63
  %add27.6.1 = add nuw nsw i64 %or25327.6.1, %or25327.6
  %arrayidx34.6 = getelementptr inbounds i64, i64* %ma, i64 6
  %38 = load i64, i64* %arrayidx34.6, align 8, !tbaa !3
  %add35.6 = add i64 %38, %add16.6.1
  %xor.i297.6 = xor i64 %add35.6, %add16.6.1
  %xor1.i298.6 = xor i64 %38, %add16.6.1
  %or.i299.6 = or i64 %xor.i297.6, %xor1.i298.6
  %xor2.i300.6 = xor i64 %or.i299.6, %add35.6
  %shr.i301.6 = lshr i64 %xor2.i300.6, 63
  %add41.6 = add i64 %shr.i301.6, %add21.6.1
  %arrayidx50.6 = getelementptr inbounds i64, i64* %mc, i64 6
  store i64 %add35.6, i64* %arrayidx50.6, align 8, !tbaa !3
  %39 = xor i64 %add41.6, -9223372036854775808
  %xor2.i294.6 = and i64 %39, %add21.6.1
  %shr.i295.6 = lshr i64 %xor2.i294.6, 63
  %add48.6 = add nsw i64 %shr.i295.6, %add27.6.1
  %40 = load i64, i64* %mc, align 8, !tbaa !3
  %41 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i.7 = and i64 %40, 4294967295
  %shr.i.7 = lshr i64 %40, 32
  %and1.i.7 = and i64 %41, 4294967295
  %shr2.i.7 = lshr i64 %41, 32
  %mul.i.7 = mul nuw i64 %and1.i.7, %and.i.7
  %mul3.i.7 = mul nuw i64 %shr2.i.7, %and.i.7
  %mul4.i.7 = mul nuw i64 %and1.i.7, %shr.i.7
  %mul5.i.7 = mul nuw i64 %shr2.i.7, %shr.i.7
  %and6.i.7 = and i64 %mul.i.7, 4294967295
  %shr7.i.7 = lshr i64 %mul.i.7, 32
  %and8.i.7 = and i64 %mul4.i.7, 4294967295
  %and9.i.7 = and i64 %mul3.i.7, 4294967295
  %add.i.7 = add nuw nsw i64 %shr7.i.7, %and8.i.7
  %add10.i.7 = add nuw nsw i64 %add.i.7, %and9.i.7
  %shr11.i.7 = lshr i64 %add10.i.7, 32
  %shl.i.7 = shl i64 %add10.i.7, 32
  %xor68.i.7 = or i64 %shl.i.7, %and6.i.7
  %shr13.i.7 = lshr i64 %mul4.i.7, 32
  %shr14.i.7 = lshr i64 %mul3.i.7, 32
  %and15.i.7 = and i64 %mul5.i.7, 4294967295
  %add16.i.7 = add nuw nsw i64 %shr13.i.7, %shr14.i.7
  %add17.i.7 = add nuw nsw i64 %add16.i.7, %and15.i.7
  %add18.i.7 = add nuw nsw i64 %add17.i.7, %shr11.i.7
  %and19.i.7 = and i64 %add18.i.7, 4294967295
  %and21.i.7 = and i64 %add18.i.7, 30064771072
  %and22.i.7 = and i64 %mul5.i.7, -4294967296
  %add23.i.7 = add i64 %and21.i.7, %and22.i.7
  %xor2569.i.7 = or i64 %add23.i.7, %and19.i.7
  %add16.7 = add i64 %xor68.i.7, %add41.6
  %xor.i315.7 = xor i64 %add16.7, %shl.i.7
  %xor1.i316.7 = xor i64 %shl.i.7, %add41.6
  %or.i317.7 = or i64 %xor.i315.7, %xor1.i316.7
  %xor2.i318.7 = xor i64 %or.i317.7, %add16.7
  %shr.i319.7 = lshr i64 %xor2.i318.7, 63
  %add20.7 = add i64 %shr.i319.7, %xor2569.i.7
  %add21.7 = add i64 %add20.7, %add48.6
  %42 = xor i64 %add20.7, -9223372036854775808
  %xor2.i312.7 = and i64 %42, %add23.i.7
  %xor.i303.7 = xor i64 %add21.7, %add20.7
  %xor1.i304.7 = xor i64 %add20.7, %add48.6
  %or.i305.7 = or i64 %xor.i303.7, %xor1.i304.7
  %xor2.i306.7 = xor i64 %or.i305.7, %add21.7
  %shr.i313328.7 = or i64 %xor2.i306.7, %xor2.i312.7
  %or25327.7 = lshr i64 %shr.i313328.7, 63
  %arrayidx9.7.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %43 = load i64, i64* %arrayidx9.7.1, align 8, !tbaa !3
  %44 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.7.1 = and i64 %43, 4294967295
  %shr.i.7.1 = lshr i64 %43, 32
  %and1.i.7.1 = and i64 %44, 4294967295
  %shr2.i.7.1 = lshr i64 %44, 32
  %mul.i.7.1 = mul nuw i64 %and1.i.7.1, %and.i.7.1
  %mul3.i.7.1 = mul nuw i64 %shr2.i.7.1, %and.i.7.1
  %mul4.i.7.1 = mul nuw i64 %and1.i.7.1, %shr.i.7.1
  %mul5.i.7.1 = mul nuw i64 %shr2.i.7.1, %shr.i.7.1
  %and6.i.7.1 = and i64 %mul.i.7.1, 4294967295
  %shr7.i.7.1 = lshr i64 %mul.i.7.1, 32
  %and8.i.7.1 = and i64 %mul4.i.7.1, 4294967295
  %and9.i.7.1 = and i64 %mul3.i.7.1, 4294967295
  %add.i.7.1 = add nuw nsw i64 %shr7.i.7.1, %and8.i.7.1
  %add10.i.7.1 = add nuw nsw i64 %add.i.7.1, %and9.i.7.1
  %shr11.i.7.1 = lshr i64 %add10.i.7.1, 32
  %shl.i.7.1 = shl i64 %add10.i.7.1, 32
  %xor68.i.7.1 = or i64 %shl.i.7.1, %and6.i.7.1
  %shr13.i.7.1 = lshr i64 %mul4.i.7.1, 32
  %shr14.i.7.1 = lshr i64 %mul3.i.7.1, 32
  %and15.i.7.1 = and i64 %mul5.i.7.1, 4294967295
  %add16.i.7.1 = add nuw nsw i64 %shr13.i.7.1, %shr14.i.7.1
  %add17.i.7.1 = add nuw nsw i64 %add16.i.7.1, %and15.i.7.1
  %add18.i.7.1 = add nuw nsw i64 %add17.i.7.1, %shr11.i.7.1
  %and19.i.7.1 = and i64 %add18.i.7.1, 4294967295
  %and21.i.7.1 = and i64 %add18.i.7.1, 30064771072
  %and22.i.7.1 = and i64 %mul5.i.7.1, -4294967296
  %add23.i.7.1 = add i64 %and21.i.7.1, %and22.i.7.1
  %xor2569.i.7.1 = or i64 %add23.i.7.1, %and19.i.7.1
  %add16.7.1 = add i64 %xor68.i.7.1, %add16.7
  %xor.i315.7.1 = xor i64 %add16.7.1, %shl.i.7.1
  %xor1.i316.7.1 = xor i64 %shl.i.7.1, %add16.7
  %or.i317.7.1 = or i64 %xor.i315.7.1, %xor1.i316.7.1
  %xor2.i318.7.1 = xor i64 %or.i317.7.1, %add16.7.1
  %shr.i319.7.1 = lshr i64 %xor2.i318.7.1, 63
  %add20.7.1 = add i64 %shr.i319.7.1, %xor2569.i.7.1
  %add21.7.1 = add i64 %add20.7.1, %add21.7
  %45 = xor i64 %add20.7.1, -9223372036854775808
  %xor2.i312.7.1 = and i64 %45, %add23.i.7.1
  %xor.i303.7.1 = xor i64 %add21.7.1, %add20.7.1
  %xor1.i304.7.1 = xor i64 %add20.7.1, %add21.7
  %or.i305.7.1 = or i64 %xor.i303.7.1, %xor1.i304.7.1
  %xor2.i306.7.1 = xor i64 %or.i305.7.1, %add21.7.1
  %shr.i313328.7.1 = or i64 %xor2.i306.7.1, %xor2.i312.7.1
  %or25327.7.1 = lshr i64 %shr.i313328.7.1, 63
  %add27.7.1 = add nuw nsw i64 %or25327.7.1, %or25327.7
  %arrayidx9.7.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %46 = load i64, i64* %arrayidx9.7.2, align 8, !tbaa !3
  %47 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.7.2 = and i64 %46, 4294967295
  %shr.i.7.2 = lshr i64 %46, 32
  %and1.i.7.2 = and i64 %47, 4294967295
  %shr2.i.7.2 = lshr i64 %47, 32
  %mul.i.7.2 = mul nuw i64 %and1.i.7.2, %and.i.7.2
  %mul3.i.7.2 = mul nuw i64 %shr2.i.7.2, %and.i.7.2
  %mul4.i.7.2 = mul nuw i64 %and1.i.7.2, %shr.i.7.2
  %mul5.i.7.2 = mul nuw i64 %shr2.i.7.2, %shr.i.7.2
  %and6.i.7.2 = and i64 %mul.i.7.2, 4294967295
  %shr7.i.7.2 = lshr i64 %mul.i.7.2, 32
  %and8.i.7.2 = and i64 %mul4.i.7.2, 4294967295
  %and9.i.7.2 = and i64 %mul3.i.7.2, 4294967295
  %add.i.7.2 = add nuw nsw i64 %shr7.i.7.2, %and8.i.7.2
  %add10.i.7.2 = add nuw nsw i64 %add.i.7.2, %and9.i.7.2
  %shr11.i.7.2 = lshr i64 %add10.i.7.2, 32
  %shl.i.7.2 = shl i64 %add10.i.7.2, 32
  %xor68.i.7.2 = or i64 %shl.i.7.2, %and6.i.7.2
  %shr13.i.7.2 = lshr i64 %mul4.i.7.2, 32
  %shr14.i.7.2 = lshr i64 %mul3.i.7.2, 32
  %and15.i.7.2 = and i64 %mul5.i.7.2, 4294967295
  %add16.i.7.2 = add nuw nsw i64 %shr13.i.7.2, %shr14.i.7.2
  %add17.i.7.2 = add nuw nsw i64 %add16.i.7.2, %and15.i.7.2
  %add18.i.7.2 = add nuw nsw i64 %add17.i.7.2, %shr11.i.7.2
  %and19.i.7.2 = and i64 %add18.i.7.2, 4294967295
  %and21.i.7.2 = and i64 %add18.i.7.2, 30064771072
  %and22.i.7.2 = and i64 %mul5.i.7.2, -4294967296
  %add23.i.7.2 = add i64 %and21.i.7.2, %and22.i.7.2
  %xor2569.i.7.2 = or i64 %add23.i.7.2, %and19.i.7.2
  %add16.7.2 = add i64 %xor68.i.7.2, %add16.7.1
  %xor.i315.7.2 = xor i64 %add16.7.2, %shl.i.7.2
  %xor1.i316.7.2 = xor i64 %shl.i.7.2, %add16.7.1
  %or.i317.7.2 = or i64 %xor.i315.7.2, %xor1.i316.7.2
  %xor2.i318.7.2 = xor i64 %or.i317.7.2, %add16.7.2
  %shr.i319.7.2 = lshr i64 %xor2.i318.7.2, 63
  %add20.7.2 = add i64 %shr.i319.7.2, %xor2569.i.7.2
  %add21.7.2 = add i64 %add20.7.2, %add21.7.1
  %48 = xor i64 %add20.7.2, -9223372036854775808
  %xor2.i312.7.2 = and i64 %48, %add23.i.7.2
  %xor.i303.7.2 = xor i64 %add21.7.2, %add20.7.2
  %xor1.i304.7.2 = xor i64 %add20.7.2, %add21.7.1
  %or.i305.7.2 = or i64 %xor.i303.7.2, %xor1.i304.7.2
  %xor2.i306.7.2 = xor i64 %or.i305.7.2, %add21.7.2
  %shr.i313328.7.2 = or i64 %xor2.i306.7.2, %xor2.i312.7.2
  %or25327.7.2 = lshr i64 %shr.i313328.7.2, 63
  %add27.7.2 = add nsw i64 %or25327.7.2, %add27.7.1
  %arrayidx34.7 = getelementptr inbounds i64, i64* %ma, i64 7
  %49 = load i64, i64* %arrayidx34.7, align 8, !tbaa !3
  %add35.7 = add i64 %49, %add16.7.2
  %xor.i297.7 = xor i64 %add35.7, %add16.7.2
  %xor1.i298.7 = xor i64 %49, %add16.7.2
  %or.i299.7 = or i64 %xor.i297.7, %xor1.i298.7
  %xor2.i300.7 = xor i64 %or.i299.7, %add35.7
  %shr.i301.7 = lshr i64 %xor2.i300.7, 63
  %add41.7 = add i64 %shr.i301.7, %add21.7.2
  %arrayidx50.7 = getelementptr inbounds i64, i64* %mc, i64 7
  store i64 %add35.7, i64* %arrayidx50.7, align 8, !tbaa !3
  %50 = xor i64 %add41.7, -9223372036854775808
  %xor2.i294.7 = and i64 %50, %add21.7.2
  %shr.i295.7 = lshr i64 %xor2.i294.7, 63
  %add48.7 = add nsw i64 %shr.i295.7, %add27.7.2
  %51 = load i64, i64* %mc, align 8, !tbaa !3
  %52 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i.8 = and i64 %51, 4294967295
  %shr.i.8 = lshr i64 %51, 32
  %and1.i.8 = and i64 %52, 4294967295
  %shr2.i.8 = lshr i64 %52, 32
  %mul.i.8 = mul nuw i64 %and1.i.8, %and.i.8
  %mul3.i.8 = mul nuw i64 %shr2.i.8, %and.i.8
  %mul4.i.8 = mul nuw i64 %and1.i.8, %shr.i.8
  %mul5.i.8 = mul nuw i64 %shr2.i.8, %shr.i.8
  %and6.i.8 = and i64 %mul.i.8, 4294967295
  %shr7.i.8 = lshr i64 %mul.i.8, 32
  %and8.i.8 = and i64 %mul4.i.8, 4294967295
  %and9.i.8 = and i64 %mul3.i.8, 4294967295
  %add.i.8 = add nuw nsw i64 %shr7.i.8, %and8.i.8
  %add10.i.8 = add nuw nsw i64 %add.i.8, %and9.i.8
  %shr11.i.8 = lshr i64 %add10.i.8, 32
  %shl.i.8 = shl i64 %add10.i.8, 32
  %xor68.i.8 = or i64 %shl.i.8, %and6.i.8
  %shr13.i.8 = lshr i64 %mul4.i.8, 32
  %shr14.i.8 = lshr i64 %mul3.i.8, 32
  %and15.i.8 = and i64 %mul5.i.8, 4294967295
  %add16.i.8 = add nuw nsw i64 %shr13.i.8, %shr14.i.8
  %add17.i.8 = add nuw nsw i64 %add16.i.8, %and15.i.8
  %add18.i.8 = add nuw nsw i64 %add17.i.8, %shr11.i.8
  %and19.i.8 = and i64 %add18.i.8, 4294967295
  %and21.i.8 = and i64 %add18.i.8, 30064771072
  %and22.i.8 = and i64 %mul5.i.8, -4294967296
  %add23.i.8 = add i64 %and21.i.8, %and22.i.8
  %xor2569.i.8 = or i64 %add23.i.8, %and19.i.8
  %add16.8 = add i64 %xor68.i.8, %add41.7
  %xor.i315.8 = xor i64 %add16.8, %shl.i.8
  %xor1.i316.8 = xor i64 %shl.i.8, %add41.7
  %or.i317.8 = or i64 %xor.i315.8, %xor1.i316.8
  %xor2.i318.8 = xor i64 %or.i317.8, %add16.8
  %shr.i319.8 = lshr i64 %xor2.i318.8, 63
  %add20.8 = add i64 %shr.i319.8, %xor2569.i.8
  %add21.8 = add i64 %add20.8, %add48.7
  %53 = xor i64 %add20.8, -9223372036854775808
  %xor2.i312.8 = and i64 %53, %add23.i.8
  %xor.i303.8 = xor i64 %add21.8, %add20.8
  %xor1.i304.8 = xor i64 %add20.8, %add48.7
  %or.i305.8 = or i64 %xor.i303.8, %xor1.i304.8
  %xor2.i306.8 = xor i64 %or.i305.8, %add21.8
  %shr.i313328.8 = or i64 %xor2.i306.8, %xor2.i312.8
  %or25327.8 = lshr i64 %shr.i313328.8, 63
  %arrayidx9.8.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %54 = load i64, i64* %arrayidx9.8.1, align 8, !tbaa !3
  %55 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i.8.1 = and i64 %54, 4294967295
  %shr.i.8.1 = lshr i64 %54, 32
  %and1.i.8.1 = and i64 %55, 4294967295
  %shr2.i.8.1 = lshr i64 %55, 32
  %mul.i.8.1 = mul nuw i64 %and1.i.8.1, %and.i.8.1
  %mul3.i.8.1 = mul nuw i64 %shr2.i.8.1, %and.i.8.1
  %mul4.i.8.1 = mul nuw i64 %and1.i.8.1, %shr.i.8.1
  %mul5.i.8.1 = mul nuw i64 %shr2.i.8.1, %shr.i.8.1
  %and6.i.8.1 = and i64 %mul.i.8.1, 4294967295
  %shr7.i.8.1 = lshr i64 %mul.i.8.1, 32
  %and8.i.8.1 = and i64 %mul4.i.8.1, 4294967295
  %and9.i.8.1 = and i64 %mul3.i.8.1, 4294967295
  %add.i.8.1 = add nuw nsw i64 %shr7.i.8.1, %and8.i.8.1
  %add10.i.8.1 = add nuw nsw i64 %add.i.8.1, %and9.i.8.1
  %shr11.i.8.1 = lshr i64 %add10.i.8.1, 32
  %shl.i.8.1 = shl i64 %add10.i.8.1, 32
  %xor68.i.8.1 = or i64 %shl.i.8.1, %and6.i.8.1
  %shr13.i.8.1 = lshr i64 %mul4.i.8.1, 32
  %shr14.i.8.1 = lshr i64 %mul3.i.8.1, 32
  %and15.i.8.1 = and i64 %mul5.i.8.1, 4294967295
  %add16.i.8.1 = add nuw nsw i64 %shr13.i.8.1, %shr14.i.8.1
  %add17.i.8.1 = add nuw nsw i64 %add16.i.8.1, %and15.i.8.1
  %add18.i.8.1 = add nuw nsw i64 %add17.i.8.1, %shr11.i.8.1
  %and19.i.8.1 = and i64 %add18.i.8.1, 4294967295
  %and21.i.8.1 = and i64 %add18.i.8.1, 30064771072
  %and22.i.8.1 = and i64 %mul5.i.8.1, -4294967296
  %add23.i.8.1 = add i64 %and21.i.8.1, %and22.i.8.1
  %xor2569.i.8.1 = or i64 %add23.i.8.1, %and19.i.8.1
  %add16.8.1 = add i64 %xor68.i.8.1, %add16.8
  %xor.i315.8.1 = xor i64 %add16.8.1, %shl.i.8.1
  %xor1.i316.8.1 = xor i64 %shl.i.8.1, %add16.8
  %or.i317.8.1 = or i64 %xor.i315.8.1, %xor1.i316.8.1
  %xor2.i318.8.1 = xor i64 %or.i317.8.1, %add16.8.1
  %shr.i319.8.1 = lshr i64 %xor2.i318.8.1, 63
  %add20.8.1 = add i64 %shr.i319.8.1, %xor2569.i.8.1
  %add21.8.1 = add i64 %add20.8.1, %add21.8
  %56 = xor i64 %add20.8.1, -9223372036854775808
  %xor2.i312.8.1 = and i64 %56, %add23.i.8.1
  %xor.i303.8.1 = xor i64 %add21.8.1, %add20.8.1
  %xor1.i304.8.1 = xor i64 %add20.8.1, %add21.8
  %or.i305.8.1 = or i64 %xor.i303.8.1, %xor1.i304.8.1
  %xor2.i306.8.1 = xor i64 %or.i305.8.1, %add21.8.1
  %shr.i313328.8.1 = or i64 %xor2.i306.8.1, %xor2.i312.8.1
  %or25327.8.1 = lshr i64 %shr.i313328.8.1, 63
  %add27.8.1 = add nuw nsw i64 %or25327.8.1, %or25327.8
  %arrayidx9.8.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %57 = load i64, i64* %arrayidx9.8.2, align 8, !tbaa !3
  %58 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.8.2 = and i64 %57, 4294967295
  %shr.i.8.2 = lshr i64 %57, 32
  %and1.i.8.2 = and i64 %58, 4294967295
  %shr2.i.8.2 = lshr i64 %58, 32
  %mul.i.8.2 = mul nuw i64 %and1.i.8.2, %and.i.8.2
  %mul3.i.8.2 = mul nuw i64 %shr2.i.8.2, %and.i.8.2
  %mul4.i.8.2 = mul nuw i64 %and1.i.8.2, %shr.i.8.2
  %mul5.i.8.2 = mul nuw i64 %shr2.i.8.2, %shr.i.8.2
  %and6.i.8.2 = and i64 %mul.i.8.2, 4294967295
  %shr7.i.8.2 = lshr i64 %mul.i.8.2, 32
  %and8.i.8.2 = and i64 %mul4.i.8.2, 4294967295
  %and9.i.8.2 = and i64 %mul3.i.8.2, 4294967295
  %add.i.8.2 = add nuw nsw i64 %shr7.i.8.2, %and8.i.8.2
  %add10.i.8.2 = add nuw nsw i64 %add.i.8.2, %and9.i.8.2
  %shr11.i.8.2 = lshr i64 %add10.i.8.2, 32
  %shl.i.8.2 = shl i64 %add10.i.8.2, 32
  %xor68.i.8.2 = or i64 %shl.i.8.2, %and6.i.8.2
  %shr13.i.8.2 = lshr i64 %mul4.i.8.2, 32
  %shr14.i.8.2 = lshr i64 %mul3.i.8.2, 32
  %and15.i.8.2 = and i64 %mul5.i.8.2, 4294967295
  %add16.i.8.2 = add nuw nsw i64 %shr13.i.8.2, %shr14.i.8.2
  %add17.i.8.2 = add nuw nsw i64 %add16.i.8.2, %and15.i.8.2
  %add18.i.8.2 = add nuw nsw i64 %add17.i.8.2, %shr11.i.8.2
  %and19.i.8.2 = and i64 %add18.i.8.2, 4294967295
  %and21.i.8.2 = and i64 %add18.i.8.2, 30064771072
  %and22.i.8.2 = and i64 %mul5.i.8.2, -4294967296
  %add23.i.8.2 = add i64 %and21.i.8.2, %and22.i.8.2
  %xor2569.i.8.2 = or i64 %add23.i.8.2, %and19.i.8.2
  %add16.8.2 = add i64 %xor68.i.8.2, %add16.8.1
  %xor.i315.8.2 = xor i64 %add16.8.2, %shl.i.8.2
  %xor1.i316.8.2 = xor i64 %shl.i.8.2, %add16.8.1
  %or.i317.8.2 = or i64 %xor.i315.8.2, %xor1.i316.8.2
  %xor2.i318.8.2 = xor i64 %or.i317.8.2, %add16.8.2
  %shr.i319.8.2 = lshr i64 %xor2.i318.8.2, 63
  %add20.8.2 = add i64 %shr.i319.8.2, %xor2569.i.8.2
  %add21.8.2 = add i64 %add20.8.2, %add21.8.1
  %59 = xor i64 %add20.8.2, -9223372036854775808
  %xor2.i312.8.2 = and i64 %59, %add23.i.8.2
  %xor.i303.8.2 = xor i64 %add21.8.2, %add20.8.2
  %xor1.i304.8.2 = xor i64 %add20.8.2, %add21.8.1
  %or.i305.8.2 = or i64 %xor.i303.8.2, %xor1.i304.8.2
  %xor2.i306.8.2 = xor i64 %or.i305.8.2, %add21.8.2
  %shr.i313328.8.2 = or i64 %xor2.i306.8.2, %xor2.i312.8.2
  %or25327.8.2 = lshr i64 %shr.i313328.8.2, 63
  %add27.8.2 = add nsw i64 %or25327.8.2, %add27.8.1
  %arrayidx9.8.3 = getelementptr inbounds i64, i64* %mc, i64 3
  %60 = load i64, i64* %arrayidx9.8.3, align 8, !tbaa !3
  %61 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.8.3 = and i64 %60, 4294967295
  %shr.i.8.3 = lshr i64 %60, 32
  %and1.i.8.3 = and i64 %61, 4294967295
  %shr2.i.8.3 = lshr i64 %61, 32
  %mul.i.8.3 = mul nuw i64 %and1.i.8.3, %and.i.8.3
  %mul3.i.8.3 = mul nuw i64 %shr2.i.8.3, %and.i.8.3
  %mul4.i.8.3 = mul nuw i64 %and1.i.8.3, %shr.i.8.3
  %mul5.i.8.3 = mul nuw i64 %shr2.i.8.3, %shr.i.8.3
  %and6.i.8.3 = and i64 %mul.i.8.3, 4294967295
  %shr7.i.8.3 = lshr i64 %mul.i.8.3, 32
  %and8.i.8.3 = and i64 %mul4.i.8.3, 4294967295
  %and9.i.8.3 = and i64 %mul3.i.8.3, 4294967295
  %add.i.8.3 = add nuw nsw i64 %shr7.i.8.3, %and8.i.8.3
  %add10.i.8.3 = add nuw nsw i64 %add.i.8.3, %and9.i.8.3
  %shr11.i.8.3 = lshr i64 %add10.i.8.3, 32
  %shl.i.8.3 = shl i64 %add10.i.8.3, 32
  %xor68.i.8.3 = or i64 %shl.i.8.3, %and6.i.8.3
  %shr13.i.8.3 = lshr i64 %mul4.i.8.3, 32
  %shr14.i.8.3 = lshr i64 %mul3.i.8.3, 32
  %and15.i.8.3 = and i64 %mul5.i.8.3, 4294967295
  %add16.i.8.3 = add nuw nsw i64 %shr13.i.8.3, %shr14.i.8.3
  %add17.i.8.3 = add nuw nsw i64 %add16.i.8.3, %and15.i.8.3
  %add18.i.8.3 = add nuw nsw i64 %add17.i.8.3, %shr11.i.8.3
  %and19.i.8.3 = and i64 %add18.i.8.3, 4294967295
  %and21.i.8.3 = and i64 %add18.i.8.3, 30064771072
  %and22.i.8.3 = and i64 %mul5.i.8.3, -4294967296
  %add23.i.8.3 = add i64 %and21.i.8.3, %and22.i.8.3
  %xor2569.i.8.3 = or i64 %add23.i.8.3, %and19.i.8.3
  %add16.8.3 = add i64 %xor68.i.8.3, %add16.8.2
  %xor.i315.8.3 = xor i64 %add16.8.3, %shl.i.8.3
  %xor1.i316.8.3 = xor i64 %shl.i.8.3, %add16.8.2
  %or.i317.8.3 = or i64 %xor.i315.8.3, %xor1.i316.8.3
  %xor2.i318.8.3 = xor i64 %or.i317.8.3, %add16.8.3
  %shr.i319.8.3 = lshr i64 %xor2.i318.8.3, 63
  %add20.8.3 = add i64 %shr.i319.8.3, %xor2569.i.8.3
  %add21.8.3 = add i64 %add20.8.3, %add21.8.2
  %62 = xor i64 %add20.8.3, -9223372036854775808
  %xor2.i312.8.3 = and i64 %62, %add23.i.8.3
  %xor.i303.8.3 = xor i64 %add21.8.3, %add20.8.3
  %xor1.i304.8.3 = xor i64 %add20.8.3, %add21.8.2
  %or.i305.8.3 = or i64 %xor.i303.8.3, %xor1.i304.8.3
  %xor2.i306.8.3 = xor i64 %or.i305.8.3, %add21.8.3
  %shr.i313328.8.3 = or i64 %xor2.i306.8.3, %xor2.i312.8.3
  %or25327.8.3 = lshr i64 %shr.i313328.8.3, 63
  %add27.8.3 = add nsw i64 %or25327.8.3, %add27.8.2
  %arrayidx34.8 = getelementptr inbounds i64, i64* %ma, i64 8
  %63 = load i64, i64* %arrayidx34.8, align 8, !tbaa !3
  %add35.8 = add i64 %63, %add16.8.3
  %xor.i297.8 = xor i64 %add35.8, %add16.8.3
  %xor1.i298.8 = xor i64 %63, %add16.8.3
  %or.i299.8 = or i64 %xor.i297.8, %xor1.i298.8
  %xor2.i300.8 = xor i64 %or.i299.8, %add35.8
  %shr.i301.8 = lshr i64 %xor2.i300.8, 63
  %add41.8 = add i64 %shr.i301.8, %add21.8.3
  %arrayidx50.8 = getelementptr inbounds i64, i64* %mc, i64 8
  store i64 %add35.8, i64* %arrayidx50.8, align 8, !tbaa !3
  %64 = xor i64 %add41.8, -9223372036854775808
  %xor2.i294.8 = and i64 %64, %add21.8.3
  %shr.i295.8 = lshr i64 %xor2.i294.8, 63
  %add48.8 = add i64 %shr.i295.8, %add27.8.3
  %65 = load i64, i64* %mc, align 8, !tbaa !3
  %66 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i.9 = and i64 %65, 4294967295
  %shr.i.9 = lshr i64 %65, 32
  %and1.i.9 = and i64 %66, 4294967295
  %shr2.i.9 = lshr i64 %66, 32
  %mul.i.9 = mul nuw i64 %and1.i.9, %and.i.9
  %mul3.i.9 = mul nuw i64 %shr2.i.9, %and.i.9
  %mul4.i.9 = mul nuw i64 %and1.i.9, %shr.i.9
  %mul5.i.9 = mul nuw i64 %shr2.i.9, %shr.i.9
  %and6.i.9 = and i64 %mul.i.9, 4294967295
  %shr7.i.9 = lshr i64 %mul.i.9, 32
  %and8.i.9 = and i64 %mul4.i.9, 4294967295
  %and9.i.9 = and i64 %mul3.i.9, 4294967295
  %add.i.9 = add nuw nsw i64 %shr7.i.9, %and8.i.9
  %add10.i.9 = add nuw nsw i64 %add.i.9, %and9.i.9
  %shr11.i.9 = lshr i64 %add10.i.9, 32
  %shl.i.9 = shl i64 %add10.i.9, 32
  %xor68.i.9 = or i64 %shl.i.9, %and6.i.9
  %shr13.i.9 = lshr i64 %mul4.i.9, 32
  %shr14.i.9 = lshr i64 %mul3.i.9, 32
  %and15.i.9 = and i64 %mul5.i.9, 4294967295
  %add16.i.9 = add nuw nsw i64 %shr13.i.9, %shr14.i.9
  %add17.i.9 = add nuw nsw i64 %add16.i.9, %and15.i.9
  %add18.i.9 = add nuw nsw i64 %add17.i.9, %shr11.i.9
  %and19.i.9 = and i64 %add18.i.9, 4294967295
  %and21.i.9 = and i64 %add18.i.9, 30064771072
  %and22.i.9 = and i64 %mul5.i.9, -4294967296
  %add23.i.9 = add i64 %and21.i.9, %and22.i.9
  %xor2569.i.9 = or i64 %add23.i.9, %and19.i.9
  %add16.9 = add i64 %xor68.i.9, %add41.8
  %xor.i315.9 = xor i64 %add16.9, %shl.i.9
  %xor1.i316.9 = xor i64 %shl.i.9, %add41.8
  %or.i317.9 = or i64 %xor.i315.9, %xor1.i316.9
  %xor2.i318.9 = xor i64 %or.i317.9, %add16.9
  %shr.i319.9 = lshr i64 %xor2.i318.9, 63
  %add20.9 = add i64 %shr.i319.9, %xor2569.i.9
  %add21.9 = add i64 %add20.9, %add48.8
  %67 = xor i64 %add20.9, -9223372036854775808
  %xor2.i312.9 = and i64 %67, %add23.i.9
  %xor.i303.9 = xor i64 %add21.9, %add20.9
  %xor1.i304.9 = xor i64 %add20.9, %add48.8
  %or.i305.9 = or i64 %xor.i303.9, %xor1.i304.9
  %xor2.i306.9 = xor i64 %or.i305.9, %add21.9
  %shr.i313328.9 = or i64 %xor2.i306.9, %xor2.i312.9
  %or25327.9 = lshr i64 %shr.i313328.9, 63
  %arrayidx9.9.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %68 = load i64, i64* %arrayidx9.9.1, align 8, !tbaa !3
  %69 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i.9.1 = and i64 %68, 4294967295
  %shr.i.9.1 = lshr i64 %68, 32
  %and1.i.9.1 = and i64 %69, 4294967295
  %shr2.i.9.1 = lshr i64 %69, 32
  %mul.i.9.1 = mul nuw i64 %and1.i.9.1, %and.i.9.1
  %mul3.i.9.1 = mul nuw i64 %shr2.i.9.1, %and.i.9.1
  %mul4.i.9.1 = mul nuw i64 %and1.i.9.1, %shr.i.9.1
  %mul5.i.9.1 = mul nuw i64 %shr2.i.9.1, %shr.i.9.1
  %and6.i.9.1 = and i64 %mul.i.9.1, 4294967295
  %shr7.i.9.1 = lshr i64 %mul.i.9.1, 32
  %and8.i.9.1 = and i64 %mul4.i.9.1, 4294967295
  %and9.i.9.1 = and i64 %mul3.i.9.1, 4294967295
  %add.i.9.1 = add nuw nsw i64 %shr7.i.9.1, %and8.i.9.1
  %add10.i.9.1 = add nuw nsw i64 %add.i.9.1, %and9.i.9.1
  %shr11.i.9.1 = lshr i64 %add10.i.9.1, 32
  %shl.i.9.1 = shl i64 %add10.i.9.1, 32
  %xor68.i.9.1 = or i64 %shl.i.9.1, %and6.i.9.1
  %shr13.i.9.1 = lshr i64 %mul4.i.9.1, 32
  %shr14.i.9.1 = lshr i64 %mul3.i.9.1, 32
  %and15.i.9.1 = and i64 %mul5.i.9.1, 4294967295
  %add16.i.9.1 = add nuw nsw i64 %shr13.i.9.1, %shr14.i.9.1
  %add17.i.9.1 = add nuw nsw i64 %add16.i.9.1, %and15.i.9.1
  %add18.i.9.1 = add nuw nsw i64 %add17.i.9.1, %shr11.i.9.1
  %and19.i.9.1 = and i64 %add18.i.9.1, 4294967295
  %and21.i.9.1 = and i64 %add18.i.9.1, 30064771072
  %and22.i.9.1 = and i64 %mul5.i.9.1, -4294967296
  %add23.i.9.1 = add i64 %and21.i.9.1, %and22.i.9.1
  %xor2569.i.9.1 = or i64 %add23.i.9.1, %and19.i.9.1
  %add16.9.1 = add i64 %xor68.i.9.1, %add16.9
  %xor.i315.9.1 = xor i64 %add16.9.1, %shl.i.9.1
  %xor1.i316.9.1 = xor i64 %shl.i.9.1, %add16.9
  %or.i317.9.1 = or i64 %xor.i315.9.1, %xor1.i316.9.1
  %xor2.i318.9.1 = xor i64 %or.i317.9.1, %add16.9.1
  %shr.i319.9.1 = lshr i64 %xor2.i318.9.1, 63
  %add20.9.1 = add i64 %shr.i319.9.1, %xor2569.i.9.1
  %add21.9.1 = add i64 %add20.9.1, %add21.9
  %70 = xor i64 %add20.9.1, -9223372036854775808
  %xor2.i312.9.1 = and i64 %70, %add23.i.9.1
  %xor.i303.9.1 = xor i64 %add21.9.1, %add20.9.1
  %xor1.i304.9.1 = xor i64 %add20.9.1, %add21.9
  %or.i305.9.1 = or i64 %xor.i303.9.1, %xor1.i304.9.1
  %xor2.i306.9.1 = xor i64 %or.i305.9.1, %add21.9.1
  %shr.i313328.9.1 = or i64 %xor2.i306.9.1, %xor2.i312.9.1
  %or25327.9.1 = lshr i64 %shr.i313328.9.1, 63
  %add27.9.1 = add nuw nsw i64 %or25327.9.1, %or25327.9
  %arrayidx9.9.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %71 = load i64, i64* %arrayidx9.9.2, align 8, !tbaa !3
  %72 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i.9.2 = and i64 %71, 4294967295
  %shr.i.9.2 = lshr i64 %71, 32
  %and1.i.9.2 = and i64 %72, 4294967295
  %shr2.i.9.2 = lshr i64 %72, 32
  %mul.i.9.2 = mul nuw i64 %and1.i.9.2, %and.i.9.2
  %mul3.i.9.2 = mul nuw i64 %shr2.i.9.2, %and.i.9.2
  %mul4.i.9.2 = mul nuw i64 %and1.i.9.2, %shr.i.9.2
  %mul5.i.9.2 = mul nuw i64 %shr2.i.9.2, %shr.i.9.2
  %and6.i.9.2 = and i64 %mul.i.9.2, 4294967295
  %shr7.i.9.2 = lshr i64 %mul.i.9.2, 32
  %and8.i.9.2 = and i64 %mul4.i.9.2, 4294967295
  %and9.i.9.2 = and i64 %mul3.i.9.2, 4294967295
  %add.i.9.2 = add nuw nsw i64 %shr7.i.9.2, %and8.i.9.2
  %add10.i.9.2 = add nuw nsw i64 %add.i.9.2, %and9.i.9.2
  %shr11.i.9.2 = lshr i64 %add10.i.9.2, 32
  %shl.i.9.2 = shl i64 %add10.i.9.2, 32
  %xor68.i.9.2 = or i64 %shl.i.9.2, %and6.i.9.2
  %shr13.i.9.2 = lshr i64 %mul4.i.9.2, 32
  %shr14.i.9.2 = lshr i64 %mul3.i.9.2, 32
  %and15.i.9.2 = and i64 %mul5.i.9.2, 4294967295
  %add16.i.9.2 = add nuw nsw i64 %shr13.i.9.2, %shr14.i.9.2
  %add17.i.9.2 = add nuw nsw i64 %add16.i.9.2, %and15.i.9.2
  %add18.i.9.2 = add nuw nsw i64 %add17.i.9.2, %shr11.i.9.2
  %and19.i.9.2 = and i64 %add18.i.9.2, 4294967295
  %and21.i.9.2 = and i64 %add18.i.9.2, 30064771072
  %and22.i.9.2 = and i64 %mul5.i.9.2, -4294967296
  %add23.i.9.2 = add i64 %and21.i.9.2, %and22.i.9.2
  %xor2569.i.9.2 = or i64 %add23.i.9.2, %and19.i.9.2
  %add16.9.2 = add i64 %xor68.i.9.2, %add16.9.1
  %xor.i315.9.2 = xor i64 %add16.9.2, %shl.i.9.2
  %xor1.i316.9.2 = xor i64 %shl.i.9.2, %add16.9.1
  %or.i317.9.2 = or i64 %xor.i315.9.2, %xor1.i316.9.2
  %xor2.i318.9.2 = xor i64 %or.i317.9.2, %add16.9.2
  %shr.i319.9.2 = lshr i64 %xor2.i318.9.2, 63
  %add20.9.2 = add i64 %shr.i319.9.2, %xor2569.i.9.2
  %add21.9.2 = add i64 %add20.9.2, %add21.9.1
  %73 = xor i64 %add20.9.2, -9223372036854775808
  %xor2.i312.9.2 = and i64 %73, %add23.i.9.2
  %xor.i303.9.2 = xor i64 %add21.9.2, %add20.9.2
  %xor1.i304.9.2 = xor i64 %add20.9.2, %add21.9.1
  %or.i305.9.2 = or i64 %xor.i303.9.2, %xor1.i304.9.2
  %xor2.i306.9.2 = xor i64 %or.i305.9.2, %add21.9.2
  %shr.i313328.9.2 = or i64 %xor2.i306.9.2, %xor2.i312.9.2
  %or25327.9.2 = lshr i64 %shr.i313328.9.2, 63
  %add27.9.2 = add nsw i64 %or25327.9.2, %add27.9.1
  %arrayidx9.9.3 = getelementptr inbounds i64, i64* %mc, i64 3
  %74 = load i64, i64* %arrayidx9.9.3, align 8, !tbaa !3
  %75 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.9.3 = and i64 %74, 4294967295
  %shr.i.9.3 = lshr i64 %74, 32
  %and1.i.9.3 = and i64 %75, 4294967295
  %shr2.i.9.3 = lshr i64 %75, 32
  %mul.i.9.3 = mul nuw i64 %and1.i.9.3, %and.i.9.3
  %mul3.i.9.3 = mul nuw i64 %shr2.i.9.3, %and.i.9.3
  %mul4.i.9.3 = mul nuw i64 %and1.i.9.3, %shr.i.9.3
  %mul5.i.9.3 = mul nuw i64 %shr2.i.9.3, %shr.i.9.3
  %and6.i.9.3 = and i64 %mul.i.9.3, 4294967295
  %shr7.i.9.3 = lshr i64 %mul.i.9.3, 32
  %and8.i.9.3 = and i64 %mul4.i.9.3, 4294967295
  %and9.i.9.3 = and i64 %mul3.i.9.3, 4294967295
  %add.i.9.3 = add nuw nsw i64 %shr7.i.9.3, %and8.i.9.3
  %add10.i.9.3 = add nuw nsw i64 %add.i.9.3, %and9.i.9.3
  %shr11.i.9.3 = lshr i64 %add10.i.9.3, 32
  %shl.i.9.3 = shl i64 %add10.i.9.3, 32
  %xor68.i.9.3 = or i64 %shl.i.9.3, %and6.i.9.3
  %shr13.i.9.3 = lshr i64 %mul4.i.9.3, 32
  %shr14.i.9.3 = lshr i64 %mul3.i.9.3, 32
  %and15.i.9.3 = and i64 %mul5.i.9.3, 4294967295
  %add16.i.9.3 = add nuw nsw i64 %shr13.i.9.3, %shr14.i.9.3
  %add17.i.9.3 = add nuw nsw i64 %add16.i.9.3, %and15.i.9.3
  %add18.i.9.3 = add nuw nsw i64 %add17.i.9.3, %shr11.i.9.3
  %and19.i.9.3 = and i64 %add18.i.9.3, 4294967295
  %and21.i.9.3 = and i64 %add18.i.9.3, 30064771072
  %and22.i.9.3 = and i64 %mul5.i.9.3, -4294967296
  %add23.i.9.3 = add i64 %and21.i.9.3, %and22.i.9.3
  %xor2569.i.9.3 = or i64 %add23.i.9.3, %and19.i.9.3
  %add16.9.3 = add i64 %xor68.i.9.3, %add16.9.2
  %xor.i315.9.3 = xor i64 %add16.9.3, %shl.i.9.3
  %xor1.i316.9.3 = xor i64 %shl.i.9.3, %add16.9.2
  %or.i317.9.3 = or i64 %xor.i315.9.3, %xor1.i316.9.3
  %xor2.i318.9.3 = xor i64 %or.i317.9.3, %add16.9.3
  %shr.i319.9.3 = lshr i64 %xor2.i318.9.3, 63
  %add20.9.3 = add i64 %shr.i319.9.3, %xor2569.i.9.3
  %add21.9.3 = add i64 %add20.9.3, %add21.9.2
  %76 = xor i64 %add20.9.3, -9223372036854775808
  %xor2.i312.9.3 = and i64 %76, %add23.i.9.3
  %xor.i303.9.3 = xor i64 %add21.9.3, %add20.9.3
  %xor1.i304.9.3 = xor i64 %add20.9.3, %add21.9.2
  %or.i305.9.3 = or i64 %xor.i303.9.3, %xor1.i304.9.3
  %xor2.i306.9.3 = xor i64 %or.i305.9.3, %add21.9.3
  %shr.i313328.9.3 = or i64 %xor2.i306.9.3, %xor2.i312.9.3
  %or25327.9.3 = lshr i64 %shr.i313328.9.3, 63
  %add27.9.3 = add nsw i64 %or25327.9.3, %add27.9.2
  %arrayidx9.9.4 = getelementptr inbounds i64, i64* %mc, i64 4
  %77 = load i64, i64* %arrayidx9.9.4, align 8, !tbaa !3
  %78 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.9.4 = and i64 %77, 4294967295
  %shr.i.9.4 = lshr i64 %77, 32
  %and1.i.9.4 = and i64 %78, 4294967295
  %shr2.i.9.4 = lshr i64 %78, 32
  %mul.i.9.4 = mul nuw i64 %and1.i.9.4, %and.i.9.4
  %mul3.i.9.4 = mul nuw i64 %shr2.i.9.4, %and.i.9.4
  %mul4.i.9.4 = mul nuw i64 %and1.i.9.4, %shr.i.9.4
  %mul5.i.9.4 = mul nuw i64 %shr2.i.9.4, %shr.i.9.4
  %and6.i.9.4 = and i64 %mul.i.9.4, 4294967295
  %shr7.i.9.4 = lshr i64 %mul.i.9.4, 32
  %and8.i.9.4 = and i64 %mul4.i.9.4, 4294967295
  %and9.i.9.4 = and i64 %mul3.i.9.4, 4294967295
  %add.i.9.4 = add nuw nsw i64 %shr7.i.9.4, %and8.i.9.4
  %add10.i.9.4 = add nuw nsw i64 %add.i.9.4, %and9.i.9.4
  %shr11.i.9.4 = lshr i64 %add10.i.9.4, 32
  %shl.i.9.4 = shl i64 %add10.i.9.4, 32
  %xor68.i.9.4 = or i64 %shl.i.9.4, %and6.i.9.4
  %shr13.i.9.4 = lshr i64 %mul4.i.9.4, 32
  %shr14.i.9.4 = lshr i64 %mul3.i.9.4, 32
  %and15.i.9.4 = and i64 %mul5.i.9.4, 4294967295
  %add16.i.9.4 = add nuw nsw i64 %shr13.i.9.4, %shr14.i.9.4
  %add17.i.9.4 = add nuw nsw i64 %add16.i.9.4, %and15.i.9.4
  %add18.i.9.4 = add nuw nsw i64 %add17.i.9.4, %shr11.i.9.4
  %and19.i.9.4 = and i64 %add18.i.9.4, 4294967295
  %and21.i.9.4 = and i64 %add18.i.9.4, 30064771072
  %and22.i.9.4 = and i64 %mul5.i.9.4, -4294967296
  %add23.i.9.4 = add i64 %and21.i.9.4, %and22.i.9.4
  %xor2569.i.9.4 = or i64 %add23.i.9.4, %and19.i.9.4
  %add16.9.4 = add i64 %xor68.i.9.4, %add16.9.3
  %xor.i315.9.4 = xor i64 %add16.9.4, %shl.i.9.4
  %xor1.i316.9.4 = xor i64 %shl.i.9.4, %add16.9.3
  %or.i317.9.4 = or i64 %xor.i315.9.4, %xor1.i316.9.4
  %xor2.i318.9.4 = xor i64 %or.i317.9.4, %add16.9.4
  %shr.i319.9.4 = lshr i64 %xor2.i318.9.4, 63
  %add20.9.4 = add i64 %shr.i319.9.4, %xor2569.i.9.4
  %add21.9.4 = add i64 %add20.9.4, %add21.9.3
  %79 = xor i64 %add20.9.4, -9223372036854775808
  %xor2.i312.9.4 = and i64 %79, %add23.i.9.4
  %xor.i303.9.4 = xor i64 %add21.9.4, %add20.9.4
  %xor1.i304.9.4 = xor i64 %add20.9.4, %add21.9.3
  %or.i305.9.4 = or i64 %xor.i303.9.4, %xor1.i304.9.4
  %xor2.i306.9.4 = xor i64 %or.i305.9.4, %add21.9.4
  %shr.i313328.9.4 = or i64 %xor2.i306.9.4, %xor2.i312.9.4
  %or25327.9.4 = lshr i64 %shr.i313328.9.4, 63
  %add27.9.4 = add i64 %or25327.9.4, %add27.9.3
  %arrayidx34.9 = getelementptr inbounds i64, i64* %ma, i64 9
  %80 = load i64, i64* %arrayidx34.9, align 8, !tbaa !3
  %add35.9 = add i64 %80, %add16.9.4
  %xor.i297.9 = xor i64 %add35.9, %add16.9.4
  %xor1.i298.9 = xor i64 %80, %add16.9.4
  %or.i299.9 = or i64 %xor.i297.9, %xor1.i298.9
  %xor2.i300.9 = xor i64 %or.i299.9, %add35.9
  %shr.i301.9 = lshr i64 %xor2.i300.9, 63
  %add41.9 = add i64 %shr.i301.9, %add21.9.4
  %arrayidx50.9 = getelementptr inbounds i64, i64* %mc, i64 9
  store i64 %add35.9, i64* %arrayidx50.9, align 8, !tbaa !3
  %81 = xor i64 %add41.9, -9223372036854775808
  %xor2.i294.9 = and i64 %81, %add21.9.4
  %shr.i295.9 = lshr i64 %xor2.i294.9, 63
  %add48.9 = add i64 %shr.i295.9, %add27.9.4
  %82 = load i64, i64* %mc, align 8, !tbaa !3
  %83 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i.10 = and i64 %82, 4294967295
  %shr.i.10 = lshr i64 %82, 32
  %and1.i.10 = and i64 %83, 4294967295
  %shr2.i.10 = lshr i64 %83, 32
  %mul.i.10 = mul nuw i64 %and1.i.10, %and.i.10
  %mul3.i.10 = mul nuw i64 %shr2.i.10, %and.i.10
  %mul4.i.10 = mul nuw i64 %and1.i.10, %shr.i.10
  %mul5.i.10 = mul nuw i64 %shr2.i.10, %shr.i.10
  %and6.i.10 = and i64 %mul.i.10, 4294967295
  %shr7.i.10 = lshr i64 %mul.i.10, 32
  %and8.i.10 = and i64 %mul4.i.10, 4294967295
  %and9.i.10 = and i64 %mul3.i.10, 4294967295
  %add.i.10 = add nuw nsw i64 %shr7.i.10, %and8.i.10
  %add10.i.10 = add nuw nsw i64 %add.i.10, %and9.i.10
  %shr11.i.10 = lshr i64 %add10.i.10, 32
  %shl.i.10 = shl i64 %add10.i.10, 32
  %xor68.i.10 = or i64 %shl.i.10, %and6.i.10
  %shr13.i.10 = lshr i64 %mul4.i.10, 32
  %shr14.i.10 = lshr i64 %mul3.i.10, 32
  %and15.i.10 = and i64 %mul5.i.10, 4294967295
  %add16.i.10 = add nuw nsw i64 %shr13.i.10, %shr14.i.10
  %add17.i.10 = add nuw nsw i64 %add16.i.10, %and15.i.10
  %add18.i.10 = add nuw nsw i64 %add17.i.10, %shr11.i.10
  %and19.i.10 = and i64 %add18.i.10, 4294967295
  %and21.i.10 = and i64 %add18.i.10, 30064771072
  %and22.i.10 = and i64 %mul5.i.10, -4294967296
  %add23.i.10 = add i64 %and21.i.10, %and22.i.10
  %xor2569.i.10 = or i64 %add23.i.10, %and19.i.10
  %add16.10 = add i64 %xor68.i.10, %add41.9
  %xor.i315.10 = xor i64 %add16.10, %shl.i.10
  %xor1.i316.10 = xor i64 %shl.i.10, %add41.9
  %or.i317.10 = or i64 %xor.i315.10, %xor1.i316.10
  %xor2.i318.10 = xor i64 %or.i317.10, %add16.10
  %shr.i319.10 = lshr i64 %xor2.i318.10, 63
  %add20.10 = add i64 %shr.i319.10, %xor2569.i.10
  %add21.10 = add i64 %add20.10, %add48.9
  %84 = xor i64 %add20.10, -9223372036854775808
  %xor2.i312.10 = and i64 %84, %add23.i.10
  %xor.i303.10 = xor i64 %add21.10, %add20.10
  %xor1.i304.10 = xor i64 %add20.10, %add48.9
  %or.i305.10 = or i64 %xor.i303.10, %xor1.i304.10
  %xor2.i306.10 = xor i64 %or.i305.10, %add21.10
  %shr.i313328.10 = or i64 %xor2.i306.10, %xor2.i312.10
  %or25327.10 = lshr i64 %shr.i313328.10, 63
  %arrayidx9.10.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %85 = load i64, i64* %arrayidx9.10.1, align 8, !tbaa !3
  %86 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i.10.1 = and i64 %85, 4294967295
  %shr.i.10.1 = lshr i64 %85, 32
  %and1.i.10.1 = and i64 %86, 4294967295
  %shr2.i.10.1 = lshr i64 %86, 32
  %mul.i.10.1 = mul nuw i64 %and1.i.10.1, %and.i.10.1
  %mul3.i.10.1 = mul nuw i64 %shr2.i.10.1, %and.i.10.1
  %mul4.i.10.1 = mul nuw i64 %and1.i.10.1, %shr.i.10.1
  %mul5.i.10.1 = mul nuw i64 %shr2.i.10.1, %shr.i.10.1
  %and6.i.10.1 = and i64 %mul.i.10.1, 4294967295
  %shr7.i.10.1 = lshr i64 %mul.i.10.1, 32
  %and8.i.10.1 = and i64 %mul4.i.10.1, 4294967295
  %and9.i.10.1 = and i64 %mul3.i.10.1, 4294967295
  %add.i.10.1 = add nuw nsw i64 %shr7.i.10.1, %and8.i.10.1
  %add10.i.10.1 = add nuw nsw i64 %add.i.10.1, %and9.i.10.1
  %shr11.i.10.1 = lshr i64 %add10.i.10.1, 32
  %shl.i.10.1 = shl i64 %add10.i.10.1, 32
  %xor68.i.10.1 = or i64 %shl.i.10.1, %and6.i.10.1
  %shr13.i.10.1 = lshr i64 %mul4.i.10.1, 32
  %shr14.i.10.1 = lshr i64 %mul3.i.10.1, 32
  %and15.i.10.1 = and i64 %mul5.i.10.1, 4294967295
  %add16.i.10.1 = add nuw nsw i64 %shr13.i.10.1, %shr14.i.10.1
  %add17.i.10.1 = add nuw nsw i64 %add16.i.10.1, %and15.i.10.1
  %add18.i.10.1 = add nuw nsw i64 %add17.i.10.1, %shr11.i.10.1
  %and19.i.10.1 = and i64 %add18.i.10.1, 4294967295
  %and21.i.10.1 = and i64 %add18.i.10.1, 30064771072
  %and22.i.10.1 = and i64 %mul5.i.10.1, -4294967296
  %add23.i.10.1 = add i64 %and21.i.10.1, %and22.i.10.1
  %xor2569.i.10.1 = or i64 %add23.i.10.1, %and19.i.10.1
  %add16.10.1 = add i64 %xor68.i.10.1, %add16.10
  %xor.i315.10.1 = xor i64 %add16.10.1, %shl.i.10.1
  %xor1.i316.10.1 = xor i64 %shl.i.10.1, %add16.10
  %or.i317.10.1 = or i64 %xor.i315.10.1, %xor1.i316.10.1
  %xor2.i318.10.1 = xor i64 %or.i317.10.1, %add16.10.1
  %shr.i319.10.1 = lshr i64 %xor2.i318.10.1, 63
  %add20.10.1 = add i64 %shr.i319.10.1, %xor2569.i.10.1
  %add21.10.1 = add i64 %add20.10.1, %add21.10
  %87 = xor i64 %add20.10.1, -9223372036854775808
  %xor2.i312.10.1 = and i64 %87, %add23.i.10.1
  %xor.i303.10.1 = xor i64 %add21.10.1, %add20.10.1
  %xor1.i304.10.1 = xor i64 %add20.10.1, %add21.10
  %or.i305.10.1 = or i64 %xor.i303.10.1, %xor1.i304.10.1
  %xor2.i306.10.1 = xor i64 %or.i305.10.1, %add21.10.1
  %shr.i313328.10.1 = or i64 %xor2.i306.10.1, %xor2.i312.10.1
  %or25327.10.1 = lshr i64 %shr.i313328.10.1, 63
  %add27.10.1 = add nuw nsw i64 %or25327.10.1, %or25327.10
  %arrayidx9.10.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %88 = load i64, i64* %arrayidx9.10.2, align 8, !tbaa !3
  %89 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i.10.2 = and i64 %88, 4294967295
  %shr.i.10.2 = lshr i64 %88, 32
  %and1.i.10.2 = and i64 %89, 4294967295
  %shr2.i.10.2 = lshr i64 %89, 32
  %mul.i.10.2 = mul nuw i64 %and1.i.10.2, %and.i.10.2
  %mul3.i.10.2 = mul nuw i64 %shr2.i.10.2, %and.i.10.2
  %mul4.i.10.2 = mul nuw i64 %and1.i.10.2, %shr.i.10.2
  %mul5.i.10.2 = mul nuw i64 %shr2.i.10.2, %shr.i.10.2
  %and6.i.10.2 = and i64 %mul.i.10.2, 4294967295
  %shr7.i.10.2 = lshr i64 %mul.i.10.2, 32
  %and8.i.10.2 = and i64 %mul4.i.10.2, 4294967295
  %and9.i.10.2 = and i64 %mul3.i.10.2, 4294967295
  %add.i.10.2 = add nuw nsw i64 %shr7.i.10.2, %and8.i.10.2
  %add10.i.10.2 = add nuw nsw i64 %add.i.10.2, %and9.i.10.2
  %shr11.i.10.2 = lshr i64 %add10.i.10.2, 32
  %shl.i.10.2 = shl i64 %add10.i.10.2, 32
  %xor68.i.10.2 = or i64 %shl.i.10.2, %and6.i.10.2
  %shr13.i.10.2 = lshr i64 %mul4.i.10.2, 32
  %shr14.i.10.2 = lshr i64 %mul3.i.10.2, 32
  %and15.i.10.2 = and i64 %mul5.i.10.2, 4294967295
  %add16.i.10.2 = add nuw nsw i64 %shr13.i.10.2, %shr14.i.10.2
  %add17.i.10.2 = add nuw nsw i64 %add16.i.10.2, %and15.i.10.2
  %add18.i.10.2 = add nuw nsw i64 %add17.i.10.2, %shr11.i.10.2
  %and19.i.10.2 = and i64 %add18.i.10.2, 4294967295
  %and21.i.10.2 = and i64 %add18.i.10.2, 30064771072
  %and22.i.10.2 = and i64 %mul5.i.10.2, -4294967296
  %add23.i.10.2 = add i64 %and21.i.10.2, %and22.i.10.2
  %xor2569.i.10.2 = or i64 %add23.i.10.2, %and19.i.10.2
  %add16.10.2 = add i64 %xor68.i.10.2, %add16.10.1
  %xor.i315.10.2 = xor i64 %add16.10.2, %shl.i.10.2
  %xor1.i316.10.2 = xor i64 %shl.i.10.2, %add16.10.1
  %or.i317.10.2 = or i64 %xor.i315.10.2, %xor1.i316.10.2
  %xor2.i318.10.2 = xor i64 %or.i317.10.2, %add16.10.2
  %shr.i319.10.2 = lshr i64 %xor2.i318.10.2, 63
  %add20.10.2 = add i64 %shr.i319.10.2, %xor2569.i.10.2
  %add21.10.2 = add i64 %add20.10.2, %add21.10.1
  %90 = xor i64 %add20.10.2, -9223372036854775808
  %xor2.i312.10.2 = and i64 %90, %add23.i.10.2
  %xor.i303.10.2 = xor i64 %add21.10.2, %add20.10.2
  %xor1.i304.10.2 = xor i64 %add20.10.2, %add21.10.1
  %or.i305.10.2 = or i64 %xor.i303.10.2, %xor1.i304.10.2
  %xor2.i306.10.2 = xor i64 %or.i305.10.2, %add21.10.2
  %shr.i313328.10.2 = or i64 %xor2.i306.10.2, %xor2.i312.10.2
  %or25327.10.2 = lshr i64 %shr.i313328.10.2, 63
  %add27.10.2 = add nsw i64 %or25327.10.2, %add27.10.1
  %arrayidx9.10.3 = getelementptr inbounds i64, i64* %mc, i64 3
  %91 = load i64, i64* %arrayidx9.10.3, align 8, !tbaa !3
  %92 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i.10.3 = and i64 %91, 4294967295
  %shr.i.10.3 = lshr i64 %91, 32
  %and1.i.10.3 = and i64 %92, 4294967295
  %shr2.i.10.3 = lshr i64 %92, 32
  %mul.i.10.3 = mul nuw i64 %and1.i.10.3, %and.i.10.3
  %mul3.i.10.3 = mul nuw i64 %shr2.i.10.3, %and.i.10.3
  %mul4.i.10.3 = mul nuw i64 %and1.i.10.3, %shr.i.10.3
  %mul5.i.10.3 = mul nuw i64 %shr2.i.10.3, %shr.i.10.3
  %and6.i.10.3 = and i64 %mul.i.10.3, 4294967295
  %shr7.i.10.3 = lshr i64 %mul.i.10.3, 32
  %and8.i.10.3 = and i64 %mul4.i.10.3, 4294967295
  %and9.i.10.3 = and i64 %mul3.i.10.3, 4294967295
  %add.i.10.3 = add nuw nsw i64 %shr7.i.10.3, %and8.i.10.3
  %add10.i.10.3 = add nuw nsw i64 %add.i.10.3, %and9.i.10.3
  %shr11.i.10.3 = lshr i64 %add10.i.10.3, 32
  %shl.i.10.3 = shl i64 %add10.i.10.3, 32
  %xor68.i.10.3 = or i64 %shl.i.10.3, %and6.i.10.3
  %shr13.i.10.3 = lshr i64 %mul4.i.10.3, 32
  %shr14.i.10.3 = lshr i64 %mul3.i.10.3, 32
  %and15.i.10.3 = and i64 %mul5.i.10.3, 4294967295
  %add16.i.10.3 = add nuw nsw i64 %shr13.i.10.3, %shr14.i.10.3
  %add17.i.10.3 = add nuw nsw i64 %add16.i.10.3, %and15.i.10.3
  %add18.i.10.3 = add nuw nsw i64 %add17.i.10.3, %shr11.i.10.3
  %and19.i.10.3 = and i64 %add18.i.10.3, 4294967295
  %and21.i.10.3 = and i64 %add18.i.10.3, 30064771072
  %and22.i.10.3 = and i64 %mul5.i.10.3, -4294967296
  %add23.i.10.3 = add i64 %and21.i.10.3, %and22.i.10.3
  %xor2569.i.10.3 = or i64 %add23.i.10.3, %and19.i.10.3
  %add16.10.3 = add i64 %xor68.i.10.3, %add16.10.2
  %xor.i315.10.3 = xor i64 %add16.10.3, %shl.i.10.3
  %xor1.i316.10.3 = xor i64 %shl.i.10.3, %add16.10.2
  %or.i317.10.3 = or i64 %xor.i315.10.3, %xor1.i316.10.3
  %xor2.i318.10.3 = xor i64 %or.i317.10.3, %add16.10.3
  %shr.i319.10.3 = lshr i64 %xor2.i318.10.3, 63
  %add20.10.3 = add i64 %shr.i319.10.3, %xor2569.i.10.3
  %add21.10.3 = add i64 %add20.10.3, %add21.10.2
  %93 = xor i64 %add20.10.3, -9223372036854775808
  %xor2.i312.10.3 = and i64 %93, %add23.i.10.3
  %xor.i303.10.3 = xor i64 %add21.10.3, %add20.10.3
  %xor1.i304.10.3 = xor i64 %add20.10.3, %add21.10.2
  %or.i305.10.3 = or i64 %xor.i303.10.3, %xor1.i304.10.3
  %xor2.i306.10.3 = xor i64 %or.i305.10.3, %add21.10.3
  %shr.i313328.10.3 = or i64 %xor2.i306.10.3, %xor2.i312.10.3
  %or25327.10.3 = lshr i64 %shr.i313328.10.3, 63
  %add27.10.3 = add nsw i64 %or25327.10.3, %add27.10.2
  %arrayidx9.10.4 = getelementptr inbounds i64, i64* %mc, i64 4
  %94 = load i64, i64* %arrayidx9.10.4, align 8, !tbaa !3
  %95 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.10.4 = and i64 %94, 4294967295
  %shr.i.10.4 = lshr i64 %94, 32
  %and1.i.10.4 = and i64 %95, 4294967295
  %shr2.i.10.4 = lshr i64 %95, 32
  %mul.i.10.4 = mul nuw i64 %and1.i.10.4, %and.i.10.4
  %mul3.i.10.4 = mul nuw i64 %shr2.i.10.4, %and.i.10.4
  %mul4.i.10.4 = mul nuw i64 %and1.i.10.4, %shr.i.10.4
  %mul5.i.10.4 = mul nuw i64 %shr2.i.10.4, %shr.i.10.4
  %and6.i.10.4 = and i64 %mul.i.10.4, 4294967295
  %shr7.i.10.4 = lshr i64 %mul.i.10.4, 32
  %and8.i.10.4 = and i64 %mul4.i.10.4, 4294967295
  %and9.i.10.4 = and i64 %mul3.i.10.4, 4294967295
  %add.i.10.4 = add nuw nsw i64 %shr7.i.10.4, %and8.i.10.4
  %add10.i.10.4 = add nuw nsw i64 %add.i.10.4, %and9.i.10.4
  %shr11.i.10.4 = lshr i64 %add10.i.10.4, 32
  %shl.i.10.4 = shl i64 %add10.i.10.4, 32
  %xor68.i.10.4 = or i64 %shl.i.10.4, %and6.i.10.4
  %shr13.i.10.4 = lshr i64 %mul4.i.10.4, 32
  %shr14.i.10.4 = lshr i64 %mul3.i.10.4, 32
  %and15.i.10.4 = and i64 %mul5.i.10.4, 4294967295
  %add16.i.10.4 = add nuw nsw i64 %shr13.i.10.4, %shr14.i.10.4
  %add17.i.10.4 = add nuw nsw i64 %add16.i.10.4, %and15.i.10.4
  %add18.i.10.4 = add nuw nsw i64 %add17.i.10.4, %shr11.i.10.4
  %and19.i.10.4 = and i64 %add18.i.10.4, 4294967295
  %and21.i.10.4 = and i64 %add18.i.10.4, 30064771072
  %and22.i.10.4 = and i64 %mul5.i.10.4, -4294967296
  %add23.i.10.4 = add i64 %and21.i.10.4, %and22.i.10.4
  %xor2569.i.10.4 = or i64 %add23.i.10.4, %and19.i.10.4
  %add16.10.4 = add i64 %xor68.i.10.4, %add16.10.3
  %xor.i315.10.4 = xor i64 %add16.10.4, %shl.i.10.4
  %xor1.i316.10.4 = xor i64 %shl.i.10.4, %add16.10.3
  %or.i317.10.4 = or i64 %xor.i315.10.4, %xor1.i316.10.4
  %xor2.i318.10.4 = xor i64 %or.i317.10.4, %add16.10.4
  %shr.i319.10.4 = lshr i64 %xor2.i318.10.4, 63
  %add20.10.4 = add i64 %shr.i319.10.4, %xor2569.i.10.4
  %add21.10.4 = add i64 %add20.10.4, %add21.10.3
  %96 = xor i64 %add20.10.4, -9223372036854775808
  %xor2.i312.10.4 = and i64 %96, %add23.i.10.4
  %xor.i303.10.4 = xor i64 %add21.10.4, %add20.10.4
  %xor1.i304.10.4 = xor i64 %add20.10.4, %add21.10.3
  %or.i305.10.4 = or i64 %xor.i303.10.4, %xor1.i304.10.4
  %xor2.i306.10.4 = xor i64 %or.i305.10.4, %add21.10.4
  %shr.i313328.10.4 = or i64 %xor2.i306.10.4, %xor2.i312.10.4
  %or25327.10.4 = lshr i64 %shr.i313328.10.4, 63
  %add27.10.4 = add i64 %or25327.10.4, %add27.10.3
  %arrayidx9.10.5 = getelementptr inbounds i64, i64* %mc, i64 5
  %97 = load i64, i64* %arrayidx9.10.5, align 8, !tbaa !3
  %98 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.10.5 = and i64 %97, 4294967295
  %shr.i.10.5 = lshr i64 %97, 32
  %and1.i.10.5 = and i64 %98, 4294967295
  %shr2.i.10.5 = lshr i64 %98, 32
  %mul.i.10.5 = mul nuw i64 %and1.i.10.5, %and.i.10.5
  %mul3.i.10.5 = mul nuw i64 %shr2.i.10.5, %and.i.10.5
  %mul4.i.10.5 = mul nuw i64 %and1.i.10.5, %shr.i.10.5
  %mul5.i.10.5 = mul nuw i64 %shr2.i.10.5, %shr.i.10.5
  %and6.i.10.5 = and i64 %mul.i.10.5, 4294967295
  %shr7.i.10.5 = lshr i64 %mul.i.10.5, 32
  %and8.i.10.5 = and i64 %mul4.i.10.5, 4294967295
  %and9.i.10.5 = and i64 %mul3.i.10.5, 4294967295
  %add.i.10.5 = add nuw nsw i64 %shr7.i.10.5, %and8.i.10.5
  %add10.i.10.5 = add nuw nsw i64 %add.i.10.5, %and9.i.10.5
  %shr11.i.10.5 = lshr i64 %add10.i.10.5, 32
  %shl.i.10.5 = shl i64 %add10.i.10.5, 32
  %xor68.i.10.5 = or i64 %shl.i.10.5, %and6.i.10.5
  %shr13.i.10.5 = lshr i64 %mul4.i.10.5, 32
  %shr14.i.10.5 = lshr i64 %mul3.i.10.5, 32
  %and15.i.10.5 = and i64 %mul5.i.10.5, 4294967295
  %add16.i.10.5 = add nuw nsw i64 %shr13.i.10.5, %shr14.i.10.5
  %add17.i.10.5 = add nuw nsw i64 %add16.i.10.5, %and15.i.10.5
  %add18.i.10.5 = add nuw nsw i64 %add17.i.10.5, %shr11.i.10.5
  %and19.i.10.5 = and i64 %add18.i.10.5, 4294967295
  %and21.i.10.5 = and i64 %add18.i.10.5, 30064771072
  %and22.i.10.5 = and i64 %mul5.i.10.5, -4294967296
  %add23.i.10.5 = add i64 %and21.i.10.5, %and22.i.10.5
  %xor2569.i.10.5 = or i64 %add23.i.10.5, %and19.i.10.5
  %add16.10.5 = add i64 %xor68.i.10.5, %add16.10.4
  %xor.i315.10.5 = xor i64 %add16.10.5, %shl.i.10.5
  %xor1.i316.10.5 = xor i64 %shl.i.10.5, %add16.10.4
  %or.i317.10.5 = or i64 %xor.i315.10.5, %xor1.i316.10.5
  %xor2.i318.10.5 = xor i64 %or.i317.10.5, %add16.10.5
  %shr.i319.10.5 = lshr i64 %xor2.i318.10.5, 63
  %add20.10.5 = add i64 %shr.i319.10.5, %xor2569.i.10.5
  %add21.10.5 = add i64 %add20.10.5, %add21.10.4
  %99 = xor i64 %add20.10.5, -9223372036854775808
  %xor2.i312.10.5 = and i64 %99, %add23.i.10.5
  %xor.i303.10.5 = xor i64 %add21.10.5, %add20.10.5
  %xor1.i304.10.5 = xor i64 %add20.10.5, %add21.10.4
  %or.i305.10.5 = or i64 %xor.i303.10.5, %xor1.i304.10.5
  %xor2.i306.10.5 = xor i64 %or.i305.10.5, %add21.10.5
  %shr.i313328.10.5 = or i64 %xor2.i306.10.5, %xor2.i312.10.5
  %or25327.10.5 = lshr i64 %shr.i313328.10.5, 63
  %add27.10.5 = add i64 %or25327.10.5, %add27.10.4
  %arrayidx34.10 = getelementptr inbounds i64, i64* %ma, i64 10
  %100 = load i64, i64* %arrayidx34.10, align 8, !tbaa !3
  %add35.10 = add i64 %100, %add16.10.5
  %xor.i297.10 = xor i64 %add35.10, %add16.10.5
  %xor1.i298.10 = xor i64 %100, %add16.10.5
  %or.i299.10 = or i64 %xor.i297.10, %xor1.i298.10
  %xor2.i300.10 = xor i64 %or.i299.10, %add35.10
  %shr.i301.10 = lshr i64 %xor2.i300.10, 63
  %add41.10 = add i64 %shr.i301.10, %add21.10.5
  %arrayidx50.10 = getelementptr inbounds i64, i64* %mc, i64 10
  store i64 %add35.10, i64* %arrayidx50.10, align 8, !tbaa !3
  %101 = xor i64 %add41.10, -9223372036854775808
  %xor2.i294.10 = and i64 %101, %add21.10.5
  %shr.i295.10 = lshr i64 %xor2.i294.10, 63
  %add48.10 = add i64 %shr.i295.10, %add27.10.5
  %102 = load i64, i64* %mc, align 8, !tbaa !3
  %103 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i.11 = and i64 %102, 4294967295
  %shr.i.11 = lshr i64 %102, 32
  %and1.i.11 = and i64 %103, 4294967295
  %shr2.i.11 = lshr i64 %103, 32
  %mul.i.11 = mul nuw i64 %and1.i.11, %and.i.11
  %mul3.i.11 = mul nuw i64 %shr2.i.11, %and.i.11
  %mul4.i.11 = mul nuw i64 %and1.i.11, %shr.i.11
  %mul5.i.11 = mul nuw i64 %shr2.i.11, %shr.i.11
  %and6.i.11 = and i64 %mul.i.11, 4294967295
  %shr7.i.11 = lshr i64 %mul.i.11, 32
  %and8.i.11 = and i64 %mul4.i.11, 4294967295
  %and9.i.11 = and i64 %mul3.i.11, 4294967295
  %add.i.11 = add nuw nsw i64 %shr7.i.11, %and8.i.11
  %add10.i.11 = add nuw nsw i64 %add.i.11, %and9.i.11
  %shr11.i.11 = lshr i64 %add10.i.11, 32
  %shl.i.11 = shl i64 %add10.i.11, 32
  %xor68.i.11 = or i64 %shl.i.11, %and6.i.11
  %shr13.i.11 = lshr i64 %mul4.i.11, 32
  %shr14.i.11 = lshr i64 %mul3.i.11, 32
  %and15.i.11 = and i64 %mul5.i.11, 4294967295
  %add16.i.11 = add nuw nsw i64 %shr13.i.11, %shr14.i.11
  %add17.i.11 = add nuw nsw i64 %add16.i.11, %and15.i.11
  %add18.i.11 = add nuw nsw i64 %add17.i.11, %shr11.i.11
  %and19.i.11 = and i64 %add18.i.11, 4294967295
  %and21.i.11 = and i64 %add18.i.11, 30064771072
  %and22.i.11 = and i64 %mul5.i.11, -4294967296
  %add23.i.11 = add i64 %and21.i.11, %and22.i.11
  %xor2569.i.11 = or i64 %add23.i.11, %and19.i.11
  %add16.11 = add i64 %xor68.i.11, %add41.10
  %xor.i315.11 = xor i64 %add16.11, %shl.i.11
  %xor1.i316.11 = xor i64 %shl.i.11, %add41.10
  %or.i317.11 = or i64 %xor.i315.11, %xor1.i316.11
  %xor2.i318.11 = xor i64 %or.i317.11, %add16.11
  %shr.i319.11 = lshr i64 %xor2.i318.11, 63
  %add20.11 = add i64 %shr.i319.11, %xor2569.i.11
  %add21.11 = add i64 %add20.11, %add48.10
  %104 = xor i64 %add20.11, -9223372036854775808
  %xor2.i312.11 = and i64 %104, %add23.i.11
  %xor.i303.11 = xor i64 %add21.11, %add20.11
  %xor1.i304.11 = xor i64 %add20.11, %add48.10
  %or.i305.11 = or i64 %xor.i303.11, %xor1.i304.11
  %xor2.i306.11 = xor i64 %or.i305.11, %add21.11
  %shr.i313328.11 = or i64 %xor2.i306.11, %xor2.i312.11
  %or25327.11 = lshr i64 %shr.i313328.11, 63
  %arrayidx9.11.1 = getelementptr inbounds i64, i64* %mc, i64 1
  %105 = load i64, i64* %arrayidx9.11.1, align 8, !tbaa !3
  %106 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i.11.1 = and i64 %105, 4294967295
  %shr.i.11.1 = lshr i64 %105, 32
  %and1.i.11.1 = and i64 %106, 4294967295
  %shr2.i.11.1 = lshr i64 %106, 32
  %mul.i.11.1 = mul nuw i64 %and1.i.11.1, %and.i.11.1
  %mul3.i.11.1 = mul nuw i64 %shr2.i.11.1, %and.i.11.1
  %mul4.i.11.1 = mul nuw i64 %and1.i.11.1, %shr.i.11.1
  %mul5.i.11.1 = mul nuw i64 %shr2.i.11.1, %shr.i.11.1
  %and6.i.11.1 = and i64 %mul.i.11.1, 4294967295
  %shr7.i.11.1 = lshr i64 %mul.i.11.1, 32
  %and8.i.11.1 = and i64 %mul4.i.11.1, 4294967295
  %and9.i.11.1 = and i64 %mul3.i.11.1, 4294967295
  %add.i.11.1 = add nuw nsw i64 %shr7.i.11.1, %and8.i.11.1
  %add10.i.11.1 = add nuw nsw i64 %add.i.11.1, %and9.i.11.1
  %shr11.i.11.1 = lshr i64 %add10.i.11.1, 32
  %shl.i.11.1 = shl i64 %add10.i.11.1, 32
  %xor68.i.11.1 = or i64 %shl.i.11.1, %and6.i.11.1
  %shr13.i.11.1 = lshr i64 %mul4.i.11.1, 32
  %shr14.i.11.1 = lshr i64 %mul3.i.11.1, 32
  %and15.i.11.1 = and i64 %mul5.i.11.1, 4294967295
  %add16.i.11.1 = add nuw nsw i64 %shr13.i.11.1, %shr14.i.11.1
  %add17.i.11.1 = add nuw nsw i64 %add16.i.11.1, %and15.i.11.1
  %add18.i.11.1 = add nuw nsw i64 %add17.i.11.1, %shr11.i.11.1
  %and19.i.11.1 = and i64 %add18.i.11.1, 4294967295
  %and21.i.11.1 = and i64 %add18.i.11.1, 30064771072
  %and22.i.11.1 = and i64 %mul5.i.11.1, -4294967296
  %add23.i.11.1 = add i64 %and21.i.11.1, %and22.i.11.1
  %xor2569.i.11.1 = or i64 %add23.i.11.1, %and19.i.11.1
  %add16.11.1 = add i64 %xor68.i.11.1, %add16.11
  %xor.i315.11.1 = xor i64 %add16.11.1, %shl.i.11.1
  %xor1.i316.11.1 = xor i64 %shl.i.11.1, %add16.11
  %or.i317.11.1 = or i64 %xor.i315.11.1, %xor1.i316.11.1
  %xor2.i318.11.1 = xor i64 %or.i317.11.1, %add16.11.1
  %shr.i319.11.1 = lshr i64 %xor2.i318.11.1, 63
  %add20.11.1 = add i64 %shr.i319.11.1, %xor2569.i.11.1
  %add21.11.1 = add i64 %add20.11.1, %add21.11
  %107 = xor i64 %add20.11.1, -9223372036854775808
  %xor2.i312.11.1 = and i64 %107, %add23.i.11.1
  %xor.i303.11.1 = xor i64 %add21.11.1, %add20.11.1
  %xor1.i304.11.1 = xor i64 %add20.11.1, %add21.11
  %or.i305.11.1 = or i64 %xor.i303.11.1, %xor1.i304.11.1
  %xor2.i306.11.1 = xor i64 %or.i305.11.1, %add21.11.1
  %shr.i313328.11.1 = or i64 %xor2.i306.11.1, %xor2.i312.11.1
  %or25327.11.1 = lshr i64 %shr.i313328.11.1, 63
  %add27.11.1 = add nuw nsw i64 %or25327.11.1, %or25327.11
  %arrayidx9.11.2 = getelementptr inbounds i64, i64* %mc, i64 2
  %108 = load i64, i64* %arrayidx9.11.2, align 8, !tbaa !3
  %109 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i.11.2 = and i64 %108, 4294967295
  %shr.i.11.2 = lshr i64 %108, 32
  %and1.i.11.2 = and i64 %109, 4294967295
  %shr2.i.11.2 = lshr i64 %109, 32
  %mul.i.11.2 = mul nuw i64 %and1.i.11.2, %and.i.11.2
  %mul3.i.11.2 = mul nuw i64 %shr2.i.11.2, %and.i.11.2
  %mul4.i.11.2 = mul nuw i64 %and1.i.11.2, %shr.i.11.2
  %mul5.i.11.2 = mul nuw i64 %shr2.i.11.2, %shr.i.11.2
  %and6.i.11.2 = and i64 %mul.i.11.2, 4294967295
  %shr7.i.11.2 = lshr i64 %mul.i.11.2, 32
  %and8.i.11.2 = and i64 %mul4.i.11.2, 4294967295
  %and9.i.11.2 = and i64 %mul3.i.11.2, 4294967295
  %add.i.11.2 = add nuw nsw i64 %shr7.i.11.2, %and8.i.11.2
  %add10.i.11.2 = add nuw nsw i64 %add.i.11.2, %and9.i.11.2
  %shr11.i.11.2 = lshr i64 %add10.i.11.2, 32
  %shl.i.11.2 = shl i64 %add10.i.11.2, 32
  %xor68.i.11.2 = or i64 %shl.i.11.2, %and6.i.11.2
  %shr13.i.11.2 = lshr i64 %mul4.i.11.2, 32
  %shr14.i.11.2 = lshr i64 %mul3.i.11.2, 32
  %and15.i.11.2 = and i64 %mul5.i.11.2, 4294967295
  %add16.i.11.2 = add nuw nsw i64 %shr13.i.11.2, %shr14.i.11.2
  %add17.i.11.2 = add nuw nsw i64 %add16.i.11.2, %and15.i.11.2
  %add18.i.11.2 = add nuw nsw i64 %add17.i.11.2, %shr11.i.11.2
  %and19.i.11.2 = and i64 %add18.i.11.2, 4294967295
  %and21.i.11.2 = and i64 %add18.i.11.2, 30064771072
  %and22.i.11.2 = and i64 %mul5.i.11.2, -4294967296
  %add23.i.11.2 = add i64 %and21.i.11.2, %and22.i.11.2
  %xor2569.i.11.2 = or i64 %add23.i.11.2, %and19.i.11.2
  %add16.11.2 = add i64 %xor68.i.11.2, %add16.11.1
  %xor.i315.11.2 = xor i64 %add16.11.2, %shl.i.11.2
  %xor1.i316.11.2 = xor i64 %shl.i.11.2, %add16.11.1
  %or.i317.11.2 = or i64 %xor.i315.11.2, %xor1.i316.11.2
  %xor2.i318.11.2 = xor i64 %or.i317.11.2, %add16.11.2
  %shr.i319.11.2 = lshr i64 %xor2.i318.11.2, 63
  %add20.11.2 = add i64 %shr.i319.11.2, %xor2569.i.11.2
  %add21.11.2 = add i64 %add20.11.2, %add21.11.1
  %110 = xor i64 %add20.11.2, -9223372036854775808
  %xor2.i312.11.2 = and i64 %110, %add23.i.11.2
  %xor.i303.11.2 = xor i64 %add21.11.2, %add20.11.2
  %xor1.i304.11.2 = xor i64 %add20.11.2, %add21.11.1
  %or.i305.11.2 = or i64 %xor.i303.11.2, %xor1.i304.11.2
  %xor2.i306.11.2 = xor i64 %or.i305.11.2, %add21.11.2
  %shr.i313328.11.2 = or i64 %xor2.i306.11.2, %xor2.i312.11.2
  %or25327.11.2 = lshr i64 %shr.i313328.11.2, 63
  %add27.11.2 = add nsw i64 %or25327.11.2, %add27.11.1
  %arrayidx9.11.3 = getelementptr inbounds i64, i64* %mc, i64 3
  %111 = load i64, i64* %arrayidx9.11.3, align 8, !tbaa !3
  %112 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i.11.3 = and i64 %111, 4294967295
  %shr.i.11.3 = lshr i64 %111, 32
  %and1.i.11.3 = and i64 %112, 4294967295
  %shr2.i.11.3 = lshr i64 %112, 32
  %mul.i.11.3 = mul nuw i64 %and1.i.11.3, %and.i.11.3
  %mul3.i.11.3 = mul nuw i64 %shr2.i.11.3, %and.i.11.3
  %mul4.i.11.3 = mul nuw i64 %and1.i.11.3, %shr.i.11.3
  %mul5.i.11.3 = mul nuw i64 %shr2.i.11.3, %shr.i.11.3
  %and6.i.11.3 = and i64 %mul.i.11.3, 4294967295
  %shr7.i.11.3 = lshr i64 %mul.i.11.3, 32
  %and8.i.11.3 = and i64 %mul4.i.11.3, 4294967295
  %and9.i.11.3 = and i64 %mul3.i.11.3, 4294967295
  %add.i.11.3 = add nuw nsw i64 %shr7.i.11.3, %and8.i.11.3
  %add10.i.11.3 = add nuw nsw i64 %add.i.11.3, %and9.i.11.3
  %shr11.i.11.3 = lshr i64 %add10.i.11.3, 32
  %shl.i.11.3 = shl i64 %add10.i.11.3, 32
  %xor68.i.11.3 = or i64 %shl.i.11.3, %and6.i.11.3
  %shr13.i.11.3 = lshr i64 %mul4.i.11.3, 32
  %shr14.i.11.3 = lshr i64 %mul3.i.11.3, 32
  %and15.i.11.3 = and i64 %mul5.i.11.3, 4294967295
  %add16.i.11.3 = add nuw nsw i64 %shr13.i.11.3, %shr14.i.11.3
  %add17.i.11.3 = add nuw nsw i64 %add16.i.11.3, %and15.i.11.3
  %add18.i.11.3 = add nuw nsw i64 %add17.i.11.3, %shr11.i.11.3
  %and19.i.11.3 = and i64 %add18.i.11.3, 4294967295
  %and21.i.11.3 = and i64 %add18.i.11.3, 30064771072
  %and22.i.11.3 = and i64 %mul5.i.11.3, -4294967296
  %add23.i.11.3 = add i64 %and21.i.11.3, %and22.i.11.3
  %xor2569.i.11.3 = or i64 %add23.i.11.3, %and19.i.11.3
  %add16.11.3 = add i64 %xor68.i.11.3, %add16.11.2
  %xor.i315.11.3 = xor i64 %add16.11.3, %shl.i.11.3
  %xor1.i316.11.3 = xor i64 %shl.i.11.3, %add16.11.2
  %or.i317.11.3 = or i64 %xor.i315.11.3, %xor1.i316.11.3
  %xor2.i318.11.3 = xor i64 %or.i317.11.3, %add16.11.3
  %shr.i319.11.3 = lshr i64 %xor2.i318.11.3, 63
  %add20.11.3 = add i64 %shr.i319.11.3, %xor2569.i.11.3
  %add21.11.3 = add i64 %add20.11.3, %add21.11.2
  %113 = xor i64 %add20.11.3, -9223372036854775808
  %xor2.i312.11.3 = and i64 %113, %add23.i.11.3
  %xor.i303.11.3 = xor i64 %add21.11.3, %add20.11.3
  %xor1.i304.11.3 = xor i64 %add20.11.3, %add21.11.2
  %or.i305.11.3 = or i64 %xor.i303.11.3, %xor1.i304.11.3
  %xor2.i306.11.3 = xor i64 %or.i305.11.3, %add21.11.3
  %shr.i313328.11.3 = or i64 %xor2.i306.11.3, %xor2.i312.11.3
  %or25327.11.3 = lshr i64 %shr.i313328.11.3, 63
  %add27.11.3 = add nsw i64 %or25327.11.3, %add27.11.2
  %arrayidx9.11.4 = getelementptr inbounds i64, i64* %mc, i64 4
  %114 = load i64, i64* %arrayidx9.11.4, align 8, !tbaa !3
  %115 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i.11.4 = and i64 %114, 4294967295
  %shr.i.11.4 = lshr i64 %114, 32
  %and1.i.11.4 = and i64 %115, 4294967295
  %shr2.i.11.4 = lshr i64 %115, 32
  %mul.i.11.4 = mul nuw i64 %and1.i.11.4, %and.i.11.4
  %mul3.i.11.4 = mul nuw i64 %shr2.i.11.4, %and.i.11.4
  %mul4.i.11.4 = mul nuw i64 %and1.i.11.4, %shr.i.11.4
  %mul5.i.11.4 = mul nuw i64 %shr2.i.11.4, %shr.i.11.4
  %and6.i.11.4 = and i64 %mul.i.11.4, 4294967295
  %shr7.i.11.4 = lshr i64 %mul.i.11.4, 32
  %and8.i.11.4 = and i64 %mul4.i.11.4, 4294967295
  %and9.i.11.4 = and i64 %mul3.i.11.4, 4294967295
  %add.i.11.4 = add nuw nsw i64 %shr7.i.11.4, %and8.i.11.4
  %add10.i.11.4 = add nuw nsw i64 %add.i.11.4, %and9.i.11.4
  %shr11.i.11.4 = lshr i64 %add10.i.11.4, 32
  %shl.i.11.4 = shl i64 %add10.i.11.4, 32
  %xor68.i.11.4 = or i64 %shl.i.11.4, %and6.i.11.4
  %shr13.i.11.4 = lshr i64 %mul4.i.11.4, 32
  %shr14.i.11.4 = lshr i64 %mul3.i.11.4, 32
  %and15.i.11.4 = and i64 %mul5.i.11.4, 4294967295
  %add16.i.11.4 = add nuw nsw i64 %shr13.i.11.4, %shr14.i.11.4
  %add17.i.11.4 = add nuw nsw i64 %add16.i.11.4, %and15.i.11.4
  %add18.i.11.4 = add nuw nsw i64 %add17.i.11.4, %shr11.i.11.4
  %and19.i.11.4 = and i64 %add18.i.11.4, 4294967295
  %and21.i.11.4 = and i64 %add18.i.11.4, 30064771072
  %and22.i.11.4 = and i64 %mul5.i.11.4, -4294967296
  %add23.i.11.4 = add i64 %and21.i.11.4, %and22.i.11.4
  %xor2569.i.11.4 = or i64 %add23.i.11.4, %and19.i.11.4
  %add16.11.4 = add i64 %xor68.i.11.4, %add16.11.3
  %xor.i315.11.4 = xor i64 %add16.11.4, %shl.i.11.4
  %xor1.i316.11.4 = xor i64 %shl.i.11.4, %add16.11.3
  %or.i317.11.4 = or i64 %xor.i315.11.4, %xor1.i316.11.4
  %xor2.i318.11.4 = xor i64 %or.i317.11.4, %add16.11.4
  %shr.i319.11.4 = lshr i64 %xor2.i318.11.4, 63
  %add20.11.4 = add i64 %shr.i319.11.4, %xor2569.i.11.4
  %add21.11.4 = add i64 %add20.11.4, %add21.11.3
  %116 = xor i64 %add20.11.4, -9223372036854775808
  %xor2.i312.11.4 = and i64 %116, %add23.i.11.4
  %xor.i303.11.4 = xor i64 %add21.11.4, %add20.11.4
  %xor1.i304.11.4 = xor i64 %add20.11.4, %add21.11.3
  %or.i305.11.4 = or i64 %xor.i303.11.4, %xor1.i304.11.4
  %xor2.i306.11.4 = xor i64 %or.i305.11.4, %add21.11.4
  %shr.i313328.11.4 = or i64 %xor2.i306.11.4, %xor2.i312.11.4
  %or25327.11.4 = lshr i64 %shr.i313328.11.4, 63
  %add27.11.4 = add i64 %or25327.11.4, %add27.11.3
  %arrayidx9.11.5 = getelementptr inbounds i64, i64* %mc, i64 5
  %117 = load i64, i64* %arrayidx9.11.5, align 8, !tbaa !3
  %118 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i.11.5 = and i64 %117, 4294967295
  %shr.i.11.5 = lshr i64 %117, 32
  %and1.i.11.5 = and i64 %118, 4294967295
  %shr2.i.11.5 = lshr i64 %118, 32
  %mul.i.11.5 = mul nuw i64 %and1.i.11.5, %and.i.11.5
  %mul3.i.11.5 = mul nuw i64 %shr2.i.11.5, %and.i.11.5
  %mul4.i.11.5 = mul nuw i64 %and1.i.11.5, %shr.i.11.5
  %mul5.i.11.5 = mul nuw i64 %shr2.i.11.5, %shr.i.11.5
  %and6.i.11.5 = and i64 %mul.i.11.5, 4294967295
  %shr7.i.11.5 = lshr i64 %mul.i.11.5, 32
  %and8.i.11.5 = and i64 %mul4.i.11.5, 4294967295
  %and9.i.11.5 = and i64 %mul3.i.11.5, 4294967295
  %add.i.11.5 = add nuw nsw i64 %shr7.i.11.5, %and8.i.11.5
  %add10.i.11.5 = add nuw nsw i64 %add.i.11.5, %and9.i.11.5
  %shr11.i.11.5 = lshr i64 %add10.i.11.5, 32
  %shl.i.11.5 = shl i64 %add10.i.11.5, 32
  %xor68.i.11.5 = or i64 %shl.i.11.5, %and6.i.11.5
  %shr13.i.11.5 = lshr i64 %mul4.i.11.5, 32
  %shr14.i.11.5 = lshr i64 %mul3.i.11.5, 32
  %and15.i.11.5 = and i64 %mul5.i.11.5, 4294967295
  %add16.i.11.5 = add nuw nsw i64 %shr13.i.11.5, %shr14.i.11.5
  %add17.i.11.5 = add nuw nsw i64 %add16.i.11.5, %and15.i.11.5
  %add18.i.11.5 = add nuw nsw i64 %add17.i.11.5, %shr11.i.11.5
  %and19.i.11.5 = and i64 %add18.i.11.5, 4294967295
  %and21.i.11.5 = and i64 %add18.i.11.5, 30064771072
  %and22.i.11.5 = and i64 %mul5.i.11.5, -4294967296
  %add23.i.11.5 = add i64 %and21.i.11.5, %and22.i.11.5
  %xor2569.i.11.5 = or i64 %add23.i.11.5, %and19.i.11.5
  %add16.11.5 = add i64 %xor68.i.11.5, %add16.11.4
  %xor.i315.11.5 = xor i64 %add16.11.5, %shl.i.11.5
  %xor1.i316.11.5 = xor i64 %shl.i.11.5, %add16.11.4
  %or.i317.11.5 = or i64 %xor.i315.11.5, %xor1.i316.11.5
  %xor2.i318.11.5 = xor i64 %or.i317.11.5, %add16.11.5
  %shr.i319.11.5 = lshr i64 %xor2.i318.11.5, 63
  %add20.11.5 = add i64 %shr.i319.11.5, %xor2569.i.11.5
  %add21.11.5 = add i64 %add20.11.5, %add21.11.4
  %119 = xor i64 %add20.11.5, -9223372036854775808
  %xor2.i312.11.5 = and i64 %119, %add23.i.11.5
  %xor.i303.11.5 = xor i64 %add21.11.5, %add20.11.5
  %xor1.i304.11.5 = xor i64 %add20.11.5, %add21.11.4
  %or.i305.11.5 = or i64 %xor.i303.11.5, %xor1.i304.11.5
  %xor2.i306.11.5 = xor i64 %or.i305.11.5, %add21.11.5
  %shr.i313328.11.5 = or i64 %xor2.i306.11.5, %xor2.i312.11.5
  %or25327.11.5 = lshr i64 %shr.i313328.11.5, 63
  %add27.11.5 = add i64 %or25327.11.5, %add27.11.4
  %arrayidx9.11.6 = getelementptr inbounds i64, i64* %mc, i64 6
  %120 = load i64, i64* %arrayidx9.11.6, align 8, !tbaa !3
  %121 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i.11.6 = and i64 %120, 4294967295
  %shr.i.11.6 = lshr i64 %120, 32
  %and1.i.11.6 = and i64 %121, 4294967295
  %shr2.i.11.6 = lshr i64 %121, 32
  %mul.i.11.6 = mul nuw i64 %and1.i.11.6, %and.i.11.6
  %mul3.i.11.6 = mul nuw i64 %shr2.i.11.6, %and.i.11.6
  %mul4.i.11.6 = mul nuw i64 %and1.i.11.6, %shr.i.11.6
  %mul5.i.11.6 = mul nuw i64 %shr2.i.11.6, %shr.i.11.6
  %and6.i.11.6 = and i64 %mul.i.11.6, 4294967295
  %shr7.i.11.6 = lshr i64 %mul.i.11.6, 32
  %and8.i.11.6 = and i64 %mul4.i.11.6, 4294967295
  %and9.i.11.6 = and i64 %mul3.i.11.6, 4294967295
  %add.i.11.6 = add nuw nsw i64 %shr7.i.11.6, %and8.i.11.6
  %add10.i.11.6 = add nuw nsw i64 %add.i.11.6, %and9.i.11.6
  %shr11.i.11.6 = lshr i64 %add10.i.11.6, 32
  %shl.i.11.6 = shl i64 %add10.i.11.6, 32
  %xor68.i.11.6 = or i64 %shl.i.11.6, %and6.i.11.6
  %shr13.i.11.6 = lshr i64 %mul4.i.11.6, 32
  %shr14.i.11.6 = lshr i64 %mul3.i.11.6, 32
  %and15.i.11.6 = and i64 %mul5.i.11.6, 4294967295
  %add16.i.11.6 = add nuw nsw i64 %shr13.i.11.6, %shr14.i.11.6
  %add17.i.11.6 = add nuw nsw i64 %add16.i.11.6, %and15.i.11.6
  %add18.i.11.6 = add nuw nsw i64 %add17.i.11.6, %shr11.i.11.6
  %and19.i.11.6 = and i64 %add18.i.11.6, 4294967295
  %and21.i.11.6 = and i64 %add18.i.11.6, 30064771072
  %and22.i.11.6 = and i64 %mul5.i.11.6, -4294967296
  %add23.i.11.6 = add i64 %and21.i.11.6, %and22.i.11.6
  %xor2569.i.11.6 = or i64 %add23.i.11.6, %and19.i.11.6
  %add16.11.6 = add i64 %xor68.i.11.6, %add16.11.5
  %xor.i315.11.6 = xor i64 %add16.11.6, %shl.i.11.6
  %xor1.i316.11.6 = xor i64 %shl.i.11.6, %add16.11.5
  %or.i317.11.6 = or i64 %xor.i315.11.6, %xor1.i316.11.6
  %xor2.i318.11.6 = xor i64 %or.i317.11.6, %add16.11.6
  %shr.i319.11.6 = lshr i64 %xor2.i318.11.6, 63
  %add20.11.6 = add i64 %shr.i319.11.6, %xor2569.i.11.6
  %add21.11.6 = add i64 %add20.11.6, %add21.11.5
  %122 = xor i64 %add20.11.6, -9223372036854775808
  %xor2.i312.11.6 = and i64 %122, %add23.i.11.6
  %xor.i303.11.6 = xor i64 %add21.11.6, %add20.11.6
  %xor1.i304.11.6 = xor i64 %add20.11.6, %add21.11.5
  %or.i305.11.6 = or i64 %xor.i303.11.6, %xor1.i304.11.6
  %xor2.i306.11.6 = xor i64 %or.i305.11.6, %add21.11.6
  %shr.i313328.11.6 = or i64 %xor2.i306.11.6, %xor2.i312.11.6
  %or25327.11.6 = lshr i64 %shr.i313328.11.6, 63
  %add27.11.6 = add i64 %or25327.11.6, %add27.11.5
  %arrayidx34.11 = getelementptr inbounds i64, i64* %ma, i64 11
  %123 = load i64, i64* %arrayidx34.11, align 8, !tbaa !3
  %add35.11 = add i64 %123, %add16.11.6
  %xor.i297.11 = xor i64 %add35.11, %add16.11.6
  %xor1.i298.11 = xor i64 %123, %add16.11.6
  %or.i299.11 = or i64 %xor.i297.11, %xor1.i298.11
  %xor2.i300.11 = xor i64 %or.i299.11, %add35.11
  %shr.i301.11 = lshr i64 %xor2.i300.11, 63
  %add41.11 = add i64 %shr.i301.11, %add21.11.6
  %arrayidx50.11 = getelementptr inbounds i64, i64* %mc, i64 11
  store i64 %add35.11, i64* %arrayidx50.11, align 8, !tbaa !3
  %124 = xor i64 %add41.11, -9223372036854775808
  %xor2.i294.11 = and i64 %124, %add21.11.6
  %shr.i295.11 = lshr i64 %xor2.i294.11, 63
  %add48.11 = add i64 %shr.i295.11, %add27.11.6
  %arrayidx74 = getelementptr inbounds i64, i64* %mc, i64 1
  %125 = load i64, i64* %arrayidx74, align 8, !tbaa !3
  %126 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262 = and i64 %125, 4294967295
  %shr.i263 = lshr i64 %125, 32
  %and1.i264 = and i64 %126, 4294967295
  %shr2.i265 = lshr i64 %126, 32
  %mul.i266 = mul nuw i64 %and1.i264, %and.i262
  %mul3.i267 = mul nuw i64 %shr2.i265, %and.i262
  %mul4.i268 = mul nuw i64 %and1.i264, %shr.i263
  %mul5.i269 = mul nuw i64 %shr2.i265, %shr.i263
  %and6.i270 = and i64 %mul.i266, 4294967295
  %shr7.i271 = lshr i64 %mul.i266, 32
  %and8.i272 = and i64 %mul4.i268, 4294967295
  %and9.i273 = and i64 %mul3.i267, 4294967295
  %add.i274 = add nuw nsw i64 %shr7.i271, %and8.i272
  %add10.i275 = add nuw nsw i64 %add.i274, %and9.i273
  %shr11.i276 = lshr i64 %add10.i275, 32
  %shl.i277 = shl i64 %add10.i275, 32
  %xor68.i278 = or i64 %shl.i277, %and6.i270
  %shr13.i279 = lshr i64 %mul4.i268, 32
  %shr14.i280 = lshr i64 %mul3.i267, 32
  %and15.i281 = and i64 %mul5.i269, 4294967295
  %add16.i282 = add nuw nsw i64 %shr13.i279, %shr14.i280
  %add17.i283 = add nuw nsw i64 %add16.i282, %and15.i281
  %add18.i284 = add nuw nsw i64 %add17.i283, %shr11.i276
  %and19.i285 = and i64 %add18.i284, 4294967295
  %and21.i287 = and i64 %add18.i284, 30064771072
  %and22.i288 = and i64 %mul5.i269, -4294967296
  %add23.i289 = add i64 %and21.i287, %and22.i288
  %xor2569.i290 = or i64 %add23.i289, %and19.i285
  %add82 = add i64 %xor68.i278, %add41.11
  %xor.i256 = xor i64 %add82, %shl.i277
  %xor1.i257 = xor i64 %shl.i277, %add41.11
  %or.i258 = or i64 %xor.i256, %xor1.i257
  %xor2.i259 = xor i64 %or.i258, %add82
  %shr.i260 = lshr i64 %xor2.i259, 63
  %add89 = add i64 %shr.i260, %xor2569.i290
  %add90 = add i64 %add89, %add48.11
  %127 = xor i64 %add89, -9223372036854775808
  %xor2.i253 = and i64 %127, %add23.i289
  %xor.i244 = xor i64 %add90, %add89
  %xor1.i245 = xor i64 %add89, %add48.11
  %or.i246 = or i64 %xor.i244, %xor1.i245
  %xor2.i247 = xor i64 %or.i246, %add90
  %shr.i254326 = or i64 %xor2.i247, %xor2.i253
  %or94325 = lshr i64 %shr.i254326, 63
  %arrayidx74.1372 = getelementptr inbounds i64, i64* %mc, i64 2
  %128 = load i64, i64* %arrayidx74.1372, align 8, !tbaa !3
  %129 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.1373 = and i64 %128, 4294967295
  %shr.i263.1374 = lshr i64 %128, 32
  %and1.i264.1375 = and i64 %129, 4294967295
  %shr2.i265.1376 = lshr i64 %129, 32
  %mul.i266.1377 = mul nuw i64 %and1.i264.1375, %and.i262.1373
  %mul3.i267.1378 = mul nuw i64 %shr2.i265.1376, %and.i262.1373
  %mul4.i268.1379 = mul nuw i64 %and1.i264.1375, %shr.i263.1374
  %mul5.i269.1380 = mul nuw i64 %shr2.i265.1376, %shr.i263.1374
  %and6.i270.1381 = and i64 %mul.i266.1377, 4294967295
  %shr7.i271.1382 = lshr i64 %mul.i266.1377, 32
  %and8.i272.1383 = and i64 %mul4.i268.1379, 4294967295
  %and9.i273.1384 = and i64 %mul3.i267.1378, 4294967295
  %add.i274.1385 = add nuw nsw i64 %shr7.i271.1382, %and8.i272.1383
  %add10.i275.1386 = add nuw nsw i64 %add.i274.1385, %and9.i273.1384
  %shr11.i276.1387 = lshr i64 %add10.i275.1386, 32
  %shl.i277.1388 = shl i64 %add10.i275.1386, 32
  %xor68.i278.1389 = or i64 %shl.i277.1388, %and6.i270.1381
  %shr13.i279.1390 = lshr i64 %mul4.i268.1379, 32
  %shr14.i280.1391 = lshr i64 %mul3.i267.1378, 32
  %and15.i281.1392 = and i64 %mul5.i269.1380, 4294967295
  %add16.i282.1393 = add nuw nsw i64 %shr13.i279.1390, %shr14.i280.1391
  %add17.i283.1394 = add nuw nsw i64 %add16.i282.1393, %and15.i281.1392
  %add18.i284.1395 = add nuw nsw i64 %add17.i283.1394, %shr11.i276.1387
  %and19.i285.1396 = and i64 %add18.i284.1395, 4294967295
  %and21.i287.1397 = and i64 %add18.i284.1395, 30064771072
  %and22.i288.1398 = and i64 %mul5.i269.1380, -4294967296
  %add23.i289.1399 = add i64 %and21.i287.1397, %and22.i288.1398
  %xor2569.i290.1400 = or i64 %add23.i289.1399, %and19.i285.1396
  %add82.1401 = add i64 %xor68.i278.1389, %add82
  %xor.i256.1402 = xor i64 %add82.1401, %shl.i277.1388
  %xor1.i257.1403 = xor i64 %shl.i277.1388, %add82
  %or.i258.1404 = or i64 %xor.i256.1402, %xor1.i257.1403
  %xor2.i259.1405 = xor i64 %or.i258.1404, %add82.1401
  %shr.i260.1406 = lshr i64 %xor2.i259.1405, 63
  %add89.1407 = add i64 %shr.i260.1406, %xor2569.i290.1400
  %add90.1408 = add i64 %add89.1407, %add90
  %130 = xor i64 %add89.1407, -9223372036854775808
  %xor2.i253.1409 = and i64 %130, %add23.i289.1399
  %xor.i244.1410 = xor i64 %add90.1408, %add89.1407
  %xor1.i245.1411 = xor i64 %add89.1407, %add90
  %or.i246.1412 = or i64 %xor.i244.1410, %xor1.i245.1411
  %xor2.i247.1413 = xor i64 %or.i246.1412, %add90.1408
  %shr.i254326.1414 = or i64 %xor2.i247.1413, %xor2.i253.1409
  %or94325.1415 = lshr i64 %shr.i254326.1414, 63
  %add96.1 = add nuw nsw i64 %or94325.1415, %or94325
  %arrayidx74.2422 = getelementptr inbounds i64, i64* %mc, i64 3
  %131 = load i64, i64* %arrayidx74.2422, align 8, !tbaa !3
  %132 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.2423 = and i64 %131, 4294967295
  %shr.i263.2424 = lshr i64 %131, 32
  %and1.i264.2425 = and i64 %132, 4294967295
  %shr2.i265.2426 = lshr i64 %132, 32
  %mul.i266.2427 = mul nuw i64 %and1.i264.2425, %and.i262.2423
  %mul3.i267.2428 = mul nuw i64 %shr2.i265.2426, %and.i262.2423
  %mul4.i268.2429 = mul nuw i64 %and1.i264.2425, %shr.i263.2424
  %mul5.i269.2430 = mul nuw i64 %shr2.i265.2426, %shr.i263.2424
  %and6.i270.2431 = and i64 %mul.i266.2427, 4294967295
  %shr7.i271.2432 = lshr i64 %mul.i266.2427, 32
  %and8.i272.2433 = and i64 %mul4.i268.2429, 4294967295
  %and9.i273.2434 = and i64 %mul3.i267.2428, 4294967295
  %add.i274.2435 = add nuw nsw i64 %shr7.i271.2432, %and8.i272.2433
  %add10.i275.2436 = add nuw nsw i64 %add.i274.2435, %and9.i273.2434
  %shr11.i276.2437 = lshr i64 %add10.i275.2436, 32
  %shl.i277.2438 = shl i64 %add10.i275.2436, 32
  %xor68.i278.2439 = or i64 %shl.i277.2438, %and6.i270.2431
  %shr13.i279.2440 = lshr i64 %mul4.i268.2429, 32
  %shr14.i280.2441 = lshr i64 %mul3.i267.2428, 32
  %and15.i281.2442 = and i64 %mul5.i269.2430, 4294967295
  %add16.i282.2443 = add nuw nsw i64 %shr13.i279.2440, %shr14.i280.2441
  %add17.i283.2444 = add nuw nsw i64 %add16.i282.2443, %and15.i281.2442
  %add18.i284.2445 = add nuw nsw i64 %add17.i283.2444, %shr11.i276.2437
  %and19.i285.2446 = and i64 %add18.i284.2445, 4294967295
  %and21.i287.2447 = and i64 %add18.i284.2445, 30064771072
  %and22.i288.2448 = and i64 %mul5.i269.2430, -4294967296
  %add23.i289.2449 = add i64 %and21.i287.2447, %and22.i288.2448
  %xor2569.i290.2450 = or i64 %add23.i289.2449, %and19.i285.2446
  %add82.2451 = add i64 %xor68.i278.2439, %add82.1401
  %xor.i256.2452 = xor i64 %add82.2451, %shl.i277.2438
  %xor1.i257.2453 = xor i64 %shl.i277.2438, %add82.1401
  %or.i258.2454 = or i64 %xor.i256.2452, %xor1.i257.2453
  %xor2.i259.2455 = xor i64 %or.i258.2454, %add82.2451
  %shr.i260.2456 = lshr i64 %xor2.i259.2455, 63
  %add89.2457 = add i64 %shr.i260.2456, %xor2569.i290.2450
  %add90.2458 = add i64 %add89.2457, %add90.1408
  %133 = xor i64 %add89.2457, -9223372036854775808
  %xor2.i253.2459 = and i64 %133, %add23.i289.2449
  %xor.i244.2460 = xor i64 %add90.2458, %add89.2457
  %xor1.i245.2461 = xor i64 %add89.2457, %add90.1408
  %or.i246.2462 = or i64 %xor.i244.2460, %xor1.i245.2461
  %xor2.i247.2463 = xor i64 %or.i246.2462, %add90.2458
  %shr.i254326.2464 = or i64 %xor2.i247.2463, %xor2.i253.2459
  %or94325.2465 = lshr i64 %shr.i254326.2464, 63
  %add96.2 = add nsw i64 %or94325.2465, %add96.1
  %arrayidx74.3472 = getelementptr inbounds i64, i64* %mc, i64 4
  %134 = load i64, i64* %arrayidx74.3472, align 8, !tbaa !3
  %135 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.3473 = and i64 %134, 4294967295
  %shr.i263.3474 = lshr i64 %134, 32
  %and1.i264.3475 = and i64 %135, 4294967295
  %shr2.i265.3476 = lshr i64 %135, 32
  %mul.i266.3477 = mul nuw i64 %and1.i264.3475, %and.i262.3473
  %mul3.i267.3478 = mul nuw i64 %shr2.i265.3476, %and.i262.3473
  %mul4.i268.3479 = mul nuw i64 %and1.i264.3475, %shr.i263.3474
  %mul5.i269.3480 = mul nuw i64 %shr2.i265.3476, %shr.i263.3474
  %and6.i270.3481 = and i64 %mul.i266.3477, 4294967295
  %shr7.i271.3482 = lshr i64 %mul.i266.3477, 32
  %and8.i272.3483 = and i64 %mul4.i268.3479, 4294967295
  %and9.i273.3484 = and i64 %mul3.i267.3478, 4294967295
  %add.i274.3485 = add nuw nsw i64 %shr7.i271.3482, %and8.i272.3483
  %add10.i275.3486 = add nuw nsw i64 %add.i274.3485, %and9.i273.3484
  %shr11.i276.3487 = lshr i64 %add10.i275.3486, 32
  %shl.i277.3488 = shl i64 %add10.i275.3486, 32
  %xor68.i278.3489 = or i64 %shl.i277.3488, %and6.i270.3481
  %shr13.i279.3490 = lshr i64 %mul4.i268.3479, 32
  %shr14.i280.3491 = lshr i64 %mul3.i267.3478, 32
  %and15.i281.3492 = and i64 %mul5.i269.3480, 4294967295
  %add16.i282.3493 = add nuw nsw i64 %shr13.i279.3490, %shr14.i280.3491
  %add17.i283.3494 = add nuw nsw i64 %add16.i282.3493, %and15.i281.3492
  %add18.i284.3495 = add nuw nsw i64 %add17.i283.3494, %shr11.i276.3487
  %and19.i285.3496 = and i64 %add18.i284.3495, 4294967295
  %and21.i287.3497 = and i64 %add18.i284.3495, 30064771072
  %and22.i288.3498 = and i64 %mul5.i269.3480, -4294967296
  %add23.i289.3499 = add i64 %and21.i287.3497, %and22.i288.3498
  %xor2569.i290.3500 = or i64 %add23.i289.3499, %and19.i285.3496
  %add82.3501 = add i64 %xor68.i278.3489, %add82.2451
  %xor.i256.3502 = xor i64 %add82.3501, %shl.i277.3488
  %xor1.i257.3503 = xor i64 %shl.i277.3488, %add82.2451
  %or.i258.3504 = or i64 %xor.i256.3502, %xor1.i257.3503
  %xor2.i259.3505 = xor i64 %or.i258.3504, %add82.3501
  %shr.i260.3506 = lshr i64 %xor2.i259.3505, 63
  %add89.3507 = add i64 %shr.i260.3506, %xor2569.i290.3500
  %add90.3508 = add i64 %add89.3507, %add90.2458
  %136 = xor i64 %add89.3507, -9223372036854775808
  %xor2.i253.3509 = and i64 %136, %add23.i289.3499
  %xor.i244.3510 = xor i64 %add90.3508, %add89.3507
  %xor1.i245.3511 = xor i64 %add89.3507, %add90.2458
  %or.i246.3512 = or i64 %xor.i244.3510, %xor1.i245.3511
  %xor2.i247.3513 = xor i64 %or.i246.3512, %add90.3508
  %shr.i254326.3514 = or i64 %xor2.i247.3513, %xor2.i253.3509
  %or94325.3515 = lshr i64 %shr.i254326.3514, 63
  %add96.3 = add nsw i64 %or94325.3515, %add96.2
  %arrayidx74.4522 = getelementptr inbounds i64, i64* %mc, i64 5
  %137 = load i64, i64* %arrayidx74.4522, align 8, !tbaa !3
  %138 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.4523 = and i64 %137, 4294967295
  %shr.i263.4524 = lshr i64 %137, 32
  %and1.i264.4525 = and i64 %138, 4294967295
  %shr2.i265.4526 = lshr i64 %138, 32
  %mul.i266.4527 = mul nuw i64 %and1.i264.4525, %and.i262.4523
  %mul3.i267.4528 = mul nuw i64 %shr2.i265.4526, %and.i262.4523
  %mul4.i268.4529 = mul nuw i64 %and1.i264.4525, %shr.i263.4524
  %mul5.i269.4530 = mul nuw i64 %shr2.i265.4526, %shr.i263.4524
  %and6.i270.4531 = and i64 %mul.i266.4527, 4294967295
  %shr7.i271.4532 = lshr i64 %mul.i266.4527, 32
  %and8.i272.4533 = and i64 %mul4.i268.4529, 4294967295
  %and9.i273.4534 = and i64 %mul3.i267.4528, 4294967295
  %add.i274.4535 = add nuw nsw i64 %shr7.i271.4532, %and8.i272.4533
  %add10.i275.4536 = add nuw nsw i64 %add.i274.4535, %and9.i273.4534
  %shr11.i276.4537 = lshr i64 %add10.i275.4536, 32
  %shl.i277.4538 = shl i64 %add10.i275.4536, 32
  %xor68.i278.4539 = or i64 %shl.i277.4538, %and6.i270.4531
  %shr13.i279.4540 = lshr i64 %mul4.i268.4529, 32
  %shr14.i280.4541 = lshr i64 %mul3.i267.4528, 32
  %and15.i281.4542 = and i64 %mul5.i269.4530, 4294967295
  %add16.i282.4543 = add nuw nsw i64 %shr13.i279.4540, %shr14.i280.4541
  %add17.i283.4544 = add nuw nsw i64 %add16.i282.4543, %and15.i281.4542
  %add18.i284.4545 = add nuw nsw i64 %add17.i283.4544, %shr11.i276.4537
  %and19.i285.4546 = and i64 %add18.i284.4545, 4294967295
  %and21.i287.4547 = and i64 %add18.i284.4545, 30064771072
  %and22.i288.4548 = and i64 %mul5.i269.4530, -4294967296
  %add23.i289.4549 = add i64 %and21.i287.4547, %and22.i288.4548
  %xor2569.i290.4550 = or i64 %add23.i289.4549, %and19.i285.4546
  %add82.4551 = add i64 %xor68.i278.4539, %add82.3501
  %xor.i256.4552 = xor i64 %add82.4551, %shl.i277.4538
  %xor1.i257.4553 = xor i64 %shl.i277.4538, %add82.3501
  %or.i258.4554 = or i64 %xor.i256.4552, %xor1.i257.4553
  %xor2.i259.4555 = xor i64 %or.i258.4554, %add82.4551
  %shr.i260.4556 = lshr i64 %xor2.i259.4555, 63
  %add89.4557 = add i64 %shr.i260.4556, %xor2569.i290.4550
  %add90.4558 = add i64 %add89.4557, %add90.3508
  %139 = xor i64 %add89.4557, -9223372036854775808
  %xor2.i253.4559 = and i64 %139, %add23.i289.4549
  %xor.i244.4560 = xor i64 %add90.4558, %add89.4557
  %xor1.i245.4561 = xor i64 %add89.4557, %add90.3508
  %or.i246.4562 = or i64 %xor.i244.4560, %xor1.i245.4561
  %xor2.i247.4563 = xor i64 %or.i246.4562, %add90.4558
  %shr.i254326.4564 = or i64 %xor2.i247.4563, %xor2.i253.4559
  %or94325.4565 = lshr i64 %shr.i254326.4564, 63
  %add96.4 = add i64 %or94325.4565, %add96.3
  %arrayidx74.5572 = getelementptr inbounds i64, i64* %mc, i64 6
  %140 = load i64, i64* %arrayidx74.5572, align 8, !tbaa !3
  %141 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.5573 = and i64 %140, 4294967295
  %shr.i263.5574 = lshr i64 %140, 32
  %and1.i264.5575 = and i64 %141, 4294967295
  %shr2.i265.5576 = lshr i64 %141, 32
  %mul.i266.5577 = mul nuw i64 %and1.i264.5575, %and.i262.5573
  %mul3.i267.5578 = mul nuw i64 %shr2.i265.5576, %and.i262.5573
  %mul4.i268.5579 = mul nuw i64 %and1.i264.5575, %shr.i263.5574
  %mul5.i269.5580 = mul nuw i64 %shr2.i265.5576, %shr.i263.5574
  %and6.i270.5581 = and i64 %mul.i266.5577, 4294967295
  %shr7.i271.5582 = lshr i64 %mul.i266.5577, 32
  %and8.i272.5583 = and i64 %mul4.i268.5579, 4294967295
  %and9.i273.5584 = and i64 %mul3.i267.5578, 4294967295
  %add.i274.5585 = add nuw nsw i64 %shr7.i271.5582, %and8.i272.5583
  %add10.i275.5586 = add nuw nsw i64 %add.i274.5585, %and9.i273.5584
  %shr11.i276.5587 = lshr i64 %add10.i275.5586, 32
  %shl.i277.5588 = shl i64 %add10.i275.5586, 32
  %xor68.i278.5589 = or i64 %shl.i277.5588, %and6.i270.5581
  %shr13.i279.5590 = lshr i64 %mul4.i268.5579, 32
  %shr14.i280.5591 = lshr i64 %mul3.i267.5578, 32
  %and15.i281.5592 = and i64 %mul5.i269.5580, 4294967295
  %add16.i282.5593 = add nuw nsw i64 %shr13.i279.5590, %shr14.i280.5591
  %add17.i283.5594 = add nuw nsw i64 %add16.i282.5593, %and15.i281.5592
  %add18.i284.5595 = add nuw nsw i64 %add17.i283.5594, %shr11.i276.5587
  %and19.i285.5596 = and i64 %add18.i284.5595, 4294967295
  %and21.i287.5597 = and i64 %add18.i284.5595, 30064771072
  %and22.i288.5598 = and i64 %mul5.i269.5580, -4294967296
  %add23.i289.5599 = add i64 %and21.i287.5597, %and22.i288.5598
  %xor2569.i290.5600 = or i64 %add23.i289.5599, %and19.i285.5596
  %add82.5601 = add i64 %xor68.i278.5589, %add82.4551
  %xor.i256.5602 = xor i64 %add82.5601, %shl.i277.5588
  %xor1.i257.5603 = xor i64 %shl.i277.5588, %add82.4551
  %or.i258.5604 = or i64 %xor.i256.5602, %xor1.i257.5603
  %xor2.i259.5605 = xor i64 %or.i258.5604, %add82.5601
  %shr.i260.5606 = lshr i64 %xor2.i259.5605, 63
  %add89.5607 = add i64 %shr.i260.5606, %xor2569.i290.5600
  %add90.5608 = add i64 %add89.5607, %add90.4558
  %142 = xor i64 %add89.5607, -9223372036854775808
  %xor2.i253.5609 = and i64 %142, %add23.i289.5599
  %xor.i244.5610 = xor i64 %add90.5608, %add89.5607
  %xor1.i245.5611 = xor i64 %add89.5607, %add90.4558
  %or.i246.5612 = or i64 %xor.i244.5610, %xor1.i245.5611
  %xor2.i247.5613 = xor i64 %or.i246.5612, %add90.5608
  %shr.i254326.5614 = or i64 %xor2.i247.5613, %xor2.i253.5609
  %or94325.5615 = lshr i64 %shr.i254326.5614, 63
  %add96.5 = add i64 %or94325.5615, %add96.4
  %arrayidx74.6622 = getelementptr inbounds i64, i64* %mc, i64 7
  %143 = load i64, i64* %arrayidx74.6622, align 8, !tbaa !3
  %144 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.6623 = and i64 %143, 4294967295
  %shr.i263.6624 = lshr i64 %143, 32
  %and1.i264.6625 = and i64 %144, 4294967295
  %shr2.i265.6626 = lshr i64 %144, 32
  %mul.i266.6627 = mul nuw i64 %and1.i264.6625, %and.i262.6623
  %mul3.i267.6628 = mul nuw i64 %shr2.i265.6626, %and.i262.6623
  %mul4.i268.6629 = mul nuw i64 %and1.i264.6625, %shr.i263.6624
  %mul5.i269.6630 = mul nuw i64 %shr2.i265.6626, %shr.i263.6624
  %and6.i270.6631 = and i64 %mul.i266.6627, 4294967295
  %shr7.i271.6632 = lshr i64 %mul.i266.6627, 32
  %and8.i272.6633 = and i64 %mul4.i268.6629, 4294967295
  %and9.i273.6634 = and i64 %mul3.i267.6628, 4294967295
  %add.i274.6635 = add nuw nsw i64 %shr7.i271.6632, %and8.i272.6633
  %add10.i275.6636 = add nuw nsw i64 %add.i274.6635, %and9.i273.6634
  %shr11.i276.6637 = lshr i64 %add10.i275.6636, 32
  %shl.i277.6638 = shl i64 %add10.i275.6636, 32
  %xor68.i278.6639 = or i64 %shl.i277.6638, %and6.i270.6631
  %shr13.i279.6640 = lshr i64 %mul4.i268.6629, 32
  %shr14.i280.6641 = lshr i64 %mul3.i267.6628, 32
  %and15.i281.6642 = and i64 %mul5.i269.6630, 4294967295
  %add16.i282.6643 = add nuw nsw i64 %shr13.i279.6640, %shr14.i280.6641
  %add17.i283.6644 = add nuw nsw i64 %add16.i282.6643, %and15.i281.6642
  %add18.i284.6645 = add nuw nsw i64 %add17.i283.6644, %shr11.i276.6637
  %and19.i285.6646 = and i64 %add18.i284.6645, 4294967295
  %and21.i287.6647 = and i64 %add18.i284.6645, 30064771072
  %and22.i288.6648 = and i64 %mul5.i269.6630, -4294967296
  %add23.i289.6649 = add i64 %and21.i287.6647, %and22.i288.6648
  %xor2569.i290.6650 = or i64 %add23.i289.6649, %and19.i285.6646
  %add82.6651 = add i64 %xor68.i278.6639, %add82.5601
  %xor.i256.6652 = xor i64 %add82.6651, %shl.i277.6638
  %xor1.i257.6653 = xor i64 %shl.i277.6638, %add82.5601
  %or.i258.6654 = or i64 %xor.i256.6652, %xor1.i257.6653
  %xor2.i259.6655 = xor i64 %or.i258.6654, %add82.6651
  %shr.i260.6656 = lshr i64 %xor2.i259.6655, 63
  %add89.6657 = add i64 %shr.i260.6656, %xor2569.i290.6650
  %add90.6658 = add i64 %add89.6657, %add90.5608
  %145 = xor i64 %add89.6657, -9223372036854775808
  %xor2.i253.6659 = and i64 %145, %add23.i289.6649
  %xor.i244.6660 = xor i64 %add90.6658, %add89.6657
  %xor1.i245.6661 = xor i64 %add89.6657, %add90.5608
  %or.i246.6662 = or i64 %xor.i244.6660, %xor1.i245.6661
  %xor2.i247.6663 = xor i64 %or.i246.6662, %add90.6658
  %shr.i254326.6664 = or i64 %xor2.i247.6663, %xor2.i253.6659
  %or94325.6665 = lshr i64 %shr.i254326.6664, 63
  %add96.6 = add i64 %or94325.6665, %add96.5
  %arrayidx104 = getelementptr inbounds i64, i64* %ma, i64 12
  %146 = load i64, i64* %arrayidx104, align 8, !tbaa !3
  %add105 = add i64 %146, %add82.6651
  %xor.i238 = xor i64 %add105, %add82.6651
  %xor1.i239 = xor i64 %146, %add82.6651
  %or.i240 = or i64 %xor.i238, %xor1.i239
  %xor2.i241 = xor i64 %or.i240, %add105
  %shr.i242 = lshr i64 %xor2.i241, 63
  %add111 = add i64 %shr.i242, %add90.6658
  store i64 %add105, i64* %mc, align 8, !tbaa !3
  %147 = xor i64 %add111, -9223372036854775808
  %xor2.i = and i64 %147, %add90.6658
  %shr.i237 = lshr i64 %xor2.i, 63
  %add118 = add i64 %shr.i237, %add96.6
  %arrayidx74.1 = getelementptr inbounds i64, i64* %mc, i64 2
  %148 = load i64, i64* %arrayidx74.1, align 8, !tbaa !3
  %149 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.1 = and i64 %148, 4294967295
  %shr.i263.1 = lshr i64 %148, 32
  %and1.i264.1 = and i64 %149, 4294967295
  %shr2.i265.1 = lshr i64 %149, 32
  %mul.i266.1 = mul nuw i64 %and1.i264.1, %and.i262.1
  %mul3.i267.1 = mul nuw i64 %shr2.i265.1, %and.i262.1
  %mul4.i268.1 = mul nuw i64 %and1.i264.1, %shr.i263.1
  %mul5.i269.1 = mul nuw i64 %shr2.i265.1, %shr.i263.1
  %and6.i270.1 = and i64 %mul.i266.1, 4294967295
  %shr7.i271.1 = lshr i64 %mul.i266.1, 32
  %and8.i272.1 = and i64 %mul4.i268.1, 4294967295
  %and9.i273.1 = and i64 %mul3.i267.1, 4294967295
  %add.i274.1 = add nuw nsw i64 %shr7.i271.1, %and8.i272.1
  %add10.i275.1 = add nuw nsw i64 %add.i274.1, %and9.i273.1
  %shr11.i276.1 = lshr i64 %add10.i275.1, 32
  %shl.i277.1 = shl i64 %add10.i275.1, 32
  %xor68.i278.1 = or i64 %shl.i277.1, %and6.i270.1
  %shr13.i279.1 = lshr i64 %mul4.i268.1, 32
  %shr14.i280.1 = lshr i64 %mul3.i267.1, 32
  %and15.i281.1 = and i64 %mul5.i269.1, 4294967295
  %add16.i282.1 = add nuw nsw i64 %shr13.i279.1, %shr14.i280.1
  %add17.i283.1 = add nuw nsw i64 %add16.i282.1, %and15.i281.1
  %add18.i284.1 = add nuw nsw i64 %add17.i283.1, %shr11.i276.1
  %and19.i285.1 = and i64 %add18.i284.1, 4294967295
  %and21.i287.1 = and i64 %add18.i284.1, 30064771072
  %and22.i288.1 = and i64 %mul5.i269.1, -4294967296
  %add23.i289.1 = add i64 %and21.i287.1, %and22.i288.1
  %xor2569.i290.1 = or i64 %add23.i289.1, %and19.i285.1
  %add82.1 = add i64 %xor68.i278.1, %add111
  %xor.i256.1 = xor i64 %add82.1, %shl.i277.1
  %xor1.i257.1 = xor i64 %shl.i277.1, %add111
  %or.i258.1 = or i64 %xor.i256.1, %xor1.i257.1
  %xor2.i259.1 = xor i64 %or.i258.1, %add82.1
  %shr.i260.1 = lshr i64 %xor2.i259.1, 63
  %add89.1 = add i64 %shr.i260.1, %xor2569.i290.1
  %add90.1 = add i64 %add89.1, %add118
  %150 = xor i64 %add89.1, -9223372036854775808
  %xor2.i253.1 = and i64 %150, %add23.i289.1
  %xor.i244.1 = xor i64 %add90.1, %add89.1
  %xor1.i245.1 = xor i64 %add89.1, %add118
  %or.i246.1 = or i64 %xor.i244.1, %xor1.i245.1
  %xor2.i247.1 = xor i64 %or.i246.1, %add90.1
  %shr.i254326.1 = or i64 %xor2.i247.1, %xor2.i253.1
  %or94325.1 = lshr i64 %shr.i254326.1, 63
  %arrayidx74.1.1 = getelementptr inbounds i64, i64* %mc, i64 3
  %151 = load i64, i64* %arrayidx74.1.1, align 8, !tbaa !3
  %152 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.1.1 = and i64 %151, 4294967295
  %shr.i263.1.1 = lshr i64 %151, 32
  %and1.i264.1.1 = and i64 %152, 4294967295
  %shr2.i265.1.1 = lshr i64 %152, 32
  %mul.i266.1.1 = mul nuw i64 %and1.i264.1.1, %and.i262.1.1
  %mul3.i267.1.1 = mul nuw i64 %shr2.i265.1.1, %and.i262.1.1
  %mul4.i268.1.1 = mul nuw i64 %and1.i264.1.1, %shr.i263.1.1
  %mul5.i269.1.1 = mul nuw i64 %shr2.i265.1.1, %shr.i263.1.1
  %and6.i270.1.1 = and i64 %mul.i266.1.1, 4294967295
  %shr7.i271.1.1 = lshr i64 %mul.i266.1.1, 32
  %and8.i272.1.1 = and i64 %mul4.i268.1.1, 4294967295
  %and9.i273.1.1 = and i64 %mul3.i267.1.1, 4294967295
  %add.i274.1.1 = add nuw nsw i64 %shr7.i271.1.1, %and8.i272.1.1
  %add10.i275.1.1 = add nuw nsw i64 %add.i274.1.1, %and9.i273.1.1
  %shr11.i276.1.1 = lshr i64 %add10.i275.1.1, 32
  %shl.i277.1.1 = shl i64 %add10.i275.1.1, 32
  %xor68.i278.1.1 = or i64 %shl.i277.1.1, %and6.i270.1.1
  %shr13.i279.1.1 = lshr i64 %mul4.i268.1.1, 32
  %shr14.i280.1.1 = lshr i64 %mul3.i267.1.1, 32
  %and15.i281.1.1 = and i64 %mul5.i269.1.1, 4294967295
  %add16.i282.1.1 = add nuw nsw i64 %shr13.i279.1.1, %shr14.i280.1.1
  %add17.i283.1.1 = add nuw nsw i64 %add16.i282.1.1, %and15.i281.1.1
  %add18.i284.1.1 = add nuw nsw i64 %add17.i283.1.1, %shr11.i276.1.1
  %and19.i285.1.1 = and i64 %add18.i284.1.1, 4294967295
  %and21.i287.1.1 = and i64 %add18.i284.1.1, 30064771072
  %and22.i288.1.1 = and i64 %mul5.i269.1.1, -4294967296
  %add23.i289.1.1 = add i64 %and21.i287.1.1, %and22.i288.1.1
  %xor2569.i290.1.1 = or i64 %add23.i289.1.1, %and19.i285.1.1
  %add82.1.1 = add i64 %xor68.i278.1.1, %add82.1
  %xor.i256.1.1 = xor i64 %add82.1.1, %shl.i277.1.1
  %xor1.i257.1.1 = xor i64 %shl.i277.1.1, %add82.1
  %or.i258.1.1 = or i64 %xor.i256.1.1, %xor1.i257.1.1
  %xor2.i259.1.1 = xor i64 %or.i258.1.1, %add82.1.1
  %shr.i260.1.1 = lshr i64 %xor2.i259.1.1, 63
  %add89.1.1 = add i64 %shr.i260.1.1, %xor2569.i290.1.1
  %add90.1.1 = add i64 %add89.1.1, %add90.1
  %153 = xor i64 %add89.1.1, -9223372036854775808
  %xor2.i253.1.1 = and i64 %153, %add23.i289.1.1
  %xor.i244.1.1 = xor i64 %add90.1.1, %add89.1.1
  %xor1.i245.1.1 = xor i64 %add89.1.1, %add90.1
  %or.i246.1.1 = or i64 %xor.i244.1.1, %xor1.i245.1.1
  %xor2.i247.1.1 = xor i64 %or.i246.1.1, %add90.1.1
  %shr.i254326.1.1 = or i64 %xor2.i247.1.1, %xor2.i253.1.1
  %or94325.1.1 = lshr i64 %shr.i254326.1.1, 63
  %add96.1.1 = add nuw nsw i64 %or94325.1.1, %or94325.1
  %arrayidx74.1.2 = getelementptr inbounds i64, i64* %mc, i64 4
  %154 = load i64, i64* %arrayidx74.1.2, align 8, !tbaa !3
  %155 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.1.2 = and i64 %154, 4294967295
  %shr.i263.1.2 = lshr i64 %154, 32
  %and1.i264.1.2 = and i64 %155, 4294967295
  %shr2.i265.1.2 = lshr i64 %155, 32
  %mul.i266.1.2 = mul nuw i64 %and1.i264.1.2, %and.i262.1.2
  %mul3.i267.1.2 = mul nuw i64 %shr2.i265.1.2, %and.i262.1.2
  %mul4.i268.1.2 = mul nuw i64 %and1.i264.1.2, %shr.i263.1.2
  %mul5.i269.1.2 = mul nuw i64 %shr2.i265.1.2, %shr.i263.1.2
  %and6.i270.1.2 = and i64 %mul.i266.1.2, 4294967295
  %shr7.i271.1.2 = lshr i64 %mul.i266.1.2, 32
  %and8.i272.1.2 = and i64 %mul4.i268.1.2, 4294967295
  %and9.i273.1.2 = and i64 %mul3.i267.1.2, 4294967295
  %add.i274.1.2 = add nuw nsw i64 %shr7.i271.1.2, %and8.i272.1.2
  %add10.i275.1.2 = add nuw nsw i64 %add.i274.1.2, %and9.i273.1.2
  %shr11.i276.1.2 = lshr i64 %add10.i275.1.2, 32
  %shl.i277.1.2 = shl i64 %add10.i275.1.2, 32
  %xor68.i278.1.2 = or i64 %shl.i277.1.2, %and6.i270.1.2
  %shr13.i279.1.2 = lshr i64 %mul4.i268.1.2, 32
  %shr14.i280.1.2 = lshr i64 %mul3.i267.1.2, 32
  %and15.i281.1.2 = and i64 %mul5.i269.1.2, 4294967295
  %add16.i282.1.2 = add nuw nsw i64 %shr13.i279.1.2, %shr14.i280.1.2
  %add17.i283.1.2 = add nuw nsw i64 %add16.i282.1.2, %and15.i281.1.2
  %add18.i284.1.2 = add nuw nsw i64 %add17.i283.1.2, %shr11.i276.1.2
  %and19.i285.1.2 = and i64 %add18.i284.1.2, 4294967295
  %and21.i287.1.2 = and i64 %add18.i284.1.2, 30064771072
  %and22.i288.1.2 = and i64 %mul5.i269.1.2, -4294967296
  %add23.i289.1.2 = add i64 %and21.i287.1.2, %and22.i288.1.2
  %xor2569.i290.1.2 = or i64 %add23.i289.1.2, %and19.i285.1.2
  %add82.1.2 = add i64 %xor68.i278.1.2, %add82.1.1
  %xor.i256.1.2 = xor i64 %add82.1.2, %shl.i277.1.2
  %xor1.i257.1.2 = xor i64 %shl.i277.1.2, %add82.1.1
  %or.i258.1.2 = or i64 %xor.i256.1.2, %xor1.i257.1.2
  %xor2.i259.1.2 = xor i64 %or.i258.1.2, %add82.1.2
  %shr.i260.1.2 = lshr i64 %xor2.i259.1.2, 63
  %add89.1.2 = add i64 %shr.i260.1.2, %xor2569.i290.1.2
  %add90.1.2 = add i64 %add89.1.2, %add90.1.1
  %156 = xor i64 %add89.1.2, -9223372036854775808
  %xor2.i253.1.2 = and i64 %156, %add23.i289.1.2
  %xor.i244.1.2 = xor i64 %add90.1.2, %add89.1.2
  %xor1.i245.1.2 = xor i64 %add89.1.2, %add90.1.1
  %or.i246.1.2 = or i64 %xor.i244.1.2, %xor1.i245.1.2
  %xor2.i247.1.2 = xor i64 %or.i246.1.2, %add90.1.2
  %shr.i254326.1.2 = or i64 %xor2.i247.1.2, %xor2.i253.1.2
  %or94325.1.2 = lshr i64 %shr.i254326.1.2, 63
  %add96.1.2 = add nsw i64 %or94325.1.2, %add96.1.1
  %arrayidx74.1.3 = getelementptr inbounds i64, i64* %mc, i64 5
  %157 = load i64, i64* %arrayidx74.1.3, align 8, !tbaa !3
  %158 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.1.3 = and i64 %157, 4294967295
  %shr.i263.1.3 = lshr i64 %157, 32
  %and1.i264.1.3 = and i64 %158, 4294967295
  %shr2.i265.1.3 = lshr i64 %158, 32
  %mul.i266.1.3 = mul nuw i64 %and1.i264.1.3, %and.i262.1.3
  %mul3.i267.1.3 = mul nuw i64 %shr2.i265.1.3, %and.i262.1.3
  %mul4.i268.1.3 = mul nuw i64 %and1.i264.1.3, %shr.i263.1.3
  %mul5.i269.1.3 = mul nuw i64 %shr2.i265.1.3, %shr.i263.1.3
  %and6.i270.1.3 = and i64 %mul.i266.1.3, 4294967295
  %shr7.i271.1.3 = lshr i64 %mul.i266.1.3, 32
  %and8.i272.1.3 = and i64 %mul4.i268.1.3, 4294967295
  %and9.i273.1.3 = and i64 %mul3.i267.1.3, 4294967295
  %add.i274.1.3 = add nuw nsw i64 %shr7.i271.1.3, %and8.i272.1.3
  %add10.i275.1.3 = add nuw nsw i64 %add.i274.1.3, %and9.i273.1.3
  %shr11.i276.1.3 = lshr i64 %add10.i275.1.3, 32
  %shl.i277.1.3 = shl i64 %add10.i275.1.3, 32
  %xor68.i278.1.3 = or i64 %shl.i277.1.3, %and6.i270.1.3
  %shr13.i279.1.3 = lshr i64 %mul4.i268.1.3, 32
  %shr14.i280.1.3 = lshr i64 %mul3.i267.1.3, 32
  %and15.i281.1.3 = and i64 %mul5.i269.1.3, 4294967295
  %add16.i282.1.3 = add nuw nsw i64 %shr13.i279.1.3, %shr14.i280.1.3
  %add17.i283.1.3 = add nuw nsw i64 %add16.i282.1.3, %and15.i281.1.3
  %add18.i284.1.3 = add nuw nsw i64 %add17.i283.1.3, %shr11.i276.1.3
  %and19.i285.1.3 = and i64 %add18.i284.1.3, 4294967295
  %and21.i287.1.3 = and i64 %add18.i284.1.3, 30064771072
  %and22.i288.1.3 = and i64 %mul5.i269.1.3, -4294967296
  %add23.i289.1.3 = add i64 %and21.i287.1.3, %and22.i288.1.3
  %xor2569.i290.1.3 = or i64 %add23.i289.1.3, %and19.i285.1.3
  %add82.1.3 = add i64 %xor68.i278.1.3, %add82.1.2
  %xor.i256.1.3 = xor i64 %add82.1.3, %shl.i277.1.3
  %xor1.i257.1.3 = xor i64 %shl.i277.1.3, %add82.1.2
  %or.i258.1.3 = or i64 %xor.i256.1.3, %xor1.i257.1.3
  %xor2.i259.1.3 = xor i64 %or.i258.1.3, %add82.1.3
  %shr.i260.1.3 = lshr i64 %xor2.i259.1.3, 63
  %add89.1.3 = add i64 %shr.i260.1.3, %xor2569.i290.1.3
  %add90.1.3 = add i64 %add89.1.3, %add90.1.2
  %159 = xor i64 %add89.1.3, -9223372036854775808
  %xor2.i253.1.3 = and i64 %159, %add23.i289.1.3
  %xor.i244.1.3 = xor i64 %add90.1.3, %add89.1.3
  %xor1.i245.1.3 = xor i64 %add89.1.3, %add90.1.2
  %or.i246.1.3 = or i64 %xor.i244.1.3, %xor1.i245.1.3
  %xor2.i247.1.3 = xor i64 %or.i246.1.3, %add90.1.3
  %shr.i254326.1.3 = or i64 %xor2.i247.1.3, %xor2.i253.1.3
  %or94325.1.3 = lshr i64 %shr.i254326.1.3, 63
  %add96.1.3 = add nsw i64 %or94325.1.3, %add96.1.2
  %arrayidx74.1.4 = getelementptr inbounds i64, i64* %mc, i64 6
  %160 = load i64, i64* %arrayidx74.1.4, align 8, !tbaa !3
  %161 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.1.4 = and i64 %160, 4294967295
  %shr.i263.1.4 = lshr i64 %160, 32
  %and1.i264.1.4 = and i64 %161, 4294967295
  %shr2.i265.1.4 = lshr i64 %161, 32
  %mul.i266.1.4 = mul nuw i64 %and1.i264.1.4, %and.i262.1.4
  %mul3.i267.1.4 = mul nuw i64 %shr2.i265.1.4, %and.i262.1.4
  %mul4.i268.1.4 = mul nuw i64 %and1.i264.1.4, %shr.i263.1.4
  %mul5.i269.1.4 = mul nuw i64 %shr2.i265.1.4, %shr.i263.1.4
  %and6.i270.1.4 = and i64 %mul.i266.1.4, 4294967295
  %shr7.i271.1.4 = lshr i64 %mul.i266.1.4, 32
  %and8.i272.1.4 = and i64 %mul4.i268.1.4, 4294967295
  %and9.i273.1.4 = and i64 %mul3.i267.1.4, 4294967295
  %add.i274.1.4 = add nuw nsw i64 %shr7.i271.1.4, %and8.i272.1.4
  %add10.i275.1.4 = add nuw nsw i64 %add.i274.1.4, %and9.i273.1.4
  %shr11.i276.1.4 = lshr i64 %add10.i275.1.4, 32
  %shl.i277.1.4 = shl i64 %add10.i275.1.4, 32
  %xor68.i278.1.4 = or i64 %shl.i277.1.4, %and6.i270.1.4
  %shr13.i279.1.4 = lshr i64 %mul4.i268.1.4, 32
  %shr14.i280.1.4 = lshr i64 %mul3.i267.1.4, 32
  %and15.i281.1.4 = and i64 %mul5.i269.1.4, 4294967295
  %add16.i282.1.4 = add nuw nsw i64 %shr13.i279.1.4, %shr14.i280.1.4
  %add17.i283.1.4 = add nuw nsw i64 %add16.i282.1.4, %and15.i281.1.4
  %add18.i284.1.4 = add nuw nsw i64 %add17.i283.1.4, %shr11.i276.1.4
  %and19.i285.1.4 = and i64 %add18.i284.1.4, 4294967295
  %and21.i287.1.4 = and i64 %add18.i284.1.4, 30064771072
  %and22.i288.1.4 = and i64 %mul5.i269.1.4, -4294967296
  %add23.i289.1.4 = add i64 %and21.i287.1.4, %and22.i288.1.4
  %xor2569.i290.1.4 = or i64 %add23.i289.1.4, %and19.i285.1.4
  %add82.1.4 = add i64 %xor68.i278.1.4, %add82.1.3
  %xor.i256.1.4 = xor i64 %add82.1.4, %shl.i277.1.4
  %xor1.i257.1.4 = xor i64 %shl.i277.1.4, %add82.1.3
  %or.i258.1.4 = or i64 %xor.i256.1.4, %xor1.i257.1.4
  %xor2.i259.1.4 = xor i64 %or.i258.1.4, %add82.1.4
  %shr.i260.1.4 = lshr i64 %xor2.i259.1.4, 63
  %add89.1.4 = add i64 %shr.i260.1.4, %xor2569.i290.1.4
  %add90.1.4 = add i64 %add89.1.4, %add90.1.3
  %162 = xor i64 %add89.1.4, -9223372036854775808
  %xor2.i253.1.4 = and i64 %162, %add23.i289.1.4
  %xor.i244.1.4 = xor i64 %add90.1.4, %add89.1.4
  %xor1.i245.1.4 = xor i64 %add89.1.4, %add90.1.3
  %or.i246.1.4 = or i64 %xor.i244.1.4, %xor1.i245.1.4
  %xor2.i247.1.4 = xor i64 %or.i246.1.4, %add90.1.4
  %shr.i254326.1.4 = or i64 %xor2.i247.1.4, %xor2.i253.1.4
  %or94325.1.4 = lshr i64 %shr.i254326.1.4, 63
  %add96.1.4 = add i64 %or94325.1.4, %add96.1.3
  %arrayidx74.1.5 = getelementptr inbounds i64, i64* %mc, i64 7
  %163 = load i64, i64* %arrayidx74.1.5, align 8, !tbaa !3
  %164 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.1.5 = and i64 %163, 4294967295
  %shr.i263.1.5 = lshr i64 %163, 32
  %and1.i264.1.5 = and i64 %164, 4294967295
  %shr2.i265.1.5 = lshr i64 %164, 32
  %mul.i266.1.5 = mul nuw i64 %and1.i264.1.5, %and.i262.1.5
  %mul3.i267.1.5 = mul nuw i64 %shr2.i265.1.5, %and.i262.1.5
  %mul4.i268.1.5 = mul nuw i64 %and1.i264.1.5, %shr.i263.1.5
  %mul5.i269.1.5 = mul nuw i64 %shr2.i265.1.5, %shr.i263.1.5
  %and6.i270.1.5 = and i64 %mul.i266.1.5, 4294967295
  %shr7.i271.1.5 = lshr i64 %mul.i266.1.5, 32
  %and8.i272.1.5 = and i64 %mul4.i268.1.5, 4294967295
  %and9.i273.1.5 = and i64 %mul3.i267.1.5, 4294967295
  %add.i274.1.5 = add nuw nsw i64 %shr7.i271.1.5, %and8.i272.1.5
  %add10.i275.1.5 = add nuw nsw i64 %add.i274.1.5, %and9.i273.1.5
  %shr11.i276.1.5 = lshr i64 %add10.i275.1.5, 32
  %shl.i277.1.5 = shl i64 %add10.i275.1.5, 32
  %xor68.i278.1.5 = or i64 %shl.i277.1.5, %and6.i270.1.5
  %shr13.i279.1.5 = lshr i64 %mul4.i268.1.5, 32
  %shr14.i280.1.5 = lshr i64 %mul3.i267.1.5, 32
  %and15.i281.1.5 = and i64 %mul5.i269.1.5, 4294967295
  %add16.i282.1.5 = add nuw nsw i64 %shr13.i279.1.5, %shr14.i280.1.5
  %add17.i283.1.5 = add nuw nsw i64 %add16.i282.1.5, %and15.i281.1.5
  %add18.i284.1.5 = add nuw nsw i64 %add17.i283.1.5, %shr11.i276.1.5
  %and19.i285.1.5 = and i64 %add18.i284.1.5, 4294967295
  %and21.i287.1.5 = and i64 %add18.i284.1.5, 30064771072
  %and22.i288.1.5 = and i64 %mul5.i269.1.5, -4294967296
  %add23.i289.1.5 = add i64 %and21.i287.1.5, %and22.i288.1.5
  %xor2569.i290.1.5 = or i64 %add23.i289.1.5, %and19.i285.1.5
  %add82.1.5 = add i64 %xor68.i278.1.5, %add82.1.4
  %xor.i256.1.5 = xor i64 %add82.1.5, %shl.i277.1.5
  %xor1.i257.1.5 = xor i64 %shl.i277.1.5, %add82.1.4
  %or.i258.1.5 = or i64 %xor.i256.1.5, %xor1.i257.1.5
  %xor2.i259.1.5 = xor i64 %or.i258.1.5, %add82.1.5
  %shr.i260.1.5 = lshr i64 %xor2.i259.1.5, 63
  %add89.1.5 = add i64 %shr.i260.1.5, %xor2569.i290.1.5
  %add90.1.5 = add i64 %add89.1.5, %add90.1.4
  %165 = xor i64 %add89.1.5, -9223372036854775808
  %xor2.i253.1.5 = and i64 %165, %add23.i289.1.5
  %xor.i244.1.5 = xor i64 %add90.1.5, %add89.1.5
  %xor1.i245.1.5 = xor i64 %add89.1.5, %add90.1.4
  %or.i246.1.5 = or i64 %xor.i244.1.5, %xor1.i245.1.5
  %xor2.i247.1.5 = xor i64 %or.i246.1.5, %add90.1.5
  %shr.i254326.1.5 = or i64 %xor2.i247.1.5, %xor2.i253.1.5
  %or94325.1.5 = lshr i64 %shr.i254326.1.5, 63
  %add96.1.5 = add i64 %or94325.1.5, %add96.1.4
  %arrayidx74.1.6 = getelementptr inbounds i64, i64* %mc, i64 8
  %166 = load i64, i64* %arrayidx74.1.6, align 8, !tbaa !3
  %167 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.1.6 = and i64 %166, 4294967295
  %shr.i263.1.6 = lshr i64 %166, 32
  %and1.i264.1.6 = and i64 %167, 4294967295
  %shr2.i265.1.6 = lshr i64 %167, 32
  %mul.i266.1.6 = mul nuw i64 %and1.i264.1.6, %and.i262.1.6
  %mul3.i267.1.6 = mul nuw i64 %shr2.i265.1.6, %and.i262.1.6
  %mul4.i268.1.6 = mul nuw i64 %and1.i264.1.6, %shr.i263.1.6
  %mul5.i269.1.6 = mul nuw i64 %shr2.i265.1.6, %shr.i263.1.6
  %and6.i270.1.6 = and i64 %mul.i266.1.6, 4294967295
  %shr7.i271.1.6 = lshr i64 %mul.i266.1.6, 32
  %and8.i272.1.6 = and i64 %mul4.i268.1.6, 4294967295
  %and9.i273.1.6 = and i64 %mul3.i267.1.6, 4294967295
  %add.i274.1.6 = add nuw nsw i64 %shr7.i271.1.6, %and8.i272.1.6
  %add10.i275.1.6 = add nuw nsw i64 %add.i274.1.6, %and9.i273.1.6
  %shr11.i276.1.6 = lshr i64 %add10.i275.1.6, 32
  %shl.i277.1.6 = shl i64 %add10.i275.1.6, 32
  %xor68.i278.1.6 = or i64 %shl.i277.1.6, %and6.i270.1.6
  %shr13.i279.1.6 = lshr i64 %mul4.i268.1.6, 32
  %shr14.i280.1.6 = lshr i64 %mul3.i267.1.6, 32
  %and15.i281.1.6 = and i64 %mul5.i269.1.6, 4294967295
  %add16.i282.1.6 = add nuw nsw i64 %shr13.i279.1.6, %shr14.i280.1.6
  %add17.i283.1.6 = add nuw nsw i64 %add16.i282.1.6, %and15.i281.1.6
  %add18.i284.1.6 = add nuw nsw i64 %add17.i283.1.6, %shr11.i276.1.6
  %and19.i285.1.6 = and i64 %add18.i284.1.6, 4294967295
  %and21.i287.1.6 = and i64 %add18.i284.1.6, 30064771072
  %and22.i288.1.6 = and i64 %mul5.i269.1.6, -4294967296
  %add23.i289.1.6 = add i64 %and21.i287.1.6, %and22.i288.1.6
  %xor2569.i290.1.6 = or i64 %add23.i289.1.6, %and19.i285.1.6
  %add82.1.6 = add i64 %xor68.i278.1.6, %add82.1.5
  %xor.i256.1.6 = xor i64 %add82.1.6, %shl.i277.1.6
  %xor1.i257.1.6 = xor i64 %shl.i277.1.6, %add82.1.5
  %or.i258.1.6 = or i64 %xor.i256.1.6, %xor1.i257.1.6
  %xor2.i259.1.6 = xor i64 %or.i258.1.6, %add82.1.6
  %shr.i260.1.6 = lshr i64 %xor2.i259.1.6, 63
  %add89.1.6 = add i64 %shr.i260.1.6, %xor2569.i290.1.6
  %add90.1.6 = add i64 %add89.1.6, %add90.1.5
  %168 = xor i64 %add89.1.6, -9223372036854775808
  %xor2.i253.1.6 = and i64 %168, %add23.i289.1.6
  %xor.i244.1.6 = xor i64 %add90.1.6, %add89.1.6
  %xor1.i245.1.6 = xor i64 %add89.1.6, %add90.1.5
  %or.i246.1.6 = or i64 %xor.i244.1.6, %xor1.i245.1.6
  %xor2.i247.1.6 = xor i64 %or.i246.1.6, %add90.1.6
  %shr.i254326.1.6 = or i64 %xor2.i247.1.6, %xor2.i253.1.6
  %or94325.1.6 = lshr i64 %shr.i254326.1.6, 63
  %add96.1.6 = add i64 %or94325.1.6, %add96.1.5
  %arrayidx104.1 = getelementptr inbounds i64, i64* %ma, i64 13
  %169 = load i64, i64* %arrayidx104.1, align 8, !tbaa !3
  %add105.1 = add i64 %169, %add82.1.6
  %xor.i238.1 = xor i64 %add105.1, %add82.1.6
  %xor1.i239.1 = xor i64 %169, %add82.1.6
  %or.i240.1 = or i64 %xor.i238.1, %xor1.i239.1
  %xor2.i241.1 = xor i64 %or.i240.1, %add105.1
  %shr.i242.1 = lshr i64 %xor2.i241.1, 63
  %add111.1 = add i64 %shr.i242.1, %add90.1.6
  store i64 %add105.1, i64* %arrayidx50.1, align 8, !tbaa !3
  %170 = xor i64 %add111.1, -9223372036854775808
  %xor2.i.1 = and i64 %170, %add90.1.6
  %shr.i237.1 = lshr i64 %xor2.i.1, 63
  %add118.1 = add i64 %shr.i237.1, %add96.1.6
  %arrayidx74.2 = getelementptr inbounds i64, i64* %mc, i64 3
  %171 = load i64, i64* %arrayidx74.2, align 8, !tbaa !3
  %172 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.2 = and i64 %171, 4294967295
  %shr.i263.2 = lshr i64 %171, 32
  %and1.i264.2 = and i64 %172, 4294967295
  %shr2.i265.2 = lshr i64 %172, 32
  %mul.i266.2 = mul nuw i64 %and1.i264.2, %and.i262.2
  %mul3.i267.2 = mul nuw i64 %shr2.i265.2, %and.i262.2
  %mul4.i268.2 = mul nuw i64 %and1.i264.2, %shr.i263.2
  %mul5.i269.2 = mul nuw i64 %shr2.i265.2, %shr.i263.2
  %and6.i270.2 = and i64 %mul.i266.2, 4294967295
  %shr7.i271.2 = lshr i64 %mul.i266.2, 32
  %and8.i272.2 = and i64 %mul4.i268.2, 4294967295
  %and9.i273.2 = and i64 %mul3.i267.2, 4294967295
  %add.i274.2 = add nuw nsw i64 %shr7.i271.2, %and8.i272.2
  %add10.i275.2 = add nuw nsw i64 %add.i274.2, %and9.i273.2
  %shr11.i276.2 = lshr i64 %add10.i275.2, 32
  %shl.i277.2 = shl i64 %add10.i275.2, 32
  %xor68.i278.2 = or i64 %shl.i277.2, %and6.i270.2
  %shr13.i279.2 = lshr i64 %mul4.i268.2, 32
  %shr14.i280.2 = lshr i64 %mul3.i267.2, 32
  %and15.i281.2 = and i64 %mul5.i269.2, 4294967295
  %add16.i282.2 = add nuw nsw i64 %shr13.i279.2, %shr14.i280.2
  %add17.i283.2 = add nuw nsw i64 %add16.i282.2, %and15.i281.2
  %add18.i284.2 = add nuw nsw i64 %add17.i283.2, %shr11.i276.2
  %and19.i285.2 = and i64 %add18.i284.2, 4294967295
  %and21.i287.2 = and i64 %add18.i284.2, 30064771072
  %and22.i288.2 = and i64 %mul5.i269.2, -4294967296
  %add23.i289.2 = add i64 %and21.i287.2, %and22.i288.2
  %xor2569.i290.2 = or i64 %add23.i289.2, %and19.i285.2
  %add82.2 = add i64 %xor68.i278.2, %add111.1
  %xor.i256.2 = xor i64 %add82.2, %shl.i277.2
  %xor1.i257.2 = xor i64 %shl.i277.2, %add111.1
  %or.i258.2 = or i64 %xor.i256.2, %xor1.i257.2
  %xor2.i259.2 = xor i64 %or.i258.2, %add82.2
  %shr.i260.2 = lshr i64 %xor2.i259.2, 63
  %add89.2 = add i64 %shr.i260.2, %xor2569.i290.2
  %add90.2 = add i64 %add89.2, %add118.1
  %173 = xor i64 %add89.2, -9223372036854775808
  %xor2.i253.2 = and i64 %173, %add23.i289.2
  %xor.i244.2 = xor i64 %add90.2, %add89.2
  %xor1.i245.2 = xor i64 %add89.2, %add118.1
  %or.i246.2 = or i64 %xor.i244.2, %xor1.i245.2
  %xor2.i247.2 = xor i64 %or.i246.2, %add90.2
  %shr.i254326.2 = or i64 %xor2.i247.2, %xor2.i253.2
  %or94325.2 = lshr i64 %shr.i254326.2, 63
  %arrayidx74.2.1 = getelementptr inbounds i64, i64* %mc, i64 4
  %174 = load i64, i64* %arrayidx74.2.1, align 8, !tbaa !3
  %175 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.2.1 = and i64 %174, 4294967295
  %shr.i263.2.1 = lshr i64 %174, 32
  %and1.i264.2.1 = and i64 %175, 4294967295
  %shr2.i265.2.1 = lshr i64 %175, 32
  %mul.i266.2.1 = mul nuw i64 %and1.i264.2.1, %and.i262.2.1
  %mul3.i267.2.1 = mul nuw i64 %shr2.i265.2.1, %and.i262.2.1
  %mul4.i268.2.1 = mul nuw i64 %and1.i264.2.1, %shr.i263.2.1
  %mul5.i269.2.1 = mul nuw i64 %shr2.i265.2.1, %shr.i263.2.1
  %and6.i270.2.1 = and i64 %mul.i266.2.1, 4294967295
  %shr7.i271.2.1 = lshr i64 %mul.i266.2.1, 32
  %and8.i272.2.1 = and i64 %mul4.i268.2.1, 4294967295
  %and9.i273.2.1 = and i64 %mul3.i267.2.1, 4294967295
  %add.i274.2.1 = add nuw nsw i64 %shr7.i271.2.1, %and8.i272.2.1
  %add10.i275.2.1 = add nuw nsw i64 %add.i274.2.1, %and9.i273.2.1
  %shr11.i276.2.1 = lshr i64 %add10.i275.2.1, 32
  %shl.i277.2.1 = shl i64 %add10.i275.2.1, 32
  %xor68.i278.2.1 = or i64 %shl.i277.2.1, %and6.i270.2.1
  %shr13.i279.2.1 = lshr i64 %mul4.i268.2.1, 32
  %shr14.i280.2.1 = lshr i64 %mul3.i267.2.1, 32
  %and15.i281.2.1 = and i64 %mul5.i269.2.1, 4294967295
  %add16.i282.2.1 = add nuw nsw i64 %shr13.i279.2.1, %shr14.i280.2.1
  %add17.i283.2.1 = add nuw nsw i64 %add16.i282.2.1, %and15.i281.2.1
  %add18.i284.2.1 = add nuw nsw i64 %add17.i283.2.1, %shr11.i276.2.1
  %and19.i285.2.1 = and i64 %add18.i284.2.1, 4294967295
  %and21.i287.2.1 = and i64 %add18.i284.2.1, 30064771072
  %and22.i288.2.1 = and i64 %mul5.i269.2.1, -4294967296
  %add23.i289.2.1 = add i64 %and21.i287.2.1, %and22.i288.2.1
  %xor2569.i290.2.1 = or i64 %add23.i289.2.1, %and19.i285.2.1
  %add82.2.1 = add i64 %xor68.i278.2.1, %add82.2
  %xor.i256.2.1 = xor i64 %add82.2.1, %shl.i277.2.1
  %xor1.i257.2.1 = xor i64 %shl.i277.2.1, %add82.2
  %or.i258.2.1 = or i64 %xor.i256.2.1, %xor1.i257.2.1
  %xor2.i259.2.1 = xor i64 %or.i258.2.1, %add82.2.1
  %shr.i260.2.1 = lshr i64 %xor2.i259.2.1, 63
  %add89.2.1 = add i64 %shr.i260.2.1, %xor2569.i290.2.1
  %add90.2.1 = add i64 %add89.2.1, %add90.2
  %176 = xor i64 %add89.2.1, -9223372036854775808
  %xor2.i253.2.1 = and i64 %176, %add23.i289.2.1
  %xor.i244.2.1 = xor i64 %add90.2.1, %add89.2.1
  %xor1.i245.2.1 = xor i64 %add89.2.1, %add90.2
  %or.i246.2.1 = or i64 %xor.i244.2.1, %xor1.i245.2.1
  %xor2.i247.2.1 = xor i64 %or.i246.2.1, %add90.2.1
  %shr.i254326.2.1 = or i64 %xor2.i247.2.1, %xor2.i253.2.1
  %or94325.2.1 = lshr i64 %shr.i254326.2.1, 63
  %add96.2.1 = add nuw nsw i64 %or94325.2.1, %or94325.2
  %arrayidx74.2.2 = getelementptr inbounds i64, i64* %mc, i64 5
  %177 = load i64, i64* %arrayidx74.2.2, align 8, !tbaa !3
  %178 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.2.2 = and i64 %177, 4294967295
  %shr.i263.2.2 = lshr i64 %177, 32
  %and1.i264.2.2 = and i64 %178, 4294967295
  %shr2.i265.2.2 = lshr i64 %178, 32
  %mul.i266.2.2 = mul nuw i64 %and1.i264.2.2, %and.i262.2.2
  %mul3.i267.2.2 = mul nuw i64 %shr2.i265.2.2, %and.i262.2.2
  %mul4.i268.2.2 = mul nuw i64 %and1.i264.2.2, %shr.i263.2.2
  %mul5.i269.2.2 = mul nuw i64 %shr2.i265.2.2, %shr.i263.2.2
  %and6.i270.2.2 = and i64 %mul.i266.2.2, 4294967295
  %shr7.i271.2.2 = lshr i64 %mul.i266.2.2, 32
  %and8.i272.2.2 = and i64 %mul4.i268.2.2, 4294967295
  %and9.i273.2.2 = and i64 %mul3.i267.2.2, 4294967295
  %add.i274.2.2 = add nuw nsw i64 %shr7.i271.2.2, %and8.i272.2.2
  %add10.i275.2.2 = add nuw nsw i64 %add.i274.2.2, %and9.i273.2.2
  %shr11.i276.2.2 = lshr i64 %add10.i275.2.2, 32
  %shl.i277.2.2 = shl i64 %add10.i275.2.2, 32
  %xor68.i278.2.2 = or i64 %shl.i277.2.2, %and6.i270.2.2
  %shr13.i279.2.2 = lshr i64 %mul4.i268.2.2, 32
  %shr14.i280.2.2 = lshr i64 %mul3.i267.2.2, 32
  %and15.i281.2.2 = and i64 %mul5.i269.2.2, 4294967295
  %add16.i282.2.2 = add nuw nsw i64 %shr13.i279.2.2, %shr14.i280.2.2
  %add17.i283.2.2 = add nuw nsw i64 %add16.i282.2.2, %and15.i281.2.2
  %add18.i284.2.2 = add nuw nsw i64 %add17.i283.2.2, %shr11.i276.2.2
  %and19.i285.2.2 = and i64 %add18.i284.2.2, 4294967295
  %and21.i287.2.2 = and i64 %add18.i284.2.2, 30064771072
  %and22.i288.2.2 = and i64 %mul5.i269.2.2, -4294967296
  %add23.i289.2.2 = add i64 %and21.i287.2.2, %and22.i288.2.2
  %xor2569.i290.2.2 = or i64 %add23.i289.2.2, %and19.i285.2.2
  %add82.2.2 = add i64 %xor68.i278.2.2, %add82.2.1
  %xor.i256.2.2 = xor i64 %add82.2.2, %shl.i277.2.2
  %xor1.i257.2.2 = xor i64 %shl.i277.2.2, %add82.2.1
  %or.i258.2.2 = or i64 %xor.i256.2.2, %xor1.i257.2.2
  %xor2.i259.2.2 = xor i64 %or.i258.2.2, %add82.2.2
  %shr.i260.2.2 = lshr i64 %xor2.i259.2.2, 63
  %add89.2.2 = add i64 %shr.i260.2.2, %xor2569.i290.2.2
  %add90.2.2 = add i64 %add89.2.2, %add90.2.1
  %179 = xor i64 %add89.2.2, -9223372036854775808
  %xor2.i253.2.2 = and i64 %179, %add23.i289.2.2
  %xor.i244.2.2 = xor i64 %add90.2.2, %add89.2.2
  %xor1.i245.2.2 = xor i64 %add89.2.2, %add90.2.1
  %or.i246.2.2 = or i64 %xor.i244.2.2, %xor1.i245.2.2
  %xor2.i247.2.2 = xor i64 %or.i246.2.2, %add90.2.2
  %shr.i254326.2.2 = or i64 %xor2.i247.2.2, %xor2.i253.2.2
  %or94325.2.2 = lshr i64 %shr.i254326.2.2, 63
  %add96.2.2 = add nsw i64 %or94325.2.2, %add96.2.1
  %arrayidx74.2.3 = getelementptr inbounds i64, i64* %mc, i64 6
  %180 = load i64, i64* %arrayidx74.2.3, align 8, !tbaa !3
  %181 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.2.3 = and i64 %180, 4294967295
  %shr.i263.2.3 = lshr i64 %180, 32
  %and1.i264.2.3 = and i64 %181, 4294967295
  %shr2.i265.2.3 = lshr i64 %181, 32
  %mul.i266.2.3 = mul nuw i64 %and1.i264.2.3, %and.i262.2.3
  %mul3.i267.2.3 = mul nuw i64 %shr2.i265.2.3, %and.i262.2.3
  %mul4.i268.2.3 = mul nuw i64 %and1.i264.2.3, %shr.i263.2.3
  %mul5.i269.2.3 = mul nuw i64 %shr2.i265.2.3, %shr.i263.2.3
  %and6.i270.2.3 = and i64 %mul.i266.2.3, 4294967295
  %shr7.i271.2.3 = lshr i64 %mul.i266.2.3, 32
  %and8.i272.2.3 = and i64 %mul4.i268.2.3, 4294967295
  %and9.i273.2.3 = and i64 %mul3.i267.2.3, 4294967295
  %add.i274.2.3 = add nuw nsw i64 %shr7.i271.2.3, %and8.i272.2.3
  %add10.i275.2.3 = add nuw nsw i64 %add.i274.2.3, %and9.i273.2.3
  %shr11.i276.2.3 = lshr i64 %add10.i275.2.3, 32
  %shl.i277.2.3 = shl i64 %add10.i275.2.3, 32
  %xor68.i278.2.3 = or i64 %shl.i277.2.3, %and6.i270.2.3
  %shr13.i279.2.3 = lshr i64 %mul4.i268.2.3, 32
  %shr14.i280.2.3 = lshr i64 %mul3.i267.2.3, 32
  %and15.i281.2.3 = and i64 %mul5.i269.2.3, 4294967295
  %add16.i282.2.3 = add nuw nsw i64 %shr13.i279.2.3, %shr14.i280.2.3
  %add17.i283.2.3 = add nuw nsw i64 %add16.i282.2.3, %and15.i281.2.3
  %add18.i284.2.3 = add nuw nsw i64 %add17.i283.2.3, %shr11.i276.2.3
  %and19.i285.2.3 = and i64 %add18.i284.2.3, 4294967295
  %and21.i287.2.3 = and i64 %add18.i284.2.3, 30064771072
  %and22.i288.2.3 = and i64 %mul5.i269.2.3, -4294967296
  %add23.i289.2.3 = add i64 %and21.i287.2.3, %and22.i288.2.3
  %xor2569.i290.2.3 = or i64 %add23.i289.2.3, %and19.i285.2.3
  %add82.2.3 = add i64 %xor68.i278.2.3, %add82.2.2
  %xor.i256.2.3 = xor i64 %add82.2.3, %shl.i277.2.3
  %xor1.i257.2.3 = xor i64 %shl.i277.2.3, %add82.2.2
  %or.i258.2.3 = or i64 %xor.i256.2.3, %xor1.i257.2.3
  %xor2.i259.2.3 = xor i64 %or.i258.2.3, %add82.2.3
  %shr.i260.2.3 = lshr i64 %xor2.i259.2.3, 63
  %add89.2.3 = add i64 %shr.i260.2.3, %xor2569.i290.2.3
  %add90.2.3 = add i64 %add89.2.3, %add90.2.2
  %182 = xor i64 %add89.2.3, -9223372036854775808
  %xor2.i253.2.3 = and i64 %182, %add23.i289.2.3
  %xor.i244.2.3 = xor i64 %add90.2.3, %add89.2.3
  %xor1.i245.2.3 = xor i64 %add89.2.3, %add90.2.2
  %or.i246.2.3 = or i64 %xor.i244.2.3, %xor1.i245.2.3
  %xor2.i247.2.3 = xor i64 %or.i246.2.3, %add90.2.3
  %shr.i254326.2.3 = or i64 %xor2.i247.2.3, %xor2.i253.2.3
  %or94325.2.3 = lshr i64 %shr.i254326.2.3, 63
  %add96.2.3 = add nsw i64 %or94325.2.3, %add96.2.2
  %arrayidx74.2.4 = getelementptr inbounds i64, i64* %mc, i64 7
  %183 = load i64, i64* %arrayidx74.2.4, align 8, !tbaa !3
  %184 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.2.4 = and i64 %183, 4294967295
  %shr.i263.2.4 = lshr i64 %183, 32
  %and1.i264.2.4 = and i64 %184, 4294967295
  %shr2.i265.2.4 = lshr i64 %184, 32
  %mul.i266.2.4 = mul nuw i64 %and1.i264.2.4, %and.i262.2.4
  %mul3.i267.2.4 = mul nuw i64 %shr2.i265.2.4, %and.i262.2.4
  %mul4.i268.2.4 = mul nuw i64 %and1.i264.2.4, %shr.i263.2.4
  %mul5.i269.2.4 = mul nuw i64 %shr2.i265.2.4, %shr.i263.2.4
  %and6.i270.2.4 = and i64 %mul.i266.2.4, 4294967295
  %shr7.i271.2.4 = lshr i64 %mul.i266.2.4, 32
  %and8.i272.2.4 = and i64 %mul4.i268.2.4, 4294967295
  %and9.i273.2.4 = and i64 %mul3.i267.2.4, 4294967295
  %add.i274.2.4 = add nuw nsw i64 %shr7.i271.2.4, %and8.i272.2.4
  %add10.i275.2.4 = add nuw nsw i64 %add.i274.2.4, %and9.i273.2.4
  %shr11.i276.2.4 = lshr i64 %add10.i275.2.4, 32
  %shl.i277.2.4 = shl i64 %add10.i275.2.4, 32
  %xor68.i278.2.4 = or i64 %shl.i277.2.4, %and6.i270.2.4
  %shr13.i279.2.4 = lshr i64 %mul4.i268.2.4, 32
  %shr14.i280.2.4 = lshr i64 %mul3.i267.2.4, 32
  %and15.i281.2.4 = and i64 %mul5.i269.2.4, 4294967295
  %add16.i282.2.4 = add nuw nsw i64 %shr13.i279.2.4, %shr14.i280.2.4
  %add17.i283.2.4 = add nuw nsw i64 %add16.i282.2.4, %and15.i281.2.4
  %add18.i284.2.4 = add nuw nsw i64 %add17.i283.2.4, %shr11.i276.2.4
  %and19.i285.2.4 = and i64 %add18.i284.2.4, 4294967295
  %and21.i287.2.4 = and i64 %add18.i284.2.4, 30064771072
  %and22.i288.2.4 = and i64 %mul5.i269.2.4, -4294967296
  %add23.i289.2.4 = add i64 %and21.i287.2.4, %and22.i288.2.4
  %xor2569.i290.2.4 = or i64 %add23.i289.2.4, %and19.i285.2.4
  %add82.2.4 = add i64 %xor68.i278.2.4, %add82.2.3
  %xor.i256.2.4 = xor i64 %add82.2.4, %shl.i277.2.4
  %xor1.i257.2.4 = xor i64 %shl.i277.2.4, %add82.2.3
  %or.i258.2.4 = or i64 %xor.i256.2.4, %xor1.i257.2.4
  %xor2.i259.2.4 = xor i64 %or.i258.2.4, %add82.2.4
  %shr.i260.2.4 = lshr i64 %xor2.i259.2.4, 63
  %add89.2.4 = add i64 %shr.i260.2.4, %xor2569.i290.2.4
  %add90.2.4 = add i64 %add89.2.4, %add90.2.3
  %185 = xor i64 %add89.2.4, -9223372036854775808
  %xor2.i253.2.4 = and i64 %185, %add23.i289.2.4
  %xor.i244.2.4 = xor i64 %add90.2.4, %add89.2.4
  %xor1.i245.2.4 = xor i64 %add89.2.4, %add90.2.3
  %or.i246.2.4 = or i64 %xor.i244.2.4, %xor1.i245.2.4
  %xor2.i247.2.4 = xor i64 %or.i246.2.4, %add90.2.4
  %shr.i254326.2.4 = or i64 %xor2.i247.2.4, %xor2.i253.2.4
  %or94325.2.4 = lshr i64 %shr.i254326.2.4, 63
  %add96.2.4 = add i64 %or94325.2.4, %add96.2.3
  %arrayidx74.2.5 = getelementptr inbounds i64, i64* %mc, i64 8
  %186 = load i64, i64* %arrayidx74.2.5, align 8, !tbaa !3
  %187 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.2.5 = and i64 %186, 4294967295
  %shr.i263.2.5 = lshr i64 %186, 32
  %and1.i264.2.5 = and i64 %187, 4294967295
  %shr2.i265.2.5 = lshr i64 %187, 32
  %mul.i266.2.5 = mul nuw i64 %and1.i264.2.5, %and.i262.2.5
  %mul3.i267.2.5 = mul nuw i64 %shr2.i265.2.5, %and.i262.2.5
  %mul4.i268.2.5 = mul nuw i64 %and1.i264.2.5, %shr.i263.2.5
  %mul5.i269.2.5 = mul nuw i64 %shr2.i265.2.5, %shr.i263.2.5
  %and6.i270.2.5 = and i64 %mul.i266.2.5, 4294967295
  %shr7.i271.2.5 = lshr i64 %mul.i266.2.5, 32
  %and8.i272.2.5 = and i64 %mul4.i268.2.5, 4294967295
  %and9.i273.2.5 = and i64 %mul3.i267.2.5, 4294967295
  %add.i274.2.5 = add nuw nsw i64 %shr7.i271.2.5, %and8.i272.2.5
  %add10.i275.2.5 = add nuw nsw i64 %add.i274.2.5, %and9.i273.2.5
  %shr11.i276.2.5 = lshr i64 %add10.i275.2.5, 32
  %shl.i277.2.5 = shl i64 %add10.i275.2.5, 32
  %xor68.i278.2.5 = or i64 %shl.i277.2.5, %and6.i270.2.5
  %shr13.i279.2.5 = lshr i64 %mul4.i268.2.5, 32
  %shr14.i280.2.5 = lshr i64 %mul3.i267.2.5, 32
  %and15.i281.2.5 = and i64 %mul5.i269.2.5, 4294967295
  %add16.i282.2.5 = add nuw nsw i64 %shr13.i279.2.5, %shr14.i280.2.5
  %add17.i283.2.5 = add nuw nsw i64 %add16.i282.2.5, %and15.i281.2.5
  %add18.i284.2.5 = add nuw nsw i64 %add17.i283.2.5, %shr11.i276.2.5
  %and19.i285.2.5 = and i64 %add18.i284.2.5, 4294967295
  %and21.i287.2.5 = and i64 %add18.i284.2.5, 30064771072
  %and22.i288.2.5 = and i64 %mul5.i269.2.5, -4294967296
  %add23.i289.2.5 = add i64 %and21.i287.2.5, %and22.i288.2.5
  %xor2569.i290.2.5 = or i64 %add23.i289.2.5, %and19.i285.2.5
  %add82.2.5 = add i64 %xor68.i278.2.5, %add82.2.4
  %xor.i256.2.5 = xor i64 %add82.2.5, %shl.i277.2.5
  %xor1.i257.2.5 = xor i64 %shl.i277.2.5, %add82.2.4
  %or.i258.2.5 = or i64 %xor.i256.2.5, %xor1.i257.2.5
  %xor2.i259.2.5 = xor i64 %or.i258.2.5, %add82.2.5
  %shr.i260.2.5 = lshr i64 %xor2.i259.2.5, 63
  %add89.2.5 = add i64 %shr.i260.2.5, %xor2569.i290.2.5
  %add90.2.5 = add i64 %add89.2.5, %add90.2.4
  %188 = xor i64 %add89.2.5, -9223372036854775808
  %xor2.i253.2.5 = and i64 %188, %add23.i289.2.5
  %xor.i244.2.5 = xor i64 %add90.2.5, %add89.2.5
  %xor1.i245.2.5 = xor i64 %add89.2.5, %add90.2.4
  %or.i246.2.5 = or i64 %xor.i244.2.5, %xor1.i245.2.5
  %xor2.i247.2.5 = xor i64 %or.i246.2.5, %add90.2.5
  %shr.i254326.2.5 = or i64 %xor2.i247.2.5, %xor2.i253.2.5
  %or94325.2.5 = lshr i64 %shr.i254326.2.5, 63
  %add96.2.5 = add i64 %or94325.2.5, %add96.2.4
  %arrayidx74.2.6 = getelementptr inbounds i64, i64* %mc, i64 9
  %189 = load i64, i64* %arrayidx74.2.6, align 8, !tbaa !3
  %190 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.2.6 = and i64 %189, 4294967295
  %shr.i263.2.6 = lshr i64 %189, 32
  %and1.i264.2.6 = and i64 %190, 4294967295
  %shr2.i265.2.6 = lshr i64 %190, 32
  %mul.i266.2.6 = mul nuw i64 %and1.i264.2.6, %and.i262.2.6
  %mul3.i267.2.6 = mul nuw i64 %shr2.i265.2.6, %and.i262.2.6
  %mul4.i268.2.6 = mul nuw i64 %and1.i264.2.6, %shr.i263.2.6
  %mul5.i269.2.6 = mul nuw i64 %shr2.i265.2.6, %shr.i263.2.6
  %and6.i270.2.6 = and i64 %mul.i266.2.6, 4294967295
  %shr7.i271.2.6 = lshr i64 %mul.i266.2.6, 32
  %and8.i272.2.6 = and i64 %mul4.i268.2.6, 4294967295
  %and9.i273.2.6 = and i64 %mul3.i267.2.6, 4294967295
  %add.i274.2.6 = add nuw nsw i64 %shr7.i271.2.6, %and8.i272.2.6
  %add10.i275.2.6 = add nuw nsw i64 %add.i274.2.6, %and9.i273.2.6
  %shr11.i276.2.6 = lshr i64 %add10.i275.2.6, 32
  %shl.i277.2.6 = shl i64 %add10.i275.2.6, 32
  %xor68.i278.2.6 = or i64 %shl.i277.2.6, %and6.i270.2.6
  %shr13.i279.2.6 = lshr i64 %mul4.i268.2.6, 32
  %shr14.i280.2.6 = lshr i64 %mul3.i267.2.6, 32
  %and15.i281.2.6 = and i64 %mul5.i269.2.6, 4294967295
  %add16.i282.2.6 = add nuw nsw i64 %shr13.i279.2.6, %shr14.i280.2.6
  %add17.i283.2.6 = add nuw nsw i64 %add16.i282.2.6, %and15.i281.2.6
  %add18.i284.2.6 = add nuw nsw i64 %add17.i283.2.6, %shr11.i276.2.6
  %and19.i285.2.6 = and i64 %add18.i284.2.6, 4294967295
  %and21.i287.2.6 = and i64 %add18.i284.2.6, 30064771072
  %and22.i288.2.6 = and i64 %mul5.i269.2.6, -4294967296
  %add23.i289.2.6 = add i64 %and21.i287.2.6, %and22.i288.2.6
  %xor2569.i290.2.6 = or i64 %add23.i289.2.6, %and19.i285.2.6
  %add82.2.6 = add i64 %xor68.i278.2.6, %add82.2.5
  %xor.i256.2.6 = xor i64 %add82.2.6, %shl.i277.2.6
  %xor1.i257.2.6 = xor i64 %shl.i277.2.6, %add82.2.5
  %or.i258.2.6 = or i64 %xor.i256.2.6, %xor1.i257.2.6
  %xor2.i259.2.6 = xor i64 %or.i258.2.6, %add82.2.6
  %shr.i260.2.6 = lshr i64 %xor2.i259.2.6, 63
  %add89.2.6 = add i64 %shr.i260.2.6, %xor2569.i290.2.6
  %add90.2.6 = add i64 %add89.2.6, %add90.2.5
  %191 = xor i64 %add89.2.6, -9223372036854775808
  %xor2.i253.2.6 = and i64 %191, %add23.i289.2.6
  %xor.i244.2.6 = xor i64 %add90.2.6, %add89.2.6
  %xor1.i245.2.6 = xor i64 %add89.2.6, %add90.2.5
  %or.i246.2.6 = or i64 %xor.i244.2.6, %xor1.i245.2.6
  %xor2.i247.2.6 = xor i64 %or.i246.2.6, %add90.2.6
  %shr.i254326.2.6 = or i64 %xor2.i247.2.6, %xor2.i253.2.6
  %or94325.2.6 = lshr i64 %shr.i254326.2.6, 63
  %add96.2.6 = add i64 %or94325.2.6, %add96.2.5
  %arrayidx104.2 = getelementptr inbounds i64, i64* %ma, i64 14
  %192 = load i64, i64* %arrayidx104.2, align 8, !tbaa !3
  %add105.2 = add i64 %192, %add82.2.6
  %xor.i238.2 = xor i64 %add105.2, %add82.2.6
  %xor1.i239.2 = xor i64 %192, %add82.2.6
  %or.i240.2 = or i64 %xor.i238.2, %xor1.i239.2
  %xor2.i241.2 = xor i64 %or.i240.2, %add105.2
  %shr.i242.2 = lshr i64 %xor2.i241.2, 63
  %add111.2 = add i64 %shr.i242.2, %add90.2.6
  store i64 %add105.2, i64* %arrayidx50.2, align 8, !tbaa !3
  %193 = xor i64 %add111.2, -9223372036854775808
  %xor2.i.2 = and i64 %193, %add90.2.6
  %shr.i237.2 = lshr i64 %xor2.i.2, 63
  %add118.2 = add i64 %shr.i237.2, %add96.2.6
  %arrayidx74.3 = getelementptr inbounds i64, i64* %mc, i64 4
  %194 = load i64, i64* %arrayidx74.3, align 8, !tbaa !3
  %195 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.3 = and i64 %194, 4294967295
  %shr.i263.3 = lshr i64 %194, 32
  %and1.i264.3 = and i64 %195, 4294967295
  %shr2.i265.3 = lshr i64 %195, 32
  %mul.i266.3 = mul nuw i64 %and1.i264.3, %and.i262.3
  %mul3.i267.3 = mul nuw i64 %shr2.i265.3, %and.i262.3
  %mul4.i268.3 = mul nuw i64 %and1.i264.3, %shr.i263.3
  %mul5.i269.3 = mul nuw i64 %shr2.i265.3, %shr.i263.3
  %and6.i270.3 = and i64 %mul.i266.3, 4294967295
  %shr7.i271.3 = lshr i64 %mul.i266.3, 32
  %and8.i272.3 = and i64 %mul4.i268.3, 4294967295
  %and9.i273.3 = and i64 %mul3.i267.3, 4294967295
  %add.i274.3 = add nuw nsw i64 %shr7.i271.3, %and8.i272.3
  %add10.i275.3 = add nuw nsw i64 %add.i274.3, %and9.i273.3
  %shr11.i276.3 = lshr i64 %add10.i275.3, 32
  %shl.i277.3 = shl i64 %add10.i275.3, 32
  %xor68.i278.3 = or i64 %shl.i277.3, %and6.i270.3
  %shr13.i279.3 = lshr i64 %mul4.i268.3, 32
  %shr14.i280.3 = lshr i64 %mul3.i267.3, 32
  %and15.i281.3 = and i64 %mul5.i269.3, 4294967295
  %add16.i282.3 = add nuw nsw i64 %shr13.i279.3, %shr14.i280.3
  %add17.i283.3 = add nuw nsw i64 %add16.i282.3, %and15.i281.3
  %add18.i284.3 = add nuw nsw i64 %add17.i283.3, %shr11.i276.3
  %and19.i285.3 = and i64 %add18.i284.3, 4294967295
  %and21.i287.3 = and i64 %add18.i284.3, 30064771072
  %and22.i288.3 = and i64 %mul5.i269.3, -4294967296
  %add23.i289.3 = add i64 %and21.i287.3, %and22.i288.3
  %xor2569.i290.3 = or i64 %add23.i289.3, %and19.i285.3
  %add82.3 = add i64 %xor68.i278.3, %add111.2
  %xor.i256.3 = xor i64 %add82.3, %shl.i277.3
  %xor1.i257.3 = xor i64 %shl.i277.3, %add111.2
  %or.i258.3 = or i64 %xor.i256.3, %xor1.i257.3
  %xor2.i259.3 = xor i64 %or.i258.3, %add82.3
  %shr.i260.3 = lshr i64 %xor2.i259.3, 63
  %add89.3 = add i64 %shr.i260.3, %xor2569.i290.3
  %add90.3 = add i64 %add89.3, %add118.2
  %196 = xor i64 %add89.3, -9223372036854775808
  %xor2.i253.3 = and i64 %196, %add23.i289.3
  %xor.i244.3 = xor i64 %add90.3, %add89.3
  %xor1.i245.3 = xor i64 %add89.3, %add118.2
  %or.i246.3 = or i64 %xor.i244.3, %xor1.i245.3
  %xor2.i247.3 = xor i64 %or.i246.3, %add90.3
  %shr.i254326.3 = or i64 %xor2.i247.3, %xor2.i253.3
  %or94325.3 = lshr i64 %shr.i254326.3, 63
  %arrayidx74.3.1 = getelementptr inbounds i64, i64* %mc, i64 5
  %197 = load i64, i64* %arrayidx74.3.1, align 8, !tbaa !3
  %198 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.3.1 = and i64 %197, 4294967295
  %shr.i263.3.1 = lshr i64 %197, 32
  %and1.i264.3.1 = and i64 %198, 4294967295
  %shr2.i265.3.1 = lshr i64 %198, 32
  %mul.i266.3.1 = mul nuw i64 %and1.i264.3.1, %and.i262.3.1
  %mul3.i267.3.1 = mul nuw i64 %shr2.i265.3.1, %and.i262.3.1
  %mul4.i268.3.1 = mul nuw i64 %and1.i264.3.1, %shr.i263.3.1
  %mul5.i269.3.1 = mul nuw i64 %shr2.i265.3.1, %shr.i263.3.1
  %and6.i270.3.1 = and i64 %mul.i266.3.1, 4294967295
  %shr7.i271.3.1 = lshr i64 %mul.i266.3.1, 32
  %and8.i272.3.1 = and i64 %mul4.i268.3.1, 4294967295
  %and9.i273.3.1 = and i64 %mul3.i267.3.1, 4294967295
  %add.i274.3.1 = add nuw nsw i64 %shr7.i271.3.1, %and8.i272.3.1
  %add10.i275.3.1 = add nuw nsw i64 %add.i274.3.1, %and9.i273.3.1
  %shr11.i276.3.1 = lshr i64 %add10.i275.3.1, 32
  %shl.i277.3.1 = shl i64 %add10.i275.3.1, 32
  %xor68.i278.3.1 = or i64 %shl.i277.3.1, %and6.i270.3.1
  %shr13.i279.3.1 = lshr i64 %mul4.i268.3.1, 32
  %shr14.i280.3.1 = lshr i64 %mul3.i267.3.1, 32
  %and15.i281.3.1 = and i64 %mul5.i269.3.1, 4294967295
  %add16.i282.3.1 = add nuw nsw i64 %shr13.i279.3.1, %shr14.i280.3.1
  %add17.i283.3.1 = add nuw nsw i64 %add16.i282.3.1, %and15.i281.3.1
  %add18.i284.3.1 = add nuw nsw i64 %add17.i283.3.1, %shr11.i276.3.1
  %and19.i285.3.1 = and i64 %add18.i284.3.1, 4294967295
  %and21.i287.3.1 = and i64 %add18.i284.3.1, 30064771072
  %and22.i288.3.1 = and i64 %mul5.i269.3.1, -4294967296
  %add23.i289.3.1 = add i64 %and21.i287.3.1, %and22.i288.3.1
  %xor2569.i290.3.1 = or i64 %add23.i289.3.1, %and19.i285.3.1
  %add82.3.1 = add i64 %xor68.i278.3.1, %add82.3
  %xor.i256.3.1 = xor i64 %add82.3.1, %shl.i277.3.1
  %xor1.i257.3.1 = xor i64 %shl.i277.3.1, %add82.3
  %or.i258.3.1 = or i64 %xor.i256.3.1, %xor1.i257.3.1
  %xor2.i259.3.1 = xor i64 %or.i258.3.1, %add82.3.1
  %shr.i260.3.1 = lshr i64 %xor2.i259.3.1, 63
  %add89.3.1 = add i64 %shr.i260.3.1, %xor2569.i290.3.1
  %add90.3.1 = add i64 %add89.3.1, %add90.3
  %199 = xor i64 %add89.3.1, -9223372036854775808
  %xor2.i253.3.1 = and i64 %199, %add23.i289.3.1
  %xor.i244.3.1 = xor i64 %add90.3.1, %add89.3.1
  %xor1.i245.3.1 = xor i64 %add89.3.1, %add90.3
  %or.i246.3.1 = or i64 %xor.i244.3.1, %xor1.i245.3.1
  %xor2.i247.3.1 = xor i64 %or.i246.3.1, %add90.3.1
  %shr.i254326.3.1 = or i64 %xor2.i247.3.1, %xor2.i253.3.1
  %or94325.3.1 = lshr i64 %shr.i254326.3.1, 63
  %add96.3.1 = add nuw nsw i64 %or94325.3.1, %or94325.3
  %arrayidx74.3.2 = getelementptr inbounds i64, i64* %mc, i64 6
  %200 = load i64, i64* %arrayidx74.3.2, align 8, !tbaa !3
  %201 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.3.2 = and i64 %200, 4294967295
  %shr.i263.3.2 = lshr i64 %200, 32
  %and1.i264.3.2 = and i64 %201, 4294967295
  %shr2.i265.3.2 = lshr i64 %201, 32
  %mul.i266.3.2 = mul nuw i64 %and1.i264.3.2, %and.i262.3.2
  %mul3.i267.3.2 = mul nuw i64 %shr2.i265.3.2, %and.i262.3.2
  %mul4.i268.3.2 = mul nuw i64 %and1.i264.3.2, %shr.i263.3.2
  %mul5.i269.3.2 = mul nuw i64 %shr2.i265.3.2, %shr.i263.3.2
  %and6.i270.3.2 = and i64 %mul.i266.3.2, 4294967295
  %shr7.i271.3.2 = lshr i64 %mul.i266.3.2, 32
  %and8.i272.3.2 = and i64 %mul4.i268.3.2, 4294967295
  %and9.i273.3.2 = and i64 %mul3.i267.3.2, 4294967295
  %add.i274.3.2 = add nuw nsw i64 %shr7.i271.3.2, %and8.i272.3.2
  %add10.i275.3.2 = add nuw nsw i64 %add.i274.3.2, %and9.i273.3.2
  %shr11.i276.3.2 = lshr i64 %add10.i275.3.2, 32
  %shl.i277.3.2 = shl i64 %add10.i275.3.2, 32
  %xor68.i278.3.2 = or i64 %shl.i277.3.2, %and6.i270.3.2
  %shr13.i279.3.2 = lshr i64 %mul4.i268.3.2, 32
  %shr14.i280.3.2 = lshr i64 %mul3.i267.3.2, 32
  %and15.i281.3.2 = and i64 %mul5.i269.3.2, 4294967295
  %add16.i282.3.2 = add nuw nsw i64 %shr13.i279.3.2, %shr14.i280.3.2
  %add17.i283.3.2 = add nuw nsw i64 %add16.i282.3.2, %and15.i281.3.2
  %add18.i284.3.2 = add nuw nsw i64 %add17.i283.3.2, %shr11.i276.3.2
  %and19.i285.3.2 = and i64 %add18.i284.3.2, 4294967295
  %and21.i287.3.2 = and i64 %add18.i284.3.2, 30064771072
  %and22.i288.3.2 = and i64 %mul5.i269.3.2, -4294967296
  %add23.i289.3.2 = add i64 %and21.i287.3.2, %and22.i288.3.2
  %xor2569.i290.3.2 = or i64 %add23.i289.3.2, %and19.i285.3.2
  %add82.3.2 = add i64 %xor68.i278.3.2, %add82.3.1
  %xor.i256.3.2 = xor i64 %add82.3.2, %shl.i277.3.2
  %xor1.i257.3.2 = xor i64 %shl.i277.3.2, %add82.3.1
  %or.i258.3.2 = or i64 %xor.i256.3.2, %xor1.i257.3.2
  %xor2.i259.3.2 = xor i64 %or.i258.3.2, %add82.3.2
  %shr.i260.3.2 = lshr i64 %xor2.i259.3.2, 63
  %add89.3.2 = add i64 %shr.i260.3.2, %xor2569.i290.3.2
  %add90.3.2 = add i64 %add89.3.2, %add90.3.1
  %202 = xor i64 %add89.3.2, -9223372036854775808
  %xor2.i253.3.2 = and i64 %202, %add23.i289.3.2
  %xor.i244.3.2 = xor i64 %add90.3.2, %add89.3.2
  %xor1.i245.3.2 = xor i64 %add89.3.2, %add90.3.1
  %or.i246.3.2 = or i64 %xor.i244.3.2, %xor1.i245.3.2
  %xor2.i247.3.2 = xor i64 %or.i246.3.2, %add90.3.2
  %shr.i254326.3.2 = or i64 %xor2.i247.3.2, %xor2.i253.3.2
  %or94325.3.2 = lshr i64 %shr.i254326.3.2, 63
  %add96.3.2 = add nsw i64 %or94325.3.2, %add96.3.1
  %arrayidx74.3.3 = getelementptr inbounds i64, i64* %mc, i64 7
  %203 = load i64, i64* %arrayidx74.3.3, align 8, !tbaa !3
  %204 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.3.3 = and i64 %203, 4294967295
  %shr.i263.3.3 = lshr i64 %203, 32
  %and1.i264.3.3 = and i64 %204, 4294967295
  %shr2.i265.3.3 = lshr i64 %204, 32
  %mul.i266.3.3 = mul nuw i64 %and1.i264.3.3, %and.i262.3.3
  %mul3.i267.3.3 = mul nuw i64 %shr2.i265.3.3, %and.i262.3.3
  %mul4.i268.3.3 = mul nuw i64 %and1.i264.3.3, %shr.i263.3.3
  %mul5.i269.3.3 = mul nuw i64 %shr2.i265.3.3, %shr.i263.3.3
  %and6.i270.3.3 = and i64 %mul.i266.3.3, 4294967295
  %shr7.i271.3.3 = lshr i64 %mul.i266.3.3, 32
  %and8.i272.3.3 = and i64 %mul4.i268.3.3, 4294967295
  %and9.i273.3.3 = and i64 %mul3.i267.3.3, 4294967295
  %add.i274.3.3 = add nuw nsw i64 %shr7.i271.3.3, %and8.i272.3.3
  %add10.i275.3.3 = add nuw nsw i64 %add.i274.3.3, %and9.i273.3.3
  %shr11.i276.3.3 = lshr i64 %add10.i275.3.3, 32
  %shl.i277.3.3 = shl i64 %add10.i275.3.3, 32
  %xor68.i278.3.3 = or i64 %shl.i277.3.3, %and6.i270.3.3
  %shr13.i279.3.3 = lshr i64 %mul4.i268.3.3, 32
  %shr14.i280.3.3 = lshr i64 %mul3.i267.3.3, 32
  %and15.i281.3.3 = and i64 %mul5.i269.3.3, 4294967295
  %add16.i282.3.3 = add nuw nsw i64 %shr13.i279.3.3, %shr14.i280.3.3
  %add17.i283.3.3 = add nuw nsw i64 %add16.i282.3.3, %and15.i281.3.3
  %add18.i284.3.3 = add nuw nsw i64 %add17.i283.3.3, %shr11.i276.3.3
  %and19.i285.3.3 = and i64 %add18.i284.3.3, 4294967295
  %and21.i287.3.3 = and i64 %add18.i284.3.3, 30064771072
  %and22.i288.3.3 = and i64 %mul5.i269.3.3, -4294967296
  %add23.i289.3.3 = add i64 %and21.i287.3.3, %and22.i288.3.3
  %xor2569.i290.3.3 = or i64 %add23.i289.3.3, %and19.i285.3.3
  %add82.3.3 = add i64 %xor68.i278.3.3, %add82.3.2
  %xor.i256.3.3 = xor i64 %add82.3.3, %shl.i277.3.3
  %xor1.i257.3.3 = xor i64 %shl.i277.3.3, %add82.3.2
  %or.i258.3.3 = or i64 %xor.i256.3.3, %xor1.i257.3.3
  %xor2.i259.3.3 = xor i64 %or.i258.3.3, %add82.3.3
  %shr.i260.3.3 = lshr i64 %xor2.i259.3.3, 63
  %add89.3.3 = add i64 %shr.i260.3.3, %xor2569.i290.3.3
  %add90.3.3 = add i64 %add89.3.3, %add90.3.2
  %205 = xor i64 %add89.3.3, -9223372036854775808
  %xor2.i253.3.3 = and i64 %205, %add23.i289.3.3
  %xor.i244.3.3 = xor i64 %add90.3.3, %add89.3.3
  %xor1.i245.3.3 = xor i64 %add89.3.3, %add90.3.2
  %or.i246.3.3 = or i64 %xor.i244.3.3, %xor1.i245.3.3
  %xor2.i247.3.3 = xor i64 %or.i246.3.3, %add90.3.3
  %shr.i254326.3.3 = or i64 %xor2.i247.3.3, %xor2.i253.3.3
  %or94325.3.3 = lshr i64 %shr.i254326.3.3, 63
  %add96.3.3 = add nsw i64 %or94325.3.3, %add96.3.2
  %arrayidx74.3.4 = getelementptr inbounds i64, i64* %mc, i64 8
  %206 = load i64, i64* %arrayidx74.3.4, align 8, !tbaa !3
  %207 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.3.4 = and i64 %206, 4294967295
  %shr.i263.3.4 = lshr i64 %206, 32
  %and1.i264.3.4 = and i64 %207, 4294967295
  %shr2.i265.3.4 = lshr i64 %207, 32
  %mul.i266.3.4 = mul nuw i64 %and1.i264.3.4, %and.i262.3.4
  %mul3.i267.3.4 = mul nuw i64 %shr2.i265.3.4, %and.i262.3.4
  %mul4.i268.3.4 = mul nuw i64 %and1.i264.3.4, %shr.i263.3.4
  %mul5.i269.3.4 = mul nuw i64 %shr2.i265.3.4, %shr.i263.3.4
  %and6.i270.3.4 = and i64 %mul.i266.3.4, 4294967295
  %shr7.i271.3.4 = lshr i64 %mul.i266.3.4, 32
  %and8.i272.3.4 = and i64 %mul4.i268.3.4, 4294967295
  %and9.i273.3.4 = and i64 %mul3.i267.3.4, 4294967295
  %add.i274.3.4 = add nuw nsw i64 %shr7.i271.3.4, %and8.i272.3.4
  %add10.i275.3.4 = add nuw nsw i64 %add.i274.3.4, %and9.i273.3.4
  %shr11.i276.3.4 = lshr i64 %add10.i275.3.4, 32
  %shl.i277.3.4 = shl i64 %add10.i275.3.4, 32
  %xor68.i278.3.4 = or i64 %shl.i277.3.4, %and6.i270.3.4
  %shr13.i279.3.4 = lshr i64 %mul4.i268.3.4, 32
  %shr14.i280.3.4 = lshr i64 %mul3.i267.3.4, 32
  %and15.i281.3.4 = and i64 %mul5.i269.3.4, 4294967295
  %add16.i282.3.4 = add nuw nsw i64 %shr13.i279.3.4, %shr14.i280.3.4
  %add17.i283.3.4 = add nuw nsw i64 %add16.i282.3.4, %and15.i281.3.4
  %add18.i284.3.4 = add nuw nsw i64 %add17.i283.3.4, %shr11.i276.3.4
  %and19.i285.3.4 = and i64 %add18.i284.3.4, 4294967295
  %and21.i287.3.4 = and i64 %add18.i284.3.4, 30064771072
  %and22.i288.3.4 = and i64 %mul5.i269.3.4, -4294967296
  %add23.i289.3.4 = add i64 %and21.i287.3.4, %and22.i288.3.4
  %xor2569.i290.3.4 = or i64 %add23.i289.3.4, %and19.i285.3.4
  %add82.3.4 = add i64 %xor68.i278.3.4, %add82.3.3
  %xor.i256.3.4 = xor i64 %add82.3.4, %shl.i277.3.4
  %xor1.i257.3.4 = xor i64 %shl.i277.3.4, %add82.3.3
  %or.i258.3.4 = or i64 %xor.i256.3.4, %xor1.i257.3.4
  %xor2.i259.3.4 = xor i64 %or.i258.3.4, %add82.3.4
  %shr.i260.3.4 = lshr i64 %xor2.i259.3.4, 63
  %add89.3.4 = add i64 %shr.i260.3.4, %xor2569.i290.3.4
  %add90.3.4 = add i64 %add89.3.4, %add90.3.3
  %208 = xor i64 %add89.3.4, -9223372036854775808
  %xor2.i253.3.4 = and i64 %208, %add23.i289.3.4
  %xor.i244.3.4 = xor i64 %add90.3.4, %add89.3.4
  %xor1.i245.3.4 = xor i64 %add89.3.4, %add90.3.3
  %or.i246.3.4 = or i64 %xor.i244.3.4, %xor1.i245.3.4
  %xor2.i247.3.4 = xor i64 %or.i246.3.4, %add90.3.4
  %shr.i254326.3.4 = or i64 %xor2.i247.3.4, %xor2.i253.3.4
  %or94325.3.4 = lshr i64 %shr.i254326.3.4, 63
  %add96.3.4 = add i64 %or94325.3.4, %add96.3.3
  %arrayidx74.3.5 = getelementptr inbounds i64, i64* %mc, i64 9
  %209 = load i64, i64* %arrayidx74.3.5, align 8, !tbaa !3
  %210 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.3.5 = and i64 %209, 4294967295
  %shr.i263.3.5 = lshr i64 %209, 32
  %and1.i264.3.5 = and i64 %210, 4294967295
  %shr2.i265.3.5 = lshr i64 %210, 32
  %mul.i266.3.5 = mul nuw i64 %and1.i264.3.5, %and.i262.3.5
  %mul3.i267.3.5 = mul nuw i64 %shr2.i265.3.5, %and.i262.3.5
  %mul4.i268.3.5 = mul nuw i64 %and1.i264.3.5, %shr.i263.3.5
  %mul5.i269.3.5 = mul nuw i64 %shr2.i265.3.5, %shr.i263.3.5
  %and6.i270.3.5 = and i64 %mul.i266.3.5, 4294967295
  %shr7.i271.3.5 = lshr i64 %mul.i266.3.5, 32
  %and8.i272.3.5 = and i64 %mul4.i268.3.5, 4294967295
  %and9.i273.3.5 = and i64 %mul3.i267.3.5, 4294967295
  %add.i274.3.5 = add nuw nsw i64 %shr7.i271.3.5, %and8.i272.3.5
  %add10.i275.3.5 = add nuw nsw i64 %add.i274.3.5, %and9.i273.3.5
  %shr11.i276.3.5 = lshr i64 %add10.i275.3.5, 32
  %shl.i277.3.5 = shl i64 %add10.i275.3.5, 32
  %xor68.i278.3.5 = or i64 %shl.i277.3.5, %and6.i270.3.5
  %shr13.i279.3.5 = lshr i64 %mul4.i268.3.5, 32
  %shr14.i280.3.5 = lshr i64 %mul3.i267.3.5, 32
  %and15.i281.3.5 = and i64 %mul5.i269.3.5, 4294967295
  %add16.i282.3.5 = add nuw nsw i64 %shr13.i279.3.5, %shr14.i280.3.5
  %add17.i283.3.5 = add nuw nsw i64 %add16.i282.3.5, %and15.i281.3.5
  %add18.i284.3.5 = add nuw nsw i64 %add17.i283.3.5, %shr11.i276.3.5
  %and19.i285.3.5 = and i64 %add18.i284.3.5, 4294967295
  %and21.i287.3.5 = and i64 %add18.i284.3.5, 30064771072
  %and22.i288.3.5 = and i64 %mul5.i269.3.5, -4294967296
  %add23.i289.3.5 = add i64 %and21.i287.3.5, %and22.i288.3.5
  %xor2569.i290.3.5 = or i64 %add23.i289.3.5, %and19.i285.3.5
  %add82.3.5 = add i64 %xor68.i278.3.5, %add82.3.4
  %xor.i256.3.5 = xor i64 %add82.3.5, %shl.i277.3.5
  %xor1.i257.3.5 = xor i64 %shl.i277.3.5, %add82.3.4
  %or.i258.3.5 = or i64 %xor.i256.3.5, %xor1.i257.3.5
  %xor2.i259.3.5 = xor i64 %or.i258.3.5, %add82.3.5
  %shr.i260.3.5 = lshr i64 %xor2.i259.3.5, 63
  %add89.3.5 = add i64 %shr.i260.3.5, %xor2569.i290.3.5
  %add90.3.5 = add i64 %add89.3.5, %add90.3.4
  %211 = xor i64 %add89.3.5, -9223372036854775808
  %xor2.i253.3.5 = and i64 %211, %add23.i289.3.5
  %xor.i244.3.5 = xor i64 %add90.3.5, %add89.3.5
  %xor1.i245.3.5 = xor i64 %add89.3.5, %add90.3.4
  %or.i246.3.5 = or i64 %xor.i244.3.5, %xor1.i245.3.5
  %xor2.i247.3.5 = xor i64 %or.i246.3.5, %add90.3.5
  %shr.i254326.3.5 = or i64 %xor2.i247.3.5, %xor2.i253.3.5
  %or94325.3.5 = lshr i64 %shr.i254326.3.5, 63
  %add96.3.5 = add i64 %or94325.3.5, %add96.3.4
  %arrayidx74.3.6 = getelementptr inbounds i64, i64* %mc, i64 10
  %212 = load i64, i64* %arrayidx74.3.6, align 8, !tbaa !3
  %213 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.3.6 = and i64 %212, 4294967295
  %shr.i263.3.6 = lshr i64 %212, 32
  %and1.i264.3.6 = and i64 %213, 4294967295
  %shr2.i265.3.6 = lshr i64 %213, 32
  %mul.i266.3.6 = mul nuw i64 %and1.i264.3.6, %and.i262.3.6
  %mul3.i267.3.6 = mul nuw i64 %shr2.i265.3.6, %and.i262.3.6
  %mul4.i268.3.6 = mul nuw i64 %and1.i264.3.6, %shr.i263.3.6
  %mul5.i269.3.6 = mul nuw i64 %shr2.i265.3.6, %shr.i263.3.6
  %and6.i270.3.6 = and i64 %mul.i266.3.6, 4294967295
  %shr7.i271.3.6 = lshr i64 %mul.i266.3.6, 32
  %and8.i272.3.6 = and i64 %mul4.i268.3.6, 4294967295
  %and9.i273.3.6 = and i64 %mul3.i267.3.6, 4294967295
  %add.i274.3.6 = add nuw nsw i64 %shr7.i271.3.6, %and8.i272.3.6
  %add10.i275.3.6 = add nuw nsw i64 %add.i274.3.6, %and9.i273.3.6
  %shr11.i276.3.6 = lshr i64 %add10.i275.3.6, 32
  %shl.i277.3.6 = shl i64 %add10.i275.3.6, 32
  %xor68.i278.3.6 = or i64 %shl.i277.3.6, %and6.i270.3.6
  %shr13.i279.3.6 = lshr i64 %mul4.i268.3.6, 32
  %shr14.i280.3.6 = lshr i64 %mul3.i267.3.6, 32
  %and15.i281.3.6 = and i64 %mul5.i269.3.6, 4294967295
  %add16.i282.3.6 = add nuw nsw i64 %shr13.i279.3.6, %shr14.i280.3.6
  %add17.i283.3.6 = add nuw nsw i64 %add16.i282.3.6, %and15.i281.3.6
  %add18.i284.3.6 = add nuw nsw i64 %add17.i283.3.6, %shr11.i276.3.6
  %and19.i285.3.6 = and i64 %add18.i284.3.6, 4294967295
  %and21.i287.3.6 = and i64 %add18.i284.3.6, 30064771072
  %and22.i288.3.6 = and i64 %mul5.i269.3.6, -4294967296
  %add23.i289.3.6 = add i64 %and21.i287.3.6, %and22.i288.3.6
  %xor2569.i290.3.6 = or i64 %add23.i289.3.6, %and19.i285.3.6
  %add82.3.6 = add i64 %xor68.i278.3.6, %add82.3.5
  %xor.i256.3.6 = xor i64 %add82.3.6, %shl.i277.3.6
  %xor1.i257.3.6 = xor i64 %shl.i277.3.6, %add82.3.5
  %or.i258.3.6 = or i64 %xor.i256.3.6, %xor1.i257.3.6
  %xor2.i259.3.6 = xor i64 %or.i258.3.6, %add82.3.6
  %shr.i260.3.6 = lshr i64 %xor2.i259.3.6, 63
  %add89.3.6 = add i64 %shr.i260.3.6, %xor2569.i290.3.6
  %add90.3.6 = add i64 %add89.3.6, %add90.3.5
  %214 = xor i64 %add89.3.6, -9223372036854775808
  %xor2.i253.3.6 = and i64 %214, %add23.i289.3.6
  %xor.i244.3.6 = xor i64 %add90.3.6, %add89.3.6
  %xor1.i245.3.6 = xor i64 %add89.3.6, %add90.3.5
  %or.i246.3.6 = or i64 %xor.i244.3.6, %xor1.i245.3.6
  %xor2.i247.3.6 = xor i64 %or.i246.3.6, %add90.3.6
  %shr.i254326.3.6 = or i64 %xor2.i247.3.6, %xor2.i253.3.6
  %or94325.3.6 = lshr i64 %shr.i254326.3.6, 63
  %add96.3.6 = add i64 %or94325.3.6, %add96.3.5
  %arrayidx104.3 = getelementptr inbounds i64, i64* %ma, i64 15
  %215 = load i64, i64* %arrayidx104.3, align 8, !tbaa !3
  %add105.3 = add i64 %215, %add82.3.6
  %xor.i238.3 = xor i64 %add105.3, %add82.3.6
  %xor1.i239.3 = xor i64 %215, %add82.3.6
  %or.i240.3 = or i64 %xor.i238.3, %xor1.i239.3
  %xor2.i241.3 = xor i64 %or.i240.3, %add105.3
  %shr.i242.3 = lshr i64 %xor2.i241.3, 63
  %add111.3 = add i64 %shr.i242.3, %add90.3.6
  store i64 %add105.3, i64* %arrayidx50.3, align 8, !tbaa !3
  %216 = xor i64 %add111.3, -9223372036854775808
  %xor2.i.3 = and i64 %216, %add90.3.6
  %shr.i237.3 = lshr i64 %xor2.i.3, 63
  %add118.3 = add i64 %shr.i237.3, %add96.3.6
  %arrayidx74.4 = getelementptr inbounds i64, i64* %mc, i64 5
  %217 = load i64, i64* %arrayidx74.4, align 8, !tbaa !3
  %218 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.4 = and i64 %217, 4294967295
  %shr.i263.4 = lshr i64 %217, 32
  %and1.i264.4 = and i64 %218, 4294967295
  %shr2.i265.4 = lshr i64 %218, 32
  %mul.i266.4 = mul nuw i64 %and1.i264.4, %and.i262.4
  %mul3.i267.4 = mul nuw i64 %shr2.i265.4, %and.i262.4
  %mul4.i268.4 = mul nuw i64 %and1.i264.4, %shr.i263.4
  %mul5.i269.4 = mul nuw i64 %shr2.i265.4, %shr.i263.4
  %and6.i270.4 = and i64 %mul.i266.4, 4294967295
  %shr7.i271.4 = lshr i64 %mul.i266.4, 32
  %and8.i272.4 = and i64 %mul4.i268.4, 4294967295
  %and9.i273.4 = and i64 %mul3.i267.4, 4294967295
  %add.i274.4 = add nuw nsw i64 %shr7.i271.4, %and8.i272.4
  %add10.i275.4 = add nuw nsw i64 %add.i274.4, %and9.i273.4
  %shr11.i276.4 = lshr i64 %add10.i275.4, 32
  %shl.i277.4 = shl i64 %add10.i275.4, 32
  %xor68.i278.4 = or i64 %shl.i277.4, %and6.i270.4
  %shr13.i279.4 = lshr i64 %mul4.i268.4, 32
  %shr14.i280.4 = lshr i64 %mul3.i267.4, 32
  %and15.i281.4 = and i64 %mul5.i269.4, 4294967295
  %add16.i282.4 = add nuw nsw i64 %shr13.i279.4, %shr14.i280.4
  %add17.i283.4 = add nuw nsw i64 %add16.i282.4, %and15.i281.4
  %add18.i284.4 = add nuw nsw i64 %add17.i283.4, %shr11.i276.4
  %and19.i285.4 = and i64 %add18.i284.4, 4294967295
  %and21.i287.4 = and i64 %add18.i284.4, 30064771072
  %and22.i288.4 = and i64 %mul5.i269.4, -4294967296
  %add23.i289.4 = add i64 %and21.i287.4, %and22.i288.4
  %xor2569.i290.4 = or i64 %add23.i289.4, %and19.i285.4
  %add82.4 = add i64 %xor68.i278.4, %add111.3
  %xor.i256.4 = xor i64 %add82.4, %shl.i277.4
  %xor1.i257.4 = xor i64 %shl.i277.4, %add111.3
  %or.i258.4 = or i64 %xor.i256.4, %xor1.i257.4
  %xor2.i259.4 = xor i64 %or.i258.4, %add82.4
  %shr.i260.4 = lshr i64 %xor2.i259.4, 63
  %add89.4 = add i64 %shr.i260.4, %xor2569.i290.4
  %add90.4 = add i64 %add89.4, %add118.3
  %219 = xor i64 %add89.4, -9223372036854775808
  %xor2.i253.4 = and i64 %219, %add23.i289.4
  %xor.i244.4 = xor i64 %add90.4, %add89.4
  %xor1.i245.4 = xor i64 %add89.4, %add118.3
  %or.i246.4 = or i64 %xor.i244.4, %xor1.i245.4
  %xor2.i247.4 = xor i64 %or.i246.4, %add90.4
  %shr.i254326.4 = or i64 %xor2.i247.4, %xor2.i253.4
  %or94325.4 = lshr i64 %shr.i254326.4, 63
  %arrayidx74.4.1 = getelementptr inbounds i64, i64* %mc, i64 6
  %220 = load i64, i64* %arrayidx74.4.1, align 8, !tbaa !3
  %221 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.4.1 = and i64 %220, 4294967295
  %shr.i263.4.1 = lshr i64 %220, 32
  %and1.i264.4.1 = and i64 %221, 4294967295
  %shr2.i265.4.1 = lshr i64 %221, 32
  %mul.i266.4.1 = mul nuw i64 %and1.i264.4.1, %and.i262.4.1
  %mul3.i267.4.1 = mul nuw i64 %shr2.i265.4.1, %and.i262.4.1
  %mul4.i268.4.1 = mul nuw i64 %and1.i264.4.1, %shr.i263.4.1
  %mul5.i269.4.1 = mul nuw i64 %shr2.i265.4.1, %shr.i263.4.1
  %and6.i270.4.1 = and i64 %mul.i266.4.1, 4294967295
  %shr7.i271.4.1 = lshr i64 %mul.i266.4.1, 32
  %and8.i272.4.1 = and i64 %mul4.i268.4.1, 4294967295
  %and9.i273.4.1 = and i64 %mul3.i267.4.1, 4294967295
  %add.i274.4.1 = add nuw nsw i64 %shr7.i271.4.1, %and8.i272.4.1
  %add10.i275.4.1 = add nuw nsw i64 %add.i274.4.1, %and9.i273.4.1
  %shr11.i276.4.1 = lshr i64 %add10.i275.4.1, 32
  %shl.i277.4.1 = shl i64 %add10.i275.4.1, 32
  %xor68.i278.4.1 = or i64 %shl.i277.4.1, %and6.i270.4.1
  %shr13.i279.4.1 = lshr i64 %mul4.i268.4.1, 32
  %shr14.i280.4.1 = lshr i64 %mul3.i267.4.1, 32
  %and15.i281.4.1 = and i64 %mul5.i269.4.1, 4294967295
  %add16.i282.4.1 = add nuw nsw i64 %shr13.i279.4.1, %shr14.i280.4.1
  %add17.i283.4.1 = add nuw nsw i64 %add16.i282.4.1, %and15.i281.4.1
  %add18.i284.4.1 = add nuw nsw i64 %add17.i283.4.1, %shr11.i276.4.1
  %and19.i285.4.1 = and i64 %add18.i284.4.1, 4294967295
  %and21.i287.4.1 = and i64 %add18.i284.4.1, 30064771072
  %and22.i288.4.1 = and i64 %mul5.i269.4.1, -4294967296
  %add23.i289.4.1 = add i64 %and21.i287.4.1, %and22.i288.4.1
  %xor2569.i290.4.1 = or i64 %add23.i289.4.1, %and19.i285.4.1
  %add82.4.1 = add i64 %xor68.i278.4.1, %add82.4
  %xor.i256.4.1 = xor i64 %add82.4.1, %shl.i277.4.1
  %xor1.i257.4.1 = xor i64 %shl.i277.4.1, %add82.4
  %or.i258.4.1 = or i64 %xor.i256.4.1, %xor1.i257.4.1
  %xor2.i259.4.1 = xor i64 %or.i258.4.1, %add82.4.1
  %shr.i260.4.1 = lshr i64 %xor2.i259.4.1, 63
  %add89.4.1 = add i64 %shr.i260.4.1, %xor2569.i290.4.1
  %add90.4.1 = add i64 %add89.4.1, %add90.4
  %222 = xor i64 %add89.4.1, -9223372036854775808
  %xor2.i253.4.1 = and i64 %222, %add23.i289.4.1
  %xor.i244.4.1 = xor i64 %add90.4.1, %add89.4.1
  %xor1.i245.4.1 = xor i64 %add89.4.1, %add90.4
  %or.i246.4.1 = or i64 %xor.i244.4.1, %xor1.i245.4.1
  %xor2.i247.4.1 = xor i64 %or.i246.4.1, %add90.4.1
  %shr.i254326.4.1 = or i64 %xor2.i247.4.1, %xor2.i253.4.1
  %or94325.4.1 = lshr i64 %shr.i254326.4.1, 63
  %add96.4.1 = add nuw nsw i64 %or94325.4.1, %or94325.4
  %arrayidx74.4.2 = getelementptr inbounds i64, i64* %mc, i64 7
  %223 = load i64, i64* %arrayidx74.4.2, align 8, !tbaa !3
  %224 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.4.2 = and i64 %223, 4294967295
  %shr.i263.4.2 = lshr i64 %223, 32
  %and1.i264.4.2 = and i64 %224, 4294967295
  %shr2.i265.4.2 = lshr i64 %224, 32
  %mul.i266.4.2 = mul nuw i64 %and1.i264.4.2, %and.i262.4.2
  %mul3.i267.4.2 = mul nuw i64 %shr2.i265.4.2, %and.i262.4.2
  %mul4.i268.4.2 = mul nuw i64 %and1.i264.4.2, %shr.i263.4.2
  %mul5.i269.4.2 = mul nuw i64 %shr2.i265.4.2, %shr.i263.4.2
  %and6.i270.4.2 = and i64 %mul.i266.4.2, 4294967295
  %shr7.i271.4.2 = lshr i64 %mul.i266.4.2, 32
  %and8.i272.4.2 = and i64 %mul4.i268.4.2, 4294967295
  %and9.i273.4.2 = and i64 %mul3.i267.4.2, 4294967295
  %add.i274.4.2 = add nuw nsw i64 %shr7.i271.4.2, %and8.i272.4.2
  %add10.i275.4.2 = add nuw nsw i64 %add.i274.4.2, %and9.i273.4.2
  %shr11.i276.4.2 = lshr i64 %add10.i275.4.2, 32
  %shl.i277.4.2 = shl i64 %add10.i275.4.2, 32
  %xor68.i278.4.2 = or i64 %shl.i277.4.2, %and6.i270.4.2
  %shr13.i279.4.2 = lshr i64 %mul4.i268.4.2, 32
  %shr14.i280.4.2 = lshr i64 %mul3.i267.4.2, 32
  %and15.i281.4.2 = and i64 %mul5.i269.4.2, 4294967295
  %add16.i282.4.2 = add nuw nsw i64 %shr13.i279.4.2, %shr14.i280.4.2
  %add17.i283.4.2 = add nuw nsw i64 %add16.i282.4.2, %and15.i281.4.2
  %add18.i284.4.2 = add nuw nsw i64 %add17.i283.4.2, %shr11.i276.4.2
  %and19.i285.4.2 = and i64 %add18.i284.4.2, 4294967295
  %and21.i287.4.2 = and i64 %add18.i284.4.2, 30064771072
  %and22.i288.4.2 = and i64 %mul5.i269.4.2, -4294967296
  %add23.i289.4.2 = add i64 %and21.i287.4.2, %and22.i288.4.2
  %xor2569.i290.4.2 = or i64 %add23.i289.4.2, %and19.i285.4.2
  %add82.4.2 = add i64 %xor68.i278.4.2, %add82.4.1
  %xor.i256.4.2 = xor i64 %add82.4.2, %shl.i277.4.2
  %xor1.i257.4.2 = xor i64 %shl.i277.4.2, %add82.4.1
  %or.i258.4.2 = or i64 %xor.i256.4.2, %xor1.i257.4.2
  %xor2.i259.4.2 = xor i64 %or.i258.4.2, %add82.4.2
  %shr.i260.4.2 = lshr i64 %xor2.i259.4.2, 63
  %add89.4.2 = add i64 %shr.i260.4.2, %xor2569.i290.4.2
  %add90.4.2 = add i64 %add89.4.2, %add90.4.1
  %225 = xor i64 %add89.4.2, -9223372036854775808
  %xor2.i253.4.2 = and i64 %225, %add23.i289.4.2
  %xor.i244.4.2 = xor i64 %add90.4.2, %add89.4.2
  %xor1.i245.4.2 = xor i64 %add89.4.2, %add90.4.1
  %or.i246.4.2 = or i64 %xor.i244.4.2, %xor1.i245.4.2
  %xor2.i247.4.2 = xor i64 %or.i246.4.2, %add90.4.2
  %shr.i254326.4.2 = or i64 %xor2.i247.4.2, %xor2.i253.4.2
  %or94325.4.2 = lshr i64 %shr.i254326.4.2, 63
  %add96.4.2 = add nsw i64 %or94325.4.2, %add96.4.1
  %arrayidx74.4.3 = getelementptr inbounds i64, i64* %mc, i64 8
  %226 = load i64, i64* %arrayidx74.4.3, align 8, !tbaa !3
  %227 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.4.3 = and i64 %226, 4294967295
  %shr.i263.4.3 = lshr i64 %226, 32
  %and1.i264.4.3 = and i64 %227, 4294967295
  %shr2.i265.4.3 = lshr i64 %227, 32
  %mul.i266.4.3 = mul nuw i64 %and1.i264.4.3, %and.i262.4.3
  %mul3.i267.4.3 = mul nuw i64 %shr2.i265.4.3, %and.i262.4.3
  %mul4.i268.4.3 = mul nuw i64 %and1.i264.4.3, %shr.i263.4.3
  %mul5.i269.4.3 = mul nuw i64 %shr2.i265.4.3, %shr.i263.4.3
  %and6.i270.4.3 = and i64 %mul.i266.4.3, 4294967295
  %shr7.i271.4.3 = lshr i64 %mul.i266.4.3, 32
  %and8.i272.4.3 = and i64 %mul4.i268.4.3, 4294967295
  %and9.i273.4.3 = and i64 %mul3.i267.4.3, 4294967295
  %add.i274.4.3 = add nuw nsw i64 %shr7.i271.4.3, %and8.i272.4.3
  %add10.i275.4.3 = add nuw nsw i64 %add.i274.4.3, %and9.i273.4.3
  %shr11.i276.4.3 = lshr i64 %add10.i275.4.3, 32
  %shl.i277.4.3 = shl i64 %add10.i275.4.3, 32
  %xor68.i278.4.3 = or i64 %shl.i277.4.3, %and6.i270.4.3
  %shr13.i279.4.3 = lshr i64 %mul4.i268.4.3, 32
  %shr14.i280.4.3 = lshr i64 %mul3.i267.4.3, 32
  %and15.i281.4.3 = and i64 %mul5.i269.4.3, 4294967295
  %add16.i282.4.3 = add nuw nsw i64 %shr13.i279.4.3, %shr14.i280.4.3
  %add17.i283.4.3 = add nuw nsw i64 %add16.i282.4.3, %and15.i281.4.3
  %add18.i284.4.3 = add nuw nsw i64 %add17.i283.4.3, %shr11.i276.4.3
  %and19.i285.4.3 = and i64 %add18.i284.4.3, 4294967295
  %and21.i287.4.3 = and i64 %add18.i284.4.3, 30064771072
  %and22.i288.4.3 = and i64 %mul5.i269.4.3, -4294967296
  %add23.i289.4.3 = add i64 %and21.i287.4.3, %and22.i288.4.3
  %xor2569.i290.4.3 = or i64 %add23.i289.4.3, %and19.i285.4.3
  %add82.4.3 = add i64 %xor68.i278.4.3, %add82.4.2
  %xor.i256.4.3 = xor i64 %add82.4.3, %shl.i277.4.3
  %xor1.i257.4.3 = xor i64 %shl.i277.4.3, %add82.4.2
  %or.i258.4.3 = or i64 %xor.i256.4.3, %xor1.i257.4.3
  %xor2.i259.4.3 = xor i64 %or.i258.4.3, %add82.4.3
  %shr.i260.4.3 = lshr i64 %xor2.i259.4.3, 63
  %add89.4.3 = add i64 %shr.i260.4.3, %xor2569.i290.4.3
  %add90.4.3 = add i64 %add89.4.3, %add90.4.2
  %228 = xor i64 %add89.4.3, -9223372036854775808
  %xor2.i253.4.3 = and i64 %228, %add23.i289.4.3
  %xor.i244.4.3 = xor i64 %add90.4.3, %add89.4.3
  %xor1.i245.4.3 = xor i64 %add89.4.3, %add90.4.2
  %or.i246.4.3 = or i64 %xor.i244.4.3, %xor1.i245.4.3
  %xor2.i247.4.3 = xor i64 %or.i246.4.3, %add90.4.3
  %shr.i254326.4.3 = or i64 %xor2.i247.4.3, %xor2.i253.4.3
  %or94325.4.3 = lshr i64 %shr.i254326.4.3, 63
  %add96.4.3 = add nsw i64 %or94325.4.3, %add96.4.2
  %arrayidx74.4.4 = getelementptr inbounds i64, i64* %mc, i64 9
  %229 = load i64, i64* %arrayidx74.4.4, align 8, !tbaa !3
  %230 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.4.4 = and i64 %229, 4294967295
  %shr.i263.4.4 = lshr i64 %229, 32
  %and1.i264.4.4 = and i64 %230, 4294967295
  %shr2.i265.4.4 = lshr i64 %230, 32
  %mul.i266.4.4 = mul nuw i64 %and1.i264.4.4, %and.i262.4.4
  %mul3.i267.4.4 = mul nuw i64 %shr2.i265.4.4, %and.i262.4.4
  %mul4.i268.4.4 = mul nuw i64 %and1.i264.4.4, %shr.i263.4.4
  %mul5.i269.4.4 = mul nuw i64 %shr2.i265.4.4, %shr.i263.4.4
  %and6.i270.4.4 = and i64 %mul.i266.4.4, 4294967295
  %shr7.i271.4.4 = lshr i64 %mul.i266.4.4, 32
  %and8.i272.4.4 = and i64 %mul4.i268.4.4, 4294967295
  %and9.i273.4.4 = and i64 %mul3.i267.4.4, 4294967295
  %add.i274.4.4 = add nuw nsw i64 %shr7.i271.4.4, %and8.i272.4.4
  %add10.i275.4.4 = add nuw nsw i64 %add.i274.4.4, %and9.i273.4.4
  %shr11.i276.4.4 = lshr i64 %add10.i275.4.4, 32
  %shl.i277.4.4 = shl i64 %add10.i275.4.4, 32
  %xor68.i278.4.4 = or i64 %shl.i277.4.4, %and6.i270.4.4
  %shr13.i279.4.4 = lshr i64 %mul4.i268.4.4, 32
  %shr14.i280.4.4 = lshr i64 %mul3.i267.4.4, 32
  %and15.i281.4.4 = and i64 %mul5.i269.4.4, 4294967295
  %add16.i282.4.4 = add nuw nsw i64 %shr13.i279.4.4, %shr14.i280.4.4
  %add17.i283.4.4 = add nuw nsw i64 %add16.i282.4.4, %and15.i281.4.4
  %add18.i284.4.4 = add nuw nsw i64 %add17.i283.4.4, %shr11.i276.4.4
  %and19.i285.4.4 = and i64 %add18.i284.4.4, 4294967295
  %and21.i287.4.4 = and i64 %add18.i284.4.4, 30064771072
  %and22.i288.4.4 = and i64 %mul5.i269.4.4, -4294967296
  %add23.i289.4.4 = add i64 %and21.i287.4.4, %and22.i288.4.4
  %xor2569.i290.4.4 = or i64 %add23.i289.4.4, %and19.i285.4.4
  %add82.4.4 = add i64 %xor68.i278.4.4, %add82.4.3
  %xor.i256.4.4 = xor i64 %add82.4.4, %shl.i277.4.4
  %xor1.i257.4.4 = xor i64 %shl.i277.4.4, %add82.4.3
  %or.i258.4.4 = or i64 %xor.i256.4.4, %xor1.i257.4.4
  %xor2.i259.4.4 = xor i64 %or.i258.4.4, %add82.4.4
  %shr.i260.4.4 = lshr i64 %xor2.i259.4.4, 63
  %add89.4.4 = add i64 %shr.i260.4.4, %xor2569.i290.4.4
  %add90.4.4 = add i64 %add89.4.4, %add90.4.3
  %231 = xor i64 %add89.4.4, -9223372036854775808
  %xor2.i253.4.4 = and i64 %231, %add23.i289.4.4
  %xor.i244.4.4 = xor i64 %add90.4.4, %add89.4.4
  %xor1.i245.4.4 = xor i64 %add89.4.4, %add90.4.3
  %or.i246.4.4 = or i64 %xor.i244.4.4, %xor1.i245.4.4
  %xor2.i247.4.4 = xor i64 %or.i246.4.4, %add90.4.4
  %shr.i254326.4.4 = or i64 %xor2.i247.4.4, %xor2.i253.4.4
  %or94325.4.4 = lshr i64 %shr.i254326.4.4, 63
  %add96.4.4 = add i64 %or94325.4.4, %add96.4.3
  %arrayidx74.4.5 = getelementptr inbounds i64, i64* %mc, i64 10
  %232 = load i64, i64* %arrayidx74.4.5, align 8, !tbaa !3
  %233 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.4.5 = and i64 %232, 4294967295
  %shr.i263.4.5 = lshr i64 %232, 32
  %and1.i264.4.5 = and i64 %233, 4294967295
  %shr2.i265.4.5 = lshr i64 %233, 32
  %mul.i266.4.5 = mul nuw i64 %and1.i264.4.5, %and.i262.4.5
  %mul3.i267.4.5 = mul nuw i64 %shr2.i265.4.5, %and.i262.4.5
  %mul4.i268.4.5 = mul nuw i64 %and1.i264.4.5, %shr.i263.4.5
  %mul5.i269.4.5 = mul nuw i64 %shr2.i265.4.5, %shr.i263.4.5
  %and6.i270.4.5 = and i64 %mul.i266.4.5, 4294967295
  %shr7.i271.4.5 = lshr i64 %mul.i266.4.5, 32
  %and8.i272.4.5 = and i64 %mul4.i268.4.5, 4294967295
  %and9.i273.4.5 = and i64 %mul3.i267.4.5, 4294967295
  %add.i274.4.5 = add nuw nsw i64 %shr7.i271.4.5, %and8.i272.4.5
  %add10.i275.4.5 = add nuw nsw i64 %add.i274.4.5, %and9.i273.4.5
  %shr11.i276.4.5 = lshr i64 %add10.i275.4.5, 32
  %shl.i277.4.5 = shl i64 %add10.i275.4.5, 32
  %xor68.i278.4.5 = or i64 %shl.i277.4.5, %and6.i270.4.5
  %shr13.i279.4.5 = lshr i64 %mul4.i268.4.5, 32
  %shr14.i280.4.5 = lshr i64 %mul3.i267.4.5, 32
  %and15.i281.4.5 = and i64 %mul5.i269.4.5, 4294967295
  %add16.i282.4.5 = add nuw nsw i64 %shr13.i279.4.5, %shr14.i280.4.5
  %add17.i283.4.5 = add nuw nsw i64 %add16.i282.4.5, %and15.i281.4.5
  %add18.i284.4.5 = add nuw nsw i64 %add17.i283.4.5, %shr11.i276.4.5
  %and19.i285.4.5 = and i64 %add18.i284.4.5, 4294967295
  %and21.i287.4.5 = and i64 %add18.i284.4.5, 30064771072
  %and22.i288.4.5 = and i64 %mul5.i269.4.5, -4294967296
  %add23.i289.4.5 = add i64 %and21.i287.4.5, %and22.i288.4.5
  %xor2569.i290.4.5 = or i64 %add23.i289.4.5, %and19.i285.4.5
  %add82.4.5 = add i64 %xor68.i278.4.5, %add82.4.4
  %xor.i256.4.5 = xor i64 %add82.4.5, %shl.i277.4.5
  %xor1.i257.4.5 = xor i64 %shl.i277.4.5, %add82.4.4
  %or.i258.4.5 = or i64 %xor.i256.4.5, %xor1.i257.4.5
  %xor2.i259.4.5 = xor i64 %or.i258.4.5, %add82.4.5
  %shr.i260.4.5 = lshr i64 %xor2.i259.4.5, 63
  %add89.4.5 = add i64 %shr.i260.4.5, %xor2569.i290.4.5
  %add90.4.5 = add i64 %add89.4.5, %add90.4.4
  %234 = xor i64 %add89.4.5, -9223372036854775808
  %xor2.i253.4.5 = and i64 %234, %add23.i289.4.5
  %xor.i244.4.5 = xor i64 %add90.4.5, %add89.4.5
  %xor1.i245.4.5 = xor i64 %add89.4.5, %add90.4.4
  %or.i246.4.5 = or i64 %xor.i244.4.5, %xor1.i245.4.5
  %xor2.i247.4.5 = xor i64 %or.i246.4.5, %add90.4.5
  %shr.i254326.4.5 = or i64 %xor2.i247.4.5, %xor2.i253.4.5
  %or94325.4.5 = lshr i64 %shr.i254326.4.5, 63
  %add96.4.5 = add i64 %or94325.4.5, %add96.4.4
  %arrayidx74.4.6 = getelementptr inbounds i64, i64* %mc, i64 11
  %235 = load i64, i64* %arrayidx74.4.6, align 8, !tbaa !3
  %236 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 5), align 8, !tbaa !3
  %and.i262.4.6 = and i64 %235, 4294967295
  %shr.i263.4.6 = lshr i64 %235, 32
  %and1.i264.4.6 = and i64 %236, 4294967295
  %shr2.i265.4.6 = lshr i64 %236, 32
  %mul.i266.4.6 = mul nuw i64 %and1.i264.4.6, %and.i262.4.6
  %mul3.i267.4.6 = mul nuw i64 %shr2.i265.4.6, %and.i262.4.6
  %mul4.i268.4.6 = mul nuw i64 %and1.i264.4.6, %shr.i263.4.6
  %mul5.i269.4.6 = mul nuw i64 %shr2.i265.4.6, %shr.i263.4.6
  %and6.i270.4.6 = and i64 %mul.i266.4.6, 4294967295
  %shr7.i271.4.6 = lshr i64 %mul.i266.4.6, 32
  %and8.i272.4.6 = and i64 %mul4.i268.4.6, 4294967295
  %and9.i273.4.6 = and i64 %mul3.i267.4.6, 4294967295
  %add.i274.4.6 = add nuw nsw i64 %shr7.i271.4.6, %and8.i272.4.6
  %add10.i275.4.6 = add nuw nsw i64 %add.i274.4.6, %and9.i273.4.6
  %shr11.i276.4.6 = lshr i64 %add10.i275.4.6, 32
  %shl.i277.4.6 = shl i64 %add10.i275.4.6, 32
  %xor68.i278.4.6 = or i64 %shl.i277.4.6, %and6.i270.4.6
  %shr13.i279.4.6 = lshr i64 %mul4.i268.4.6, 32
  %shr14.i280.4.6 = lshr i64 %mul3.i267.4.6, 32
  %and15.i281.4.6 = and i64 %mul5.i269.4.6, 4294967295
  %add16.i282.4.6 = add nuw nsw i64 %shr13.i279.4.6, %shr14.i280.4.6
  %add17.i283.4.6 = add nuw nsw i64 %add16.i282.4.6, %and15.i281.4.6
  %add18.i284.4.6 = add nuw nsw i64 %add17.i283.4.6, %shr11.i276.4.6
  %and19.i285.4.6 = and i64 %add18.i284.4.6, 4294967295
  %and21.i287.4.6 = and i64 %add18.i284.4.6, 30064771072
  %and22.i288.4.6 = and i64 %mul5.i269.4.6, -4294967296
  %add23.i289.4.6 = add i64 %and21.i287.4.6, %and22.i288.4.6
  %xor2569.i290.4.6 = or i64 %add23.i289.4.6, %and19.i285.4.6
  %add82.4.6 = add i64 %xor68.i278.4.6, %add82.4.5
  %xor.i256.4.6 = xor i64 %add82.4.6, %shl.i277.4.6
  %xor1.i257.4.6 = xor i64 %shl.i277.4.6, %add82.4.5
  %or.i258.4.6 = or i64 %xor.i256.4.6, %xor1.i257.4.6
  %xor2.i259.4.6 = xor i64 %or.i258.4.6, %add82.4.6
  %shr.i260.4.6 = lshr i64 %xor2.i259.4.6, 63
  %add89.4.6 = add i64 %shr.i260.4.6, %xor2569.i290.4.6
  %add90.4.6 = add i64 %add89.4.6, %add90.4.5
  %237 = xor i64 %add89.4.6, -9223372036854775808
  %xor2.i253.4.6 = and i64 %237, %add23.i289.4.6
  %xor.i244.4.6 = xor i64 %add90.4.6, %add89.4.6
  %xor1.i245.4.6 = xor i64 %add89.4.6, %add90.4.5
  %or.i246.4.6 = or i64 %xor.i244.4.6, %xor1.i245.4.6
  %xor2.i247.4.6 = xor i64 %or.i246.4.6, %add90.4.6
  %shr.i254326.4.6 = or i64 %xor2.i247.4.6, %xor2.i253.4.6
  %or94325.4.6 = lshr i64 %shr.i254326.4.6, 63
  %add96.4.6 = add i64 %or94325.4.6, %add96.4.5
  %arrayidx104.4 = getelementptr inbounds i64, i64* %ma, i64 16
  %238 = load i64, i64* %arrayidx104.4, align 8, !tbaa !3
  %add105.4 = add i64 %238, %add82.4.6
  %xor.i238.4 = xor i64 %add105.4, %add82.4.6
  %xor1.i239.4 = xor i64 %238, %add82.4.6
  %or.i240.4 = or i64 %xor.i238.4, %xor1.i239.4
  %xor2.i241.4 = xor i64 %or.i240.4, %add105.4
  %shr.i242.4 = lshr i64 %xor2.i241.4, 63
  %add111.4 = add i64 %shr.i242.4, %add90.4.6
  store i64 %add105.4, i64* %arrayidx50.4, align 8, !tbaa !3
  %239 = xor i64 %add111.4, -9223372036854775808
  %xor2.i.4 = and i64 %239, %add90.4.6
  %shr.i237.4 = lshr i64 %xor2.i.4, 63
  %add118.4 = add i64 %shr.i237.4, %add96.4.6
  %arrayidx74.5 = getelementptr inbounds i64, i64* %mc, i64 6
  %240 = load i64, i64* %arrayidx74.5, align 8, !tbaa !3
  %241 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.5 = and i64 %240, 4294967295
  %shr.i263.5 = lshr i64 %240, 32
  %and1.i264.5 = and i64 %241, 4294967295
  %shr2.i265.5 = lshr i64 %241, 32
  %mul.i266.5 = mul nuw i64 %and1.i264.5, %and.i262.5
  %mul3.i267.5 = mul nuw i64 %shr2.i265.5, %and.i262.5
  %mul4.i268.5 = mul nuw i64 %and1.i264.5, %shr.i263.5
  %mul5.i269.5 = mul nuw i64 %shr2.i265.5, %shr.i263.5
  %and6.i270.5 = and i64 %mul.i266.5, 4294967295
  %shr7.i271.5 = lshr i64 %mul.i266.5, 32
  %and8.i272.5 = and i64 %mul4.i268.5, 4294967295
  %and9.i273.5 = and i64 %mul3.i267.5, 4294967295
  %add.i274.5 = add nuw nsw i64 %shr7.i271.5, %and8.i272.5
  %add10.i275.5 = add nuw nsw i64 %add.i274.5, %and9.i273.5
  %shr11.i276.5 = lshr i64 %add10.i275.5, 32
  %shl.i277.5 = shl i64 %add10.i275.5, 32
  %xor68.i278.5 = or i64 %shl.i277.5, %and6.i270.5
  %shr13.i279.5 = lshr i64 %mul4.i268.5, 32
  %shr14.i280.5 = lshr i64 %mul3.i267.5, 32
  %and15.i281.5 = and i64 %mul5.i269.5, 4294967295
  %add16.i282.5 = add nuw nsw i64 %shr13.i279.5, %shr14.i280.5
  %add17.i283.5 = add nuw nsw i64 %add16.i282.5, %and15.i281.5
  %add18.i284.5 = add nuw nsw i64 %add17.i283.5, %shr11.i276.5
  %and19.i285.5 = and i64 %add18.i284.5, 4294967295
  %and21.i287.5 = and i64 %add18.i284.5, 30064771072
  %and22.i288.5 = and i64 %mul5.i269.5, -4294967296
  %add23.i289.5 = add i64 %and21.i287.5, %and22.i288.5
  %xor2569.i290.5 = or i64 %add23.i289.5, %and19.i285.5
  %add82.5 = add i64 %xor68.i278.5, %add111.4
  %xor.i256.5 = xor i64 %add82.5, %shl.i277.5
  %xor1.i257.5 = xor i64 %shl.i277.5, %add111.4
  %or.i258.5 = or i64 %xor.i256.5, %xor1.i257.5
  %xor2.i259.5 = xor i64 %or.i258.5, %add82.5
  %shr.i260.5 = lshr i64 %xor2.i259.5, 63
  %add89.5 = add i64 %shr.i260.5, %xor2569.i290.5
  %add90.5 = add i64 %add89.5, %add118.4
  %242 = xor i64 %add89.5, -9223372036854775808
  %xor2.i253.5 = and i64 %242, %add23.i289.5
  %xor.i244.5 = xor i64 %add90.5, %add89.5
  %xor1.i245.5 = xor i64 %add89.5, %add118.4
  %or.i246.5 = or i64 %xor.i244.5, %xor1.i245.5
  %xor2.i247.5 = xor i64 %or.i246.5, %add90.5
  %shr.i254326.5 = or i64 %xor2.i247.5, %xor2.i253.5
  %or94325.5 = lshr i64 %shr.i254326.5, 63
  %arrayidx74.5.1 = getelementptr inbounds i64, i64* %mc, i64 7
  %243 = load i64, i64* %arrayidx74.5.1, align 8, !tbaa !3
  %244 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.5.1 = and i64 %243, 4294967295
  %shr.i263.5.1 = lshr i64 %243, 32
  %and1.i264.5.1 = and i64 %244, 4294967295
  %shr2.i265.5.1 = lshr i64 %244, 32
  %mul.i266.5.1 = mul nuw i64 %and1.i264.5.1, %and.i262.5.1
  %mul3.i267.5.1 = mul nuw i64 %shr2.i265.5.1, %and.i262.5.1
  %mul4.i268.5.1 = mul nuw i64 %and1.i264.5.1, %shr.i263.5.1
  %mul5.i269.5.1 = mul nuw i64 %shr2.i265.5.1, %shr.i263.5.1
  %and6.i270.5.1 = and i64 %mul.i266.5.1, 4294967295
  %shr7.i271.5.1 = lshr i64 %mul.i266.5.1, 32
  %and8.i272.5.1 = and i64 %mul4.i268.5.1, 4294967295
  %and9.i273.5.1 = and i64 %mul3.i267.5.1, 4294967295
  %add.i274.5.1 = add nuw nsw i64 %shr7.i271.5.1, %and8.i272.5.1
  %add10.i275.5.1 = add nuw nsw i64 %add.i274.5.1, %and9.i273.5.1
  %shr11.i276.5.1 = lshr i64 %add10.i275.5.1, 32
  %shl.i277.5.1 = shl i64 %add10.i275.5.1, 32
  %xor68.i278.5.1 = or i64 %shl.i277.5.1, %and6.i270.5.1
  %shr13.i279.5.1 = lshr i64 %mul4.i268.5.1, 32
  %shr14.i280.5.1 = lshr i64 %mul3.i267.5.1, 32
  %and15.i281.5.1 = and i64 %mul5.i269.5.1, 4294967295
  %add16.i282.5.1 = add nuw nsw i64 %shr13.i279.5.1, %shr14.i280.5.1
  %add17.i283.5.1 = add nuw nsw i64 %add16.i282.5.1, %and15.i281.5.1
  %add18.i284.5.1 = add nuw nsw i64 %add17.i283.5.1, %shr11.i276.5.1
  %and19.i285.5.1 = and i64 %add18.i284.5.1, 4294967295
  %and21.i287.5.1 = and i64 %add18.i284.5.1, 30064771072
  %and22.i288.5.1 = and i64 %mul5.i269.5.1, -4294967296
  %add23.i289.5.1 = add i64 %and21.i287.5.1, %and22.i288.5.1
  %xor2569.i290.5.1 = or i64 %add23.i289.5.1, %and19.i285.5.1
  %add82.5.1 = add i64 %xor68.i278.5.1, %add82.5
  %xor.i256.5.1 = xor i64 %add82.5.1, %shl.i277.5.1
  %xor1.i257.5.1 = xor i64 %shl.i277.5.1, %add82.5
  %or.i258.5.1 = or i64 %xor.i256.5.1, %xor1.i257.5.1
  %xor2.i259.5.1 = xor i64 %or.i258.5.1, %add82.5.1
  %shr.i260.5.1 = lshr i64 %xor2.i259.5.1, 63
  %add89.5.1 = add i64 %shr.i260.5.1, %xor2569.i290.5.1
  %add90.5.1 = add i64 %add89.5.1, %add90.5
  %245 = xor i64 %add89.5.1, -9223372036854775808
  %xor2.i253.5.1 = and i64 %245, %add23.i289.5.1
  %xor.i244.5.1 = xor i64 %add90.5.1, %add89.5.1
  %xor1.i245.5.1 = xor i64 %add89.5.1, %add90.5
  %or.i246.5.1 = or i64 %xor.i244.5.1, %xor1.i245.5.1
  %xor2.i247.5.1 = xor i64 %or.i246.5.1, %add90.5.1
  %shr.i254326.5.1 = or i64 %xor2.i247.5.1, %xor2.i253.5.1
  %or94325.5.1 = lshr i64 %shr.i254326.5.1, 63
  %add96.5.1 = add nuw nsw i64 %or94325.5.1, %or94325.5
  %arrayidx74.5.2 = getelementptr inbounds i64, i64* %mc, i64 8
  %246 = load i64, i64* %arrayidx74.5.2, align 8, !tbaa !3
  %247 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.5.2 = and i64 %246, 4294967295
  %shr.i263.5.2 = lshr i64 %246, 32
  %and1.i264.5.2 = and i64 %247, 4294967295
  %shr2.i265.5.2 = lshr i64 %247, 32
  %mul.i266.5.2 = mul nuw i64 %and1.i264.5.2, %and.i262.5.2
  %mul3.i267.5.2 = mul nuw i64 %shr2.i265.5.2, %and.i262.5.2
  %mul4.i268.5.2 = mul nuw i64 %and1.i264.5.2, %shr.i263.5.2
  %mul5.i269.5.2 = mul nuw i64 %shr2.i265.5.2, %shr.i263.5.2
  %and6.i270.5.2 = and i64 %mul.i266.5.2, 4294967295
  %shr7.i271.5.2 = lshr i64 %mul.i266.5.2, 32
  %and8.i272.5.2 = and i64 %mul4.i268.5.2, 4294967295
  %and9.i273.5.2 = and i64 %mul3.i267.5.2, 4294967295
  %add.i274.5.2 = add nuw nsw i64 %shr7.i271.5.2, %and8.i272.5.2
  %add10.i275.5.2 = add nuw nsw i64 %add.i274.5.2, %and9.i273.5.2
  %shr11.i276.5.2 = lshr i64 %add10.i275.5.2, 32
  %shl.i277.5.2 = shl i64 %add10.i275.5.2, 32
  %xor68.i278.5.2 = or i64 %shl.i277.5.2, %and6.i270.5.2
  %shr13.i279.5.2 = lshr i64 %mul4.i268.5.2, 32
  %shr14.i280.5.2 = lshr i64 %mul3.i267.5.2, 32
  %and15.i281.5.2 = and i64 %mul5.i269.5.2, 4294967295
  %add16.i282.5.2 = add nuw nsw i64 %shr13.i279.5.2, %shr14.i280.5.2
  %add17.i283.5.2 = add nuw nsw i64 %add16.i282.5.2, %and15.i281.5.2
  %add18.i284.5.2 = add nuw nsw i64 %add17.i283.5.2, %shr11.i276.5.2
  %and19.i285.5.2 = and i64 %add18.i284.5.2, 4294967295
  %and21.i287.5.2 = and i64 %add18.i284.5.2, 30064771072
  %and22.i288.5.2 = and i64 %mul5.i269.5.2, -4294967296
  %add23.i289.5.2 = add i64 %and21.i287.5.2, %and22.i288.5.2
  %xor2569.i290.5.2 = or i64 %add23.i289.5.2, %and19.i285.5.2
  %add82.5.2 = add i64 %xor68.i278.5.2, %add82.5.1
  %xor.i256.5.2 = xor i64 %add82.5.2, %shl.i277.5.2
  %xor1.i257.5.2 = xor i64 %shl.i277.5.2, %add82.5.1
  %or.i258.5.2 = or i64 %xor.i256.5.2, %xor1.i257.5.2
  %xor2.i259.5.2 = xor i64 %or.i258.5.2, %add82.5.2
  %shr.i260.5.2 = lshr i64 %xor2.i259.5.2, 63
  %add89.5.2 = add i64 %shr.i260.5.2, %xor2569.i290.5.2
  %add90.5.2 = add i64 %add89.5.2, %add90.5.1
  %248 = xor i64 %add89.5.2, -9223372036854775808
  %xor2.i253.5.2 = and i64 %248, %add23.i289.5.2
  %xor.i244.5.2 = xor i64 %add90.5.2, %add89.5.2
  %xor1.i245.5.2 = xor i64 %add89.5.2, %add90.5.1
  %or.i246.5.2 = or i64 %xor.i244.5.2, %xor1.i245.5.2
  %xor2.i247.5.2 = xor i64 %or.i246.5.2, %add90.5.2
  %shr.i254326.5.2 = or i64 %xor2.i247.5.2, %xor2.i253.5.2
  %or94325.5.2 = lshr i64 %shr.i254326.5.2, 63
  %add96.5.2 = add nsw i64 %or94325.5.2, %add96.5.1
  %arrayidx74.5.3 = getelementptr inbounds i64, i64* %mc, i64 9
  %249 = load i64, i64* %arrayidx74.5.3, align 8, !tbaa !3
  %250 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.5.3 = and i64 %249, 4294967295
  %shr.i263.5.3 = lshr i64 %249, 32
  %and1.i264.5.3 = and i64 %250, 4294967295
  %shr2.i265.5.3 = lshr i64 %250, 32
  %mul.i266.5.3 = mul nuw i64 %and1.i264.5.3, %and.i262.5.3
  %mul3.i267.5.3 = mul nuw i64 %shr2.i265.5.3, %and.i262.5.3
  %mul4.i268.5.3 = mul nuw i64 %and1.i264.5.3, %shr.i263.5.3
  %mul5.i269.5.3 = mul nuw i64 %shr2.i265.5.3, %shr.i263.5.3
  %and6.i270.5.3 = and i64 %mul.i266.5.3, 4294967295
  %shr7.i271.5.3 = lshr i64 %mul.i266.5.3, 32
  %and8.i272.5.3 = and i64 %mul4.i268.5.3, 4294967295
  %and9.i273.5.3 = and i64 %mul3.i267.5.3, 4294967295
  %add.i274.5.3 = add nuw nsw i64 %shr7.i271.5.3, %and8.i272.5.3
  %add10.i275.5.3 = add nuw nsw i64 %add.i274.5.3, %and9.i273.5.3
  %shr11.i276.5.3 = lshr i64 %add10.i275.5.3, 32
  %shl.i277.5.3 = shl i64 %add10.i275.5.3, 32
  %xor68.i278.5.3 = or i64 %shl.i277.5.3, %and6.i270.5.3
  %shr13.i279.5.3 = lshr i64 %mul4.i268.5.3, 32
  %shr14.i280.5.3 = lshr i64 %mul3.i267.5.3, 32
  %and15.i281.5.3 = and i64 %mul5.i269.5.3, 4294967295
  %add16.i282.5.3 = add nuw nsw i64 %shr13.i279.5.3, %shr14.i280.5.3
  %add17.i283.5.3 = add nuw nsw i64 %add16.i282.5.3, %and15.i281.5.3
  %add18.i284.5.3 = add nuw nsw i64 %add17.i283.5.3, %shr11.i276.5.3
  %and19.i285.5.3 = and i64 %add18.i284.5.3, 4294967295
  %and21.i287.5.3 = and i64 %add18.i284.5.3, 30064771072
  %and22.i288.5.3 = and i64 %mul5.i269.5.3, -4294967296
  %add23.i289.5.3 = add i64 %and21.i287.5.3, %and22.i288.5.3
  %xor2569.i290.5.3 = or i64 %add23.i289.5.3, %and19.i285.5.3
  %add82.5.3 = add i64 %xor68.i278.5.3, %add82.5.2
  %xor.i256.5.3 = xor i64 %add82.5.3, %shl.i277.5.3
  %xor1.i257.5.3 = xor i64 %shl.i277.5.3, %add82.5.2
  %or.i258.5.3 = or i64 %xor.i256.5.3, %xor1.i257.5.3
  %xor2.i259.5.3 = xor i64 %or.i258.5.3, %add82.5.3
  %shr.i260.5.3 = lshr i64 %xor2.i259.5.3, 63
  %add89.5.3 = add i64 %shr.i260.5.3, %xor2569.i290.5.3
  %add90.5.3 = add i64 %add89.5.3, %add90.5.2
  %251 = xor i64 %add89.5.3, -9223372036854775808
  %xor2.i253.5.3 = and i64 %251, %add23.i289.5.3
  %xor.i244.5.3 = xor i64 %add90.5.3, %add89.5.3
  %xor1.i245.5.3 = xor i64 %add89.5.3, %add90.5.2
  %or.i246.5.3 = or i64 %xor.i244.5.3, %xor1.i245.5.3
  %xor2.i247.5.3 = xor i64 %or.i246.5.3, %add90.5.3
  %shr.i254326.5.3 = or i64 %xor2.i247.5.3, %xor2.i253.5.3
  %or94325.5.3 = lshr i64 %shr.i254326.5.3, 63
  %add96.5.3 = add nsw i64 %or94325.5.3, %add96.5.2
  %arrayidx74.5.4 = getelementptr inbounds i64, i64* %mc, i64 10
  %252 = load i64, i64* %arrayidx74.5.4, align 8, !tbaa !3
  %253 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.5.4 = and i64 %252, 4294967295
  %shr.i263.5.4 = lshr i64 %252, 32
  %and1.i264.5.4 = and i64 %253, 4294967295
  %shr2.i265.5.4 = lshr i64 %253, 32
  %mul.i266.5.4 = mul nuw i64 %and1.i264.5.4, %and.i262.5.4
  %mul3.i267.5.4 = mul nuw i64 %shr2.i265.5.4, %and.i262.5.4
  %mul4.i268.5.4 = mul nuw i64 %and1.i264.5.4, %shr.i263.5.4
  %mul5.i269.5.4 = mul nuw i64 %shr2.i265.5.4, %shr.i263.5.4
  %and6.i270.5.4 = and i64 %mul.i266.5.4, 4294967295
  %shr7.i271.5.4 = lshr i64 %mul.i266.5.4, 32
  %and8.i272.5.4 = and i64 %mul4.i268.5.4, 4294967295
  %and9.i273.5.4 = and i64 %mul3.i267.5.4, 4294967295
  %add.i274.5.4 = add nuw nsw i64 %shr7.i271.5.4, %and8.i272.5.4
  %add10.i275.5.4 = add nuw nsw i64 %add.i274.5.4, %and9.i273.5.4
  %shr11.i276.5.4 = lshr i64 %add10.i275.5.4, 32
  %shl.i277.5.4 = shl i64 %add10.i275.5.4, 32
  %xor68.i278.5.4 = or i64 %shl.i277.5.4, %and6.i270.5.4
  %shr13.i279.5.4 = lshr i64 %mul4.i268.5.4, 32
  %shr14.i280.5.4 = lshr i64 %mul3.i267.5.4, 32
  %and15.i281.5.4 = and i64 %mul5.i269.5.4, 4294967295
  %add16.i282.5.4 = add nuw nsw i64 %shr13.i279.5.4, %shr14.i280.5.4
  %add17.i283.5.4 = add nuw nsw i64 %add16.i282.5.4, %and15.i281.5.4
  %add18.i284.5.4 = add nuw nsw i64 %add17.i283.5.4, %shr11.i276.5.4
  %and19.i285.5.4 = and i64 %add18.i284.5.4, 4294967295
  %and21.i287.5.4 = and i64 %add18.i284.5.4, 30064771072
  %and22.i288.5.4 = and i64 %mul5.i269.5.4, -4294967296
  %add23.i289.5.4 = add i64 %and21.i287.5.4, %and22.i288.5.4
  %xor2569.i290.5.4 = or i64 %add23.i289.5.4, %and19.i285.5.4
  %add82.5.4 = add i64 %xor68.i278.5.4, %add82.5.3
  %xor.i256.5.4 = xor i64 %add82.5.4, %shl.i277.5.4
  %xor1.i257.5.4 = xor i64 %shl.i277.5.4, %add82.5.3
  %or.i258.5.4 = or i64 %xor.i256.5.4, %xor1.i257.5.4
  %xor2.i259.5.4 = xor i64 %or.i258.5.4, %add82.5.4
  %shr.i260.5.4 = lshr i64 %xor2.i259.5.4, 63
  %add89.5.4 = add i64 %shr.i260.5.4, %xor2569.i290.5.4
  %add90.5.4 = add i64 %add89.5.4, %add90.5.3
  %254 = xor i64 %add89.5.4, -9223372036854775808
  %xor2.i253.5.4 = and i64 %254, %add23.i289.5.4
  %xor.i244.5.4 = xor i64 %add90.5.4, %add89.5.4
  %xor1.i245.5.4 = xor i64 %add89.5.4, %add90.5.3
  %or.i246.5.4 = or i64 %xor.i244.5.4, %xor1.i245.5.4
  %xor2.i247.5.4 = xor i64 %or.i246.5.4, %add90.5.4
  %shr.i254326.5.4 = or i64 %xor2.i247.5.4, %xor2.i253.5.4
  %or94325.5.4 = lshr i64 %shr.i254326.5.4, 63
  %add96.5.4 = add i64 %or94325.5.4, %add96.5.3
  %arrayidx74.5.5 = getelementptr inbounds i64, i64* %mc, i64 11
  %255 = load i64, i64* %arrayidx74.5.5, align 8, !tbaa !3
  %256 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 6), align 16, !tbaa !3
  %and.i262.5.5 = and i64 %255, 4294967295
  %shr.i263.5.5 = lshr i64 %255, 32
  %and1.i264.5.5 = and i64 %256, 4294967295
  %shr2.i265.5.5 = lshr i64 %256, 32
  %mul.i266.5.5 = mul nuw i64 %and1.i264.5.5, %and.i262.5.5
  %mul3.i267.5.5 = mul nuw i64 %shr2.i265.5.5, %and.i262.5.5
  %mul4.i268.5.5 = mul nuw i64 %and1.i264.5.5, %shr.i263.5.5
  %mul5.i269.5.5 = mul nuw i64 %shr2.i265.5.5, %shr.i263.5.5
  %and6.i270.5.5 = and i64 %mul.i266.5.5, 4294967295
  %shr7.i271.5.5 = lshr i64 %mul.i266.5.5, 32
  %and8.i272.5.5 = and i64 %mul4.i268.5.5, 4294967295
  %and9.i273.5.5 = and i64 %mul3.i267.5.5, 4294967295
  %add.i274.5.5 = add nuw nsw i64 %shr7.i271.5.5, %and8.i272.5.5
  %add10.i275.5.5 = add nuw nsw i64 %add.i274.5.5, %and9.i273.5.5
  %shr11.i276.5.5 = lshr i64 %add10.i275.5.5, 32
  %shl.i277.5.5 = shl i64 %add10.i275.5.5, 32
  %xor68.i278.5.5 = or i64 %shl.i277.5.5, %and6.i270.5.5
  %shr13.i279.5.5 = lshr i64 %mul4.i268.5.5, 32
  %shr14.i280.5.5 = lshr i64 %mul3.i267.5.5, 32
  %and15.i281.5.5 = and i64 %mul5.i269.5.5, 4294967295
  %add16.i282.5.5 = add nuw nsw i64 %shr13.i279.5.5, %shr14.i280.5.5
  %add17.i283.5.5 = add nuw nsw i64 %add16.i282.5.5, %and15.i281.5.5
  %add18.i284.5.5 = add nuw nsw i64 %add17.i283.5.5, %shr11.i276.5.5
  %and19.i285.5.5 = and i64 %add18.i284.5.5, 4294967295
  %and21.i287.5.5 = and i64 %add18.i284.5.5, 30064771072
  %and22.i288.5.5 = and i64 %mul5.i269.5.5, -4294967296
  %add23.i289.5.5 = add i64 %and21.i287.5.5, %and22.i288.5.5
  %xor2569.i290.5.5 = or i64 %add23.i289.5.5, %and19.i285.5.5
  %add82.5.5 = add i64 %xor68.i278.5.5, %add82.5.4
  %xor.i256.5.5 = xor i64 %add82.5.5, %shl.i277.5.5
  %xor1.i257.5.5 = xor i64 %shl.i277.5.5, %add82.5.4
  %or.i258.5.5 = or i64 %xor.i256.5.5, %xor1.i257.5.5
  %xor2.i259.5.5 = xor i64 %or.i258.5.5, %add82.5.5
  %shr.i260.5.5 = lshr i64 %xor2.i259.5.5, 63
  %add89.5.5 = add i64 %shr.i260.5.5, %xor2569.i290.5.5
  %add90.5.5 = add i64 %add89.5.5, %add90.5.4
  %257 = xor i64 %add89.5.5, -9223372036854775808
  %xor2.i253.5.5 = and i64 %257, %add23.i289.5.5
  %xor.i244.5.5 = xor i64 %add90.5.5, %add89.5.5
  %xor1.i245.5.5 = xor i64 %add89.5.5, %add90.5.4
  %or.i246.5.5 = or i64 %xor.i244.5.5, %xor1.i245.5.5
  %xor2.i247.5.5 = xor i64 %or.i246.5.5, %add90.5.5
  %shr.i254326.5.5 = or i64 %xor2.i247.5.5, %xor2.i253.5.5
  %or94325.5.5 = lshr i64 %shr.i254326.5.5, 63
  %add96.5.5 = add i64 %or94325.5.5, %add96.5.4
  %arrayidx104.5 = getelementptr inbounds i64, i64* %ma, i64 17
  %258 = load i64, i64* %arrayidx104.5, align 8, !tbaa !3
  %add105.5 = add i64 %258, %add82.5.5
  %xor.i238.5 = xor i64 %add105.5, %add82.5.5
  %xor1.i239.5 = xor i64 %258, %add82.5.5
  %or.i240.5 = or i64 %xor.i238.5, %xor1.i239.5
  %xor2.i241.5 = xor i64 %or.i240.5, %add105.5
  %shr.i242.5 = lshr i64 %xor2.i241.5, 63
  %add111.5 = add i64 %shr.i242.5, %add90.5.5
  store i64 %add105.5, i64* %arrayidx50.5, align 8, !tbaa !3
  %259 = xor i64 %add111.5, -9223372036854775808
  %xor2.i.5 = and i64 %259, %add90.5.5
  %shr.i237.5 = lshr i64 %xor2.i.5, 63
  %add118.5 = add i64 %shr.i237.5, %add96.5.5
  %arrayidx74.6 = getelementptr inbounds i64, i64* %mc, i64 7
  %260 = load i64, i64* %arrayidx74.6, align 8, !tbaa !3
  %261 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.6 = and i64 %260, 4294967295
  %shr.i263.6 = lshr i64 %260, 32
  %and1.i264.6 = and i64 %261, 4294967295
  %shr2.i265.6 = lshr i64 %261, 32
  %mul.i266.6 = mul nuw i64 %and1.i264.6, %and.i262.6
  %mul3.i267.6 = mul nuw i64 %shr2.i265.6, %and.i262.6
  %mul4.i268.6 = mul nuw i64 %and1.i264.6, %shr.i263.6
  %mul5.i269.6 = mul nuw i64 %shr2.i265.6, %shr.i263.6
  %and6.i270.6 = and i64 %mul.i266.6, 4294967295
  %shr7.i271.6 = lshr i64 %mul.i266.6, 32
  %and8.i272.6 = and i64 %mul4.i268.6, 4294967295
  %and9.i273.6 = and i64 %mul3.i267.6, 4294967295
  %add.i274.6 = add nuw nsw i64 %shr7.i271.6, %and8.i272.6
  %add10.i275.6 = add nuw nsw i64 %add.i274.6, %and9.i273.6
  %shr11.i276.6 = lshr i64 %add10.i275.6, 32
  %shl.i277.6 = shl i64 %add10.i275.6, 32
  %xor68.i278.6 = or i64 %shl.i277.6, %and6.i270.6
  %shr13.i279.6 = lshr i64 %mul4.i268.6, 32
  %shr14.i280.6 = lshr i64 %mul3.i267.6, 32
  %and15.i281.6 = and i64 %mul5.i269.6, 4294967295
  %add16.i282.6 = add nuw nsw i64 %shr13.i279.6, %shr14.i280.6
  %add17.i283.6 = add nuw nsw i64 %add16.i282.6, %and15.i281.6
  %add18.i284.6 = add nuw nsw i64 %add17.i283.6, %shr11.i276.6
  %and19.i285.6 = and i64 %add18.i284.6, 4294967295
  %and21.i287.6 = and i64 %add18.i284.6, 30064771072
  %and22.i288.6 = and i64 %mul5.i269.6, -4294967296
  %add23.i289.6 = add i64 %and21.i287.6, %and22.i288.6
  %xor2569.i290.6 = or i64 %add23.i289.6, %and19.i285.6
  %add82.6 = add i64 %xor68.i278.6, %add111.5
  %xor.i256.6 = xor i64 %add82.6, %shl.i277.6
  %xor1.i257.6 = xor i64 %shl.i277.6, %add111.5
  %or.i258.6 = or i64 %xor.i256.6, %xor1.i257.6
  %xor2.i259.6 = xor i64 %or.i258.6, %add82.6
  %shr.i260.6 = lshr i64 %xor2.i259.6, 63
  %add89.6 = add i64 %shr.i260.6, %xor2569.i290.6
  %add90.6 = add i64 %add89.6, %add118.5
  %262 = xor i64 %add89.6, -9223372036854775808
  %xor2.i253.6 = and i64 %262, %add23.i289.6
  %xor.i244.6 = xor i64 %add90.6, %add89.6
  %xor1.i245.6 = xor i64 %add89.6, %add118.5
  %or.i246.6 = or i64 %xor.i244.6, %xor1.i245.6
  %xor2.i247.6 = xor i64 %or.i246.6, %add90.6
  %shr.i254326.6 = or i64 %xor2.i247.6, %xor2.i253.6
  %or94325.6 = lshr i64 %shr.i254326.6, 63
  %arrayidx74.6.1 = getelementptr inbounds i64, i64* %mc, i64 8
  %263 = load i64, i64* %arrayidx74.6.1, align 8, !tbaa !3
  %264 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.6.1 = and i64 %263, 4294967295
  %shr.i263.6.1 = lshr i64 %263, 32
  %and1.i264.6.1 = and i64 %264, 4294967295
  %shr2.i265.6.1 = lshr i64 %264, 32
  %mul.i266.6.1 = mul nuw i64 %and1.i264.6.1, %and.i262.6.1
  %mul3.i267.6.1 = mul nuw i64 %shr2.i265.6.1, %and.i262.6.1
  %mul4.i268.6.1 = mul nuw i64 %and1.i264.6.1, %shr.i263.6.1
  %mul5.i269.6.1 = mul nuw i64 %shr2.i265.6.1, %shr.i263.6.1
  %and6.i270.6.1 = and i64 %mul.i266.6.1, 4294967295
  %shr7.i271.6.1 = lshr i64 %mul.i266.6.1, 32
  %and8.i272.6.1 = and i64 %mul4.i268.6.1, 4294967295
  %and9.i273.6.1 = and i64 %mul3.i267.6.1, 4294967295
  %add.i274.6.1 = add nuw nsw i64 %shr7.i271.6.1, %and8.i272.6.1
  %add10.i275.6.1 = add nuw nsw i64 %add.i274.6.1, %and9.i273.6.1
  %shr11.i276.6.1 = lshr i64 %add10.i275.6.1, 32
  %shl.i277.6.1 = shl i64 %add10.i275.6.1, 32
  %xor68.i278.6.1 = or i64 %shl.i277.6.1, %and6.i270.6.1
  %shr13.i279.6.1 = lshr i64 %mul4.i268.6.1, 32
  %shr14.i280.6.1 = lshr i64 %mul3.i267.6.1, 32
  %and15.i281.6.1 = and i64 %mul5.i269.6.1, 4294967295
  %add16.i282.6.1 = add nuw nsw i64 %shr13.i279.6.1, %shr14.i280.6.1
  %add17.i283.6.1 = add nuw nsw i64 %add16.i282.6.1, %and15.i281.6.1
  %add18.i284.6.1 = add nuw nsw i64 %add17.i283.6.1, %shr11.i276.6.1
  %and19.i285.6.1 = and i64 %add18.i284.6.1, 4294967295
  %and21.i287.6.1 = and i64 %add18.i284.6.1, 30064771072
  %and22.i288.6.1 = and i64 %mul5.i269.6.1, -4294967296
  %add23.i289.6.1 = add i64 %and21.i287.6.1, %and22.i288.6.1
  %xor2569.i290.6.1 = or i64 %add23.i289.6.1, %and19.i285.6.1
  %add82.6.1 = add i64 %xor68.i278.6.1, %add82.6
  %xor.i256.6.1 = xor i64 %add82.6.1, %shl.i277.6.1
  %xor1.i257.6.1 = xor i64 %shl.i277.6.1, %add82.6
  %or.i258.6.1 = or i64 %xor.i256.6.1, %xor1.i257.6.1
  %xor2.i259.6.1 = xor i64 %or.i258.6.1, %add82.6.1
  %shr.i260.6.1 = lshr i64 %xor2.i259.6.1, 63
  %add89.6.1 = add i64 %shr.i260.6.1, %xor2569.i290.6.1
  %add90.6.1 = add i64 %add89.6.1, %add90.6
  %265 = xor i64 %add89.6.1, -9223372036854775808
  %xor2.i253.6.1 = and i64 %265, %add23.i289.6.1
  %xor.i244.6.1 = xor i64 %add90.6.1, %add89.6.1
  %xor1.i245.6.1 = xor i64 %add89.6.1, %add90.6
  %or.i246.6.1 = or i64 %xor.i244.6.1, %xor1.i245.6.1
  %xor2.i247.6.1 = xor i64 %or.i246.6.1, %add90.6.1
  %shr.i254326.6.1 = or i64 %xor2.i247.6.1, %xor2.i253.6.1
  %or94325.6.1 = lshr i64 %shr.i254326.6.1, 63
  %add96.6.1 = add nuw nsw i64 %or94325.6.1, %or94325.6
  %arrayidx74.6.2 = getelementptr inbounds i64, i64* %mc, i64 9
  %266 = load i64, i64* %arrayidx74.6.2, align 8, !tbaa !3
  %267 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.6.2 = and i64 %266, 4294967295
  %shr.i263.6.2 = lshr i64 %266, 32
  %and1.i264.6.2 = and i64 %267, 4294967295
  %shr2.i265.6.2 = lshr i64 %267, 32
  %mul.i266.6.2 = mul nuw i64 %and1.i264.6.2, %and.i262.6.2
  %mul3.i267.6.2 = mul nuw i64 %shr2.i265.6.2, %and.i262.6.2
  %mul4.i268.6.2 = mul nuw i64 %and1.i264.6.2, %shr.i263.6.2
  %mul5.i269.6.2 = mul nuw i64 %shr2.i265.6.2, %shr.i263.6.2
  %and6.i270.6.2 = and i64 %mul.i266.6.2, 4294967295
  %shr7.i271.6.2 = lshr i64 %mul.i266.6.2, 32
  %and8.i272.6.2 = and i64 %mul4.i268.6.2, 4294967295
  %and9.i273.6.2 = and i64 %mul3.i267.6.2, 4294967295
  %add.i274.6.2 = add nuw nsw i64 %shr7.i271.6.2, %and8.i272.6.2
  %add10.i275.6.2 = add nuw nsw i64 %add.i274.6.2, %and9.i273.6.2
  %shr11.i276.6.2 = lshr i64 %add10.i275.6.2, 32
  %shl.i277.6.2 = shl i64 %add10.i275.6.2, 32
  %xor68.i278.6.2 = or i64 %shl.i277.6.2, %and6.i270.6.2
  %shr13.i279.6.2 = lshr i64 %mul4.i268.6.2, 32
  %shr14.i280.6.2 = lshr i64 %mul3.i267.6.2, 32
  %and15.i281.6.2 = and i64 %mul5.i269.6.2, 4294967295
  %add16.i282.6.2 = add nuw nsw i64 %shr13.i279.6.2, %shr14.i280.6.2
  %add17.i283.6.2 = add nuw nsw i64 %add16.i282.6.2, %and15.i281.6.2
  %add18.i284.6.2 = add nuw nsw i64 %add17.i283.6.2, %shr11.i276.6.2
  %and19.i285.6.2 = and i64 %add18.i284.6.2, 4294967295
  %and21.i287.6.2 = and i64 %add18.i284.6.2, 30064771072
  %and22.i288.6.2 = and i64 %mul5.i269.6.2, -4294967296
  %add23.i289.6.2 = add i64 %and21.i287.6.2, %and22.i288.6.2
  %xor2569.i290.6.2 = or i64 %add23.i289.6.2, %and19.i285.6.2
  %add82.6.2 = add i64 %xor68.i278.6.2, %add82.6.1
  %xor.i256.6.2 = xor i64 %add82.6.2, %shl.i277.6.2
  %xor1.i257.6.2 = xor i64 %shl.i277.6.2, %add82.6.1
  %or.i258.6.2 = or i64 %xor.i256.6.2, %xor1.i257.6.2
  %xor2.i259.6.2 = xor i64 %or.i258.6.2, %add82.6.2
  %shr.i260.6.2 = lshr i64 %xor2.i259.6.2, 63
  %add89.6.2 = add i64 %shr.i260.6.2, %xor2569.i290.6.2
  %add90.6.2 = add i64 %add89.6.2, %add90.6.1
  %268 = xor i64 %add89.6.2, -9223372036854775808
  %xor2.i253.6.2 = and i64 %268, %add23.i289.6.2
  %xor.i244.6.2 = xor i64 %add90.6.2, %add89.6.2
  %xor1.i245.6.2 = xor i64 %add89.6.2, %add90.6.1
  %or.i246.6.2 = or i64 %xor.i244.6.2, %xor1.i245.6.2
  %xor2.i247.6.2 = xor i64 %or.i246.6.2, %add90.6.2
  %shr.i254326.6.2 = or i64 %xor2.i247.6.2, %xor2.i253.6.2
  %or94325.6.2 = lshr i64 %shr.i254326.6.2, 63
  %add96.6.2 = add nsw i64 %or94325.6.2, %add96.6.1
  %arrayidx74.6.3 = getelementptr inbounds i64, i64* %mc, i64 10
  %269 = load i64, i64* %arrayidx74.6.3, align 8, !tbaa !3
  %270 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.6.3 = and i64 %269, 4294967295
  %shr.i263.6.3 = lshr i64 %269, 32
  %and1.i264.6.3 = and i64 %270, 4294967295
  %shr2.i265.6.3 = lshr i64 %270, 32
  %mul.i266.6.3 = mul nuw i64 %and1.i264.6.3, %and.i262.6.3
  %mul3.i267.6.3 = mul nuw i64 %shr2.i265.6.3, %and.i262.6.3
  %mul4.i268.6.3 = mul nuw i64 %and1.i264.6.3, %shr.i263.6.3
  %mul5.i269.6.3 = mul nuw i64 %shr2.i265.6.3, %shr.i263.6.3
  %and6.i270.6.3 = and i64 %mul.i266.6.3, 4294967295
  %shr7.i271.6.3 = lshr i64 %mul.i266.6.3, 32
  %and8.i272.6.3 = and i64 %mul4.i268.6.3, 4294967295
  %and9.i273.6.3 = and i64 %mul3.i267.6.3, 4294967295
  %add.i274.6.3 = add nuw nsw i64 %shr7.i271.6.3, %and8.i272.6.3
  %add10.i275.6.3 = add nuw nsw i64 %add.i274.6.3, %and9.i273.6.3
  %shr11.i276.6.3 = lshr i64 %add10.i275.6.3, 32
  %shl.i277.6.3 = shl i64 %add10.i275.6.3, 32
  %xor68.i278.6.3 = or i64 %shl.i277.6.3, %and6.i270.6.3
  %shr13.i279.6.3 = lshr i64 %mul4.i268.6.3, 32
  %shr14.i280.6.3 = lshr i64 %mul3.i267.6.3, 32
  %and15.i281.6.3 = and i64 %mul5.i269.6.3, 4294967295
  %add16.i282.6.3 = add nuw nsw i64 %shr13.i279.6.3, %shr14.i280.6.3
  %add17.i283.6.3 = add nuw nsw i64 %add16.i282.6.3, %and15.i281.6.3
  %add18.i284.6.3 = add nuw nsw i64 %add17.i283.6.3, %shr11.i276.6.3
  %and19.i285.6.3 = and i64 %add18.i284.6.3, 4294967295
  %and21.i287.6.3 = and i64 %add18.i284.6.3, 30064771072
  %and22.i288.6.3 = and i64 %mul5.i269.6.3, -4294967296
  %add23.i289.6.3 = add i64 %and21.i287.6.3, %and22.i288.6.3
  %xor2569.i290.6.3 = or i64 %add23.i289.6.3, %and19.i285.6.3
  %add82.6.3 = add i64 %xor68.i278.6.3, %add82.6.2
  %xor.i256.6.3 = xor i64 %add82.6.3, %shl.i277.6.3
  %xor1.i257.6.3 = xor i64 %shl.i277.6.3, %add82.6.2
  %or.i258.6.3 = or i64 %xor.i256.6.3, %xor1.i257.6.3
  %xor2.i259.6.3 = xor i64 %or.i258.6.3, %add82.6.3
  %shr.i260.6.3 = lshr i64 %xor2.i259.6.3, 63
  %add89.6.3 = add i64 %shr.i260.6.3, %xor2569.i290.6.3
  %add90.6.3 = add i64 %add89.6.3, %add90.6.2
  %271 = xor i64 %add89.6.3, -9223372036854775808
  %xor2.i253.6.3 = and i64 %271, %add23.i289.6.3
  %xor.i244.6.3 = xor i64 %add90.6.3, %add89.6.3
  %xor1.i245.6.3 = xor i64 %add89.6.3, %add90.6.2
  %or.i246.6.3 = or i64 %xor.i244.6.3, %xor1.i245.6.3
  %xor2.i247.6.3 = xor i64 %or.i246.6.3, %add90.6.3
  %shr.i254326.6.3 = or i64 %xor2.i247.6.3, %xor2.i253.6.3
  %or94325.6.3 = lshr i64 %shr.i254326.6.3, 63
  %add96.6.3 = add nsw i64 %or94325.6.3, %add96.6.2
  %arrayidx74.6.4 = getelementptr inbounds i64, i64* %mc, i64 11
  %272 = load i64, i64* %arrayidx74.6.4, align 8, !tbaa !3
  %273 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 7), align 8, !tbaa !3
  %and.i262.6.4 = and i64 %272, 4294967295
  %shr.i263.6.4 = lshr i64 %272, 32
  %and1.i264.6.4 = and i64 %273, 4294967295
  %shr2.i265.6.4 = lshr i64 %273, 32
  %mul.i266.6.4 = mul nuw i64 %and1.i264.6.4, %and.i262.6.4
  %mul3.i267.6.4 = mul nuw i64 %shr2.i265.6.4, %and.i262.6.4
  %mul4.i268.6.4 = mul nuw i64 %and1.i264.6.4, %shr.i263.6.4
  %mul5.i269.6.4 = mul nuw i64 %shr2.i265.6.4, %shr.i263.6.4
  %and6.i270.6.4 = and i64 %mul.i266.6.4, 4294967295
  %shr7.i271.6.4 = lshr i64 %mul.i266.6.4, 32
  %and8.i272.6.4 = and i64 %mul4.i268.6.4, 4294967295
  %and9.i273.6.4 = and i64 %mul3.i267.6.4, 4294967295
  %add.i274.6.4 = add nuw nsw i64 %shr7.i271.6.4, %and8.i272.6.4
  %add10.i275.6.4 = add nuw nsw i64 %add.i274.6.4, %and9.i273.6.4
  %shr11.i276.6.4 = lshr i64 %add10.i275.6.4, 32
  %shl.i277.6.4 = shl i64 %add10.i275.6.4, 32
  %xor68.i278.6.4 = or i64 %shl.i277.6.4, %and6.i270.6.4
  %shr13.i279.6.4 = lshr i64 %mul4.i268.6.4, 32
  %shr14.i280.6.4 = lshr i64 %mul3.i267.6.4, 32
  %and15.i281.6.4 = and i64 %mul5.i269.6.4, 4294967295
  %add16.i282.6.4 = add nuw nsw i64 %shr13.i279.6.4, %shr14.i280.6.4
  %add17.i283.6.4 = add nuw nsw i64 %add16.i282.6.4, %and15.i281.6.4
  %add18.i284.6.4 = add nuw nsw i64 %add17.i283.6.4, %shr11.i276.6.4
  %and19.i285.6.4 = and i64 %add18.i284.6.4, 4294967295
  %and21.i287.6.4 = and i64 %add18.i284.6.4, 30064771072
  %and22.i288.6.4 = and i64 %mul5.i269.6.4, -4294967296
  %add23.i289.6.4 = add i64 %and21.i287.6.4, %and22.i288.6.4
  %xor2569.i290.6.4 = or i64 %add23.i289.6.4, %and19.i285.6.4
  %add82.6.4 = add i64 %xor68.i278.6.4, %add82.6.3
  %xor.i256.6.4 = xor i64 %add82.6.4, %shl.i277.6.4
  %xor1.i257.6.4 = xor i64 %shl.i277.6.4, %add82.6.3
  %or.i258.6.4 = or i64 %xor.i256.6.4, %xor1.i257.6.4
  %xor2.i259.6.4 = xor i64 %or.i258.6.4, %add82.6.4
  %shr.i260.6.4 = lshr i64 %xor2.i259.6.4, 63
  %add89.6.4 = add i64 %shr.i260.6.4, %xor2569.i290.6.4
  %add90.6.4 = add i64 %add89.6.4, %add90.6.3
  %274 = xor i64 %add89.6.4, -9223372036854775808
  %xor2.i253.6.4 = and i64 %274, %add23.i289.6.4
  %xor.i244.6.4 = xor i64 %add90.6.4, %add89.6.4
  %xor1.i245.6.4 = xor i64 %add89.6.4, %add90.6.3
  %or.i246.6.4 = or i64 %xor.i244.6.4, %xor1.i245.6.4
  %xor2.i247.6.4 = xor i64 %or.i246.6.4, %add90.6.4
  %shr.i254326.6.4 = or i64 %xor2.i247.6.4, %xor2.i253.6.4
  %or94325.6.4 = lshr i64 %shr.i254326.6.4, 63
  %add96.6.4 = add i64 %or94325.6.4, %add96.6.3
  %arrayidx104.6 = getelementptr inbounds i64, i64* %ma, i64 18
  %275 = load i64, i64* %arrayidx104.6, align 8, !tbaa !3
  %add105.6 = add i64 %275, %add82.6.4
  %xor.i238.6 = xor i64 %add105.6, %add82.6.4
  %xor1.i239.6 = xor i64 %275, %add82.6.4
  %or.i240.6 = or i64 %xor.i238.6, %xor1.i239.6
  %xor2.i241.6 = xor i64 %or.i240.6, %add105.6
  %shr.i242.6 = lshr i64 %xor2.i241.6, 63
  %add111.6 = add i64 %shr.i242.6, %add90.6.4
  store i64 %add105.6, i64* %arrayidx50.6, align 8, !tbaa !3
  %276 = xor i64 %add111.6, -9223372036854775808
  %xor2.i.6 = and i64 %276, %add90.6.4
  %shr.i237.6 = lshr i64 %xor2.i.6, 63
  %add118.6 = add i64 %shr.i237.6, %add96.6.4
  %arrayidx74.7 = getelementptr inbounds i64, i64* %mc, i64 8
  %277 = load i64, i64* %arrayidx74.7, align 8, !tbaa !3
  %278 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.7 = and i64 %277, 4294967295
  %shr.i263.7 = lshr i64 %277, 32
  %and1.i264.7 = and i64 %278, 4294967295
  %shr2.i265.7 = lshr i64 %278, 32
  %mul.i266.7 = mul nuw i64 %and1.i264.7, %and.i262.7
  %mul3.i267.7 = mul nuw i64 %shr2.i265.7, %and.i262.7
  %mul4.i268.7 = mul nuw i64 %and1.i264.7, %shr.i263.7
  %mul5.i269.7 = mul nuw i64 %shr2.i265.7, %shr.i263.7
  %and6.i270.7 = and i64 %mul.i266.7, 4294967295
  %shr7.i271.7 = lshr i64 %mul.i266.7, 32
  %and8.i272.7 = and i64 %mul4.i268.7, 4294967295
  %and9.i273.7 = and i64 %mul3.i267.7, 4294967295
  %add.i274.7 = add nuw nsw i64 %shr7.i271.7, %and8.i272.7
  %add10.i275.7 = add nuw nsw i64 %add.i274.7, %and9.i273.7
  %shr11.i276.7 = lshr i64 %add10.i275.7, 32
  %shl.i277.7 = shl i64 %add10.i275.7, 32
  %xor68.i278.7 = or i64 %shl.i277.7, %and6.i270.7
  %shr13.i279.7 = lshr i64 %mul4.i268.7, 32
  %shr14.i280.7 = lshr i64 %mul3.i267.7, 32
  %and15.i281.7 = and i64 %mul5.i269.7, 4294967295
  %add16.i282.7 = add nuw nsw i64 %shr13.i279.7, %shr14.i280.7
  %add17.i283.7 = add nuw nsw i64 %add16.i282.7, %and15.i281.7
  %add18.i284.7 = add nuw nsw i64 %add17.i283.7, %shr11.i276.7
  %and19.i285.7 = and i64 %add18.i284.7, 4294967295
  %and21.i287.7 = and i64 %add18.i284.7, 30064771072
  %and22.i288.7 = and i64 %mul5.i269.7, -4294967296
  %add23.i289.7 = add i64 %and21.i287.7, %and22.i288.7
  %xor2569.i290.7 = or i64 %add23.i289.7, %and19.i285.7
  %add82.7 = add i64 %xor68.i278.7, %add111.6
  %xor.i256.7 = xor i64 %add82.7, %shl.i277.7
  %xor1.i257.7 = xor i64 %shl.i277.7, %add111.6
  %or.i258.7 = or i64 %xor.i256.7, %xor1.i257.7
  %xor2.i259.7 = xor i64 %or.i258.7, %add82.7
  %shr.i260.7 = lshr i64 %xor2.i259.7, 63
  %add89.7 = add i64 %shr.i260.7, %xor2569.i290.7
  %add90.7 = add i64 %add89.7, %add118.6
  %279 = xor i64 %add89.7, -9223372036854775808
  %xor2.i253.7 = and i64 %279, %add23.i289.7
  %xor.i244.7 = xor i64 %add90.7, %add89.7
  %xor1.i245.7 = xor i64 %add89.7, %add118.6
  %or.i246.7 = or i64 %xor.i244.7, %xor1.i245.7
  %xor2.i247.7 = xor i64 %or.i246.7, %add90.7
  %shr.i254326.7 = or i64 %xor2.i247.7, %xor2.i253.7
  %or94325.7 = lshr i64 %shr.i254326.7, 63
  %arrayidx74.7.1 = getelementptr inbounds i64, i64* %mc, i64 9
  %280 = load i64, i64* %arrayidx74.7.1, align 8, !tbaa !3
  %281 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.7.1 = and i64 %280, 4294967295
  %shr.i263.7.1 = lshr i64 %280, 32
  %and1.i264.7.1 = and i64 %281, 4294967295
  %shr2.i265.7.1 = lshr i64 %281, 32
  %mul.i266.7.1 = mul nuw i64 %and1.i264.7.1, %and.i262.7.1
  %mul3.i267.7.1 = mul nuw i64 %shr2.i265.7.1, %and.i262.7.1
  %mul4.i268.7.1 = mul nuw i64 %and1.i264.7.1, %shr.i263.7.1
  %mul5.i269.7.1 = mul nuw i64 %shr2.i265.7.1, %shr.i263.7.1
  %and6.i270.7.1 = and i64 %mul.i266.7.1, 4294967295
  %shr7.i271.7.1 = lshr i64 %mul.i266.7.1, 32
  %and8.i272.7.1 = and i64 %mul4.i268.7.1, 4294967295
  %and9.i273.7.1 = and i64 %mul3.i267.7.1, 4294967295
  %add.i274.7.1 = add nuw nsw i64 %shr7.i271.7.1, %and8.i272.7.1
  %add10.i275.7.1 = add nuw nsw i64 %add.i274.7.1, %and9.i273.7.1
  %shr11.i276.7.1 = lshr i64 %add10.i275.7.1, 32
  %shl.i277.7.1 = shl i64 %add10.i275.7.1, 32
  %xor68.i278.7.1 = or i64 %shl.i277.7.1, %and6.i270.7.1
  %shr13.i279.7.1 = lshr i64 %mul4.i268.7.1, 32
  %shr14.i280.7.1 = lshr i64 %mul3.i267.7.1, 32
  %and15.i281.7.1 = and i64 %mul5.i269.7.1, 4294967295
  %add16.i282.7.1 = add nuw nsw i64 %shr13.i279.7.1, %shr14.i280.7.1
  %add17.i283.7.1 = add nuw nsw i64 %add16.i282.7.1, %and15.i281.7.1
  %add18.i284.7.1 = add nuw nsw i64 %add17.i283.7.1, %shr11.i276.7.1
  %and19.i285.7.1 = and i64 %add18.i284.7.1, 4294967295
  %and21.i287.7.1 = and i64 %add18.i284.7.1, 30064771072
  %and22.i288.7.1 = and i64 %mul5.i269.7.1, -4294967296
  %add23.i289.7.1 = add i64 %and21.i287.7.1, %and22.i288.7.1
  %xor2569.i290.7.1 = or i64 %add23.i289.7.1, %and19.i285.7.1
  %add82.7.1 = add i64 %xor68.i278.7.1, %add82.7
  %xor.i256.7.1 = xor i64 %add82.7.1, %shl.i277.7.1
  %xor1.i257.7.1 = xor i64 %shl.i277.7.1, %add82.7
  %or.i258.7.1 = or i64 %xor.i256.7.1, %xor1.i257.7.1
  %xor2.i259.7.1 = xor i64 %or.i258.7.1, %add82.7.1
  %shr.i260.7.1 = lshr i64 %xor2.i259.7.1, 63
  %add89.7.1 = add i64 %shr.i260.7.1, %xor2569.i290.7.1
  %add90.7.1 = add i64 %add89.7.1, %add90.7
  %282 = xor i64 %add89.7.1, -9223372036854775808
  %xor2.i253.7.1 = and i64 %282, %add23.i289.7.1
  %xor.i244.7.1 = xor i64 %add90.7.1, %add89.7.1
  %xor1.i245.7.1 = xor i64 %add89.7.1, %add90.7
  %or.i246.7.1 = or i64 %xor.i244.7.1, %xor1.i245.7.1
  %xor2.i247.7.1 = xor i64 %or.i246.7.1, %add90.7.1
  %shr.i254326.7.1 = or i64 %xor2.i247.7.1, %xor2.i253.7.1
  %or94325.7.1 = lshr i64 %shr.i254326.7.1, 63
  %add96.7.1 = add nuw nsw i64 %or94325.7.1, %or94325.7
  %arrayidx74.7.2 = getelementptr inbounds i64, i64* %mc, i64 10
  %283 = load i64, i64* %arrayidx74.7.2, align 8, !tbaa !3
  %284 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.7.2 = and i64 %283, 4294967295
  %shr.i263.7.2 = lshr i64 %283, 32
  %and1.i264.7.2 = and i64 %284, 4294967295
  %shr2.i265.7.2 = lshr i64 %284, 32
  %mul.i266.7.2 = mul nuw i64 %and1.i264.7.2, %and.i262.7.2
  %mul3.i267.7.2 = mul nuw i64 %shr2.i265.7.2, %and.i262.7.2
  %mul4.i268.7.2 = mul nuw i64 %and1.i264.7.2, %shr.i263.7.2
  %mul5.i269.7.2 = mul nuw i64 %shr2.i265.7.2, %shr.i263.7.2
  %and6.i270.7.2 = and i64 %mul.i266.7.2, 4294967295
  %shr7.i271.7.2 = lshr i64 %mul.i266.7.2, 32
  %and8.i272.7.2 = and i64 %mul4.i268.7.2, 4294967295
  %and9.i273.7.2 = and i64 %mul3.i267.7.2, 4294967295
  %add.i274.7.2 = add nuw nsw i64 %shr7.i271.7.2, %and8.i272.7.2
  %add10.i275.7.2 = add nuw nsw i64 %add.i274.7.2, %and9.i273.7.2
  %shr11.i276.7.2 = lshr i64 %add10.i275.7.2, 32
  %shl.i277.7.2 = shl i64 %add10.i275.7.2, 32
  %xor68.i278.7.2 = or i64 %shl.i277.7.2, %and6.i270.7.2
  %shr13.i279.7.2 = lshr i64 %mul4.i268.7.2, 32
  %shr14.i280.7.2 = lshr i64 %mul3.i267.7.2, 32
  %and15.i281.7.2 = and i64 %mul5.i269.7.2, 4294967295
  %add16.i282.7.2 = add nuw nsw i64 %shr13.i279.7.2, %shr14.i280.7.2
  %add17.i283.7.2 = add nuw nsw i64 %add16.i282.7.2, %and15.i281.7.2
  %add18.i284.7.2 = add nuw nsw i64 %add17.i283.7.2, %shr11.i276.7.2
  %and19.i285.7.2 = and i64 %add18.i284.7.2, 4294967295
  %and21.i287.7.2 = and i64 %add18.i284.7.2, 30064771072
  %and22.i288.7.2 = and i64 %mul5.i269.7.2, -4294967296
  %add23.i289.7.2 = add i64 %and21.i287.7.2, %and22.i288.7.2
  %xor2569.i290.7.2 = or i64 %add23.i289.7.2, %and19.i285.7.2
  %add82.7.2 = add i64 %xor68.i278.7.2, %add82.7.1
  %xor.i256.7.2 = xor i64 %add82.7.2, %shl.i277.7.2
  %xor1.i257.7.2 = xor i64 %shl.i277.7.2, %add82.7.1
  %or.i258.7.2 = or i64 %xor.i256.7.2, %xor1.i257.7.2
  %xor2.i259.7.2 = xor i64 %or.i258.7.2, %add82.7.2
  %shr.i260.7.2 = lshr i64 %xor2.i259.7.2, 63
  %add89.7.2 = add i64 %shr.i260.7.2, %xor2569.i290.7.2
  %add90.7.2 = add i64 %add89.7.2, %add90.7.1
  %285 = xor i64 %add89.7.2, -9223372036854775808
  %xor2.i253.7.2 = and i64 %285, %add23.i289.7.2
  %xor.i244.7.2 = xor i64 %add90.7.2, %add89.7.2
  %xor1.i245.7.2 = xor i64 %add89.7.2, %add90.7.1
  %or.i246.7.2 = or i64 %xor.i244.7.2, %xor1.i245.7.2
  %xor2.i247.7.2 = xor i64 %or.i246.7.2, %add90.7.2
  %shr.i254326.7.2 = or i64 %xor2.i247.7.2, %xor2.i253.7.2
  %or94325.7.2 = lshr i64 %shr.i254326.7.2, 63
  %add96.7.2 = add nsw i64 %or94325.7.2, %add96.7.1
  %arrayidx74.7.3 = getelementptr inbounds i64, i64* %mc, i64 11
  %286 = load i64, i64* %arrayidx74.7.3, align 8, !tbaa !3
  %287 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 8), align 16, !tbaa !3
  %and.i262.7.3 = and i64 %286, 4294967295
  %shr.i263.7.3 = lshr i64 %286, 32
  %and1.i264.7.3 = and i64 %287, 4294967295
  %shr2.i265.7.3 = lshr i64 %287, 32
  %mul.i266.7.3 = mul nuw i64 %and1.i264.7.3, %and.i262.7.3
  %mul3.i267.7.3 = mul nuw i64 %shr2.i265.7.3, %and.i262.7.3
  %mul4.i268.7.3 = mul nuw i64 %and1.i264.7.3, %shr.i263.7.3
  %mul5.i269.7.3 = mul nuw i64 %shr2.i265.7.3, %shr.i263.7.3
  %and6.i270.7.3 = and i64 %mul.i266.7.3, 4294967295
  %shr7.i271.7.3 = lshr i64 %mul.i266.7.3, 32
  %and8.i272.7.3 = and i64 %mul4.i268.7.3, 4294967295
  %and9.i273.7.3 = and i64 %mul3.i267.7.3, 4294967295
  %add.i274.7.3 = add nuw nsw i64 %shr7.i271.7.3, %and8.i272.7.3
  %add10.i275.7.3 = add nuw nsw i64 %add.i274.7.3, %and9.i273.7.3
  %shr11.i276.7.3 = lshr i64 %add10.i275.7.3, 32
  %shl.i277.7.3 = shl i64 %add10.i275.7.3, 32
  %xor68.i278.7.3 = or i64 %shl.i277.7.3, %and6.i270.7.3
  %shr13.i279.7.3 = lshr i64 %mul4.i268.7.3, 32
  %shr14.i280.7.3 = lshr i64 %mul3.i267.7.3, 32
  %and15.i281.7.3 = and i64 %mul5.i269.7.3, 4294967295
  %add16.i282.7.3 = add nuw nsw i64 %shr13.i279.7.3, %shr14.i280.7.3
  %add17.i283.7.3 = add nuw nsw i64 %add16.i282.7.3, %and15.i281.7.3
  %add18.i284.7.3 = add nuw nsw i64 %add17.i283.7.3, %shr11.i276.7.3
  %and19.i285.7.3 = and i64 %add18.i284.7.3, 4294967295
  %and21.i287.7.3 = and i64 %add18.i284.7.3, 30064771072
  %and22.i288.7.3 = and i64 %mul5.i269.7.3, -4294967296
  %add23.i289.7.3 = add i64 %and21.i287.7.3, %and22.i288.7.3
  %xor2569.i290.7.3 = or i64 %add23.i289.7.3, %and19.i285.7.3
  %add82.7.3 = add i64 %xor68.i278.7.3, %add82.7.2
  %xor.i256.7.3 = xor i64 %add82.7.3, %shl.i277.7.3
  %xor1.i257.7.3 = xor i64 %shl.i277.7.3, %add82.7.2
  %or.i258.7.3 = or i64 %xor.i256.7.3, %xor1.i257.7.3
  %xor2.i259.7.3 = xor i64 %or.i258.7.3, %add82.7.3
  %shr.i260.7.3 = lshr i64 %xor2.i259.7.3, 63
  %add89.7.3 = add i64 %shr.i260.7.3, %xor2569.i290.7.3
  %add90.7.3 = add i64 %add89.7.3, %add90.7.2
  %288 = xor i64 %add89.7.3, -9223372036854775808
  %xor2.i253.7.3 = and i64 %288, %add23.i289.7.3
  %xor.i244.7.3 = xor i64 %add90.7.3, %add89.7.3
  %xor1.i245.7.3 = xor i64 %add89.7.3, %add90.7.2
  %or.i246.7.3 = or i64 %xor.i244.7.3, %xor1.i245.7.3
  %xor2.i247.7.3 = xor i64 %or.i246.7.3, %add90.7.3
  %shr.i254326.7.3 = or i64 %xor2.i247.7.3, %xor2.i253.7.3
  %or94325.7.3 = lshr i64 %shr.i254326.7.3, 63
  %add96.7.3 = add nsw i64 %or94325.7.3, %add96.7.2
  %arrayidx104.7 = getelementptr inbounds i64, i64* %ma, i64 19
  %289 = load i64, i64* %arrayidx104.7, align 8, !tbaa !3
  %add105.7 = add i64 %289, %add82.7.3
  %xor.i238.7 = xor i64 %add105.7, %add82.7.3
  %xor1.i239.7 = xor i64 %289, %add82.7.3
  %or.i240.7 = or i64 %xor.i238.7, %xor1.i239.7
  %xor2.i241.7 = xor i64 %or.i240.7, %add105.7
  %shr.i242.7 = lshr i64 %xor2.i241.7, 63
  %add111.7 = add i64 %shr.i242.7, %add90.7.3
  store i64 %add105.7, i64* %arrayidx50.7, align 8, !tbaa !3
  %290 = xor i64 %add111.7, -9223372036854775808
  %xor2.i.7 = and i64 %290, %add90.7.3
  %shr.i237.7 = lshr i64 %xor2.i.7, 63
  %add118.7 = add i64 %shr.i237.7, %add96.7.3
  %arrayidx74.8 = getelementptr inbounds i64, i64* %mc, i64 9
  %291 = load i64, i64* %arrayidx74.8, align 8, !tbaa !3
  %292 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.8 = and i64 %291, 4294967295
  %shr.i263.8 = lshr i64 %291, 32
  %and1.i264.8 = and i64 %292, 4294967295
  %shr2.i265.8 = lshr i64 %292, 32
  %mul.i266.8 = mul nuw i64 %and1.i264.8, %and.i262.8
  %mul3.i267.8 = mul nuw i64 %shr2.i265.8, %and.i262.8
  %mul4.i268.8 = mul nuw i64 %and1.i264.8, %shr.i263.8
  %mul5.i269.8 = mul nuw i64 %shr2.i265.8, %shr.i263.8
  %and6.i270.8 = and i64 %mul.i266.8, 4294967295
  %shr7.i271.8 = lshr i64 %mul.i266.8, 32
  %and8.i272.8 = and i64 %mul4.i268.8, 4294967295
  %and9.i273.8 = and i64 %mul3.i267.8, 4294967295
  %add.i274.8 = add nuw nsw i64 %shr7.i271.8, %and8.i272.8
  %add10.i275.8 = add nuw nsw i64 %add.i274.8, %and9.i273.8
  %shr11.i276.8 = lshr i64 %add10.i275.8, 32
  %shl.i277.8 = shl i64 %add10.i275.8, 32
  %xor68.i278.8 = or i64 %shl.i277.8, %and6.i270.8
  %shr13.i279.8 = lshr i64 %mul4.i268.8, 32
  %shr14.i280.8 = lshr i64 %mul3.i267.8, 32
  %and15.i281.8 = and i64 %mul5.i269.8, 4294967295
  %add16.i282.8 = add nuw nsw i64 %shr13.i279.8, %shr14.i280.8
  %add17.i283.8 = add nuw nsw i64 %add16.i282.8, %and15.i281.8
  %add18.i284.8 = add nuw nsw i64 %add17.i283.8, %shr11.i276.8
  %and19.i285.8 = and i64 %add18.i284.8, 4294967295
  %and21.i287.8 = and i64 %add18.i284.8, 30064771072
  %and22.i288.8 = and i64 %mul5.i269.8, -4294967296
  %add23.i289.8 = add i64 %and21.i287.8, %and22.i288.8
  %xor2569.i290.8 = or i64 %add23.i289.8, %and19.i285.8
  %add82.8 = add i64 %xor68.i278.8, %add111.7
  %xor.i256.8 = xor i64 %add82.8, %shl.i277.8
  %xor1.i257.8 = xor i64 %shl.i277.8, %add111.7
  %or.i258.8 = or i64 %xor.i256.8, %xor1.i257.8
  %xor2.i259.8 = xor i64 %or.i258.8, %add82.8
  %shr.i260.8 = lshr i64 %xor2.i259.8, 63
  %add89.8 = add i64 %shr.i260.8, %xor2569.i290.8
  %add90.8 = add i64 %add89.8, %add118.7
  %293 = xor i64 %add89.8, -9223372036854775808
  %xor2.i253.8 = and i64 %293, %add23.i289.8
  %xor.i244.8 = xor i64 %add90.8, %add89.8
  %xor1.i245.8 = xor i64 %add89.8, %add118.7
  %or.i246.8 = or i64 %xor.i244.8, %xor1.i245.8
  %xor2.i247.8 = xor i64 %or.i246.8, %add90.8
  %shr.i254326.8 = or i64 %xor2.i247.8, %xor2.i253.8
  %or94325.8 = lshr i64 %shr.i254326.8, 63
  %arrayidx74.8.1 = getelementptr inbounds i64, i64* %mc, i64 10
  %294 = load i64, i64* %arrayidx74.8.1, align 8, !tbaa !3
  %295 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.8.1 = and i64 %294, 4294967295
  %shr.i263.8.1 = lshr i64 %294, 32
  %and1.i264.8.1 = and i64 %295, 4294967295
  %shr2.i265.8.1 = lshr i64 %295, 32
  %mul.i266.8.1 = mul nuw i64 %and1.i264.8.1, %and.i262.8.1
  %mul3.i267.8.1 = mul nuw i64 %shr2.i265.8.1, %and.i262.8.1
  %mul4.i268.8.1 = mul nuw i64 %and1.i264.8.1, %shr.i263.8.1
  %mul5.i269.8.1 = mul nuw i64 %shr2.i265.8.1, %shr.i263.8.1
  %and6.i270.8.1 = and i64 %mul.i266.8.1, 4294967295
  %shr7.i271.8.1 = lshr i64 %mul.i266.8.1, 32
  %and8.i272.8.1 = and i64 %mul4.i268.8.1, 4294967295
  %and9.i273.8.1 = and i64 %mul3.i267.8.1, 4294967295
  %add.i274.8.1 = add nuw nsw i64 %shr7.i271.8.1, %and8.i272.8.1
  %add10.i275.8.1 = add nuw nsw i64 %add.i274.8.1, %and9.i273.8.1
  %shr11.i276.8.1 = lshr i64 %add10.i275.8.1, 32
  %shl.i277.8.1 = shl i64 %add10.i275.8.1, 32
  %xor68.i278.8.1 = or i64 %shl.i277.8.1, %and6.i270.8.1
  %shr13.i279.8.1 = lshr i64 %mul4.i268.8.1, 32
  %shr14.i280.8.1 = lshr i64 %mul3.i267.8.1, 32
  %and15.i281.8.1 = and i64 %mul5.i269.8.1, 4294967295
  %add16.i282.8.1 = add nuw nsw i64 %shr13.i279.8.1, %shr14.i280.8.1
  %add17.i283.8.1 = add nuw nsw i64 %add16.i282.8.1, %and15.i281.8.1
  %add18.i284.8.1 = add nuw nsw i64 %add17.i283.8.1, %shr11.i276.8.1
  %and19.i285.8.1 = and i64 %add18.i284.8.1, 4294967295
  %and21.i287.8.1 = and i64 %add18.i284.8.1, 30064771072
  %and22.i288.8.1 = and i64 %mul5.i269.8.1, -4294967296
  %add23.i289.8.1 = add i64 %and21.i287.8.1, %and22.i288.8.1
  %xor2569.i290.8.1 = or i64 %add23.i289.8.1, %and19.i285.8.1
  %add82.8.1 = add i64 %xor68.i278.8.1, %add82.8
  %xor.i256.8.1 = xor i64 %add82.8.1, %shl.i277.8.1
  %xor1.i257.8.1 = xor i64 %shl.i277.8.1, %add82.8
  %or.i258.8.1 = or i64 %xor.i256.8.1, %xor1.i257.8.1
  %xor2.i259.8.1 = xor i64 %or.i258.8.1, %add82.8.1
  %shr.i260.8.1 = lshr i64 %xor2.i259.8.1, 63
  %add89.8.1 = add i64 %shr.i260.8.1, %xor2569.i290.8.1
  %add90.8.1 = add i64 %add89.8.1, %add90.8
  %296 = xor i64 %add89.8.1, -9223372036854775808
  %xor2.i253.8.1 = and i64 %296, %add23.i289.8.1
  %xor.i244.8.1 = xor i64 %add90.8.1, %add89.8.1
  %xor1.i245.8.1 = xor i64 %add89.8.1, %add90.8
  %or.i246.8.1 = or i64 %xor.i244.8.1, %xor1.i245.8.1
  %xor2.i247.8.1 = xor i64 %or.i246.8.1, %add90.8.1
  %shr.i254326.8.1 = or i64 %xor2.i247.8.1, %xor2.i253.8.1
  %or94325.8.1 = lshr i64 %shr.i254326.8.1, 63
  %add96.8.1 = add nuw nsw i64 %or94325.8.1, %or94325.8
  %arrayidx74.8.2 = getelementptr inbounds i64, i64* %mc, i64 11
  %297 = load i64, i64* %arrayidx74.8.2, align 8, !tbaa !3
  %298 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 9), align 8, !tbaa !3
  %and.i262.8.2 = and i64 %297, 4294967295
  %shr.i263.8.2 = lshr i64 %297, 32
  %and1.i264.8.2 = and i64 %298, 4294967295
  %shr2.i265.8.2 = lshr i64 %298, 32
  %mul.i266.8.2 = mul nuw i64 %and1.i264.8.2, %and.i262.8.2
  %mul3.i267.8.2 = mul nuw i64 %shr2.i265.8.2, %and.i262.8.2
  %mul4.i268.8.2 = mul nuw i64 %and1.i264.8.2, %shr.i263.8.2
  %mul5.i269.8.2 = mul nuw i64 %shr2.i265.8.2, %shr.i263.8.2
  %and6.i270.8.2 = and i64 %mul.i266.8.2, 4294967295
  %shr7.i271.8.2 = lshr i64 %mul.i266.8.2, 32
  %and8.i272.8.2 = and i64 %mul4.i268.8.2, 4294967295
  %and9.i273.8.2 = and i64 %mul3.i267.8.2, 4294967295
  %add.i274.8.2 = add nuw nsw i64 %shr7.i271.8.2, %and8.i272.8.2
  %add10.i275.8.2 = add nuw nsw i64 %add.i274.8.2, %and9.i273.8.2
  %shr11.i276.8.2 = lshr i64 %add10.i275.8.2, 32
  %shl.i277.8.2 = shl i64 %add10.i275.8.2, 32
  %xor68.i278.8.2 = or i64 %shl.i277.8.2, %and6.i270.8.2
  %shr13.i279.8.2 = lshr i64 %mul4.i268.8.2, 32
  %shr14.i280.8.2 = lshr i64 %mul3.i267.8.2, 32
  %and15.i281.8.2 = and i64 %mul5.i269.8.2, 4294967295
  %add16.i282.8.2 = add nuw nsw i64 %shr13.i279.8.2, %shr14.i280.8.2
  %add17.i283.8.2 = add nuw nsw i64 %add16.i282.8.2, %and15.i281.8.2
  %add18.i284.8.2 = add nuw nsw i64 %add17.i283.8.2, %shr11.i276.8.2
  %and19.i285.8.2 = and i64 %add18.i284.8.2, 4294967295
  %and21.i287.8.2 = and i64 %add18.i284.8.2, 30064771072
  %and22.i288.8.2 = and i64 %mul5.i269.8.2, -4294967296
  %add23.i289.8.2 = add i64 %and21.i287.8.2, %and22.i288.8.2
  %xor2569.i290.8.2 = or i64 %add23.i289.8.2, %and19.i285.8.2
  %add82.8.2 = add i64 %xor68.i278.8.2, %add82.8.1
  %xor.i256.8.2 = xor i64 %add82.8.2, %shl.i277.8.2
  %xor1.i257.8.2 = xor i64 %shl.i277.8.2, %add82.8.1
  %or.i258.8.2 = or i64 %xor.i256.8.2, %xor1.i257.8.2
  %xor2.i259.8.2 = xor i64 %or.i258.8.2, %add82.8.2
  %shr.i260.8.2 = lshr i64 %xor2.i259.8.2, 63
  %add89.8.2 = add i64 %shr.i260.8.2, %xor2569.i290.8.2
  %add90.8.2 = add i64 %add89.8.2, %add90.8.1
  %299 = xor i64 %add89.8.2, -9223372036854775808
  %xor2.i253.8.2 = and i64 %299, %add23.i289.8.2
  %xor.i244.8.2 = xor i64 %add90.8.2, %add89.8.2
  %xor1.i245.8.2 = xor i64 %add89.8.2, %add90.8.1
  %or.i246.8.2 = or i64 %xor.i244.8.2, %xor1.i245.8.2
  %xor2.i247.8.2 = xor i64 %or.i246.8.2, %add90.8.2
  %shr.i254326.8.2 = or i64 %xor2.i247.8.2, %xor2.i253.8.2
  %or94325.8.2 = lshr i64 %shr.i254326.8.2, 63
  %add96.8.2 = add nsw i64 %or94325.8.2, %add96.8.1
  %arrayidx104.8 = getelementptr inbounds i64, i64* %ma, i64 20
  %300 = load i64, i64* %arrayidx104.8, align 8, !tbaa !3
  %add105.8 = add i64 %300, %add82.8.2
  %xor.i238.8 = xor i64 %add105.8, %add82.8.2
  %xor1.i239.8 = xor i64 %300, %add82.8.2
  %or.i240.8 = or i64 %xor.i238.8, %xor1.i239.8
  %xor2.i241.8 = xor i64 %or.i240.8, %add105.8
  %shr.i242.8 = lshr i64 %xor2.i241.8, 63
  %add111.8 = add i64 %shr.i242.8, %add90.8.2
  store i64 %add105.8, i64* %arrayidx50.8, align 8, !tbaa !3
  %301 = xor i64 %add111.8, -9223372036854775808
  %xor2.i.8 = and i64 %301, %add90.8.2
  %shr.i237.8 = lshr i64 %xor2.i.8, 63
  %add118.8 = add nsw i64 %shr.i237.8, %add96.8.2
  %arrayidx74.9 = getelementptr inbounds i64, i64* %mc, i64 10
  %302 = load i64, i64* %arrayidx74.9, align 8, !tbaa !3
  %303 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.9 = and i64 %302, 4294967295
  %shr.i263.9 = lshr i64 %302, 32
  %and1.i264.9 = and i64 %303, 4294967295
  %shr2.i265.9 = lshr i64 %303, 32
  %mul.i266.9 = mul nuw i64 %and1.i264.9, %and.i262.9
  %mul3.i267.9 = mul nuw i64 %shr2.i265.9, %and.i262.9
  %mul4.i268.9 = mul nuw i64 %and1.i264.9, %shr.i263.9
  %mul5.i269.9 = mul nuw i64 %shr2.i265.9, %shr.i263.9
  %and6.i270.9 = and i64 %mul.i266.9, 4294967295
  %shr7.i271.9 = lshr i64 %mul.i266.9, 32
  %and8.i272.9 = and i64 %mul4.i268.9, 4294967295
  %and9.i273.9 = and i64 %mul3.i267.9, 4294967295
  %add.i274.9 = add nuw nsw i64 %shr7.i271.9, %and8.i272.9
  %add10.i275.9 = add nuw nsw i64 %add.i274.9, %and9.i273.9
  %shr11.i276.9 = lshr i64 %add10.i275.9, 32
  %shl.i277.9 = shl i64 %add10.i275.9, 32
  %xor68.i278.9 = or i64 %shl.i277.9, %and6.i270.9
  %shr13.i279.9 = lshr i64 %mul4.i268.9, 32
  %shr14.i280.9 = lshr i64 %mul3.i267.9, 32
  %and15.i281.9 = and i64 %mul5.i269.9, 4294967295
  %add16.i282.9 = add nuw nsw i64 %shr13.i279.9, %shr14.i280.9
  %add17.i283.9 = add nuw nsw i64 %add16.i282.9, %and15.i281.9
  %add18.i284.9 = add nuw nsw i64 %add17.i283.9, %shr11.i276.9
  %and19.i285.9 = and i64 %add18.i284.9, 4294967295
  %and21.i287.9 = and i64 %add18.i284.9, 30064771072
  %and22.i288.9 = and i64 %mul5.i269.9, -4294967296
  %add23.i289.9 = add i64 %and21.i287.9, %and22.i288.9
  %xor2569.i290.9 = or i64 %add23.i289.9, %and19.i285.9
  %add82.9 = add i64 %xor68.i278.9, %add111.8
  %xor.i256.9 = xor i64 %add82.9, %shl.i277.9
  %xor1.i257.9 = xor i64 %shl.i277.9, %add111.8
  %or.i258.9 = or i64 %xor.i256.9, %xor1.i257.9
  %xor2.i259.9 = xor i64 %or.i258.9, %add82.9
  %shr.i260.9 = lshr i64 %xor2.i259.9, 63
  %add89.9 = add i64 %shr.i260.9, %xor2569.i290.9
  %add90.9 = add i64 %add89.9, %add118.8
  %304 = xor i64 %add89.9, -9223372036854775808
  %xor2.i253.9 = and i64 %304, %add23.i289.9
  %xor.i244.9 = xor i64 %add90.9, %add89.9
  %xor1.i245.9 = xor i64 %add89.9, %add118.8
  %or.i246.9 = or i64 %xor.i244.9, %xor1.i245.9
  %xor2.i247.9 = xor i64 %or.i246.9, %add90.9
  %shr.i254326.9 = or i64 %xor2.i247.9, %xor2.i253.9
  %or94325.9 = lshr i64 %shr.i254326.9, 63
  %arrayidx74.9.1 = getelementptr inbounds i64, i64* %mc, i64 11
  %305 = load i64, i64* %arrayidx74.9.1, align 8, !tbaa !3
  %306 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 10), align 16, !tbaa !3
  %and.i262.9.1 = and i64 %305, 4294967295
  %shr.i263.9.1 = lshr i64 %305, 32
  %and1.i264.9.1 = and i64 %306, 4294967295
  %shr2.i265.9.1 = lshr i64 %306, 32
  %mul.i266.9.1 = mul nuw i64 %and1.i264.9.1, %and.i262.9.1
  %mul3.i267.9.1 = mul nuw i64 %shr2.i265.9.1, %and.i262.9.1
  %mul4.i268.9.1 = mul nuw i64 %and1.i264.9.1, %shr.i263.9.1
  %mul5.i269.9.1 = mul nuw i64 %shr2.i265.9.1, %shr.i263.9.1
  %and6.i270.9.1 = and i64 %mul.i266.9.1, 4294967295
  %shr7.i271.9.1 = lshr i64 %mul.i266.9.1, 32
  %and8.i272.9.1 = and i64 %mul4.i268.9.1, 4294967295
  %and9.i273.9.1 = and i64 %mul3.i267.9.1, 4294967295
  %add.i274.9.1 = add nuw nsw i64 %shr7.i271.9.1, %and8.i272.9.1
  %add10.i275.9.1 = add nuw nsw i64 %add.i274.9.1, %and9.i273.9.1
  %shr11.i276.9.1 = lshr i64 %add10.i275.9.1, 32
  %shl.i277.9.1 = shl i64 %add10.i275.9.1, 32
  %xor68.i278.9.1 = or i64 %shl.i277.9.1, %and6.i270.9.1
  %shr13.i279.9.1 = lshr i64 %mul4.i268.9.1, 32
  %shr14.i280.9.1 = lshr i64 %mul3.i267.9.1, 32
  %and15.i281.9.1 = and i64 %mul5.i269.9.1, 4294967295
  %add16.i282.9.1 = add nuw nsw i64 %shr13.i279.9.1, %shr14.i280.9.1
  %add17.i283.9.1 = add nuw nsw i64 %add16.i282.9.1, %and15.i281.9.1
  %add18.i284.9.1 = add nuw nsw i64 %add17.i283.9.1, %shr11.i276.9.1
  %and19.i285.9.1 = and i64 %add18.i284.9.1, 4294967295
  %and21.i287.9.1 = and i64 %add18.i284.9.1, 30064771072
  %and22.i288.9.1 = and i64 %mul5.i269.9.1, -4294967296
  %add23.i289.9.1 = add i64 %and21.i287.9.1, %and22.i288.9.1
  %xor2569.i290.9.1 = or i64 %add23.i289.9.1, %and19.i285.9.1
  %add82.9.1 = add i64 %xor68.i278.9.1, %add82.9
  %xor.i256.9.1 = xor i64 %add82.9.1, %shl.i277.9.1
  %xor1.i257.9.1 = xor i64 %shl.i277.9.1, %add82.9
  %or.i258.9.1 = or i64 %xor.i256.9.1, %xor1.i257.9.1
  %xor2.i259.9.1 = xor i64 %or.i258.9.1, %add82.9.1
  %shr.i260.9.1 = lshr i64 %xor2.i259.9.1, 63
  %add89.9.1 = add i64 %shr.i260.9.1, %xor2569.i290.9.1
  %add90.9.1 = add i64 %add89.9.1, %add90.9
  %307 = xor i64 %add89.9.1, -9223372036854775808
  %xor2.i253.9.1 = and i64 %307, %add23.i289.9.1
  %xor.i244.9.1 = xor i64 %add90.9.1, %add89.9.1
  %xor1.i245.9.1 = xor i64 %add89.9.1, %add90.9
  %or.i246.9.1 = or i64 %xor.i244.9.1, %xor1.i245.9.1
  %xor2.i247.9.1 = xor i64 %or.i246.9.1, %add90.9.1
  %shr.i254326.9.1 = or i64 %xor2.i247.9.1, %xor2.i253.9.1
  %or94325.9.1 = lshr i64 %shr.i254326.9.1, 63
  %add96.9.1 = add nuw nsw i64 %or94325.9.1, %or94325.9
  %arrayidx104.9 = getelementptr inbounds i64, i64* %ma, i64 21
  %308 = load i64, i64* %arrayidx104.9, align 8, !tbaa !3
  %add105.9 = add i64 %308, %add82.9.1
  %xor.i238.9 = xor i64 %add105.9, %add82.9.1
  %xor1.i239.9 = xor i64 %308, %add82.9.1
  %or.i240.9 = or i64 %xor.i238.9, %xor1.i239.9
  %xor2.i241.9 = xor i64 %or.i240.9, %add105.9
  %shr.i242.9 = lshr i64 %xor2.i241.9, 63
  %add111.9 = add i64 %shr.i242.9, %add90.9.1
  store i64 %add105.9, i64* %arrayidx50.9, align 8, !tbaa !3
  %309 = xor i64 %add111.9, -9223372036854775808
  %xor2.i.9 = and i64 %309, %add90.9.1
  %shr.i237.9 = lshr i64 %xor2.i.9, 63
  %add118.9 = add nsw i64 %shr.i237.9, %add96.9.1
  %arrayidx74.10 = getelementptr inbounds i64, i64* %mc, i64 11
  %310 = load i64, i64* %arrayidx74.10, align 8, !tbaa !3
  %311 = load i64, i64* getelementptr inbounds ([12 x i64], [12 x i64]* @p751p1, i64 0, i64 11), align 8, !tbaa !3
  %and.i262.10 = and i64 %310, 4294967295
  %shr.i263.10 = lshr i64 %310, 32
  %and1.i264.10 = and i64 %311, 4294967295
  %shr2.i265.10 = lshr i64 %311, 32
  %mul.i266.10 = mul nuw i64 %and1.i264.10, %and.i262.10
  %mul3.i267.10 = mul nuw i64 %shr2.i265.10, %and.i262.10
  %mul4.i268.10 = mul nuw i64 %and1.i264.10, %shr.i263.10
  %mul5.i269.10 = mul nuw i64 %shr2.i265.10, %shr.i263.10
  %and6.i270.10 = and i64 %mul.i266.10, 4294967295
  %shr7.i271.10 = lshr i64 %mul.i266.10, 32
  %and8.i272.10 = and i64 %mul4.i268.10, 4294967295
  %and9.i273.10 = and i64 %mul3.i267.10, 4294967295
  %add.i274.10 = add nuw nsw i64 %shr7.i271.10, %and8.i272.10
  %add10.i275.10 = add nuw nsw i64 %add.i274.10, %and9.i273.10
  %shr11.i276.10 = lshr i64 %add10.i275.10, 32
  %shl.i277.10 = shl i64 %add10.i275.10, 32
  %xor68.i278.10 = or i64 %shl.i277.10, %and6.i270.10
  %shr13.i279.10 = lshr i64 %mul4.i268.10, 32
  %shr14.i280.10 = lshr i64 %mul3.i267.10, 32
  %and15.i281.10 = and i64 %mul5.i269.10, 4294967295
  %add16.i282.10 = add nuw nsw i64 %shr13.i279.10, %shr14.i280.10
  %add17.i283.10 = add nuw nsw i64 %add16.i282.10, %and15.i281.10
  %add18.i284.10 = add nuw nsw i64 %add17.i283.10, %shr11.i276.10
  %and19.i285.10 = and i64 %add18.i284.10, 4294967295
  %and21.i287.10 = and i64 %add18.i284.10, 30064771072
  %and22.i288.10 = and i64 %mul5.i269.10, -4294967296
  %add23.i289.10 = add i64 %and21.i287.10, %and22.i288.10
  %xor2569.i290.10 = or i64 %add23.i289.10, %and19.i285.10
  %add82.10 = add i64 %xor68.i278.10, %add111.9
  %xor.i256.10 = xor i64 %add82.10, %shl.i277.10
  %xor1.i257.10 = xor i64 %shl.i277.10, %add111.9
  %or.i258.10 = or i64 %xor.i256.10, %xor1.i257.10
  %xor2.i259.10 = xor i64 %or.i258.10, %add82.10
  %shr.i260.10 = lshr i64 %xor2.i259.10, 63
  %add89.10 = add i64 %shr.i260.10, %xor2569.i290.10
  %add90.10 = add i64 %add89.10, %add118.9
  %arrayidx104.10 = getelementptr inbounds i64, i64* %ma, i64 22
  %312 = load i64, i64* %arrayidx104.10, align 8, !tbaa !3
  %add105.10 = add i64 %312, %add82.10
  %xor.i238.10 = xor i64 %add105.10, %add82.10
  %xor1.i239.10 = xor i64 %312, %add82.10
  %or.i240.10 = or i64 %xor.i238.10, %xor1.i239.10
  %xor2.i241.10 = xor i64 %or.i240.10, %add105.10
  %shr.i242.10 = lshr i64 %xor2.i241.10, 63
  %add111.10 = add i64 %shr.i242.10, %add90.10
  store i64 %add105.10, i64* %arrayidx50.10, align 8, !tbaa !3
  %arrayidx127 = getelementptr inbounds i64, i64* %ma, i64 23
  %313 = load i64, i64* %arrayidx127, align 8, !tbaa !3
  %add128 = add i64 %313, %add111.10
  store i64 %add128, i64* %arrayidx50.11, align 8, !tbaa !3
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1) #4

attributes #0 = { inlinehint norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="haswell" "target-features"="+aes,+avx,+avx2,+bmi,+bmi2,+cmov,+cx16,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-adx,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vpopcntdq,-cldemote,-clflushopt,-clwb,-clzero,-fma4,-gfni,-lwp,-movdir64b,-movdiri,-mwaitx,-pconfig,-pku,-prefetchwt1,-prfchw,-ptwrite,-rdpid,-rdseed,-rtm,-sgx,-sha,-shstk,-sse4a,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,-xsavec,-xsaves" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { argmemonly nounwind }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{!"clang version 7.0.0 (tags/RELEASE_700/final)"}
!3 = !{!4, !4, i64 0}
!4 = !{!"long long", !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C/C++ TBAA"}

; ModuleID = 'chacha.c'
source_filename = "chacha.c"
target datalayout = "e-m:o-p:32:32-f64:32:64-v64:32:64-v128:32:128-a:0:32-n32-S32"
target triple = "armv4t-apple-macosx10.17.0"

%struct.ECRYPT_ctx = type { [16 x i32] }

@sigma = internal unnamed_addr constant [16 x i8] c"expand 32-byte k", align 1
@tau = internal unnamed_addr constant [16 x i8] c"expand 16-byte k", align 1

; Function Attrs: nounwind ssp
define void @salsa20_wordtobyte(i8* nocapture %output, i32* nocapture readonly %input) local_unnamed_addr #0 {
entry:
  %input544 = bitcast i32* %input to i8*
  %x = alloca [16 x i32], align 4
  %0 = bitcast [16 x i32]* %x to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %0) #4
  call void @llvm.memcpy.p0i8.p0i8.i32(i8* nonnull align 4 %0, i8* align 4 %input544, i32 64, i1 false)
  %arrayidx5 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 0
  %arrayidx6 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 4
  %arrayidx8 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 12
  %arrayidx14 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 8
  %arrayidx56 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 1
  %arrayidx57 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 5
  %arrayidx60 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 13
  %arrayidx70 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 9
  %arrayidx112 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 2
  %arrayidx113 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 6
  %arrayidx116 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 14
  %arrayidx126 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 10
  %arrayidx168 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 3
  %arrayidx169 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 7
  %arrayidx172 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 15
  %arrayidx182 = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 11
  %arrayidx5.promoted = load i32, i32* %arrayidx5, align 4, !tbaa !4
  %arrayidx6.promoted = load i32, i32* %arrayidx6, align 4, !tbaa !4
  %arrayidx8.promoted = load i32, i32* %arrayidx8, align 4, !tbaa !4
  %arrayidx14.promoted = load i32, i32* %arrayidx14, align 4, !tbaa !4
  %arrayidx56.promoted = load i32, i32* %arrayidx56, align 4, !tbaa !4
  %arrayidx57.promoted = load i32, i32* %arrayidx57, align 4, !tbaa !4
  %arrayidx60.promoted = load i32, i32* %arrayidx60, align 4, !tbaa !4
  %arrayidx70.promoted = load i32, i32* %arrayidx70, align 4, !tbaa !4
  %arrayidx112.promoted = load i32, i32* %arrayidx112, align 4, !tbaa !4
  %arrayidx113.promoted = load i32, i32* %arrayidx113, align 4, !tbaa !4
  %arrayidx116.promoted = load i32, i32* %arrayidx116, align 4, !tbaa !4
  %arrayidx126.promoted = load i32, i32* %arrayidx126, align 4, !tbaa !4
  %arrayidx168.promoted = load i32, i32* %arrayidx168, align 4, !tbaa !4
  %arrayidx169.promoted = load i32, i32* %arrayidx169, align 4, !tbaa !4
  %arrayidx172.promoted = load i32, i32* %arrayidx172, align 4, !tbaa !4
  %arrayidx182.promoted = load i32, i32* %arrayidx182, align 4, !tbaa !4
  %add = add i32 %arrayidx6.promoted, %arrayidx5.promoted
  %xor = xor i32 %arrayidx8.promoted, %add
  %shl = shl i32 %xor, 16
  %shr = lshr i32 %xor, 16
  %or = or i32 %shl, %shr
  %add16 = add i32 %or, %arrayidx14.promoted
  %xor20 = xor i32 %add16, %arrayidx6.promoted
  %shl21 = shl i32 %xor20, 12
  %shr25 = lshr i32 %xor20, 20
  %or26 = or i32 %shl21, %shr25
  %add30 = add i32 %or26, %add
  %xor34 = xor i32 %add30, %or
  %shl35 = shl i32 %xor34, 8
  %shr39 = lshr i32 %xor34, 24
  %or40 = or i32 %shl35, %shr39
  %add44 = add i32 %or40, %add16
  %xor48 = xor i32 %add44, %or26
  %shl49 = shl i32 %xor48, 7
  %shr53 = lshr i32 %xor48, 25
  %or54 = or i32 %shl49, %shr53
  %add58 = add i32 %arrayidx57.promoted, %arrayidx56.promoted
  %xor62 = xor i32 %arrayidx60.promoted, %add58
  %shl63 = shl i32 %xor62, 16
  %shr67 = lshr i32 %xor62, 16
  %or68 = or i32 %shl63, %shr67
  %add72 = add i32 %or68, %arrayidx70.promoted
  %xor76 = xor i32 %add72, %arrayidx57.promoted
  %shl77 = shl i32 %xor76, 12
  %shr81 = lshr i32 %xor76, 20
  %or82 = or i32 %shl77, %shr81
  %add86 = add i32 %or82, %add58
  %xor90 = xor i32 %add86, %or68
  %shl91 = shl i32 %xor90, 8
  %shr95 = lshr i32 %xor90, 24
  %or96 = or i32 %shl91, %shr95
  %add100 = add i32 %or96, %add72
  %xor104 = xor i32 %add100, %or82
  %shl105 = shl i32 %xor104, 7
  %shr109 = lshr i32 %xor104, 25
  %or110 = or i32 %shl105, %shr109
  %add114 = add i32 %arrayidx113.promoted, %arrayidx112.promoted
  %xor118 = xor i32 %arrayidx116.promoted, %add114
  %shl119 = shl i32 %xor118, 16
  %shr123 = lshr i32 %xor118, 16
  %or124 = or i32 %shl119, %shr123
  %add128 = add i32 %or124, %arrayidx126.promoted
  %xor132 = xor i32 %add128, %arrayidx113.promoted
  %shl133 = shl i32 %xor132, 12
  %shr137 = lshr i32 %xor132, 20
  %or138 = or i32 %shl133, %shr137
  %add142 = add i32 %or138, %add114
  %xor146 = xor i32 %add142, %or124
  %shl147 = shl i32 %xor146, 8
  %shr151 = lshr i32 %xor146, 24
  %or152 = or i32 %shl147, %shr151
  %add156 = add i32 %or152, %add128
  %xor160 = xor i32 %add156, %or138
  %shl161 = shl i32 %xor160, 7
  %shr165 = lshr i32 %xor160, 25
  %or166 = or i32 %shl161, %shr165
  %add170 = add i32 %arrayidx169.promoted, %arrayidx168.promoted
  %xor174 = xor i32 %arrayidx172.promoted, %add170
  %shl175 = shl i32 %xor174, 16
  %shr179 = lshr i32 %xor174, 16
  %or180 = or i32 %shl175, %shr179
  %add184 = add i32 %or180, %arrayidx182.promoted
  %xor188 = xor i32 %add184, %arrayidx169.promoted
  %shl189 = shl i32 %xor188, 12
  %shr193 = lshr i32 %xor188, 20
  %or194 = or i32 %shl189, %shr193
  %add198 = add i32 %or194, %add170
  %xor202 = xor i32 %add198, %or180
  %shl203 = shl i32 %xor202, 8
  %shr207 = lshr i32 %xor202, 24
  %or208 = or i32 %shl203, %shr207
  %add212 = add i32 %or208, %add184
  %xor216 = xor i32 %add212, %or194
  %shl217 = shl i32 %xor216, 7
  %shr221 = lshr i32 %xor216, 25
  %or222 = or i32 %shl217, %shr221
  %add226 = add i32 %or110, %add30
  %xor230 = xor i32 %or208, %add226
  %shl231 = shl i32 %xor230, 16
  %shr235 = lshr i32 %xor230, 16
  %or236 = or i32 %shl231, %shr235
  %add240 = add i32 %or236, %add156
  %xor244 = xor i32 %add240, %or110
  %shl245 = shl i32 %xor244, 12
  %shr249 = lshr i32 %xor244, 20
  %or250 = or i32 %shl245, %shr249
  %add254 = add i32 %or250, %add226
  %xor258 = xor i32 %add254, %or236
  %shl259 = shl i32 %xor258, 8
  %shr263 = lshr i32 %xor258, 24
  %or264 = or i32 %shl259, %shr263
  %add268 = add i32 %or264, %add240
  %xor272 = xor i32 %add268, %or250
  %shl273 = shl i32 %xor272, 7
  %shr277 = lshr i32 %xor272, 25
  %or278 = or i32 %shl273, %shr277
  %add282 = add i32 %or166, %add86
  %xor286 = xor i32 %add282, %or40
  %shl287 = shl i32 %xor286, 16
  %shr291 = lshr i32 %xor286, 16
  %or292 = or i32 %shl287, %shr291
  %add296 = add i32 %add212, %or292
  %xor300 = xor i32 %add296, %or166
  %shl301 = shl i32 %xor300, 12
  %shr305 = lshr i32 %xor300, 20
  %or306 = or i32 %shl301, %shr305
  %add310 = add i32 %or306, %add282
  %xor314 = xor i32 %add310, %or292
  %shl315 = shl i32 %xor314, 8
  %shr319 = lshr i32 %xor314, 24
  %or320 = or i32 %shl315, %shr319
  %add324 = add i32 %or320, %add296
  %xor328 = xor i32 %add324, %or306
  %shl329 = shl i32 %xor328, 7
  %shr333 = lshr i32 %xor328, 25
  %or334 = or i32 %shl329, %shr333
  %add338 = add i32 %or222, %add142
  %xor342 = xor i32 %add338, %or96
  %shl343 = shl i32 %xor342, 16
  %shr347 = lshr i32 %xor342, 16
  %or348 = or i32 %shl343, %shr347
  %add352 = add i32 %or348, %add44
  %xor356 = xor i32 %add352, %or222
  %shl357 = shl i32 %xor356, 12
  %shr361 = lshr i32 %xor356, 20
  %or362 = or i32 %shl357, %shr361
  %add366 = add i32 %or362, %add338
  %xor370 = xor i32 %add366, %or348
  %shl371 = shl i32 %xor370, 8
  %shr375 = lshr i32 %xor370, 24
  %or376 = or i32 %shl371, %shr375
  %add380 = add i32 %or376, %add352
  %xor384 = xor i32 %add380, %or362
  %shl385 = shl i32 %xor384, 7
  %shr389 = lshr i32 %xor384, 25
  %or390 = or i32 %shl385, %shr389
  %add394 = add i32 %add198, %or54
  %xor398 = xor i32 %add394, %or152
  %shl399 = shl i32 %xor398, 16
  %shr403 = lshr i32 %xor398, 16
  %or404 = or i32 %shl399, %shr403
  %add408 = add i32 %or404, %add100
  %xor412 = xor i32 %add408, %or54
  %shl413 = shl i32 %xor412, 12
  %shr417 = lshr i32 %xor412, 20
  %or418 = or i32 %shl413, %shr417
  %add422 = add i32 %or418, %add394
  %xor426 = xor i32 %add422, %or404
  %shl427 = shl i32 %xor426, 8
  %shr431 = lshr i32 %xor426, 24
  %or432 = or i32 %shl427, %shr431
  %add436 = add i32 %or432, %add408
  %xor440 = xor i32 %add436, %or418
  %shl441 = shl i32 %xor440, 7
  %shr445 = lshr i32 %xor440, 25
  %or446 = or i32 %shl441, %shr445
  %add.1 = add i32 %or446, %add254
  %xor.1 = xor i32 %or320, %add.1
  %shl.1 = shl i32 %xor.1, 16
  %shr.1 = lshr i32 %xor.1, 16
  %or.1 = or i32 %shl.1, %shr.1
  %add16.1 = add i32 %or.1, %add380
  %xor20.1 = xor i32 %add16.1, %or446
  %shl21.1 = shl i32 %xor20.1, 12
  %shr25.1 = lshr i32 %xor20.1, 20
  %or26.1 = or i32 %shl21.1, %shr25.1
  %add30.1 = add i32 %or26.1, %add.1
  %xor34.1 = xor i32 %add30.1, %or.1
  %shl35.1 = shl i32 %xor34.1, 8
  %shr39.1 = lshr i32 %xor34.1, 24
  %or40.1 = or i32 %shl35.1, %shr39.1
  %add44.1 = add i32 %or40.1, %add16.1
  %xor48.1 = xor i32 %add44.1, %or26.1
  %shl49.1 = shl i32 %xor48.1, 7
  %shr53.1 = lshr i32 %xor48.1, 25
  %or54.1 = or i32 %shl49.1, %shr53.1
  %add58.1 = add i32 %or278, %add310
  %xor62.1 = xor i32 %or376, %add58.1
  %shl63.1 = shl i32 %xor62.1, 16
  %shr67.1 = lshr i32 %xor62.1, 16
  %or68.1 = or i32 %shl63.1, %shr67.1
  %add72.1 = add i32 %or68.1, %add436
  %xor76.1 = xor i32 %add72.1, %or278
  %shl77.1 = shl i32 %xor76.1, 12
  %shr81.1 = lshr i32 %xor76.1, 20
  %or82.1 = or i32 %shl77.1, %shr81.1
  %add86.1 = add i32 %or82.1, %add58.1
  %xor90.1 = xor i32 %add86.1, %or68.1
  %shl91.1 = shl i32 %xor90.1, 8
  %shr95.1 = lshr i32 %xor90.1, 24
  %or96.1 = or i32 %shl91.1, %shr95.1
  %add100.1 = add i32 %or96.1, %add72.1
  %xor104.1 = xor i32 %add100.1, %or82.1
  %shl105.1 = shl i32 %xor104.1, 7
  %shr109.1 = lshr i32 %xor104.1, 25
  %or110.1 = or i32 %shl105.1, %shr109.1
  %add114.1 = add i32 %or334, %add366
  %xor118.1 = xor i32 %or432, %add114.1
  %shl119.1 = shl i32 %xor118.1, 16
  %shr123.1 = lshr i32 %xor118.1, 16
  %or124.1 = or i32 %shl119.1, %shr123.1
  %add128.1 = add i32 %or124.1, %add268
  %xor132.1 = xor i32 %add128.1, %or334
  %shl133.1 = shl i32 %xor132.1, 12
  %shr137.1 = lshr i32 %xor132.1, 20
  %or138.1 = or i32 %shl133.1, %shr137.1
  %add142.1 = add i32 %or138.1, %add114.1
  %xor146.1 = xor i32 %add142.1, %or124.1
  %shl147.1 = shl i32 %xor146.1, 8
  %shr151.1 = lshr i32 %xor146.1, 24
  %or152.1 = or i32 %shl147.1, %shr151.1
  %add156.1 = add i32 %or152.1, %add128.1
  %xor160.1 = xor i32 %add156.1, %or138.1
  %shl161.1 = shl i32 %xor160.1, 7
  %shr165.1 = lshr i32 %xor160.1, 25
  %or166.1 = or i32 %shl161.1, %shr165.1
  %add170.1 = add i32 %or390, %add422
  %xor174.1 = xor i32 %or264, %add170.1
  %shl175.1 = shl i32 %xor174.1, 16
  %shr179.1 = lshr i32 %xor174.1, 16
  %or180.1 = or i32 %shl175.1, %shr179.1
  %add184.1 = add i32 %or180.1, %add324
  %xor188.1 = xor i32 %add184.1, %or390
  %shl189.1 = shl i32 %xor188.1, 12
  %shr193.1 = lshr i32 %xor188.1, 20
  %or194.1 = or i32 %shl189.1, %shr193.1
  %add198.1 = add i32 %or194.1, %add170.1
  %xor202.1 = xor i32 %add198.1, %or180.1
  %shl203.1 = shl i32 %xor202.1, 8
  %shr207.1 = lshr i32 %xor202.1, 24
  %or208.1 = or i32 %shl203.1, %shr207.1
  %add212.1 = add i32 %or208.1, %add184.1
  %xor216.1 = xor i32 %add212.1, %or194.1
  %shl217.1 = shl i32 %xor216.1, 7
  %shr221.1 = lshr i32 %xor216.1, 25
  %or222.1 = or i32 %shl217.1, %shr221.1
  %add226.1 = add i32 %or110.1, %add30.1
  %xor230.1 = xor i32 %or208.1, %add226.1
  %shl231.1 = shl i32 %xor230.1, 16
  %shr235.1 = lshr i32 %xor230.1, 16
  %or236.1 = or i32 %shl231.1, %shr235.1
  %add240.1 = add i32 %or236.1, %add156.1
  %xor244.1 = xor i32 %add240.1, %or110.1
  %shl245.1 = shl i32 %xor244.1, 12
  %shr249.1 = lshr i32 %xor244.1, 20
  %or250.1 = or i32 %shl245.1, %shr249.1
  %add254.1 = add i32 %or250.1, %add226.1
  %xor258.1 = xor i32 %add254.1, %or236.1
  %shl259.1 = shl i32 %xor258.1, 8
  %shr263.1 = lshr i32 %xor258.1, 24
  %or264.1 = or i32 %shl259.1, %shr263.1
  %add268.1 = add i32 %or264.1, %add240.1
  %xor272.1 = xor i32 %add268.1, %or250.1
  %shl273.1 = shl i32 %xor272.1, 7
  %shr277.1 = lshr i32 %xor272.1, 25
  %or278.1 = or i32 %shl273.1, %shr277.1
  %add282.1 = add i32 %or166.1, %add86.1
  %xor286.1 = xor i32 %add282.1, %or40.1
  %shl287.1 = shl i32 %xor286.1, 16
  %shr291.1 = lshr i32 %xor286.1, 16
  %or292.1 = or i32 %shl287.1, %shr291.1
  %add296.1 = add i32 %add212.1, %or292.1
  %xor300.1 = xor i32 %add296.1, %or166.1
  %shl301.1 = shl i32 %xor300.1, 12
  %shr305.1 = lshr i32 %xor300.1, 20
  %or306.1 = or i32 %shl301.1, %shr305.1
  %add310.1 = add i32 %or306.1, %add282.1
  %xor314.1 = xor i32 %add310.1, %or292.1
  %shl315.1 = shl i32 %xor314.1, 8
  %shr319.1 = lshr i32 %xor314.1, 24
  %or320.1 = or i32 %shl315.1, %shr319.1
  %add324.1 = add i32 %or320.1, %add296.1
  %xor328.1 = xor i32 %add324.1, %or306.1
  %shl329.1 = shl i32 %xor328.1, 7
  %shr333.1 = lshr i32 %xor328.1, 25
  %or334.1 = or i32 %shl329.1, %shr333.1
  %add338.1 = add i32 %or222.1, %add142.1
  %xor342.1 = xor i32 %add338.1, %or96.1
  %shl343.1 = shl i32 %xor342.1, 16
  %shr347.1 = lshr i32 %xor342.1, 16
  %or348.1 = or i32 %shl343.1, %shr347.1
  %add352.1 = add i32 %or348.1, %add44.1
  %xor356.1 = xor i32 %add352.1, %or222.1
  %shl357.1 = shl i32 %xor356.1, 12
  %shr361.1 = lshr i32 %xor356.1, 20
  %or362.1 = or i32 %shl357.1, %shr361.1
  %add366.1 = add i32 %or362.1, %add338.1
  %xor370.1 = xor i32 %add366.1, %or348.1
  %shl371.1 = shl i32 %xor370.1, 8
  %shr375.1 = lshr i32 %xor370.1, 24
  %or376.1 = or i32 %shl371.1, %shr375.1
  %add380.1 = add i32 %or376.1, %add352.1
  %xor384.1 = xor i32 %add380.1, %or362.1
  %shl385.1 = shl i32 %xor384.1, 7
  %shr389.1 = lshr i32 %xor384.1, 25
  %or390.1 = or i32 %shl385.1, %shr389.1
  %add394.1 = add i32 %add198.1, %or54.1
  %xor398.1 = xor i32 %add394.1, %or152.1
  %shl399.1 = shl i32 %xor398.1, 16
  %shr403.1 = lshr i32 %xor398.1, 16
  %or404.1 = or i32 %shl399.1, %shr403.1
  %add408.1 = add i32 %or404.1, %add100.1
  %xor412.1 = xor i32 %add408.1, %or54.1
  %shl413.1 = shl i32 %xor412.1, 12
  %shr417.1 = lshr i32 %xor412.1, 20
  %or418.1 = or i32 %shl413.1, %shr417.1
  %add422.1 = add i32 %or418.1, %add394.1
  %xor426.1 = xor i32 %add422.1, %or404.1
  %shl427.1 = shl i32 %xor426.1, 8
  %shr431.1 = lshr i32 %xor426.1, 24
  %or432.1 = or i32 %shl427.1, %shr431.1
  %add436.1 = add i32 %or432.1, %add408.1
  %xor440.1 = xor i32 %add436.1, %or418.1
  %shl441.1 = shl i32 %xor440.1, 7
  %shr445.1 = lshr i32 %xor440.1, 25
  %or446.1 = or i32 %shl441.1, %shr445.1
  %add.2 = add i32 %or446.1, %add254.1
  %xor.2 = xor i32 %or320.1, %add.2
  %shl.2 = shl i32 %xor.2, 16
  %shr.2 = lshr i32 %xor.2, 16
  %or.2 = or i32 %shl.2, %shr.2
  %add16.2 = add i32 %or.2, %add380.1
  %xor20.2 = xor i32 %add16.2, %or446.1
  %shl21.2 = shl i32 %xor20.2, 12
  %shr25.2 = lshr i32 %xor20.2, 20
  %or26.2 = or i32 %shl21.2, %shr25.2
  %add30.2 = add i32 %or26.2, %add.2
  %xor34.2 = xor i32 %add30.2, %or.2
  %shl35.2 = shl i32 %xor34.2, 8
  %shr39.2 = lshr i32 %xor34.2, 24
  %or40.2 = or i32 %shl35.2, %shr39.2
  %add44.2 = add i32 %or40.2, %add16.2
  %xor48.2 = xor i32 %add44.2, %or26.2
  %shl49.2 = shl i32 %xor48.2, 7
  %shr53.2 = lshr i32 %xor48.2, 25
  %or54.2 = or i32 %shl49.2, %shr53.2
  %add58.2 = add i32 %or278.1, %add310.1
  %xor62.2 = xor i32 %or376.1, %add58.2
  %shl63.2 = shl i32 %xor62.2, 16
  %shr67.2 = lshr i32 %xor62.2, 16
  %or68.2 = or i32 %shl63.2, %shr67.2
  %add72.2 = add i32 %or68.2, %add436.1
  %xor76.2 = xor i32 %add72.2, %or278.1
  %shl77.2 = shl i32 %xor76.2, 12
  %shr81.2 = lshr i32 %xor76.2, 20
  %or82.2 = or i32 %shl77.2, %shr81.2
  %add86.2 = add i32 %or82.2, %add58.2
  %xor90.2 = xor i32 %add86.2, %or68.2
  %shl91.2 = shl i32 %xor90.2, 8
  %shr95.2 = lshr i32 %xor90.2, 24
  %or96.2 = or i32 %shl91.2, %shr95.2
  %add100.2 = add i32 %or96.2, %add72.2
  %xor104.2 = xor i32 %add100.2, %or82.2
  %shl105.2 = shl i32 %xor104.2, 7
  %shr109.2 = lshr i32 %xor104.2, 25
  %or110.2 = or i32 %shl105.2, %shr109.2
  %add114.2 = add i32 %or334.1, %add366.1
  %xor118.2 = xor i32 %or432.1, %add114.2
  %shl119.2 = shl i32 %xor118.2, 16
  %shr123.2 = lshr i32 %xor118.2, 16
  %or124.2 = or i32 %shl119.2, %shr123.2
  %add128.2 = add i32 %or124.2, %add268.1
  %xor132.2 = xor i32 %add128.2, %or334.1
  %shl133.2 = shl i32 %xor132.2, 12
  %shr137.2 = lshr i32 %xor132.2, 20
  %or138.2 = or i32 %shl133.2, %shr137.2
  %add142.2 = add i32 %or138.2, %add114.2
  %xor146.2 = xor i32 %add142.2, %or124.2
  %shl147.2 = shl i32 %xor146.2, 8
  %shr151.2 = lshr i32 %xor146.2, 24
  %or152.2 = or i32 %shl147.2, %shr151.2
  %add156.2 = add i32 %or152.2, %add128.2
  %xor160.2 = xor i32 %add156.2, %or138.2
  %shl161.2 = shl i32 %xor160.2, 7
  %shr165.2 = lshr i32 %xor160.2, 25
  %or166.2 = or i32 %shl161.2, %shr165.2
  %add170.2 = add i32 %or390.1, %add422.1
  %xor174.2 = xor i32 %or264.1, %add170.2
  %shl175.2 = shl i32 %xor174.2, 16
  %shr179.2 = lshr i32 %xor174.2, 16
  %or180.2 = or i32 %shl175.2, %shr179.2
  %add184.2 = add i32 %or180.2, %add324.1
  %xor188.2 = xor i32 %add184.2, %or390.1
  %shl189.2 = shl i32 %xor188.2, 12
  %shr193.2 = lshr i32 %xor188.2, 20
  %or194.2 = or i32 %shl189.2, %shr193.2
  %add198.2 = add i32 %or194.2, %add170.2
  %xor202.2 = xor i32 %add198.2, %or180.2
  %shl203.2 = shl i32 %xor202.2, 8
  %shr207.2 = lshr i32 %xor202.2, 24
  %or208.2 = or i32 %shl203.2, %shr207.2
  %add212.2 = add i32 %or208.2, %add184.2
  %xor216.2 = xor i32 %add212.2, %or194.2
  %shl217.2 = shl i32 %xor216.2, 7
  %shr221.2 = lshr i32 %xor216.2, 25
  %or222.2 = or i32 %shl217.2, %shr221.2
  %add226.2 = add i32 %or110.2, %add30.2
  %xor230.2 = xor i32 %or208.2, %add226.2
  %shl231.2 = shl i32 %xor230.2, 16
  %shr235.2 = lshr i32 %xor230.2, 16
  %or236.2 = or i32 %shl231.2, %shr235.2
  %add240.2 = add i32 %or236.2, %add156.2
  %xor244.2 = xor i32 %add240.2, %or110.2
  %shl245.2 = shl i32 %xor244.2, 12
  %shr249.2 = lshr i32 %xor244.2, 20
  %or250.2 = or i32 %shl245.2, %shr249.2
  %add254.2 = add i32 %or250.2, %add226.2
  %xor258.2 = xor i32 %add254.2, %or236.2
  %shl259.2 = shl i32 %xor258.2, 8
  %shr263.2 = lshr i32 %xor258.2, 24
  %or264.2 = or i32 %shl259.2, %shr263.2
  %add268.2 = add i32 %or264.2, %add240.2
  %xor272.2 = xor i32 %add268.2, %or250.2
  %shl273.2 = shl i32 %xor272.2, 7
  %shr277.2 = lshr i32 %xor272.2, 25
  %or278.2 = or i32 %shl273.2, %shr277.2
  %add282.2 = add i32 %or166.2, %add86.2
  %xor286.2 = xor i32 %add282.2, %or40.2
  %shl287.2 = shl i32 %xor286.2, 16
  %shr291.2 = lshr i32 %xor286.2, 16
  %or292.2 = or i32 %shl287.2, %shr291.2
  %add296.2 = add i32 %add212.2, %or292.2
  %xor300.2 = xor i32 %add296.2, %or166.2
  %shl301.2 = shl i32 %xor300.2, 12
  %shr305.2 = lshr i32 %xor300.2, 20
  %or306.2 = or i32 %shl301.2, %shr305.2
  %add310.2 = add i32 %or306.2, %add282.2
  %xor314.2 = xor i32 %add310.2, %or292.2
  %shl315.2 = shl i32 %xor314.2, 8
  %shr319.2 = lshr i32 %xor314.2, 24
  %or320.2 = or i32 %shl315.2, %shr319.2
  %add324.2 = add i32 %or320.2, %add296.2
  %xor328.2 = xor i32 %add324.2, %or306.2
  %shl329.2 = shl i32 %xor328.2, 7
  %shr333.2 = lshr i32 %xor328.2, 25
  %or334.2 = or i32 %shl329.2, %shr333.2
  %add338.2 = add i32 %or222.2, %add142.2
  %xor342.2 = xor i32 %add338.2, %or96.2
  %shl343.2 = shl i32 %xor342.2, 16
  %shr347.2 = lshr i32 %xor342.2, 16
  %or348.2 = or i32 %shl343.2, %shr347.2
  %add352.2 = add i32 %or348.2, %add44.2
  %xor356.2 = xor i32 %add352.2, %or222.2
  %shl357.2 = shl i32 %xor356.2, 12
  %shr361.2 = lshr i32 %xor356.2, 20
  %or362.2 = or i32 %shl357.2, %shr361.2
  %add366.2 = add i32 %or362.2, %add338.2
  %xor370.2 = xor i32 %add366.2, %or348.2
  %shl371.2 = shl i32 %xor370.2, 8
  %shr375.2 = lshr i32 %xor370.2, 24
  %or376.2 = or i32 %shl371.2, %shr375.2
  %add380.2 = add i32 %or376.2, %add352.2
  %xor384.2 = xor i32 %add380.2, %or362.2
  %shl385.2 = shl i32 %xor384.2, 7
  %shr389.2 = lshr i32 %xor384.2, 25
  %or390.2 = or i32 %shl385.2, %shr389.2
  %add394.2 = add i32 %add198.2, %or54.2
  %xor398.2 = xor i32 %add394.2, %or152.2
  %shl399.2 = shl i32 %xor398.2, 16
  %shr403.2 = lshr i32 %xor398.2, 16
  %or404.2 = or i32 %shl399.2, %shr403.2
  %add408.2 = add i32 %or404.2, %add100.2
  %xor412.2 = xor i32 %add408.2, %or54.2
  %shl413.2 = shl i32 %xor412.2, 12
  %shr417.2 = lshr i32 %xor412.2, 20
  %or418.2 = or i32 %shl413.2, %shr417.2
  %add422.2 = add i32 %or418.2, %add394.2
  %xor426.2 = xor i32 %add422.2, %or404.2
  %shl427.2 = shl i32 %xor426.2, 8
  %shr431.2 = lshr i32 %xor426.2, 24
  %or432.2 = or i32 %shl427.2, %shr431.2
  %add436.2 = add i32 %or432.2, %add408.2
  %xor440.2 = xor i32 %add436.2, %or418.2
  %shl441.2 = shl i32 %xor440.2, 7
  %shr445.2 = lshr i32 %xor440.2, 25
  %or446.2 = or i32 %shl441.2, %shr445.2
  %add.3 = add i32 %or446.2, %add254.2
  %xor.3 = xor i32 %or320.2, %add.3
  %shl.3 = shl i32 %xor.3, 16
  %shr.3 = lshr i32 %xor.3, 16
  %or.3 = or i32 %shl.3, %shr.3
  %add16.3 = add i32 %or.3, %add380.2
  %xor20.3 = xor i32 %add16.3, %or446.2
  %shl21.3 = shl i32 %xor20.3, 12
  %shr25.3 = lshr i32 %xor20.3, 20
  %or26.3 = or i32 %shl21.3, %shr25.3
  %add30.3 = add i32 %or26.3, %add.3
  %xor34.3 = xor i32 %add30.3, %or.3
  %shl35.3 = shl i32 %xor34.3, 8
  %shr39.3 = lshr i32 %xor34.3, 24
  %or40.3 = or i32 %shl35.3, %shr39.3
  %add44.3 = add i32 %or40.3, %add16.3
  %xor48.3 = xor i32 %add44.3, %or26.3
  %shl49.3 = shl i32 %xor48.3, 7
  %shr53.3 = lshr i32 %xor48.3, 25
  %or54.3 = or i32 %shl49.3, %shr53.3
  %add58.3 = add i32 %or278.2, %add310.2
  %xor62.3 = xor i32 %or376.2, %add58.3
  %shl63.3 = shl i32 %xor62.3, 16
  %shr67.3 = lshr i32 %xor62.3, 16
  %or68.3 = or i32 %shl63.3, %shr67.3
  %add72.3 = add i32 %or68.3, %add436.2
  %xor76.3 = xor i32 %add72.3, %or278.2
  %shl77.3 = shl i32 %xor76.3, 12
  %shr81.3 = lshr i32 %xor76.3, 20
  %or82.3 = or i32 %shl77.3, %shr81.3
  %add86.3 = add i32 %or82.3, %add58.3
  %xor90.3 = xor i32 %add86.3, %or68.3
  %shl91.3 = shl i32 %xor90.3, 8
  %shr95.3 = lshr i32 %xor90.3, 24
  %or96.3 = or i32 %shl91.3, %shr95.3
  %add100.3 = add i32 %or96.3, %add72.3
  %xor104.3 = xor i32 %add100.3, %or82.3
  %shl105.3 = shl i32 %xor104.3, 7
  %shr109.3 = lshr i32 %xor104.3, 25
  %or110.3 = or i32 %shl105.3, %shr109.3
  %add114.3 = add i32 %or334.2, %add366.2
  %xor118.3 = xor i32 %or432.2, %add114.3
  %shl119.3 = shl i32 %xor118.3, 16
  %shr123.3 = lshr i32 %xor118.3, 16
  %or124.3 = or i32 %shl119.3, %shr123.3
  %add128.3 = add i32 %or124.3, %add268.2
  %xor132.3 = xor i32 %add128.3, %or334.2
  %shl133.3 = shl i32 %xor132.3, 12
  %shr137.3 = lshr i32 %xor132.3, 20
  %or138.3 = or i32 %shl133.3, %shr137.3
  %add142.3 = add i32 %or138.3, %add114.3
  %xor146.3 = xor i32 %add142.3, %or124.3
  %shl147.3 = shl i32 %xor146.3, 8
  %shr151.3 = lshr i32 %xor146.3, 24
  %or152.3 = or i32 %shl147.3, %shr151.3
  %add156.3 = add i32 %or152.3, %add128.3
  %xor160.3 = xor i32 %add156.3, %or138.3
  %shl161.3 = shl i32 %xor160.3, 7
  %shr165.3 = lshr i32 %xor160.3, 25
  %or166.3 = or i32 %shl161.3, %shr165.3
  %add170.3 = add i32 %or390.2, %add422.2
  %xor174.3 = xor i32 %or264.2, %add170.3
  %shl175.3 = shl i32 %xor174.3, 16
  %shr179.3 = lshr i32 %xor174.3, 16
  %or180.3 = or i32 %shl175.3, %shr179.3
  %add184.3 = add i32 %or180.3, %add324.2
  %xor188.3 = xor i32 %add184.3, %or390.2
  %shl189.3 = shl i32 %xor188.3, 12
  %shr193.3 = lshr i32 %xor188.3, 20
  %or194.3 = or i32 %shl189.3, %shr193.3
  %add198.3 = add i32 %or194.3, %add170.3
  %xor202.3 = xor i32 %add198.3, %or180.3
  %shl203.3 = shl i32 %xor202.3, 8
  %shr207.3 = lshr i32 %xor202.3, 24
  %or208.3 = or i32 %shl203.3, %shr207.3
  %add212.3 = add i32 %or208.3, %add184.3
  %xor216.3 = xor i32 %add212.3, %or194.3
  %shl217.3 = shl i32 %xor216.3, 7
  %shr221.3 = lshr i32 %xor216.3, 25
  %or222.3 = or i32 %shl217.3, %shr221.3
  %add226.3 = add i32 %or110.3, %add30.3
  %xor230.3 = xor i32 %or208.3, %add226.3
  %shl231.3 = shl i32 %xor230.3, 16
  %shr235.3 = lshr i32 %xor230.3, 16
  %or236.3 = or i32 %shl231.3, %shr235.3
  %add240.3 = add i32 %or236.3, %add156.3
  %xor244.3 = xor i32 %add240.3, %or110.3
  %shl245.3 = shl i32 %xor244.3, 12
  %shr249.3 = lshr i32 %xor244.3, 20
  %or250.3 = or i32 %shl245.3, %shr249.3
  %add254.3 = add i32 %or250.3, %add226.3
  %xor258.3 = xor i32 %add254.3, %or236.3
  %shl259.3 = shl i32 %xor258.3, 8
  %shr263.3 = lshr i32 %xor258.3, 24
  %or264.3 = or i32 %shl259.3, %shr263.3
  %add268.3 = add i32 %or264.3, %add240.3
  %xor272.3 = xor i32 %add268.3, %or250.3
  %shl273.3 = shl i32 %xor272.3, 7
  %shr277.3 = lshr i32 %xor272.3, 25
  %or278.3 = or i32 %shl273.3, %shr277.3
  %add282.3 = add i32 %or166.3, %add86.3
  %xor286.3 = xor i32 %add282.3, %or40.3
  %shl287.3 = shl i32 %xor286.3, 16
  %shr291.3 = lshr i32 %xor286.3, 16
  %or292.3 = or i32 %shl287.3, %shr291.3
  %add296.3 = add i32 %add212.3, %or292.3
  %xor300.3 = xor i32 %add296.3, %or166.3
  %shl301.3 = shl i32 %xor300.3, 12
  %shr305.3 = lshr i32 %xor300.3, 20
  %or306.3 = or i32 %shl301.3, %shr305.3
  %add310.3 = add i32 %or306.3, %add282.3
  %xor314.3 = xor i32 %add310.3, %or292.3
  %shl315.3 = shl i32 %xor314.3, 8
  %shr319.3 = lshr i32 %xor314.3, 24
  %or320.3 = or i32 %shl315.3, %shr319.3
  %add324.3 = add i32 %or320.3, %add296.3
  %xor328.3 = xor i32 %add324.3, %or306.3
  %shl329.3 = shl i32 %xor328.3, 7
  %shr333.3 = lshr i32 %xor328.3, 25
  %or334.3 = or i32 %shl329.3, %shr333.3
  %add338.3 = add i32 %or222.3, %add142.3
  %xor342.3 = xor i32 %add338.3, %or96.3
  %shl343.3 = shl i32 %xor342.3, 16
  %shr347.3 = lshr i32 %xor342.3, 16
  %or348.3 = or i32 %shl343.3, %shr347.3
  %add352.3 = add i32 %or348.3, %add44.3
  %xor356.3 = xor i32 %add352.3, %or222.3
  %shl357.3 = shl i32 %xor356.3, 12
  %shr361.3 = lshr i32 %xor356.3, 20
  %or362.3 = or i32 %shl357.3, %shr361.3
  %add366.3 = add i32 %or362.3, %add338.3
  %xor370.3 = xor i32 %add366.3, %or348.3
  %shl371.3 = shl i32 %xor370.3, 8
  %shr375.3 = lshr i32 %xor370.3, 24
  %or376.3 = or i32 %shl371.3, %shr375.3
  %add380.3 = add i32 %or376.3, %add352.3
  %xor384.3 = xor i32 %add380.3, %or362.3
  %shl385.3 = shl i32 %xor384.3, 7
  %shr389.3 = lshr i32 %xor384.3, 25
  %or390.3 = or i32 %shl385.3, %shr389.3
  %add394.3 = add i32 %add198.3, %or54.3
  %xor398.3 = xor i32 %add394.3, %or152.3
  %shl399.3 = shl i32 %xor398.3, 16
  %shr403.3 = lshr i32 %xor398.3, 16
  %or404.3 = or i32 %shl399.3, %shr403.3
  %add408.3 = add i32 %or404.3, %add100.3
  %xor412.3 = xor i32 %add408.3, %or54.3
  %shl413.3 = shl i32 %xor412.3, 12
  %shr417.3 = lshr i32 %xor412.3, 20
  %or418.3 = or i32 %shl413.3, %shr417.3
  %add422.3 = add i32 %or418.3, %add394.3
  %xor426.3 = xor i32 %add422.3, %or404.3
  %shl427.3 = shl i32 %xor426.3, 8
  %shr431.3 = lshr i32 %xor426.3, 24
  %or432.3 = or i32 %shl427.3, %shr431.3
  %add436.3 = add i32 %or432.3, %add408.3
  %xor440.3 = xor i32 %add436.3, %or418.3
  %shl441.3 = shl i32 %xor440.3, 7
  %shr445.3 = lshr i32 %xor440.3, 25
  %or446.3 = or i32 %shl441.3, %shr445.3
  %add.4 = add i32 %or446.3, %add254.3
  %xor.4 = xor i32 %or320.3, %add.4
  %shl.4 = shl i32 %xor.4, 16
  %shr.4 = lshr i32 %xor.4, 16
  %or.4 = or i32 %shl.4, %shr.4
  %add16.4 = add i32 %or.4, %add380.3
  %xor20.4 = xor i32 %add16.4, %or446.3
  %shl21.4 = shl i32 %xor20.4, 12
  %shr25.4 = lshr i32 %xor20.4, 20
  %or26.4 = or i32 %shl21.4, %shr25.4
  %add30.4 = add i32 %or26.4, %add.4
  %xor34.4 = xor i32 %add30.4, %or.4
  %shl35.4 = shl i32 %xor34.4, 8
  %shr39.4 = lshr i32 %xor34.4, 24
  %or40.4 = or i32 %shl35.4, %shr39.4
  %add44.4 = add i32 %or40.4, %add16.4
  %xor48.4 = xor i32 %add44.4, %or26.4
  %shl49.4 = shl i32 %xor48.4, 7
  %shr53.4 = lshr i32 %xor48.4, 25
  %or54.4 = or i32 %shl49.4, %shr53.4
  %add58.4 = add i32 %or278.3, %add310.3
  %xor62.4 = xor i32 %or376.3, %add58.4
  %shl63.4 = shl i32 %xor62.4, 16
  %shr67.4 = lshr i32 %xor62.4, 16
  %or68.4 = or i32 %shl63.4, %shr67.4
  %add72.4 = add i32 %or68.4, %add436.3
  %xor76.4 = xor i32 %add72.4, %or278.3
  %shl77.4 = shl i32 %xor76.4, 12
  %shr81.4 = lshr i32 %xor76.4, 20
  %or82.4 = or i32 %shl77.4, %shr81.4
  %add86.4 = add i32 %or82.4, %add58.4
  %xor90.4 = xor i32 %add86.4, %or68.4
  %shl91.4 = shl i32 %xor90.4, 8
  %shr95.4 = lshr i32 %xor90.4, 24
  %or96.4 = or i32 %shl91.4, %shr95.4
  %add100.4 = add i32 %or96.4, %add72.4
  %xor104.4 = xor i32 %add100.4, %or82.4
  %shl105.4 = shl i32 %xor104.4, 7
  %shr109.4 = lshr i32 %xor104.4, 25
  %or110.4 = or i32 %shl105.4, %shr109.4
  %add114.4 = add i32 %or334.3, %add366.3
  %xor118.4 = xor i32 %or432.3, %add114.4
  %shl119.4 = shl i32 %xor118.4, 16
  %shr123.4 = lshr i32 %xor118.4, 16
  %or124.4 = or i32 %shl119.4, %shr123.4
  %add128.4 = add i32 %or124.4, %add268.3
  %xor132.4 = xor i32 %add128.4, %or334.3
  %shl133.4 = shl i32 %xor132.4, 12
  %shr137.4 = lshr i32 %xor132.4, 20
  %or138.4 = or i32 %shl133.4, %shr137.4
  %add142.4 = add i32 %or138.4, %add114.4
  %xor146.4 = xor i32 %add142.4, %or124.4
  %shl147.4 = shl i32 %xor146.4, 8
  %shr151.4 = lshr i32 %xor146.4, 24
  %or152.4 = or i32 %shl147.4, %shr151.4
  %add156.4 = add i32 %or152.4, %add128.4
  %xor160.4 = xor i32 %add156.4, %or138.4
  %shl161.4 = shl i32 %xor160.4, 7
  %shr165.4 = lshr i32 %xor160.4, 25
  %or166.4 = or i32 %shl161.4, %shr165.4
  %add170.4 = add i32 %or390.3, %add422.3
  %xor174.4 = xor i32 %or264.3, %add170.4
  %shl175.4 = shl i32 %xor174.4, 16
  %shr179.4 = lshr i32 %xor174.4, 16
  %or180.4 = or i32 %shl175.4, %shr179.4
  %add184.4 = add i32 %or180.4, %add324.3
  %xor188.4 = xor i32 %add184.4, %or390.3
  %shl189.4 = shl i32 %xor188.4, 12
  %shr193.4 = lshr i32 %xor188.4, 20
  %or194.4 = or i32 %shl189.4, %shr193.4
  %add198.4 = add i32 %or194.4, %add170.4
  %xor202.4 = xor i32 %add198.4, %or180.4
  %shl203.4 = shl i32 %xor202.4, 8
  %shr207.4 = lshr i32 %xor202.4, 24
  %or208.4 = or i32 %shl203.4, %shr207.4
  %add212.4 = add i32 %or208.4, %add184.4
  %xor216.4 = xor i32 %add212.4, %or194.4
  %shl217.4 = shl i32 %xor216.4, 7
  %shr221.4 = lshr i32 %xor216.4, 25
  %or222.4 = or i32 %shl217.4, %shr221.4
  %add226.4 = add i32 %or110.4, %add30.4
  %xor230.4 = xor i32 %or208.4, %add226.4
  %shl231.4 = shl i32 %xor230.4, 16
  %shr235.4 = lshr i32 %xor230.4, 16
  %or236.4 = or i32 %shl231.4, %shr235.4
  %add240.4 = add i32 %or236.4, %add156.4
  %xor244.4 = xor i32 %add240.4, %or110.4
  %shl245.4 = shl i32 %xor244.4, 12
  %shr249.4 = lshr i32 %xor244.4, 20
  %or250.4 = or i32 %shl245.4, %shr249.4
  %add254.4 = add i32 %or250.4, %add226.4
  %xor258.4 = xor i32 %add254.4, %or236.4
  %shl259.4 = shl i32 %xor258.4, 8
  %shr263.4 = lshr i32 %xor258.4, 24
  %or264.4 = or i32 %shl259.4, %shr263.4
  %add268.4 = add i32 %or264.4, %add240.4
  %xor272.4 = xor i32 %add268.4, %or250.4
  %shl273.4 = shl i32 %xor272.4, 7
  %shr277.4 = lshr i32 %xor272.4, 25
  %or278.4 = or i32 %shl273.4, %shr277.4
  %add282.4 = add i32 %or166.4, %add86.4
  %xor286.4 = xor i32 %add282.4, %or40.4
  %shl287.4 = shl i32 %xor286.4, 16
  %shr291.4 = lshr i32 %xor286.4, 16
  %or292.4 = or i32 %shl287.4, %shr291.4
  %add296.4 = add i32 %add212.4, %or292.4
  %xor300.4 = xor i32 %add296.4, %or166.4
  %shl301.4 = shl i32 %xor300.4, 12
  %shr305.4 = lshr i32 %xor300.4, 20
  %or306.4 = or i32 %shl301.4, %shr305.4
  %add310.4 = add i32 %or306.4, %add282.4
  %xor314.4 = xor i32 %add310.4, %or292.4
  %shl315.4 = shl i32 %xor314.4, 8
  %shr319.4 = lshr i32 %xor314.4, 24
  %or320.4 = or i32 %shl315.4, %shr319.4
  %add324.4 = add i32 %or320.4, %add296.4
  %xor328.4 = xor i32 %add324.4, %or306.4
  %shl329.4 = shl i32 %xor328.4, 7
  %shr333.4 = lshr i32 %xor328.4, 25
  %or334.4 = or i32 %shl329.4, %shr333.4
  %add338.4 = add i32 %or222.4, %add142.4
  %xor342.4 = xor i32 %add338.4, %or96.4
  %shl343.4 = shl i32 %xor342.4, 16
  %shr347.4 = lshr i32 %xor342.4, 16
  %or348.4 = or i32 %shl343.4, %shr347.4
  %add352.4 = add i32 %or348.4, %add44.4
  %xor356.4 = xor i32 %add352.4, %or222.4
  %shl357.4 = shl i32 %xor356.4, 12
  %shr361.4 = lshr i32 %xor356.4, 20
  %or362.4 = or i32 %shl357.4, %shr361.4
  %add366.4 = add i32 %or362.4, %add338.4
  %xor370.4 = xor i32 %add366.4, %or348.4
  %shl371.4 = shl i32 %xor370.4, 8
  %shr375.4 = lshr i32 %xor370.4, 24
  %or376.4 = or i32 %shl371.4, %shr375.4
  %add380.4 = add i32 %or376.4, %add352.4
  %xor384.4 = xor i32 %add380.4, %or362.4
  %shl385.4 = shl i32 %xor384.4, 7
  %shr389.4 = lshr i32 %xor384.4, 25
  %or390.4 = or i32 %shl385.4, %shr389.4
  %add394.4 = add i32 %add198.4, %or54.4
  %xor398.4 = xor i32 %add394.4, %or152.4
  %shl399.4 = shl i32 %xor398.4, 16
  %shr403.4 = lshr i32 %xor398.4, 16
  %or404.4 = or i32 %shl399.4, %shr403.4
  %add408.4 = add i32 %or404.4, %add100.4
  %xor412.4 = xor i32 %add408.4, %or54.4
  %shl413.4 = shl i32 %xor412.4, 12
  %shr417.4 = lshr i32 %xor412.4, 20
  %or418.4 = or i32 %shl413.4, %shr417.4
  %add422.4 = add i32 %or418.4, %add394.4
  %xor426.4 = xor i32 %add422.4, %or404.4
  %shl427.4 = shl i32 %xor426.4, 8
  %shr431.4 = lshr i32 %xor426.4, 24
  %or432.4 = or i32 %shl427.4, %shr431.4
  %add436.4 = add i32 %or432.4, %add408.4
  %xor440.4 = xor i32 %add436.4, %or418.4
  %shl441.4 = shl i32 %xor440.4, 7
  %shr445.4 = lshr i32 %xor440.4, 25
  %or446.4 = or i32 %shl441.4, %shr445.4
  %add.5 = add i32 %or446.4, %add254.4
  %xor.5 = xor i32 %or320.4, %add.5
  %shl.5 = shl i32 %xor.5, 16
  %shr.5 = lshr i32 %xor.5, 16
  %or.5 = or i32 %shl.5, %shr.5
  %add16.5 = add i32 %or.5, %add380.4
  %xor20.5 = xor i32 %add16.5, %or446.4
  %shl21.5 = shl i32 %xor20.5, 12
  %shr25.5 = lshr i32 %xor20.5, 20
  %or26.5 = or i32 %shl21.5, %shr25.5
  %add30.5 = add i32 %or26.5, %add.5
  %xor34.5 = xor i32 %add30.5, %or.5
  %shl35.5 = shl i32 %xor34.5, 8
  %shr39.5 = lshr i32 %xor34.5, 24
  %or40.5 = or i32 %shl35.5, %shr39.5
  %add44.5 = add i32 %or40.5, %add16.5
  %xor48.5 = xor i32 %add44.5, %or26.5
  %shl49.5 = shl i32 %xor48.5, 7
  %shr53.5 = lshr i32 %xor48.5, 25
  %or54.5 = or i32 %shl49.5, %shr53.5
  %add58.5 = add i32 %or278.4, %add310.4
  %xor62.5 = xor i32 %or376.4, %add58.5
  %shl63.5 = shl i32 %xor62.5, 16
  %shr67.5 = lshr i32 %xor62.5, 16
  %or68.5 = or i32 %shl63.5, %shr67.5
  %add72.5 = add i32 %or68.5, %add436.4
  %xor76.5 = xor i32 %add72.5, %or278.4
  %shl77.5 = shl i32 %xor76.5, 12
  %shr81.5 = lshr i32 %xor76.5, 20
  %or82.5 = or i32 %shl77.5, %shr81.5
  %add86.5 = add i32 %or82.5, %add58.5
  %xor90.5 = xor i32 %add86.5, %or68.5
  %shl91.5 = shl i32 %xor90.5, 8
  %shr95.5 = lshr i32 %xor90.5, 24
  %or96.5 = or i32 %shl91.5, %shr95.5
  %add100.5 = add i32 %or96.5, %add72.5
  %xor104.5 = xor i32 %add100.5, %or82.5
  %shl105.5 = shl i32 %xor104.5, 7
  %shr109.5 = lshr i32 %xor104.5, 25
  %or110.5 = or i32 %shl105.5, %shr109.5
  %add114.5 = add i32 %or334.4, %add366.4
  %xor118.5 = xor i32 %or432.4, %add114.5
  %shl119.5 = shl i32 %xor118.5, 16
  %shr123.5 = lshr i32 %xor118.5, 16
  %or124.5 = or i32 %shl119.5, %shr123.5
  %add128.5 = add i32 %or124.5, %add268.4
  %xor132.5 = xor i32 %add128.5, %or334.4
  %shl133.5 = shl i32 %xor132.5, 12
  %shr137.5 = lshr i32 %xor132.5, 20
  %or138.5 = or i32 %shl133.5, %shr137.5
  %add142.5 = add i32 %or138.5, %add114.5
  %xor146.5 = xor i32 %add142.5, %or124.5
  %shl147.5 = shl i32 %xor146.5, 8
  %shr151.5 = lshr i32 %xor146.5, 24
  %or152.5 = or i32 %shl147.5, %shr151.5
  %add156.5 = add i32 %or152.5, %add128.5
  %xor160.5 = xor i32 %add156.5, %or138.5
  %shl161.5 = shl i32 %xor160.5, 7
  %shr165.5 = lshr i32 %xor160.5, 25
  %or166.5 = or i32 %shl161.5, %shr165.5
  %add170.5 = add i32 %or390.4, %add422.4
  %xor174.5 = xor i32 %or264.4, %add170.5
  %shl175.5 = shl i32 %xor174.5, 16
  %shr179.5 = lshr i32 %xor174.5, 16
  %or180.5 = or i32 %shl175.5, %shr179.5
  %add184.5 = add i32 %or180.5, %add324.4
  %xor188.5 = xor i32 %add184.5, %or390.4
  %shl189.5 = shl i32 %xor188.5, 12
  %shr193.5 = lshr i32 %xor188.5, 20
  %or194.5 = or i32 %shl189.5, %shr193.5
  %add198.5 = add i32 %or194.5, %add170.5
  %xor202.5 = xor i32 %add198.5, %or180.5
  %shl203.5 = shl i32 %xor202.5, 8
  %shr207.5 = lshr i32 %xor202.5, 24
  %or208.5 = or i32 %shl203.5, %shr207.5
  %add212.5 = add i32 %or208.5, %add184.5
  %xor216.5 = xor i32 %add212.5, %or194.5
  %shl217.5 = shl i32 %xor216.5, 7
  %shr221.5 = lshr i32 %xor216.5, 25
  %or222.5 = or i32 %shl217.5, %shr221.5
  %add226.5 = add i32 %or110.5, %add30.5
  %xor230.5 = xor i32 %or208.5, %add226.5
  %shl231.5 = shl i32 %xor230.5, 16
  %shr235.5 = lshr i32 %xor230.5, 16
  %or236.5 = or i32 %shl231.5, %shr235.5
  %add240.5 = add i32 %or236.5, %add156.5
  %xor244.5 = xor i32 %add240.5, %or110.5
  %shl245.5 = shl i32 %xor244.5, 12
  %shr249.5 = lshr i32 %xor244.5, 20
  %or250.5 = or i32 %shl245.5, %shr249.5
  %add254.5 = add i32 %or250.5, %add226.5
  %xor258.5 = xor i32 %add254.5, %or236.5
  %shl259.5 = shl i32 %xor258.5, 8
  %shr263.5 = lshr i32 %xor258.5, 24
  %or264.5 = or i32 %shl259.5, %shr263.5
  %add268.5 = add i32 %or264.5, %add240.5
  %xor272.5 = xor i32 %add268.5, %or250.5
  %shl273.5 = shl i32 %xor272.5, 7
  %shr277.5 = lshr i32 %xor272.5, 25
  %or278.5 = or i32 %shl273.5, %shr277.5
  %add282.5 = add i32 %or166.5, %add86.5
  %xor286.5 = xor i32 %add282.5, %or40.5
  %shl287.5 = shl i32 %xor286.5, 16
  %shr291.5 = lshr i32 %xor286.5, 16
  %or292.5 = or i32 %shl287.5, %shr291.5
  %add296.5 = add i32 %add212.5, %or292.5
  %xor300.5 = xor i32 %add296.5, %or166.5
  %shl301.5 = shl i32 %xor300.5, 12
  %shr305.5 = lshr i32 %xor300.5, 20
  %or306.5 = or i32 %shl301.5, %shr305.5
  %add310.5 = add i32 %or306.5, %add282.5
  %xor314.5 = xor i32 %add310.5, %or292.5
  %shl315.5 = shl i32 %xor314.5, 8
  %shr319.5 = lshr i32 %xor314.5, 24
  %or320.5 = or i32 %shl315.5, %shr319.5
  %add324.5 = add i32 %or320.5, %add296.5
  %xor328.5 = xor i32 %add324.5, %or306.5
  %shl329.5 = shl i32 %xor328.5, 7
  %shr333.5 = lshr i32 %xor328.5, 25
  %or334.5 = or i32 %shl329.5, %shr333.5
  %add338.5 = add i32 %or222.5, %add142.5
  %xor342.5 = xor i32 %add338.5, %or96.5
  %shl343.5 = shl i32 %xor342.5, 16
  %shr347.5 = lshr i32 %xor342.5, 16
  %or348.5 = or i32 %shl343.5, %shr347.5
  %add352.5 = add i32 %or348.5, %add44.5
  %xor356.5 = xor i32 %add352.5, %or222.5
  %shl357.5 = shl i32 %xor356.5, 12
  %shr361.5 = lshr i32 %xor356.5, 20
  %or362.5 = or i32 %shl357.5, %shr361.5
  %add366.5 = add i32 %or362.5, %add338.5
  %xor370.5 = xor i32 %add366.5, %or348.5
  %shl371.5 = shl i32 %xor370.5, 8
  %shr375.5 = lshr i32 %xor370.5, 24
  %or376.5 = or i32 %shl371.5, %shr375.5
  %add380.5 = add i32 %or376.5, %add352.5
  %xor384.5 = xor i32 %add380.5, %or362.5
  %shl385.5 = shl i32 %xor384.5, 7
  %shr389.5 = lshr i32 %xor384.5, 25
  %or390.5 = or i32 %shl385.5, %shr389.5
  %add394.5 = add i32 %add198.5, %or54.5
  %xor398.5 = xor i32 %add394.5, %or152.5
  %shl399.5 = shl i32 %xor398.5, 16
  %shr403.5 = lshr i32 %xor398.5, 16
  %or404.5 = or i32 %shl399.5, %shr403.5
  %add408.5 = add i32 %or404.5, %add100.5
  %xor412.5 = xor i32 %add408.5, %or54.5
  %shl413.5 = shl i32 %xor412.5, 12
  %shr417.5 = lshr i32 %xor412.5, 20
  %or418.5 = or i32 %shl413.5, %shr417.5
  %add422.5 = add i32 %or418.5, %add394.5
  %xor426.5 = xor i32 %add422.5, %or404.5
  %shl427.5 = shl i32 %xor426.5, 8
  %shr431.5 = lshr i32 %xor426.5, 24
  %or432.5 = or i32 %shl427.5, %shr431.5
  %add436.5 = add i32 %or432.5, %add408.5
  %xor440.5 = xor i32 %add436.5, %or418.5
  %shl441.5 = shl i32 %xor440.5, 7
  %shr445.5 = lshr i32 %xor440.5, 25
  %or446.5 = or i32 %shl441.5, %shr445.5
  %add.6 = add i32 %or446.5, %add254.5
  %xor.6 = xor i32 %or320.5, %add.6
  %shl.6 = shl i32 %xor.6, 16
  %shr.6 = lshr i32 %xor.6, 16
  %or.6 = or i32 %shl.6, %shr.6
  %add16.6 = add i32 %or.6, %add380.5
  %xor20.6 = xor i32 %add16.6, %or446.5
  %shl21.6 = shl i32 %xor20.6, 12
  %shr25.6 = lshr i32 %xor20.6, 20
  %or26.6 = or i32 %shl21.6, %shr25.6
  %add30.6 = add i32 %or26.6, %add.6
  %xor34.6 = xor i32 %add30.6, %or.6
  %shl35.6 = shl i32 %xor34.6, 8
  %shr39.6 = lshr i32 %xor34.6, 24
  %or40.6 = or i32 %shl35.6, %shr39.6
  %add44.6 = add i32 %or40.6, %add16.6
  %xor48.6 = xor i32 %add44.6, %or26.6
  %shl49.6 = shl i32 %xor48.6, 7
  %shr53.6 = lshr i32 %xor48.6, 25
  %or54.6 = or i32 %shl49.6, %shr53.6
  %add58.6 = add i32 %or278.5, %add310.5
  %xor62.6 = xor i32 %or376.5, %add58.6
  %shl63.6 = shl i32 %xor62.6, 16
  %shr67.6 = lshr i32 %xor62.6, 16
  %or68.6 = or i32 %shl63.6, %shr67.6
  %add72.6 = add i32 %or68.6, %add436.5
  %xor76.6 = xor i32 %add72.6, %or278.5
  %shl77.6 = shl i32 %xor76.6, 12
  %shr81.6 = lshr i32 %xor76.6, 20
  %or82.6 = or i32 %shl77.6, %shr81.6
  %add86.6 = add i32 %or82.6, %add58.6
  %xor90.6 = xor i32 %add86.6, %or68.6
  %shl91.6 = shl i32 %xor90.6, 8
  %shr95.6 = lshr i32 %xor90.6, 24
  %or96.6 = or i32 %shl91.6, %shr95.6
  %add100.6 = add i32 %or96.6, %add72.6
  %xor104.6 = xor i32 %add100.6, %or82.6
  %shl105.6 = shl i32 %xor104.6, 7
  %shr109.6 = lshr i32 %xor104.6, 25
  %or110.6 = or i32 %shl105.6, %shr109.6
  %add114.6 = add i32 %or334.5, %add366.5
  %xor118.6 = xor i32 %or432.5, %add114.6
  %shl119.6 = shl i32 %xor118.6, 16
  %shr123.6 = lshr i32 %xor118.6, 16
  %or124.6 = or i32 %shl119.6, %shr123.6
  %add128.6 = add i32 %or124.6, %add268.5
  %xor132.6 = xor i32 %add128.6, %or334.5
  %shl133.6 = shl i32 %xor132.6, 12
  %shr137.6 = lshr i32 %xor132.6, 20
  %or138.6 = or i32 %shl133.6, %shr137.6
  %add142.6 = add i32 %or138.6, %add114.6
  %xor146.6 = xor i32 %add142.6, %or124.6
  %shl147.6 = shl i32 %xor146.6, 8
  %shr151.6 = lshr i32 %xor146.6, 24
  %or152.6 = or i32 %shl147.6, %shr151.6
  %add156.6 = add i32 %or152.6, %add128.6
  %xor160.6 = xor i32 %add156.6, %or138.6
  %shl161.6 = shl i32 %xor160.6, 7
  %shr165.6 = lshr i32 %xor160.6, 25
  %or166.6 = or i32 %shl161.6, %shr165.6
  %add170.6 = add i32 %or390.5, %add422.5
  %xor174.6 = xor i32 %or264.5, %add170.6
  %shl175.6 = shl i32 %xor174.6, 16
  %shr179.6 = lshr i32 %xor174.6, 16
  %or180.6 = or i32 %shl175.6, %shr179.6
  %add184.6 = add i32 %or180.6, %add324.5
  %xor188.6 = xor i32 %add184.6, %or390.5
  %shl189.6 = shl i32 %xor188.6, 12
  %shr193.6 = lshr i32 %xor188.6, 20
  %or194.6 = or i32 %shl189.6, %shr193.6
  %add198.6 = add i32 %or194.6, %add170.6
  %xor202.6 = xor i32 %add198.6, %or180.6
  %shl203.6 = shl i32 %xor202.6, 8
  %shr207.6 = lshr i32 %xor202.6, 24
  %or208.6 = or i32 %shl203.6, %shr207.6
  %add212.6 = add i32 %or208.6, %add184.6
  %xor216.6 = xor i32 %add212.6, %or194.6
  %shl217.6 = shl i32 %xor216.6, 7
  %shr221.6 = lshr i32 %xor216.6, 25
  %or222.6 = or i32 %shl217.6, %shr221.6
  %add226.6 = add i32 %or110.6, %add30.6
  %xor230.6 = xor i32 %or208.6, %add226.6
  %shl231.6 = shl i32 %xor230.6, 16
  %shr235.6 = lshr i32 %xor230.6, 16
  %or236.6 = or i32 %shl231.6, %shr235.6
  %add240.6 = add i32 %or236.6, %add156.6
  %xor244.6 = xor i32 %add240.6, %or110.6
  %shl245.6 = shl i32 %xor244.6, 12
  %shr249.6 = lshr i32 %xor244.6, 20
  %or250.6 = or i32 %shl245.6, %shr249.6
  %add254.6 = add i32 %or250.6, %add226.6
  %xor258.6 = xor i32 %add254.6, %or236.6
  %shl259.6 = shl i32 %xor258.6, 8
  %shr263.6 = lshr i32 %xor258.6, 24
  %or264.6 = or i32 %shl259.6, %shr263.6
  %add268.6 = add i32 %or264.6, %add240.6
  %xor272.6 = xor i32 %add268.6, %or250.6
  %shl273.6 = shl i32 %xor272.6, 7
  %shr277.6 = lshr i32 %xor272.6, 25
  %or278.6 = or i32 %shl273.6, %shr277.6
  %add282.6 = add i32 %or166.6, %add86.6
  %xor286.6 = xor i32 %add282.6, %or40.6
  %shl287.6 = shl i32 %xor286.6, 16
  %shr291.6 = lshr i32 %xor286.6, 16
  %or292.6 = or i32 %shl287.6, %shr291.6
  %add296.6 = add i32 %add212.6, %or292.6
  %xor300.6 = xor i32 %add296.6, %or166.6
  %shl301.6 = shl i32 %xor300.6, 12
  %shr305.6 = lshr i32 %xor300.6, 20
  %or306.6 = or i32 %shl301.6, %shr305.6
  %add310.6 = add i32 %or306.6, %add282.6
  %xor314.6 = xor i32 %add310.6, %or292.6
  %shl315.6 = shl i32 %xor314.6, 8
  %shr319.6 = lshr i32 %xor314.6, 24
  %or320.6 = or i32 %shl315.6, %shr319.6
  %add324.6 = add i32 %or320.6, %add296.6
  %xor328.6 = xor i32 %add324.6, %or306.6
  %shl329.6 = shl i32 %xor328.6, 7
  %shr333.6 = lshr i32 %xor328.6, 25
  %or334.6 = or i32 %shl329.6, %shr333.6
  %add338.6 = add i32 %or222.6, %add142.6
  %xor342.6 = xor i32 %add338.6, %or96.6
  %shl343.6 = shl i32 %xor342.6, 16
  %shr347.6 = lshr i32 %xor342.6, 16
  %or348.6 = or i32 %shl343.6, %shr347.6
  %add352.6 = add i32 %or348.6, %add44.6
  %xor356.6 = xor i32 %add352.6, %or222.6
  %shl357.6 = shl i32 %xor356.6, 12
  %shr361.6 = lshr i32 %xor356.6, 20
  %or362.6 = or i32 %shl357.6, %shr361.6
  %add366.6 = add i32 %or362.6, %add338.6
  %xor370.6 = xor i32 %add366.6, %or348.6
  %shl371.6 = shl i32 %xor370.6, 8
  %shr375.6 = lshr i32 %xor370.6, 24
  %or376.6 = or i32 %shl371.6, %shr375.6
  %add380.6 = add i32 %or376.6, %add352.6
  %xor384.6 = xor i32 %add380.6, %or362.6
  %shl385.6 = shl i32 %xor384.6, 7
  %shr389.6 = lshr i32 %xor384.6, 25
  %or390.6 = or i32 %shl385.6, %shr389.6
  %add394.6 = add i32 %add198.6, %or54.6
  %xor398.6 = xor i32 %add394.6, %or152.6
  %shl399.6 = shl i32 %xor398.6, 16
  %shr403.6 = lshr i32 %xor398.6, 16
  %or404.6 = or i32 %shl399.6, %shr403.6
  %add408.6 = add i32 %or404.6, %add100.6
  %xor412.6 = xor i32 %add408.6, %or54.6
  %shl413.6 = shl i32 %xor412.6, 12
  %shr417.6 = lshr i32 %xor412.6, 20
  %or418.6 = or i32 %shl413.6, %shr417.6
  %add422.6 = add i32 %or418.6, %add394.6
  %xor426.6 = xor i32 %add422.6, %or404.6
  %shl427.6 = shl i32 %xor426.6, 8
  %shr431.6 = lshr i32 %xor426.6, 24
  %or432.6 = or i32 %shl427.6, %shr431.6
  %add436.6 = add i32 %or432.6, %add408.6
  %xor440.6 = xor i32 %add436.6, %or418.6
  %shl441.6 = shl i32 %xor440.6, 7
  %shr445.6 = lshr i32 %xor440.6, 25
  %or446.6 = or i32 %shl441.6, %shr445.6
  %add.7 = add i32 %or446.6, %add254.6
  %xor.7 = xor i32 %or320.6, %add.7
  %shl.7 = shl i32 %xor.7, 16
  %shr.7 = lshr i32 %xor.7, 16
  %or.7 = or i32 %shl.7, %shr.7
  %add16.7 = add i32 %or.7, %add380.6
  %xor20.7 = xor i32 %add16.7, %or446.6
  %shl21.7 = shl i32 %xor20.7, 12
  %shr25.7 = lshr i32 %xor20.7, 20
  %or26.7 = or i32 %shl21.7, %shr25.7
  %add30.7 = add i32 %or26.7, %add.7
  %xor34.7 = xor i32 %add30.7, %or.7
  %shl35.7 = shl i32 %xor34.7, 8
  %shr39.7 = lshr i32 %xor34.7, 24
  %or40.7 = or i32 %shl35.7, %shr39.7
  %add44.7 = add i32 %or40.7, %add16.7
  %xor48.7 = xor i32 %add44.7, %or26.7
  %shl49.7 = shl i32 %xor48.7, 7
  %shr53.7 = lshr i32 %xor48.7, 25
  %or54.7 = or i32 %shl49.7, %shr53.7
  %add58.7 = add i32 %or278.6, %add310.6
  %xor62.7 = xor i32 %or376.6, %add58.7
  %shl63.7 = shl i32 %xor62.7, 16
  %shr67.7 = lshr i32 %xor62.7, 16
  %or68.7 = or i32 %shl63.7, %shr67.7
  %add72.7 = add i32 %or68.7, %add436.6
  %xor76.7 = xor i32 %add72.7, %or278.6
  %shl77.7 = shl i32 %xor76.7, 12
  %shr81.7 = lshr i32 %xor76.7, 20
  %or82.7 = or i32 %shl77.7, %shr81.7
  %add86.7 = add i32 %or82.7, %add58.7
  %xor90.7 = xor i32 %add86.7, %or68.7
  %shl91.7 = shl i32 %xor90.7, 8
  %shr95.7 = lshr i32 %xor90.7, 24
  %or96.7 = or i32 %shl91.7, %shr95.7
  %add100.7 = add i32 %or96.7, %add72.7
  %xor104.7 = xor i32 %add100.7, %or82.7
  %shl105.7 = shl i32 %xor104.7, 7
  %shr109.7 = lshr i32 %xor104.7, 25
  %or110.7 = or i32 %shl105.7, %shr109.7
  %add114.7 = add i32 %or334.6, %add366.6
  %xor118.7 = xor i32 %or432.6, %add114.7
  %shl119.7 = shl i32 %xor118.7, 16
  %shr123.7 = lshr i32 %xor118.7, 16
  %or124.7 = or i32 %shl119.7, %shr123.7
  %add128.7 = add i32 %or124.7, %add268.6
  %xor132.7 = xor i32 %add128.7, %or334.6
  %shl133.7 = shl i32 %xor132.7, 12
  %shr137.7 = lshr i32 %xor132.7, 20
  %or138.7 = or i32 %shl133.7, %shr137.7
  %add142.7 = add i32 %or138.7, %add114.7
  %xor146.7 = xor i32 %add142.7, %or124.7
  %shl147.7 = shl i32 %xor146.7, 8
  %shr151.7 = lshr i32 %xor146.7, 24
  %or152.7 = or i32 %shl147.7, %shr151.7
  %add156.7 = add i32 %or152.7, %add128.7
  %xor160.7 = xor i32 %add156.7, %or138.7
  %shl161.7 = shl i32 %xor160.7, 7
  %shr165.7 = lshr i32 %xor160.7, 25
  %or166.7 = or i32 %shl161.7, %shr165.7
  %add170.7 = add i32 %or390.6, %add422.6
  %xor174.7 = xor i32 %or264.6, %add170.7
  %shl175.7 = shl i32 %xor174.7, 16
  %shr179.7 = lshr i32 %xor174.7, 16
  %or180.7 = or i32 %shl175.7, %shr179.7
  %add184.7 = add i32 %or180.7, %add324.6
  %xor188.7 = xor i32 %add184.7, %or390.6
  %shl189.7 = shl i32 %xor188.7, 12
  %shr193.7 = lshr i32 %xor188.7, 20
  %or194.7 = or i32 %shl189.7, %shr193.7
  %add198.7 = add i32 %or194.7, %add170.7
  %xor202.7 = xor i32 %add198.7, %or180.7
  %shl203.7 = shl i32 %xor202.7, 8
  %shr207.7 = lshr i32 %xor202.7, 24
  %or208.7 = or i32 %shl203.7, %shr207.7
  %add212.7 = add i32 %or208.7, %add184.7
  %xor216.7 = xor i32 %add212.7, %or194.7
  %shl217.7 = shl i32 %xor216.7, 7
  %shr221.7 = lshr i32 %xor216.7, 25
  %or222.7 = or i32 %shl217.7, %shr221.7
  %add226.7 = add i32 %or110.7, %add30.7
  %xor230.7 = xor i32 %or208.7, %add226.7
  %shl231.7 = shl i32 %xor230.7, 16
  %shr235.7 = lshr i32 %xor230.7, 16
  %or236.7 = or i32 %shl231.7, %shr235.7
  %add240.7 = add i32 %or236.7, %add156.7
  %xor244.7 = xor i32 %add240.7, %or110.7
  %shl245.7 = shl i32 %xor244.7, 12
  %shr249.7 = lshr i32 %xor244.7, 20
  %or250.7 = or i32 %shl245.7, %shr249.7
  %add254.7 = add i32 %or250.7, %add226.7
  %xor258.7 = xor i32 %add254.7, %or236.7
  %shl259.7 = shl i32 %xor258.7, 8
  %shr263.7 = lshr i32 %xor258.7, 24
  %or264.7 = or i32 %shl259.7, %shr263.7
  %add268.7 = add i32 %or264.7, %add240.7
  %xor272.7 = xor i32 %add268.7, %or250.7
  %shl273.7 = shl i32 %xor272.7, 7
  %shr277.7 = lshr i32 %xor272.7, 25
  %or278.7 = or i32 %shl273.7, %shr277.7
  %add282.7 = add i32 %or166.7, %add86.7
  %xor286.7 = xor i32 %add282.7, %or40.7
  %shl287.7 = shl i32 %xor286.7, 16
  %shr291.7 = lshr i32 %xor286.7, 16
  %or292.7 = or i32 %shl287.7, %shr291.7
  %add296.7 = add i32 %add212.7, %or292.7
  %xor300.7 = xor i32 %add296.7, %or166.7
  %shl301.7 = shl i32 %xor300.7, 12
  %shr305.7 = lshr i32 %xor300.7, 20
  %or306.7 = or i32 %shl301.7, %shr305.7
  %add310.7 = add i32 %or306.7, %add282.7
  %xor314.7 = xor i32 %add310.7, %or292.7
  %shl315.7 = shl i32 %xor314.7, 8
  %shr319.7 = lshr i32 %xor314.7, 24
  %or320.7 = or i32 %shl315.7, %shr319.7
  %add324.7 = add i32 %or320.7, %add296.7
  %xor328.7 = xor i32 %add324.7, %or306.7
  %shl329.7 = shl i32 %xor328.7, 7
  %shr333.7 = lshr i32 %xor328.7, 25
  %or334.7 = or i32 %shl329.7, %shr333.7
  %add338.7 = add i32 %or222.7, %add142.7
  %xor342.7 = xor i32 %add338.7, %or96.7
  %shl343.7 = shl i32 %xor342.7, 16
  %shr347.7 = lshr i32 %xor342.7, 16
  %or348.7 = or i32 %shl343.7, %shr347.7
  %add352.7 = add i32 %or348.7, %add44.7
  %xor356.7 = xor i32 %add352.7, %or222.7
  %shl357.7 = shl i32 %xor356.7, 12
  %shr361.7 = lshr i32 %xor356.7, 20
  %or362.7 = or i32 %shl357.7, %shr361.7
  %add366.7 = add i32 %or362.7, %add338.7
  %xor370.7 = xor i32 %add366.7, %or348.7
  %shl371.7 = shl i32 %xor370.7, 8
  %shr375.7 = lshr i32 %xor370.7, 24
  %or376.7 = or i32 %shl371.7, %shr375.7
  %add380.7 = add i32 %or376.7, %add352.7
  %xor384.7 = xor i32 %add380.7, %or362.7
  %shl385.7 = shl i32 %xor384.7, 7
  %shr389.7 = lshr i32 %xor384.7, 25
  %or390.7 = or i32 %shl385.7, %shr389.7
  %add394.7 = add i32 %add198.7, %or54.7
  %xor398.7 = xor i32 %add394.7, %or152.7
  %shl399.7 = shl i32 %xor398.7, 16
  %shr403.7 = lshr i32 %xor398.7, 16
  %or404.7 = or i32 %shl399.7, %shr403.7
  %add408.7 = add i32 %or404.7, %add100.7
  %xor412.7 = xor i32 %add408.7, %or54.7
  %shl413.7 = shl i32 %xor412.7, 12
  %shr417.7 = lshr i32 %xor412.7, 20
  %or418.7 = or i32 %shl413.7, %shr417.7
  %add422.7 = add i32 %or418.7, %add394.7
  %xor426.7 = xor i32 %add422.7, %or404.7
  %shl427.7 = shl i32 %xor426.7, 8
  %shr431.7 = lshr i32 %xor426.7, 24
  %or432.7 = or i32 %shl427.7, %shr431.7
  %add436.7 = add i32 %or432.7, %add408.7
  %xor440.7 = xor i32 %add436.7, %or418.7
  %shl441.7 = shl i32 %xor440.7, 7
  %shr445.7 = lshr i32 %xor440.7, 25
  %or446.7 = or i32 %shl441.7, %shr445.7
  %add.8 = add i32 %or446.7, %add254.7
  %xor.8 = xor i32 %or320.7, %add.8
  %shl.8 = shl i32 %xor.8, 16
  %shr.8 = lshr i32 %xor.8, 16
  %or.8 = or i32 %shl.8, %shr.8
  %add16.8 = add i32 %or.8, %add380.7
  %xor20.8 = xor i32 %add16.8, %or446.7
  %shl21.8 = shl i32 %xor20.8, 12
  %shr25.8 = lshr i32 %xor20.8, 20
  %or26.8 = or i32 %shl21.8, %shr25.8
  %add30.8 = add i32 %or26.8, %add.8
  %xor34.8 = xor i32 %add30.8, %or.8
  %shl35.8 = shl i32 %xor34.8, 8
  %shr39.8 = lshr i32 %xor34.8, 24
  %or40.8 = or i32 %shl35.8, %shr39.8
  %add44.8 = add i32 %or40.8, %add16.8
  %xor48.8 = xor i32 %add44.8, %or26.8
  %shl49.8 = shl i32 %xor48.8, 7
  %shr53.8 = lshr i32 %xor48.8, 25
  %or54.8 = or i32 %shl49.8, %shr53.8
  %add58.8 = add i32 %or278.7, %add310.7
  %xor62.8 = xor i32 %or376.7, %add58.8
  %shl63.8 = shl i32 %xor62.8, 16
  %shr67.8 = lshr i32 %xor62.8, 16
  %or68.8 = or i32 %shl63.8, %shr67.8
  %add72.8 = add i32 %or68.8, %add436.7
  %xor76.8 = xor i32 %add72.8, %or278.7
  %shl77.8 = shl i32 %xor76.8, 12
  %shr81.8 = lshr i32 %xor76.8, 20
  %or82.8 = or i32 %shl77.8, %shr81.8
  %add86.8 = add i32 %or82.8, %add58.8
  %xor90.8 = xor i32 %add86.8, %or68.8
  %shl91.8 = shl i32 %xor90.8, 8
  %shr95.8 = lshr i32 %xor90.8, 24
  %or96.8 = or i32 %shl91.8, %shr95.8
  %add100.8 = add i32 %or96.8, %add72.8
  %xor104.8 = xor i32 %add100.8, %or82.8
  %shl105.8 = shl i32 %xor104.8, 7
  %shr109.8 = lshr i32 %xor104.8, 25
  %or110.8 = or i32 %shl105.8, %shr109.8
  %add114.8 = add i32 %or334.7, %add366.7
  %xor118.8 = xor i32 %or432.7, %add114.8
  %shl119.8 = shl i32 %xor118.8, 16
  %shr123.8 = lshr i32 %xor118.8, 16
  %or124.8 = or i32 %shl119.8, %shr123.8
  %add128.8 = add i32 %or124.8, %add268.7
  %xor132.8 = xor i32 %add128.8, %or334.7
  %shl133.8 = shl i32 %xor132.8, 12
  %shr137.8 = lshr i32 %xor132.8, 20
  %or138.8 = or i32 %shl133.8, %shr137.8
  %add142.8 = add i32 %or138.8, %add114.8
  %xor146.8 = xor i32 %add142.8, %or124.8
  %shl147.8 = shl i32 %xor146.8, 8
  %shr151.8 = lshr i32 %xor146.8, 24
  %or152.8 = or i32 %shl147.8, %shr151.8
  %add156.8 = add i32 %or152.8, %add128.8
  %xor160.8 = xor i32 %add156.8, %or138.8
  %shl161.8 = shl i32 %xor160.8, 7
  %shr165.8 = lshr i32 %xor160.8, 25
  %or166.8 = or i32 %shl161.8, %shr165.8
  %add170.8 = add i32 %or390.7, %add422.7
  %xor174.8 = xor i32 %or264.7, %add170.8
  %shl175.8 = shl i32 %xor174.8, 16
  %shr179.8 = lshr i32 %xor174.8, 16
  %or180.8 = or i32 %shl175.8, %shr179.8
  %add184.8 = add i32 %or180.8, %add324.7
  %xor188.8 = xor i32 %add184.8, %or390.7
  %shl189.8 = shl i32 %xor188.8, 12
  %shr193.8 = lshr i32 %xor188.8, 20
  %or194.8 = or i32 %shl189.8, %shr193.8
  %add198.8 = add i32 %or194.8, %add170.8
  %xor202.8 = xor i32 %add198.8, %or180.8
  %shl203.8 = shl i32 %xor202.8, 8
  %shr207.8 = lshr i32 %xor202.8, 24
  %or208.8 = or i32 %shl203.8, %shr207.8
  %add212.8 = add i32 %or208.8, %add184.8
  %xor216.8 = xor i32 %add212.8, %or194.8
  %shl217.8 = shl i32 %xor216.8, 7
  %shr221.8 = lshr i32 %xor216.8, 25
  %or222.8 = or i32 %shl217.8, %shr221.8
  %add226.8 = add i32 %or110.8, %add30.8
  %xor230.8 = xor i32 %or208.8, %add226.8
  %shl231.8 = shl i32 %xor230.8, 16
  %shr235.8 = lshr i32 %xor230.8, 16
  %or236.8 = or i32 %shl231.8, %shr235.8
  %add240.8 = add i32 %or236.8, %add156.8
  %xor244.8 = xor i32 %add240.8, %or110.8
  %shl245.8 = shl i32 %xor244.8, 12
  %shr249.8 = lshr i32 %xor244.8, 20
  %or250.8 = or i32 %shl245.8, %shr249.8
  %add254.8 = add i32 %or250.8, %add226.8
  %xor258.8 = xor i32 %add254.8, %or236.8
  %shl259.8 = shl i32 %xor258.8, 8
  %shr263.8 = lshr i32 %xor258.8, 24
  %or264.8 = or i32 %shl259.8, %shr263.8
  %add268.8 = add i32 %or264.8, %add240.8
  %xor272.8 = xor i32 %add268.8, %or250.8
  %shl273.8 = shl i32 %xor272.8, 7
  %shr277.8 = lshr i32 %xor272.8, 25
  %or278.8 = or i32 %shl273.8, %shr277.8
  %add282.8 = add i32 %or166.8, %add86.8
  %xor286.8 = xor i32 %add282.8, %or40.8
  %shl287.8 = shl i32 %xor286.8, 16
  %shr291.8 = lshr i32 %xor286.8, 16
  %or292.8 = or i32 %shl287.8, %shr291.8
  %add296.8 = add i32 %add212.8, %or292.8
  %xor300.8 = xor i32 %add296.8, %or166.8
  %shl301.8 = shl i32 %xor300.8, 12
  %shr305.8 = lshr i32 %xor300.8, 20
  %or306.8 = or i32 %shl301.8, %shr305.8
  %add310.8 = add i32 %or306.8, %add282.8
  %xor314.8 = xor i32 %add310.8, %or292.8
  %shl315.8 = shl i32 %xor314.8, 8
  %shr319.8 = lshr i32 %xor314.8, 24
  %or320.8 = or i32 %shl315.8, %shr319.8
  %add324.8 = add i32 %or320.8, %add296.8
  %xor328.8 = xor i32 %add324.8, %or306.8
  %shl329.8 = shl i32 %xor328.8, 7
  %shr333.8 = lshr i32 %xor328.8, 25
  %or334.8 = or i32 %shl329.8, %shr333.8
  %add338.8 = add i32 %or222.8, %add142.8
  %xor342.8 = xor i32 %add338.8, %or96.8
  %shl343.8 = shl i32 %xor342.8, 16
  %shr347.8 = lshr i32 %xor342.8, 16
  %or348.8 = or i32 %shl343.8, %shr347.8
  %add352.8 = add i32 %or348.8, %add44.8
  %xor356.8 = xor i32 %add352.8, %or222.8
  %shl357.8 = shl i32 %xor356.8, 12
  %shr361.8 = lshr i32 %xor356.8, 20
  %or362.8 = or i32 %shl357.8, %shr361.8
  %add366.8 = add i32 %or362.8, %add338.8
  %xor370.8 = xor i32 %add366.8, %or348.8
  %shl371.8 = shl i32 %xor370.8, 8
  %shr375.8 = lshr i32 %xor370.8, 24
  %or376.8 = or i32 %shl371.8, %shr375.8
  %add380.8 = add i32 %or376.8, %add352.8
  %xor384.8 = xor i32 %add380.8, %or362.8
  %shl385.8 = shl i32 %xor384.8, 7
  %shr389.8 = lshr i32 %xor384.8, 25
  %or390.8 = or i32 %shl385.8, %shr389.8
  %add394.8 = add i32 %add198.8, %or54.8
  %xor398.8 = xor i32 %add394.8, %or152.8
  %shl399.8 = shl i32 %xor398.8, 16
  %shr403.8 = lshr i32 %xor398.8, 16
  %or404.8 = or i32 %shl399.8, %shr403.8
  %add408.8 = add i32 %or404.8, %add100.8
  %xor412.8 = xor i32 %add408.8, %or54.8
  %shl413.8 = shl i32 %xor412.8, 12
  %shr417.8 = lshr i32 %xor412.8, 20
  %or418.8 = or i32 %shl413.8, %shr417.8
  %add422.8 = add i32 %or418.8, %add394.8
  %xor426.8 = xor i32 %add422.8, %or404.8
  %shl427.8 = shl i32 %xor426.8, 8
  %shr431.8 = lshr i32 %xor426.8, 24
  %or432.8 = or i32 %shl427.8, %shr431.8
  %add436.8 = add i32 %or432.8, %add408.8
  %xor440.8 = xor i32 %add436.8, %or418.8
  %shl441.8 = shl i32 %xor440.8, 7
  %shr445.8 = lshr i32 %xor440.8, 25
  %or446.8 = or i32 %shl441.8, %shr445.8
  %add.9 = add i32 %or446.8, %add254.8
  %xor.9 = xor i32 %or320.8, %add.9
  %shl.9 = shl i32 %xor.9, 16
  %shr.9 = lshr i32 %xor.9, 16
  %or.9 = or i32 %shl.9, %shr.9
  %add16.9 = add i32 %or.9, %add380.8
  %xor20.9 = xor i32 %add16.9, %or446.8
  %shl21.9 = shl i32 %xor20.9, 12
  %shr25.9 = lshr i32 %xor20.9, 20
  %or26.9 = or i32 %shl21.9, %shr25.9
  %add30.9 = add i32 %or26.9, %add.9
  %xor34.9 = xor i32 %add30.9, %or.9
  %shl35.9 = shl i32 %xor34.9, 8
  %shr39.9 = lshr i32 %xor34.9, 24
  %or40.9 = or i32 %shl35.9, %shr39.9
  %add44.9 = add i32 %or40.9, %add16.9
  %xor48.9 = xor i32 %add44.9, %or26.9
  %shl49.9 = shl i32 %xor48.9, 7
  %shr53.9 = lshr i32 %xor48.9, 25
  %or54.9 = or i32 %shl49.9, %shr53.9
  %add58.9 = add i32 %or278.8, %add310.8
  %xor62.9 = xor i32 %or376.8, %add58.9
  %shl63.9 = shl i32 %xor62.9, 16
  %shr67.9 = lshr i32 %xor62.9, 16
  %or68.9 = or i32 %shl63.9, %shr67.9
  %add72.9 = add i32 %or68.9, %add436.8
  %xor76.9 = xor i32 %add72.9, %or278.8
  %shl77.9 = shl i32 %xor76.9, 12
  %shr81.9 = lshr i32 %xor76.9, 20
  %or82.9 = or i32 %shl77.9, %shr81.9
  %add86.9 = add i32 %or82.9, %add58.9
  %xor90.9 = xor i32 %add86.9, %or68.9
  %shl91.9 = shl i32 %xor90.9, 8
  %shr95.9 = lshr i32 %xor90.9, 24
  %or96.9 = or i32 %shl91.9, %shr95.9
  %add100.9 = add i32 %or96.9, %add72.9
  %xor104.9 = xor i32 %add100.9, %or82.9
  %shl105.9 = shl i32 %xor104.9, 7
  %shr109.9 = lshr i32 %xor104.9, 25
  %or110.9 = or i32 %shl105.9, %shr109.9
  %add114.9 = add i32 %or334.8, %add366.8
  %xor118.9 = xor i32 %or432.8, %add114.9
  %shl119.9 = shl i32 %xor118.9, 16
  %shr123.9 = lshr i32 %xor118.9, 16
  %or124.9 = or i32 %shl119.9, %shr123.9
  %add128.9 = add i32 %or124.9, %add268.8
  %xor132.9 = xor i32 %add128.9, %or334.8
  %shl133.9 = shl i32 %xor132.9, 12
  %shr137.9 = lshr i32 %xor132.9, 20
  %or138.9 = or i32 %shl133.9, %shr137.9
  %add142.9 = add i32 %or138.9, %add114.9
  %xor146.9 = xor i32 %add142.9, %or124.9
  %shl147.9 = shl i32 %xor146.9, 8
  %shr151.9 = lshr i32 %xor146.9, 24
  %or152.9 = or i32 %shl147.9, %shr151.9
  %add156.9 = add i32 %or152.9, %add128.9
  %xor160.9 = xor i32 %add156.9, %or138.9
  %shl161.9 = shl i32 %xor160.9, 7
  %shr165.9 = lshr i32 %xor160.9, 25
  %or166.9 = or i32 %shl161.9, %shr165.9
  %add170.9 = add i32 %or390.8, %add422.8
  %xor174.9 = xor i32 %or264.8, %add170.9
  %shl175.9 = shl i32 %xor174.9, 16
  %shr179.9 = lshr i32 %xor174.9, 16
  %or180.9 = or i32 %shl175.9, %shr179.9
  %add184.9 = add i32 %or180.9, %add324.8
  %xor188.9 = xor i32 %add184.9, %or390.8
  %shl189.9 = shl i32 %xor188.9, 12
  %shr193.9 = lshr i32 %xor188.9, 20
  %or194.9 = or i32 %shl189.9, %shr193.9
  %add198.9 = add i32 %or194.9, %add170.9
  %xor202.9 = xor i32 %add198.9, %or180.9
  %shl203.9 = shl i32 %xor202.9, 8
  %shr207.9 = lshr i32 %xor202.9, 24
  %or208.9 = or i32 %shl203.9, %shr207.9
  %add212.9 = add i32 %or208.9, %add184.9
  %xor216.9 = xor i32 %add212.9, %or194.9
  %shl217.9 = shl i32 %xor216.9, 7
  %shr221.9 = lshr i32 %xor216.9, 25
  %or222.9 = or i32 %shl217.9, %shr221.9
  %add226.9 = add i32 %or110.9, %add30.9
  %xor230.9 = xor i32 %or208.9, %add226.9
  %shl231.9 = shl i32 %xor230.9, 16
  %shr235.9 = lshr i32 %xor230.9, 16
  %or236.9 = or i32 %shl231.9, %shr235.9
  %add240.9 = add i32 %or236.9, %add156.9
  %xor244.9 = xor i32 %add240.9, %or110.9
  %shl245.9 = shl i32 %xor244.9, 12
  %shr249.9 = lshr i32 %xor244.9, 20
  %or250.9 = or i32 %shl245.9, %shr249.9
  %add254.9 = add i32 %or250.9, %add226.9
  %xor258.9 = xor i32 %add254.9, %or236.9
  %shl259.9 = shl i32 %xor258.9, 8
  %shr263.9 = lshr i32 %xor258.9, 24
  %or264.9 = or i32 %shl259.9, %shr263.9
  %add268.9 = add i32 %or264.9, %add240.9
  %xor272.9 = xor i32 %add268.9, %or250.9
  %shl273.9 = shl i32 %xor272.9, 7
  %shr277.9 = lshr i32 %xor272.9, 25
  %or278.9 = or i32 %shl273.9, %shr277.9
  %add282.9 = add i32 %or166.9, %add86.9
  %xor286.9 = xor i32 %add282.9, %or40.9
  %shl287.9 = shl i32 %xor286.9, 16
  %shr291.9 = lshr i32 %xor286.9, 16
  %or292.9 = or i32 %shl287.9, %shr291.9
  %add296.9 = add i32 %add212.9, %or292.9
  %xor300.9 = xor i32 %add296.9, %or166.9
  %shl301.9 = shl i32 %xor300.9, 12
  %shr305.9 = lshr i32 %xor300.9, 20
  %or306.9 = or i32 %shl301.9, %shr305.9
  %add310.9 = add i32 %or306.9, %add282.9
  %xor314.9 = xor i32 %add310.9, %or292.9
  %shl315.9 = shl i32 %xor314.9, 8
  %shr319.9 = lshr i32 %xor314.9, 24
  %or320.9 = or i32 %shl315.9, %shr319.9
  %add324.9 = add i32 %or320.9, %add296.9
  %xor328.9 = xor i32 %add324.9, %or306.9
  %shl329.9 = shl i32 %xor328.9, 7
  %shr333.9 = lshr i32 %xor328.9, 25
  %or334.9 = or i32 %shl329.9, %shr333.9
  %add338.9 = add i32 %or222.9, %add142.9
  %xor342.9 = xor i32 %add338.9, %or96.9
  %shl343.9 = shl i32 %xor342.9, 16
  %shr347.9 = lshr i32 %xor342.9, 16
  %or348.9 = or i32 %shl343.9, %shr347.9
  %add352.9 = add i32 %or348.9, %add44.9
  %xor356.9 = xor i32 %add352.9, %or222.9
  %shl357.9 = shl i32 %xor356.9, 12
  %shr361.9 = lshr i32 %xor356.9, 20
  %or362.9 = or i32 %shl357.9, %shr361.9
  %add366.9 = add i32 %or362.9, %add338.9
  %xor370.9 = xor i32 %add366.9, %or348.9
  %shl371.9 = shl i32 %xor370.9, 8
  %shr375.9 = lshr i32 %xor370.9, 24
  %or376.9 = or i32 %shl371.9, %shr375.9
  %add380.9 = add i32 %or376.9, %add352.9
  %xor384.9 = xor i32 %add380.9, %or362.9
  %shl385.9 = shl i32 %xor384.9, 7
  %shr389.9 = lshr i32 %xor384.9, 25
  %or390.9 = or i32 %shl385.9, %shr389.9
  %add394.9 = add i32 %add198.9, %or54.9
  %xor398.9 = xor i32 %add394.9, %or152.9
  %shl399.9 = shl i32 %xor398.9, 16
  %shr403.9 = lshr i32 %xor398.9, 16
  %or404.9 = or i32 %shl399.9, %shr403.9
  %add408.9 = add i32 %or404.9, %add100.9
  %xor412.9 = xor i32 %add408.9, %or54.9
  %shl413.9 = shl i32 %xor412.9, 12
  %shr417.9 = lshr i32 %xor412.9, 20
  %or418.9 = or i32 %shl413.9, %shr417.9
  %add422.9 = add i32 %or418.9, %add394.9
  %xor426.9 = xor i32 %add422.9, %or404.9
  %shl427.9 = shl i32 %xor426.9, 8
  %shr431.9 = lshr i32 %xor426.9, 24
  %or432.9 = or i32 %shl427.9, %shr431.9
  %add436.9 = add i32 %or432.9, %add408.9
  %xor440.9 = xor i32 %add436.9, %or418.9
  %shl441.9 = shl i32 %xor440.9, 7
  %shr445.9 = lshr i32 %xor440.9, 25
  %or446.9 = or i32 %shl441.9, %shr445.9
  %1 = load i32, i32* %input, align 4, !tbaa !4
  %add455 = add i32 %1, %add254.9
  store i32 %add455, i32* %arrayidx5, align 4, !tbaa !4
  %arrayidx454.1 = getelementptr inbounds i32, i32* %input, i32 1
  %2 = load i32, i32* %arrayidx454.1, align 4, !tbaa !4
  %add455.1 = add i32 %2, %add310.9
  store i32 %add455.1, i32* %arrayidx56, align 4, !tbaa !4
  %arrayidx454.2 = getelementptr inbounds i32, i32* %input, i32 2
  %3 = load i32, i32* %arrayidx454.2, align 4, !tbaa !4
  %add455.2 = add i32 %3, %add366.9
  store i32 %add455.2, i32* %arrayidx112, align 4, !tbaa !4
  %arrayidx454.3 = getelementptr inbounds i32, i32* %input, i32 3
  %4 = load i32, i32* %arrayidx454.3, align 4, !tbaa !4
  %add455.3 = add i32 %4, %add422.9
  store i32 %add455.3, i32* %arrayidx168, align 4, !tbaa !4
  %arrayidx454.4 = getelementptr inbounds i32, i32* %input, i32 4
  %5 = load i32, i32* %arrayidx454.4, align 4, !tbaa !4
  %add455.4 = add i32 %5, %or446.9
  store i32 %add455.4, i32* %arrayidx6, align 4, !tbaa !4
  %arrayidx454.5 = getelementptr inbounds i32, i32* %input, i32 5
  %6 = load i32, i32* %arrayidx454.5, align 4, !tbaa !4
  %add455.5 = add i32 %6, %or278.9
  store i32 %add455.5, i32* %arrayidx57, align 4, !tbaa !4
  %arrayidx454.6 = getelementptr inbounds i32, i32* %input, i32 6
  %7 = load i32, i32* %arrayidx454.6, align 4, !tbaa !4
  %add455.6 = add i32 %7, %or334.9
  store i32 %add455.6, i32* %arrayidx113, align 4, !tbaa !4
  %arrayidx454.7 = getelementptr inbounds i32, i32* %input, i32 7
  %8 = load i32, i32* %arrayidx454.7, align 4, !tbaa !4
  %add455.7 = add i32 %8, %or390.9
  store i32 %add455.7, i32* %arrayidx169, align 4, !tbaa !4
  %arrayidx454.8 = getelementptr inbounds i32, i32* %input, i32 8
  %9 = load i32, i32* %arrayidx454.8, align 4, !tbaa !4
  %add455.8 = add i32 %9, %add380.9
  store i32 %add455.8, i32* %arrayidx14, align 4, !tbaa !4
  %arrayidx454.9 = getelementptr inbounds i32, i32* %input, i32 9
  %10 = load i32, i32* %arrayidx454.9, align 4, !tbaa !4
  %add455.9 = add i32 %10, %add436.9
  store i32 %add455.9, i32* %arrayidx70, align 4, !tbaa !4
  %arrayidx454.10 = getelementptr inbounds i32, i32* %input, i32 10
  %11 = load i32, i32* %arrayidx454.10, align 4, !tbaa !4
  %add455.10 = add i32 %11, %add268.9
  store i32 %add455.10, i32* %arrayidx126, align 4, !tbaa !4
  %arrayidx454.11 = getelementptr inbounds i32, i32* %input, i32 11
  %12 = load i32, i32* %arrayidx454.11, align 4, !tbaa !4
  %add455.11 = add i32 %12, %add324.9
  store i32 %add455.11, i32* %arrayidx182, align 4, !tbaa !4
  %arrayidx454.12 = getelementptr inbounds i32, i32* %input, i32 12
  %13 = load i32, i32* %arrayidx454.12, align 4, !tbaa !4
  %add455.12 = add i32 %13, %or320.9
  store i32 %add455.12, i32* %arrayidx8, align 4, !tbaa !4
  %arrayidx454.13 = getelementptr inbounds i32, i32* %input, i32 13
  %14 = load i32, i32* %arrayidx454.13, align 4, !tbaa !4
  %add455.13 = add i32 %14, %or376.9
  store i32 %add455.13, i32* %arrayidx60, align 4, !tbaa !4
  %arrayidx454.14 = getelementptr inbounds i32, i32* %input, i32 14
  %15 = load i32, i32* %arrayidx454.14, align 4, !tbaa !4
  %add455.14 = add i32 %15, %or432.9
  store i32 %add455.14, i32* %arrayidx116, align 4, !tbaa !4
  %arrayidx454.15 = getelementptr inbounds i32, i32* %input, i32 15
  %16 = load i32, i32* %arrayidx454.15, align 4, !tbaa !4
  %add455.15 = add i32 %16, %or264.9
  store i32 %add455.15, i32* %arrayidx172, align 4, !tbaa !4
  %conv465545 = trunc i32 %add455 to i8
  store i8 %conv465545, i8* %output, align 1, !tbaa !8
  %shr468546 = lshr i32 %add455, 8
  %conv472547 = trunc i32 %shr468546 to i8
  %arrayidx475548 = getelementptr inbounds i8, i8* %output, i32 1
  store i8 %conv472547, i8* %arrayidx475548, align 1, !tbaa !8
  %shr477549 = lshr i32 %add455, 16
  %conv481550 = trunc i32 %shr477549 to i8
  %arrayidx484551 = getelementptr inbounds i8, i8* %output, i32 2
  store i8 %conv481550, i8* %arrayidx484551, align 1, !tbaa !8
  %shr486552 = lshr i32 %add455, 24
  %conv490553 = trunc i32 %shr486552 to i8
  %arrayidx493554 = getelementptr inbounds i8, i8* %output, i32 3
  store i8 %conv490553, i8* %arrayidx493554, align 1, !tbaa !8
  br label %do.body.do.body_crit_edge

do.body.do.body_crit_edge:                        ; preds = %entry, %do.body.do.body_crit_edge
  %inc495555 = phi i32 [ 1, %entry ], [ %inc495, %do.body.do.body_crit_edge ]
  %arrayidx463.phi.trans.insert = getelementptr inbounds [16 x i32], [16 x i32]* %x, i32 0, i32 %inc495555
  %.pre = load i32, i32* %arrayidx463.phi.trans.insert, align 4, !tbaa !4
  %conv465 = trunc i32 %.pre to i8
  %mul = shl nsw i32 %inc495555, 2
  %add.ptr = getelementptr inbounds i8, i8* %output, i32 %mul
  store i8 %conv465, i8* %add.ptr, align 1, !tbaa !8
  %shr468 = lshr i32 %.pre, 8
  %conv472 = trunc i32 %shr468 to i8
  %arrayidx475 = getelementptr inbounds i8, i8* %add.ptr, i32 1
  store i8 %conv472, i8* %arrayidx475, align 1, !tbaa !8
  %shr477 = lshr i32 %.pre, 16
  %conv481 = trunc i32 %shr477 to i8
  %arrayidx484 = getelementptr inbounds i8, i8* %add.ptr, i32 2
  store i8 %conv481, i8* %arrayidx484, align 1, !tbaa !8
  %shr486 = lshr i32 %.pre, 24
  %conv490 = trunc i32 %shr486 to i8
  %arrayidx493 = getelementptr inbounds i8, i8* %add.ptr, i32 3
  store i8 %conv490, i8* %arrayidx493, align 1, !tbaa !8
  %inc495 = add nuw nsw i32 %inc495555, 1
  %exitcond = icmp eq i32 %inc495, 16
  br i1 %exitcond, label %for.end496, label %do.body.do.body_crit_edge

for.end496:                                       ; preds = %do.body.do.body_crit_edge
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %0) #4
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64, i8* nocapture) #1

; Function Attrs: norecurse nounwind readnone ssp
define void @ECRYPT_init() local_unnamed_addr #2 {
entry:
  ret void
}

; Function Attrs: norecurse nounwind ssp
define void @ECRYPT_keysetup(%struct.ECRYPT_ctx* nocapture %x, i8* nocapture readonly %k, i32 %kbits, i32 %ivbits) local_unnamed_addr #3 {
entry:
  %0 = load i8, i8* %k, align 1, !tbaa !8
  %conv = zext i8 %0 to i32
  %arrayidx2 = getelementptr inbounds i8, i8* %k, i32 1
  %1 = load i8, i8* %arrayidx2, align 1, !tbaa !8
  %conv3 = zext i8 %1 to i32
  %shl = shl nuw nsw i32 %conv3, 8
  %or = or i32 %shl, %conv
  %arrayidx5 = getelementptr inbounds i8, i8* %k, i32 2
  %2 = load i8, i8* %arrayidx5, align 1, !tbaa !8
  %conv6 = zext i8 %2 to i32
  %shl7 = shl nuw nsw i32 %conv6, 16
  %or8 = or i32 %or, %shl7
  %arrayidx10 = getelementptr inbounds i8, i8* %k, i32 3
  %3 = load i8, i8* %arrayidx10, align 1, !tbaa !8
  %conv11 = zext i8 %3 to i32
  %shl12 = shl nuw i32 %conv11, 24
  %or13 = or i32 %or8, %shl12
  %arrayidx14 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 4
  store i32 %or13, i32* %arrayidx14, align 4, !tbaa !4
  %add.ptr15 = getelementptr inbounds i8, i8* %k, i32 4
  %4 = load i8, i8* %add.ptr15, align 1, !tbaa !8
  %conv17 = zext i8 %4 to i32
  %arrayidx19 = getelementptr inbounds i8, i8* %k, i32 5
  %5 = load i8, i8* %arrayidx19, align 1, !tbaa !8
  %conv20 = zext i8 %5 to i32
  %shl21 = shl nuw nsw i32 %conv20, 8
  %or22 = or i32 %shl21, %conv17
  %arrayidx24 = getelementptr inbounds i8, i8* %k, i32 6
  %6 = load i8, i8* %arrayidx24, align 1, !tbaa !8
  %conv25 = zext i8 %6 to i32
  %shl26 = shl nuw nsw i32 %conv25, 16
  %or27 = or i32 %or22, %shl26
  %arrayidx29 = getelementptr inbounds i8, i8* %k, i32 7
  %7 = load i8, i8* %arrayidx29, align 1, !tbaa !8
  %conv30 = zext i8 %7 to i32
  %shl31 = shl nuw i32 %conv30, 24
  %or32 = or i32 %or27, %shl31
  %arrayidx34 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 5
  store i32 %or32, i32* %arrayidx34, align 4, !tbaa !4
  %add.ptr35 = getelementptr inbounds i8, i8* %k, i32 8
  %8 = load i8, i8* %add.ptr35, align 1, !tbaa !8
  %conv37 = zext i8 %8 to i32
  %arrayidx39 = getelementptr inbounds i8, i8* %k, i32 9
  %9 = load i8, i8* %arrayidx39, align 1, !tbaa !8
  %conv40 = zext i8 %9 to i32
  %shl41 = shl nuw nsw i32 %conv40, 8
  %or42 = or i32 %shl41, %conv37
  %arrayidx44 = getelementptr inbounds i8, i8* %k, i32 10
  %10 = load i8, i8* %arrayidx44, align 1, !tbaa !8
  %conv45 = zext i8 %10 to i32
  %shl46 = shl nuw nsw i32 %conv45, 16
  %or47 = or i32 %or42, %shl46
  %arrayidx49 = getelementptr inbounds i8, i8* %k, i32 11
  %11 = load i8, i8* %arrayidx49, align 1, !tbaa !8
  %conv50 = zext i8 %11 to i32
  %shl51 = shl nuw i32 %conv50, 24
  %or52 = or i32 %or47, %shl51
  %arrayidx54 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 6
  store i32 %or52, i32* %arrayidx54, align 4, !tbaa !4
  %add.ptr55 = getelementptr inbounds i8, i8* %k, i32 12
  %12 = load i8, i8* %add.ptr55, align 1, !tbaa !8
  %conv57 = zext i8 %12 to i32
  %arrayidx59 = getelementptr inbounds i8, i8* %k, i32 13
  %13 = load i8, i8* %arrayidx59, align 1, !tbaa !8
  %conv60 = zext i8 %13 to i32
  %shl61 = shl nuw nsw i32 %conv60, 8
  %or62 = or i32 %shl61, %conv57
  %arrayidx64 = getelementptr inbounds i8, i8* %k, i32 14
  %14 = load i8, i8* %arrayidx64, align 1, !tbaa !8
  %conv65 = zext i8 %14 to i32
  %shl66 = shl nuw nsw i32 %conv65, 16
  %or67 = or i32 %or62, %shl66
  %arrayidx69 = getelementptr inbounds i8, i8* %k, i32 15
  %15 = load i8, i8* %arrayidx69, align 1, !tbaa !8
  %conv70 = zext i8 %15 to i32
  %shl71 = shl nuw i32 %conv70, 24
  %or72 = or i32 %or67, %shl71
  %arrayidx74 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 7
  store i32 %or72, i32* %arrayidx74, align 4, !tbaa !4
  %cmp = icmp eq i32 %kbits, 256
  %add.ptr76 = getelementptr inbounds i8, i8* %k, i32 16
  %k.addr.0 = select i1 %cmp, i8* %add.ptr76, i8* %k
  %constants.0 = select i1 %cmp, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @sigma, i32 0, i32 0), i8* getelementptr inbounds ([16 x i8], [16 x i8]* @tau, i32 0, i32 0)
  %16 = load i8, i8* %k.addr.0, align 1, !tbaa !8
  %conv79 = zext i8 %16 to i32
  %arrayidx81 = getelementptr inbounds i8, i8* %k.addr.0, i32 1
  %17 = load i8, i8* %arrayidx81, align 1, !tbaa !8
  %conv82 = zext i8 %17 to i32
  %shl83 = shl nuw nsw i32 %conv82, 8
  %or84 = or i32 %shl83, %conv79
  %arrayidx86 = getelementptr inbounds i8, i8* %k.addr.0, i32 2
  %18 = load i8, i8* %arrayidx86, align 1, !tbaa !8
  %conv87 = zext i8 %18 to i32
  %shl88 = shl nuw nsw i32 %conv87, 16
  %or89 = or i32 %or84, %shl88
  %arrayidx91 = getelementptr inbounds i8, i8* %k.addr.0, i32 3
  %19 = load i8, i8* %arrayidx91, align 1, !tbaa !8
  %conv92 = zext i8 %19 to i32
  %shl93 = shl nuw i32 %conv92, 24
  %or94 = or i32 %or89, %shl93
  %arrayidx96 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 8
  store i32 %or94, i32* %arrayidx96, align 4, !tbaa !4
  %add.ptr97 = getelementptr inbounds i8, i8* %k.addr.0, i32 4
  %20 = load i8, i8* %add.ptr97, align 1, !tbaa !8
  %conv99 = zext i8 %20 to i32
  %arrayidx101 = getelementptr inbounds i8, i8* %k.addr.0, i32 5
  %21 = load i8, i8* %arrayidx101, align 1, !tbaa !8
  %conv102 = zext i8 %21 to i32
  %shl103 = shl nuw nsw i32 %conv102, 8
  %or104 = or i32 %shl103, %conv99
  %arrayidx106 = getelementptr inbounds i8, i8* %k.addr.0, i32 6
  %22 = load i8, i8* %arrayidx106, align 1, !tbaa !8
  %conv107 = zext i8 %22 to i32
  %shl108 = shl nuw nsw i32 %conv107, 16
  %or109 = or i32 %or104, %shl108
  %arrayidx111 = getelementptr inbounds i8, i8* %k.addr.0, i32 7
  %23 = load i8, i8* %arrayidx111, align 1, !tbaa !8
  %conv112 = zext i8 %23 to i32
  %shl113 = shl nuw i32 %conv112, 24
  %or114 = or i32 %or109, %shl113
  %arrayidx116 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 9
  store i32 %or114, i32* %arrayidx116, align 4, !tbaa !4
  %add.ptr117 = getelementptr inbounds i8, i8* %k.addr.0, i32 8
  %24 = load i8, i8* %add.ptr117, align 1, !tbaa !8
  %conv119 = zext i8 %24 to i32
  %arrayidx121 = getelementptr inbounds i8, i8* %k.addr.0, i32 9
  %25 = load i8, i8* %arrayidx121, align 1, !tbaa !8
  %conv122 = zext i8 %25 to i32
  %shl123 = shl nuw nsw i32 %conv122, 8
  %or124 = or i32 %shl123, %conv119
  %arrayidx126 = getelementptr inbounds i8, i8* %k.addr.0, i32 10
  %26 = load i8, i8* %arrayidx126, align 1, !tbaa !8
  %conv127 = zext i8 %26 to i32
  %shl128 = shl nuw nsw i32 %conv127, 16
  %or129 = or i32 %or124, %shl128
  %arrayidx131 = getelementptr inbounds i8, i8* %k.addr.0, i32 11
  %27 = load i8, i8* %arrayidx131, align 1, !tbaa !8
  %conv132 = zext i8 %27 to i32
  %shl133 = shl nuw i32 %conv132, 24
  %or134 = or i32 %or129, %shl133
  %arrayidx136 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 10
  store i32 %or134, i32* %arrayidx136, align 4, !tbaa !4
  %add.ptr137 = getelementptr inbounds i8, i8* %k.addr.0, i32 12
  %28 = load i8, i8* %add.ptr137, align 1, !tbaa !8
  %conv139 = zext i8 %28 to i32
  %arrayidx141 = getelementptr inbounds i8, i8* %k.addr.0, i32 13
  %29 = load i8, i8* %arrayidx141, align 1, !tbaa !8
  %conv142 = zext i8 %29 to i32
  %shl143 = shl nuw nsw i32 %conv142, 8
  %or144 = or i32 %shl143, %conv139
  %arrayidx146 = getelementptr inbounds i8, i8* %k.addr.0, i32 14
  %30 = load i8, i8* %arrayidx146, align 1, !tbaa !8
  %conv147 = zext i8 %30 to i32
  %shl148 = shl nuw nsw i32 %conv147, 16
  %or149 = or i32 %or144, %shl148
  %arrayidx151 = getelementptr inbounds i8, i8* %k.addr.0, i32 15
  %31 = load i8, i8* %arrayidx151, align 1, !tbaa !8
  %conv152 = zext i8 %31 to i32
  %shl153 = shl nuw i32 %conv152, 24
  %or154 = or i32 %or149, %shl153
  %arrayidx156 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 11
  store i32 %or154, i32* %arrayidx156, align 4, !tbaa !4
  %32 = load i8, i8* %constants.0, align 1, !tbaa !8
  %conv159 = sext i8 %32 to i32
  %arrayidx161 = getelementptr inbounds i8, i8* %constants.0, i32 1
  %33 = load i8, i8* %arrayidx161, align 1, !tbaa !8
  %conv162 = sext i8 %33 to i32
  %shl163 = shl nsw i32 %conv162, 8
  %or164 = or i32 %shl163, %conv159
  %arrayidx166 = getelementptr inbounds i8, i8* %constants.0, i32 2
  %34 = load i8, i8* %arrayidx166, align 1, !tbaa !8
  %conv167 = sext i8 %34 to i32
  %shl168 = shl nsw i32 %conv167, 16
  %or169 = or i32 %or164, %shl168
  %arrayidx171 = getelementptr inbounds i8, i8* %constants.0, i32 3
  %35 = load i8, i8* %arrayidx171, align 1, !tbaa !8
  %conv172296 = zext i8 %35 to i32
  %shl173 = shl nuw i32 %conv172296, 24
  %or174 = or i32 %or169, %shl173
  %arrayidx176 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 0
  store i32 %or174, i32* %arrayidx176, align 4, !tbaa !4
  %add.ptr177 = getelementptr inbounds i8, i8* %constants.0, i32 4
  %36 = load i8, i8* %add.ptr177, align 1, !tbaa !8
  %conv179 = sext i8 %36 to i32
  %arrayidx181 = getelementptr inbounds i8, i8* %constants.0, i32 5
  %37 = load i8, i8* %arrayidx181, align 1, !tbaa !8
  %conv182 = sext i8 %37 to i32
  %shl183 = shl nsw i32 %conv182, 8
  %or184 = or i32 %shl183, %conv179
  %arrayidx186 = getelementptr inbounds i8, i8* %constants.0, i32 6
  %38 = load i8, i8* %arrayidx186, align 1, !tbaa !8
  %conv187 = sext i8 %38 to i32
  %shl188 = shl nsw i32 %conv187, 16
  %or189 = or i32 %or184, %shl188
  %arrayidx191 = getelementptr inbounds i8, i8* %constants.0, i32 7
  %39 = load i8, i8* %arrayidx191, align 1, !tbaa !8
  %conv192297 = zext i8 %39 to i32
  %shl193 = shl nuw i32 %conv192297, 24
  %or194 = or i32 %or189, %shl193
  %arrayidx196 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 1
  store i32 %or194, i32* %arrayidx196, align 4, !tbaa !4
  %add.ptr197 = getelementptr inbounds i8, i8* %constants.0, i32 8
  %40 = load i8, i8* %add.ptr197, align 1, !tbaa !8
  %conv199 = sext i8 %40 to i32
  %arrayidx201 = getelementptr inbounds i8, i8* %constants.0, i32 9
  %41 = load i8, i8* %arrayidx201, align 1, !tbaa !8
  %conv202 = sext i8 %41 to i32
  %shl203 = shl nsw i32 %conv202, 8
  %or204 = or i32 %shl203, %conv199
  %arrayidx206 = getelementptr inbounds i8, i8* %constants.0, i32 10
  %42 = load i8, i8* %arrayidx206, align 1, !tbaa !8
  %conv207 = sext i8 %42 to i32
  %shl208 = shl nsw i32 %conv207, 16
  %or209 = or i32 %or204, %shl208
  %arrayidx211 = getelementptr inbounds i8, i8* %constants.0, i32 11
  %43 = load i8, i8* %arrayidx211, align 1, !tbaa !8
  %conv212298 = zext i8 %43 to i32
  %shl213 = shl nuw i32 %conv212298, 24
  %or214 = or i32 %or209, %shl213
  %arrayidx216 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 2
  store i32 %or214, i32* %arrayidx216, align 4, !tbaa !4
  %add.ptr217 = getelementptr inbounds i8, i8* %constants.0, i32 12
  %44 = load i8, i8* %add.ptr217, align 1, !tbaa !8
  %conv219 = sext i8 %44 to i32
  %arrayidx221 = getelementptr inbounds i8, i8* %constants.0, i32 13
  %45 = load i8, i8* %arrayidx221, align 1, !tbaa !8
  %conv222 = sext i8 %45 to i32
  %shl223 = shl nsw i32 %conv222, 8
  %or224 = or i32 %shl223, %conv219
  %arrayidx226 = getelementptr inbounds i8, i8* %constants.0, i32 14
  %46 = load i8, i8* %arrayidx226, align 1, !tbaa !8
  %conv227 = sext i8 %46 to i32
  %shl228 = shl nsw i32 %conv227, 16
  %or229 = or i32 %or224, %shl228
  %arrayidx231 = getelementptr inbounds i8, i8* %constants.0, i32 15
  %47 = load i8, i8* %arrayidx231, align 1, !tbaa !8
  %conv232299 = zext i8 %47 to i32
  %shl233 = shl nuw i32 %conv232299, 24
  %or234 = or i32 %or229, %shl233
  %arrayidx236 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 3
  store i32 %or234, i32* %arrayidx236, align 4, !tbaa !4
  ret void
}

; Function Attrs: norecurse nounwind ssp
define void @ECRYPT_ivsetup(%struct.ECRYPT_ctx* nocapture %x, i8* nocapture readonly %iv) local_unnamed_addr #3 {
entry:
  %arrayidx = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 12
  store i32 0, i32* %arrayidx, align 4, !tbaa !4
  %arrayidx2 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 13
  store i32 0, i32* %arrayidx2, align 4, !tbaa !4
  %0 = load i8, i8* %iv, align 1, !tbaa !8
  %conv = zext i8 %0 to i32
  %arrayidx5 = getelementptr inbounds i8, i8* %iv, i32 1
  %1 = load i8, i8* %arrayidx5, align 1, !tbaa !8
  %conv6 = zext i8 %1 to i32
  %shl = shl nuw nsw i32 %conv6, 8
  %or = or i32 %shl, %conv
  %arrayidx8 = getelementptr inbounds i8, i8* %iv, i32 2
  %2 = load i8, i8* %arrayidx8, align 1, !tbaa !8
  %conv9 = zext i8 %2 to i32
  %shl10 = shl nuw nsw i32 %conv9, 16
  %or11 = or i32 %or, %shl10
  %arrayidx13 = getelementptr inbounds i8, i8* %iv, i32 3
  %3 = load i8, i8* %arrayidx13, align 1, !tbaa !8
  %conv14 = zext i8 %3 to i32
  %shl15 = shl nuw i32 %conv14, 24
  %or16 = or i32 %or11, %shl15
  %arrayidx18 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 14
  store i32 %or16, i32* %arrayidx18, align 4, !tbaa !4
  %add.ptr19 = getelementptr inbounds i8, i8* %iv, i32 4
  %4 = load i8, i8* %add.ptr19, align 1, !tbaa !8
  %conv21 = zext i8 %4 to i32
  %arrayidx23 = getelementptr inbounds i8, i8* %iv, i32 5
  %5 = load i8, i8* %arrayidx23, align 1, !tbaa !8
  %conv24 = zext i8 %5 to i32
  %shl25 = shl nuw nsw i32 %conv24, 8
  %or26 = or i32 %shl25, %conv21
  %arrayidx28 = getelementptr inbounds i8, i8* %iv, i32 6
  %6 = load i8, i8* %arrayidx28, align 1, !tbaa !8
  %conv29 = zext i8 %6 to i32
  %shl30 = shl nuw nsw i32 %conv29, 16
  %or31 = or i32 %or26, %shl30
  %arrayidx33 = getelementptr inbounds i8, i8* %iv, i32 7
  %7 = load i8, i8* %arrayidx33, align 1, !tbaa !8
  %conv34 = zext i8 %7 to i32
  %shl35 = shl nuw i32 %conv34, 24
  %or36 = or i32 %or31, %shl35
  %arrayidx38 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 15
  store i32 %or36, i32* %arrayidx38, align 4, !tbaa !4
  ret void
}

; Function Attrs: nounwind ssp
define void @ECRYPT_encrypt_bytes(%struct.ECRYPT_ctx* nocapture %x, i8* nocapture readonly %m, i8* nocapture %c, i32 %bytes) local_unnamed_addr #0 {
entry:
  %output = alloca [64 x i8], align 1
  %0 = getelementptr inbounds [64 x i8], [64 x i8]* %output, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %0) #4
  %tobool = icmp eq i32 %bytes, 0
  br i1 %tobool, label %cleanup, label %for.cond.preheader

for.cond.preheader:                               ; preds = %entry
  %arraydecay1 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 0
  %arrayidx = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 12
  %arrayidx10 = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 13
  %1 = xor i32 %bytes, -1
  %2 = icmp ugt i32 %1, -65
  %umax = select i1 %2, i32 %1, i32 -65
  %3 = add i32 %umax, %bytes
  %4 = add i32 %3, 64
  %5 = and i32 %4, -64
  %scevgep68 = getelementptr i8, i8* %c, i32 %5
  br label %for.cond

for.cond:                                         ; preds = %for.cond.preheader, %for.end37
  %m.addr.0 = phi i8* [ %add.ptr38, %for.end37 ], [ %m, %for.cond.preheader ]
  %c.addr.0 = phi i8* [ %add.ptr, %for.end37 ], [ %c, %for.cond.preheader ]
  %bytes.addr.0 = phi i32 [ %sub, %for.end37 ], [ %bytes, %for.cond.preheader ]
  call void @salsa20_wordtobyte(i8* nonnull %0, i32* %arraydecay1)
  %6 = load i32, i32* %arrayidx, align 4, !tbaa !4
  %add = add i32 %6, 1
  store i32 %add, i32* %arrayidx, align 4, !tbaa !4
  %tobool7 = icmp eq i32 %add, 0
  br i1 %tobool7, label %if.then8, label %if.end14

if.then8:                                         ; preds = %for.cond
  %7 = load i32, i32* %arrayidx10, align 4, !tbaa !4
  %add11 = add i32 %7, 1
  store i32 %add11, i32* %arrayidx10, align 4, !tbaa !4
  br label %if.end14

if.end14:                                         ; preds = %for.cond, %if.then8
  %cmp = icmp ult i32 %bytes.addr.0, 65
  br i1 %cmp, label %for.cond16.preheader, label %for.body27

for.cond16.preheader:                             ; preds = %if.end14
  %scevgep = getelementptr i8, i8* %m, i32 %5
  %8 = sub i32 %bytes, %5
  %cmp1764 = icmp eq i32 %8, 0
  br i1 %cmp1764, label %cleanup, label %for.body

for.body:                                         ; preds = %for.cond16.preheader, %for.body
  %i.065 = phi i32 [ %inc, %for.body ], [ 0, %for.cond16.preheader ]
  %arrayidx18 = getelementptr inbounds i8, i8* %scevgep, i32 %i.065
  %9 = load i8, i8* %arrayidx18, align 1, !tbaa !8
  %arrayidx19 = getelementptr inbounds [64 x i8], [64 x i8]* %output, i32 0, i32 %i.065
  %10 = load i8, i8* %arrayidx19, align 1, !tbaa !8
  %xor63 = xor i8 %10, %9
  %arrayidx22 = getelementptr inbounds i8, i8* %scevgep68, i32 %i.065
  store i8 %xor63, i8* %arrayidx22, align 1, !tbaa !8
  %inc = add nuw nsw i32 %i.065, 1
  %exitcond = icmp eq i32 %inc, %8
  br i1 %exitcond, label %cleanup, label %for.body

for.body27:                                       ; preds = %if.end14, %for.body27
  %i.166 = phi i32 [ %inc36, %for.body27 ], [ 0, %if.end14 ]
  %arrayidx28 = getelementptr inbounds i8, i8* %m.addr.0, i32 %i.166
  %11 = load i8, i8* %arrayidx28, align 1, !tbaa !8
  %arrayidx30 = getelementptr inbounds [64 x i8], [64 x i8]* %output, i32 0, i32 %i.166
  %12 = load i8, i8* %arrayidx30, align 1, !tbaa !8
  %xor3262 = xor i8 %12, %11
  %arrayidx34 = getelementptr inbounds i8, i8* %c.addr.0, i32 %i.166
  store i8 %xor3262, i8* %arrayidx34, align 1, !tbaa !8
  %inc36 = add nuw nsw i32 %i.166, 1
  %exitcond67 = icmp eq i32 %inc36, 64
  br i1 %exitcond67, label %for.end37, label %for.body27

for.end37:                                        ; preds = %for.body27
  %sub = add i32 %bytes.addr.0, -64
  %add.ptr = getelementptr inbounds i8, i8* %c.addr.0, i32 64
  %add.ptr38 = getelementptr inbounds i8, i8* %m.addr.0, i32 64
  br label %for.cond

cleanup:                                          ; preds = %for.body, %for.cond16.preheader, %entry
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %0) #4
  ret void
}

; Function Attrs: nounwind ssp
define void @ECRYPT_decrypt_bytes(%struct.ECRYPT_ctx* nocapture %x, i8* nocapture readonly %c, i8* nocapture %m, i32 %bytes) local_unnamed_addr #0 {
entry:
  %output.i = alloca [64 x i8], align 1
  %0 = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %0) #4
  %tobool.i = icmp eq i32 %bytes, 0
  br i1 %tobool.i, label %ECRYPT_encrypt_bytes.exit, label %for.cond.preheader.i

for.cond.preheader.i:                             ; preds = %entry
  %arraydecay1.i = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 0
  %arrayidx.i = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 12
  %arrayidx10.i = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 13
  %1 = xor i32 %bytes, -1
  %2 = icmp ugt i32 %1, -65
  %umax.i = select i1 %2, i32 %1, i32 -65
  %3 = add i32 %bytes, 64
  %4 = add i32 %3, %umax.i
  %5 = and i32 %4, -64
  br label %for.cond.i

for.cond.i:                                       ; preds = %for.end37.i, %for.cond.preheader.i
  %m.addr.0.i = phi i8* [ %add.ptr38.i, %for.end37.i ], [ %c, %for.cond.preheader.i ]
  %c.addr.0.i = phi i8* [ %add.ptr.i, %for.end37.i ], [ %m, %for.cond.preheader.i ]
  %bytes.addr.0.i = phi i32 [ %sub.i, %for.end37.i ], [ %bytes, %for.cond.preheader.i ]
  call void @salsa20_wordtobyte(i8* nonnull %0, i32* %arraydecay1.i) #4
  %6 = load i32, i32* %arrayidx.i, align 4, !tbaa !4
  %add.i = add i32 %6, 1
  store i32 %add.i, i32* %arrayidx.i, align 4, !tbaa !4
  %tobool7.i = icmp eq i32 %add.i, 0
  br i1 %tobool7.i, label %if.then8.i, label %if.end14.i

if.then8.i:                                       ; preds = %for.cond.i
  %7 = load i32, i32* %arrayidx10.i, align 4, !tbaa !4
  %add11.i = add i32 %7, 1
  store i32 %add11.i, i32* %arrayidx10.i, align 4, !tbaa !4
  br label %if.end14.i

if.end14.i:                                       ; preds = %if.then8.i, %for.cond.i
  %cmp.i = icmp ult i32 %bytes.addr.0.i, 65
  br i1 %cmp.i, label %for.cond16.preheader.i, label %for.body27.i

for.cond16.preheader.i:                           ; preds = %if.end14.i
  %scevgep68.i = getelementptr i8, i8* %m, i32 %5
  %scevgep.i = getelementptr i8, i8* %c, i32 %5
  %8 = sub i32 %bytes, %5
  %cmp1764.i = icmp eq i32 %8, 0
  br i1 %cmp1764.i, label %ECRYPT_encrypt_bytes.exit, label %for.body.i

for.body.i:                                       ; preds = %for.cond16.preheader.i, %for.body.i
  %i.065.i = phi i32 [ %inc.i, %for.body.i ], [ 0, %for.cond16.preheader.i ]
  %arrayidx18.i = getelementptr inbounds i8, i8* %scevgep.i, i32 %i.065.i
  %9 = load i8, i8* %arrayidx18.i, align 1, !tbaa !8
  %arrayidx19.i = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 %i.065.i
  %10 = load i8, i8* %arrayidx19.i, align 1, !tbaa !8
  %xor63.i = xor i8 %10, %9
  %arrayidx22.i = getelementptr inbounds i8, i8* %scevgep68.i, i32 %i.065.i
  store i8 %xor63.i, i8* %arrayidx22.i, align 1, !tbaa !8
  %inc.i = add nuw nsw i32 %i.065.i, 1
  %exitcond.i = icmp eq i32 %inc.i, %8
  br i1 %exitcond.i, label %ECRYPT_encrypt_bytes.exit, label %for.body.i

for.body27.i:                                     ; preds = %if.end14.i, %for.body27.i
  %i.166.i = phi i32 [ %inc36.i, %for.body27.i ], [ 0, %if.end14.i ]
  %arrayidx28.i = getelementptr inbounds i8, i8* %m.addr.0.i, i32 %i.166.i
  %11 = load i8, i8* %arrayidx28.i, align 1, !tbaa !8
  %arrayidx30.i = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 %i.166.i
  %12 = load i8, i8* %arrayidx30.i, align 1, !tbaa !8
  %xor3262.i = xor i8 %12, %11
  %arrayidx34.i = getelementptr inbounds i8, i8* %c.addr.0.i, i32 %i.166.i
  store i8 %xor3262.i, i8* %arrayidx34.i, align 1, !tbaa !8
  %inc36.i = add nuw nsw i32 %i.166.i, 1
  %exitcond67.i = icmp eq i32 %inc36.i, 64
  br i1 %exitcond67.i, label %for.end37.i, label %for.body27.i

for.end37.i:                                      ; preds = %for.body27.i
  %sub.i = add i32 %bytes.addr.0.i, -64
  %add.ptr.i = getelementptr inbounds i8, i8* %c.addr.0.i, i32 64
  %add.ptr38.i = getelementptr inbounds i8, i8* %m.addr.0.i, i32 64
  br label %for.cond.i

ECRYPT_encrypt_bytes.exit:                        ; preds = %for.body.i, %entry, %for.cond16.preheader.i
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %0) #4
  ret void
}

; Function Attrs: nounwind ssp
define void @ECRYPT_keystream_bytes(%struct.ECRYPT_ctx* nocapture %x, i8* nocapture %stream, i32 %bytes) local_unnamed_addr #0 {
entry:
  %output.i = alloca [64 x i8], align 1
  %cmp7 = icmp eq i32 %bytes, 0
  br i1 %cmp7, label %for.end.thread, label %for.cond.preheader.i

for.end.thread:                                   ; preds = %entry
  %0 = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %0) #4
  br label %ECRYPT_encrypt_bytes.exit

for.cond.preheader.i:                             ; preds = %entry
  call void @llvm.memset.p0i8.i32(i8* align 1 %stream, i8 0, i32 %bytes, i1 false)
  %1 = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %1) #4
  %arraydecay1.i = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 0
  %arrayidx.i = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 12
  %arrayidx10.i = getelementptr inbounds %struct.ECRYPT_ctx, %struct.ECRYPT_ctx* %x, i32 0, i32 0, i32 13
  %2 = xor i32 %bytes, -1
  %3 = icmp ugt i32 %2, -65
  %umax.i = select i1 %3, i32 %2, i32 -65
  %4 = add i32 %bytes, 64
  %5 = add i32 %4, %umax.i
  %6 = and i32 %5, -64
  br label %for.cond.i

for.cond.i:                                       ; preds = %for.end37.i, %for.cond.preheader.i
  %m.addr.0.i = phi i8* [ %add.ptr38.i, %for.end37.i ], [ %stream, %for.cond.preheader.i ]
  %bytes.addr.0.i = phi i32 [ %sub.i, %for.end37.i ], [ %bytes, %for.cond.preheader.i ]
  call void @salsa20_wordtobyte(i8* nonnull %1, i32* %arraydecay1.i) #4
  %7 = load i32, i32* %arrayidx.i, align 4, !tbaa !4
  %add.i = add i32 %7, 1
  store i32 %add.i, i32* %arrayidx.i, align 4, !tbaa !4
  %tobool7.i = icmp eq i32 %add.i, 0
  br i1 %tobool7.i, label %if.then8.i, label %if.end14.i

if.then8.i:                                       ; preds = %for.cond.i
  %8 = load i32, i32* %arrayidx10.i, align 4, !tbaa !4
  %add11.i = add i32 %8, 1
  store i32 %add11.i, i32* %arrayidx10.i, align 4, !tbaa !4
  br label %if.end14.i

if.end14.i:                                       ; preds = %if.then8.i, %for.cond.i
  %cmp.i = icmp ult i32 %bytes.addr.0.i, 65
  br i1 %cmp.i, label %for.cond16.preheader.i, label %for.body27.i

for.cond16.preheader.i:                           ; preds = %if.end14.i
  %scevgep68.i = getelementptr i8, i8* %stream, i32 %6
  %9 = sub i32 %bytes, %6
  %cmp1764.i = icmp eq i32 %9, 0
  br i1 %cmp1764.i, label %ECRYPT_encrypt_bytes.exit, label %for.body.i

for.body.i:                                       ; preds = %for.cond16.preheader.i, %for.body.i
  %i.065.i = phi i32 [ %inc.i, %for.body.i ], [ 0, %for.cond16.preheader.i ]
  %arrayidx18.i = getelementptr inbounds i8, i8* %scevgep68.i, i32 %i.065.i
  %10 = load i8, i8* %arrayidx18.i, align 1, !tbaa !8
  %arrayidx19.i = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 %i.065.i
  %11 = load i8, i8* %arrayidx19.i, align 1, !tbaa !8
  %xor63.i = xor i8 %11, %10
  store i8 %xor63.i, i8* %arrayidx18.i, align 1, !tbaa !8
  %inc.i = add nuw nsw i32 %i.065.i, 1
  %exitcond.i = icmp eq i32 %inc.i, %9
  br i1 %exitcond.i, label %ECRYPT_encrypt_bytes.exit, label %for.body.i

for.body27.i:                                     ; preds = %if.end14.i, %for.body27.i
  %i.166.i = phi i32 [ %inc36.i, %for.body27.i ], [ 0, %if.end14.i ]
  %arrayidx28.i = getelementptr inbounds i8, i8* %m.addr.0.i, i32 %i.166.i
  %12 = load i8, i8* %arrayidx28.i, align 1, !tbaa !8
  %arrayidx30.i = getelementptr inbounds [64 x i8], [64 x i8]* %output.i, i32 0, i32 %i.166.i
  %13 = load i8, i8* %arrayidx30.i, align 1, !tbaa !8
  %xor3262.i = xor i8 %13, %12
  store i8 %xor3262.i, i8* %arrayidx28.i, align 1, !tbaa !8
  %inc36.i = add nuw nsw i32 %i.166.i, 1
  %exitcond67.i = icmp eq i32 %inc36.i, 64
  br i1 %exitcond67.i, label %for.end37.i, label %for.body27.i

for.end37.i:                                      ; preds = %for.body27.i
  %sub.i = add i32 %bytes.addr.0.i, -64
  %add.ptr38.i = getelementptr inbounds i8, i8* %m.addr.0.i, i32 64
  br label %for.cond.i

ECRYPT_encrypt_bytes.exit:                        ; preds = %for.body.i, %for.end.thread, %for.cond16.preheader.i
  %14 = phi i8* [ %0, %for.end.thread ], [ %1, %for.cond16.preheader.i ], [ %1, %for.body.i ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %14) #4
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i32(i8* nocapture writeonly, i8* nocapture readonly, i32, i1) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i32(i8* nocapture writeonly, i8, i32, i1) #1

attributes #0 = { nounwind ssp "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="arm7tdmi" "target-features"="+armv4t,+soft-float,+strict-align,-crypto,-d16,-fp-armv8,-fp-only-sp,-fp16,-neon,-thumb-mode,-vfp2,-vfp3,-vfp4" "unsafe-fp-math"="false" "use-soft-float"="true" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { norecurse nounwind readnone ssp "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="arm7tdmi" "target-features"="+armv4t,+soft-float,+strict-align,-crypto,-d16,-fp-armv8,-fp-only-sp,-fp16,-neon,-thumb-mode,-vfp2,-vfp3,-vfp4" "unsafe-fp-math"="false" "use-soft-float"="true" }
attributes #3 = { norecurse nounwind ssp "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="arm7tdmi" "target-features"="+armv4t,+soft-float,+strict-align,-crypto,-d16,-fp-armv8,-fp-only-sp,-fp16,-neon,-thumb-mode,-vfp2,-vfp3,-vfp4" "unsafe-fp-math"="false" "use-soft-float"="true" }
attributes #4 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 1, !"min_enum_size", i32 4}
!2 = !{i32 7, !"PIC Level", i32 2}
!3 = !{!"clang version 7.0.0 (tags/RELEASE_700/final)"}
!4 = !{!5, !5, i64 0}
!5 = !{!"int", !6, i64 0}
!6 = !{!"omnipotent char", !7, i64 0}
!7 = !{!"Simple C/C++ TBAA"}
!8 = !{!6, !6, i64 0}

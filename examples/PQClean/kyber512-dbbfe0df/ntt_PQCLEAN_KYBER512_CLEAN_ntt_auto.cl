(* quine: -v -isafety -jobs 16 -no_carry_constraint -slicing ntt_PQCLEAN_KYBER512_CLEAN_ntt_auto.cl
Parsing Cryptoline file:                [OK]            4.384880 seconds
Checking well-formedness:               [OK]            0.048862 seconds
Transforming to SSA form:               [OK]            0.149796 seconds
Rewriting assignments:                  [OK]            109.958373 seconds
Verifying program safety:               [OK]            328.761871 seconds
Verifying range assertions:             [OK]            233.618574 seconds
Verifying range specification:          [OK]            476.341644 seconds
Rewriting value-preserved casting:      [OK]            0.000994 seconds
Verifying algebraic assertions:         [OK]            1.292216 seconds
Verifying algebraic specification:      [OK]            111.266753 seconds
Verification result:                    [OK]            1265.853208 seconds
*)

(* R = 2**16, q = 3329, invq = 62209 *)
(* R == 2285 == -1044 (mod q) *)
(* zeta = 17, zeta**256 == 1 (mod q) *)
(* f[x] in Z_q[x] *)
(* Z_q[x] ~= Z_q[x]/(x**256 + 1) = Z_q[x]/(x**256 - zeta**128)
          ~= Z_q[x]/(x**128 - zeta**64) * Z_q[x]/(x**128 - zeta**192)
          ~= Z_q[x]/(x**64 -  zeta**32) * Z_q[x]/(x**64 - zeta**160) *
             Z_q[x]/(x**64 -  zeta**96) * Z_q[x]/(x**64 - zeta**224)
          ~= Z_q[x]/(x**32 -  zeta**16) * Z_q[x]/(x**32 - zeta**144) *
             Z_q[x]/(x**32 -  zeta**80) * Z_q[x]/(x**32 - zeta**208) *
             Z_q[x]/(x**32 -  zeta**48) * Z_q[x]/(x**32 - zeta**176) *
             Z_q[x]/(x**32 - zeta**112) * Z_q[x]/(x**32 - zeta**240)
          ~= Z_q[x]/(x**16 -   zeta**8) * Z_q[x]/(x**16 - zeta**136) *
             Z_q[x]/(x**16 -  zeta**72) * Z_q[x]/(x**16 - zeta**200) *
             Z_q[x]/(x**16 -  zeta**40) * Z_q[x]/(x**16 - zeta**168) *
             Z_q[x]/(x**16 - zeta**104) * Z_q[x]/(x**16 - zeta**232) *
             Z_q[x]/(x**16 -  zeta**24) * Z_q[x]/(x**16 - zeta**152) *
             Z_q[x]/(x**16 -  zeta**88) * Z_q[x]/(x**16 - zeta**216) *
             Z_q[x]/(x**16 -  zeta**56) * Z_q[x]/(x**16 - zeta**184) *
             Z_q[x]/(x**16 - zeta**120) * Z_q[x]/(x**16 - zeta**248)
          ~= Z_q[x]/( x**8 -   zeta**4) * Z_q[x]/(x** 8 - zeta**132) *
             Z_q[x]/( x**8 -  zeta**68) * Z_q[x]/(x** 8 - zeta**196) *
             Z_q[x]/( x**8 -  zeta**36) * Z_q[x]/(x** 8 - zeta**164) *
             Z_q[x]/( x**8 - zeta**100) * Z_q[x]/(x** 8 - zeta**228) *
             Z_q[x]/( x**8 -  zeta**20) * Z_q[x]/(x** 8 - zeta**148) *
             Z_q[x]/( x**8 -  zeta**84) * Z_q[x]/(x** 8 - zeta**212) *
             Z_q[x]/( x**8 -  zeta**52) * Z_q[x]/(x** 8 - zeta**180) *
             Z_q[x]/( x**8 - zeta**116) * Z_q[x]/(x** 8 - zeta**244) *
             Z_q[x]/( x**8 -  zeta**12) * Z_q[x]/(x** 8 - zeta**140) *
             Z_q[x]/( x**8 -  zeta**76) * Z_q[x]/(x** 8 - zeta**204) *
             Z_q[x]/( x**8 -  zeta**44) * Z_q[x]/(x** 8 - zeta**172) *
             Z_q[x]/( x**8 - zeta**108) * Z_q[x]/(x** 8 - zeta**236) *
             Z_q[x]/( x**8 -  zeta**28) * Z_q[x]/(x** 8 - zeta**156) *
             Z_q[x]/( x**8 -  zeta**92) * Z_q[x]/(x** 8 - zeta**220) *
             Z_q[x]/( x**8 -  zeta**60) * Z_q[x]/(x** 8 - zeta**188) *
             Z_q[x]/( x**8 - zeta**124) * Z_q[x]/(x** 8 - zeta**252)
*)

proc PQCLEAN_KYBER512_CLEAN_montgomery_reduce (sint32 v_a, sint16 ret) =
{ true && true }

assert true
    && and [ (-3329)@32 * (2**15)@32 <=s v_a, v_a <s 3329@32 * (2**15)@32 ];

assume eqmod (ret * (2**16)) v_a 3329
    && and [ (-3329)@16 <s ret, ret <s 3329@16 ];

{ true && true }


proc main (  sint16 a_0,   sint16 a_2,   sint16 a_4,   sint16 a_6,
             sint16 a_8,  sint16 a_10,  sint16 a_12,  sint16 a_14,
            sint16 a_16,  sint16 a_18,  sint16 a_20,  sint16 a_22,
            sint16 a_24,  sint16 a_26,  sint16 a_28,  sint16 a_30,
            sint16 a_32,  sint16 a_34,  sint16 a_36,  sint16 a_38,
            sint16 a_40,  sint16 a_42,  sint16 a_44,  sint16 a_46,
            sint16 a_48,  sint16 a_50,  sint16 a_52,  sint16 a_54,
            sint16 a_56,  sint16 a_58,  sint16 a_60,  sint16 a_62,
            sint16 a_64,  sint16 a_66,  sint16 a_68,  sint16 a_70,
            sint16 a_72,  sint16 a_74,  sint16 a_76,  sint16 a_78,
            sint16 a_80,  sint16 a_82,  sint16 a_84,  sint16 a_86,
            sint16 a_88,  sint16 a_90,  sint16 a_92,  sint16 a_94,
            sint16 a_96,  sint16 a_98, sint16 a_100, sint16 a_102,
           sint16 a_104, sint16 a_106, sint16 a_108, sint16 a_110,
           sint16 a_112, sint16 a_114, sint16 a_116, sint16 a_118,
           sint16 a_120, sint16 a_122, sint16 a_124, sint16 a_126,
           sint16 a_128, sint16 a_130, sint16 a_132, sint16 a_134,
           sint16 a_136, sint16 a_138, sint16 a_140, sint16 a_142,
           sint16 a_144, sint16 a_146, sint16 a_148, sint16 a_150,
           sint16 a_152, sint16 a_154, sint16 a_156, sint16 a_158,
           sint16 a_160, sint16 a_162, sint16 a_164, sint16 a_166,
           sint16 a_168, sint16 a_170, sint16 a_172, sint16 a_174,
           sint16 a_176, sint16 a_178, sint16 a_180, sint16 a_182,
           sint16 a_184, sint16 a_186, sint16 a_188, sint16 a_190,
           sint16 a_192, sint16 a_194, sint16 a_196, sint16 a_198,
           sint16 a_200, sint16 a_202, sint16 a_204, sint16 a_206,
           sint16 a_208, sint16 a_210, sint16 a_212, sint16 a_214,
           sint16 a_216, sint16 a_218, sint16 a_220, sint16 a_222,
           sint16 a_224, sint16 a_226, sint16 a_228, sint16 a_230,
           sint16 a_232, sint16 a_234, sint16 a_236, sint16 a_238,
           sint16 a_240, sint16 a_242, sint16 a_244, sint16 a_246,
           sint16 a_248, sint16 a_250, sint16 a_252, sint16 a_254,
           sint16 a_256, sint16 a_258, sint16 a_260, sint16 a_262,
           sint16 a_264, sint16 a_266, sint16 a_268, sint16 a_270,
           sint16 a_272, sint16 a_274, sint16 a_276, sint16 a_278,
           sint16 a_280, sint16 a_282, sint16 a_284, sint16 a_286,
           sint16 a_288, sint16 a_290, sint16 a_292, sint16 a_294,
           sint16 a_296, sint16 a_298, sint16 a_300, sint16 a_302,
           sint16 a_304, sint16 a_306, sint16 a_308, sint16 a_310,
           sint16 a_312, sint16 a_314, sint16 a_316, sint16 a_318,
           sint16 a_320, sint16 a_322, sint16 a_324, sint16 a_326,
           sint16 a_328, sint16 a_330, sint16 a_332, sint16 a_334,
           sint16 a_336, sint16 a_338, sint16 a_340, sint16 a_342,
           sint16 a_344, sint16 a_346, sint16 a_348, sint16 a_350,
           sint16 a_352, sint16 a_354, sint16 a_356, sint16 a_358,
           sint16 a_360, sint16 a_362, sint16 a_364, sint16 a_366,
           sint16 a_368, sint16 a_370, sint16 a_372, sint16 a_374,
           sint16 a_376, sint16 a_378, sint16 a_380, sint16 a_382,
           sint16 a_384, sint16 a_386, sint16 a_388, sint16 a_390,
           sint16 a_392, sint16 a_394, sint16 a_396, sint16 a_398,
           sint16 a_400, sint16 a_402, sint16 a_404, sint16 a_406,
           sint16 a_408, sint16 a_410, sint16 a_412, sint16 a_414,
           sint16 a_416, sint16 a_418, sint16 a_420, sint16 a_422,
           sint16 a_424, sint16 a_426, sint16 a_428, sint16 a_430,
           sint16 a_432, sint16 a_434, sint16 a_436, sint16 a_438,
           sint16 a_440, sint16 a_442, sint16 a_444, sint16 a_446,
           sint16 a_448, sint16 a_450, sint16 a_452, sint16 a_454,
           sint16 a_456, sint16 a_458, sint16 a_460, sint16 a_462,
           sint16 a_464, sint16 a_466, sint16 a_468, sint16 a_470,
           sint16 a_472, sint16 a_474, sint16 a_476, sint16 a_478,
           sint16 a_480, sint16 a_482, sint16 a_484, sint16 a_486,
           sint16 a_488, sint16 a_490, sint16 a_492, sint16 a_494,
           sint16 a_496, sint16 a_498, sint16 a_500, sint16 a_502,
           sint16 a_504, sint16 a_506, sint16 a_508, sint16 a_510) =
{
  true
&&
   and [
(-3329)@16 <s   a_0,   a_0 <s 3329@16, (-3329)@16 <s   a_2,   a_2 <s 3329@16, 
(-3329)@16 <s   a_4,   a_4 <s 3329@16, (-3329)@16 <s   a_6,   a_6 <s 3329@16, 
(-3329)@16 <s   a_8,   a_8 <s 3329@16, (-3329)@16 <s  a_10,  a_10 <s 3329@16, 
(-3329)@16 <s  a_12,  a_12 <s 3329@16, (-3329)@16 <s  a_14,  a_14 <s 3329@16, 
(-3329)@16 <s  a_16,  a_16 <s 3329@16, (-3329)@16 <s  a_18,  a_18 <s 3329@16, 
(-3329)@16 <s  a_20,  a_20 <s 3329@16, (-3329)@16 <s  a_22,  a_22 <s 3329@16, 
(-3329)@16 <s  a_24,  a_24 <s 3329@16, (-3329)@16 <s  a_26,  a_26 <s 3329@16, 
(-3329)@16 <s  a_28,  a_28 <s 3329@16, (-3329)@16 <s  a_30,  a_30 <s 3329@16, 
(-3329)@16 <s  a_32,  a_32 <s 3329@16, (-3329)@16 <s  a_34,  a_34 <s 3329@16, 
(-3329)@16 <s  a_36,  a_36 <s 3329@16, (-3329)@16 <s  a_38,  a_38 <s 3329@16, 
(-3329)@16 <s  a_40,  a_40 <s 3329@16, (-3329)@16 <s  a_42,  a_42 <s 3329@16, 
(-3329)@16 <s  a_44,  a_44 <s 3329@16, (-3329)@16 <s  a_46,  a_46 <s 3329@16, 
(-3329)@16 <s  a_48,  a_48 <s 3329@16, (-3329)@16 <s  a_50,  a_50 <s 3329@16, 
(-3329)@16 <s  a_52,  a_52 <s 3329@16, (-3329)@16 <s  a_54,  a_54 <s 3329@16, 
(-3329)@16 <s  a_56,  a_56 <s 3329@16, (-3329)@16 <s  a_58,  a_58 <s 3329@16, 
(-3329)@16 <s  a_60,  a_60 <s 3329@16, (-3329)@16 <s  a_62,  a_62 <s 3329@16, 
(-3329)@16 <s  a_64,  a_64 <s 3329@16, (-3329)@16 <s  a_66,  a_66 <s 3329@16, 
(-3329)@16 <s  a_68,  a_68 <s 3329@16, (-3329)@16 <s  a_70,  a_70 <s 3329@16, 
(-3329)@16 <s  a_72,  a_72 <s 3329@16, (-3329)@16 <s  a_74,  a_74 <s 3329@16, 
(-3329)@16 <s  a_76,  a_76 <s 3329@16, (-3329)@16 <s  a_78,  a_78 <s 3329@16, 
(-3329)@16 <s  a_80,  a_80 <s 3329@16, (-3329)@16 <s  a_82,  a_82 <s 3329@16, 
(-3329)@16 <s  a_84,  a_84 <s 3329@16, (-3329)@16 <s  a_86,  a_86 <s 3329@16, 
(-3329)@16 <s  a_88,  a_88 <s 3329@16, (-3329)@16 <s  a_90,  a_90 <s 3329@16, 
(-3329)@16 <s  a_92,  a_92 <s 3329@16, (-3329)@16 <s  a_94,  a_94 <s 3329@16, 
(-3329)@16 <s  a_96,  a_96 <s 3329@16, (-3329)@16 <s  a_98,  a_98 <s 3329@16, 
(-3329)@16 <s a_100, a_100 <s 3329@16, (-3329)@16 <s a_102, a_102 <s 3329@16, 
(-3329)@16 <s a_104, a_104 <s 3329@16, (-3329)@16 <s a_106, a_106 <s 3329@16, 
(-3329)@16 <s a_108, a_108 <s 3329@16, (-3329)@16 <s a_110, a_110 <s 3329@16, 
(-3329)@16 <s a_112, a_112 <s 3329@16, (-3329)@16 <s a_114, a_114 <s 3329@16, 
(-3329)@16 <s a_116, a_116 <s 3329@16, (-3329)@16 <s a_118, a_118 <s 3329@16, 
(-3329)@16 <s a_120, a_120 <s 3329@16, (-3329)@16 <s a_122, a_122 <s 3329@16, 
(-3329)@16 <s a_124, a_124 <s 3329@16, (-3329)@16 <s a_126, a_126 <s 3329@16, 
(-3329)@16 <s a_128, a_128 <s 3329@16, (-3329)@16 <s a_130, a_130 <s 3329@16, 
(-3329)@16 <s a_132, a_132 <s 3329@16, (-3329)@16 <s a_134, a_134 <s 3329@16, 
(-3329)@16 <s a_136, a_136 <s 3329@16, (-3329)@16 <s a_138, a_138 <s 3329@16, 
(-3329)@16 <s a_140, a_140 <s 3329@16, (-3329)@16 <s a_142, a_142 <s 3329@16, 
(-3329)@16 <s a_144, a_144 <s 3329@16, (-3329)@16 <s a_146, a_146 <s 3329@16, 
(-3329)@16 <s a_148, a_148 <s 3329@16, (-3329)@16 <s a_150, a_150 <s 3329@16, 
(-3329)@16 <s a_152, a_152 <s 3329@16, (-3329)@16 <s a_154, a_154 <s 3329@16, 
(-3329)@16 <s a_156, a_156 <s 3329@16, (-3329)@16 <s a_158, a_158 <s 3329@16, 
(-3329)@16 <s a_160, a_160 <s 3329@16, (-3329)@16 <s a_162, a_162 <s 3329@16, 
(-3329)@16 <s a_164, a_164 <s 3329@16, (-3329)@16 <s a_166, a_166 <s 3329@16, 
(-3329)@16 <s a_168, a_168 <s 3329@16, (-3329)@16 <s a_170, a_170 <s 3329@16, 
(-3329)@16 <s a_172, a_172 <s 3329@16, (-3329)@16 <s a_174, a_174 <s 3329@16, 
(-3329)@16 <s a_176, a_176 <s 3329@16, (-3329)@16 <s a_178, a_178 <s 3329@16, 
(-3329)@16 <s a_180, a_180 <s 3329@16, (-3329)@16 <s a_182, a_182 <s 3329@16, 
(-3329)@16 <s a_184, a_184 <s 3329@16, (-3329)@16 <s a_186, a_186 <s 3329@16, 
(-3329)@16 <s a_188, a_188 <s 3329@16, (-3329)@16 <s a_190, a_190 <s 3329@16, 
(-3329)@16 <s a_192, a_192 <s 3329@16, (-3329)@16 <s a_194, a_194 <s 3329@16, 
(-3329)@16 <s a_196, a_196 <s 3329@16, (-3329)@16 <s a_198, a_198 <s 3329@16, 
(-3329)@16 <s a_200, a_200 <s 3329@16, (-3329)@16 <s a_202, a_202 <s 3329@16, 
(-3329)@16 <s a_204, a_204 <s 3329@16, (-3329)@16 <s a_206, a_206 <s 3329@16, 
(-3329)@16 <s a_208, a_208 <s 3329@16, (-3329)@16 <s a_210, a_210 <s 3329@16, 
(-3329)@16 <s a_212, a_212 <s 3329@16, (-3329)@16 <s a_214, a_214 <s 3329@16, 
(-3329)@16 <s a_216, a_216 <s 3329@16, (-3329)@16 <s a_218, a_218 <s 3329@16, 
(-3329)@16 <s a_220, a_220 <s 3329@16, (-3329)@16 <s a_222, a_222 <s 3329@16, 
(-3329)@16 <s a_224, a_224 <s 3329@16, (-3329)@16 <s a_226, a_226 <s 3329@16, 
(-3329)@16 <s a_228, a_228 <s 3329@16, (-3329)@16 <s a_230, a_230 <s 3329@16, 
(-3329)@16 <s a_232, a_232 <s 3329@16, (-3329)@16 <s a_234, a_234 <s 3329@16, 
(-3329)@16 <s a_236, a_236 <s 3329@16, (-3329)@16 <s a_238, a_238 <s 3329@16, 
(-3329)@16 <s a_240, a_240 <s 3329@16, (-3329)@16 <s a_242, a_242 <s 3329@16, 
(-3329)@16 <s a_244, a_244 <s 3329@16, (-3329)@16 <s a_246, a_246 <s 3329@16, 
(-3329)@16 <s a_248, a_248 <s 3329@16, (-3329)@16 <s a_250, a_250 <s 3329@16, 
(-3329)@16 <s a_252, a_252 <s 3329@16, (-3329)@16 <s a_254, a_254 <s 3329@16, 
(-3329)@16 <s a_256, a_256 <s 3329@16, (-3329)@16 <s a_258, a_258 <s 3329@16, 
(-3329)@16 <s a_260, a_260 <s 3329@16, (-3329)@16 <s a_262, a_262 <s 3329@16, 
(-3329)@16 <s a_264, a_264 <s 3329@16, (-3329)@16 <s a_266, a_266 <s 3329@16, 
(-3329)@16 <s a_268, a_268 <s 3329@16, (-3329)@16 <s a_270, a_270 <s 3329@16, 
(-3329)@16 <s a_272, a_272 <s 3329@16, (-3329)@16 <s a_274, a_274 <s 3329@16, 
(-3329)@16 <s a_276, a_276 <s 3329@16, (-3329)@16 <s a_278, a_278 <s 3329@16, 
(-3329)@16 <s a_280, a_280 <s 3329@16, (-3329)@16 <s a_282, a_282 <s 3329@16, 
(-3329)@16 <s a_284, a_284 <s 3329@16, (-3329)@16 <s a_286, a_286 <s 3329@16, 
(-3329)@16 <s a_288, a_288 <s 3329@16, (-3329)@16 <s a_290, a_290 <s 3329@16, 
(-3329)@16 <s a_292, a_292 <s 3329@16, (-3329)@16 <s a_294, a_294 <s 3329@16, 
(-3329)@16 <s a_296, a_296 <s 3329@16, (-3329)@16 <s a_298, a_298 <s 3329@16, 
(-3329)@16 <s a_300, a_300 <s 3329@16, (-3329)@16 <s a_302, a_302 <s 3329@16, 
(-3329)@16 <s a_304, a_304 <s 3329@16, (-3329)@16 <s a_306, a_306 <s 3329@16, 
(-3329)@16 <s a_308, a_308 <s 3329@16, (-3329)@16 <s a_310, a_310 <s 3329@16, 
(-3329)@16 <s a_312, a_312 <s 3329@16, (-3329)@16 <s a_314, a_314 <s 3329@16, 
(-3329)@16 <s a_316, a_316 <s 3329@16, (-3329)@16 <s a_318, a_318 <s 3329@16, 
(-3329)@16 <s a_320, a_320 <s 3329@16, (-3329)@16 <s a_322, a_322 <s 3329@16, 
(-3329)@16 <s a_324, a_324 <s 3329@16, (-3329)@16 <s a_326, a_326 <s 3329@16, 
(-3329)@16 <s a_328, a_328 <s 3329@16, (-3329)@16 <s a_330, a_330 <s 3329@16, 
(-3329)@16 <s a_332, a_332 <s 3329@16, (-3329)@16 <s a_334, a_334 <s 3329@16, 
(-3329)@16 <s a_336, a_336 <s 3329@16, (-3329)@16 <s a_338, a_338 <s 3329@16, 
(-3329)@16 <s a_340, a_340 <s 3329@16, (-3329)@16 <s a_342, a_342 <s 3329@16, 
(-3329)@16 <s a_344, a_344 <s 3329@16, (-3329)@16 <s a_346, a_346 <s 3329@16, 
(-3329)@16 <s a_348, a_348 <s 3329@16, (-3329)@16 <s a_350, a_350 <s 3329@16, 
(-3329)@16 <s a_352, a_352 <s 3329@16, (-3329)@16 <s a_354, a_354 <s 3329@16, 
(-3329)@16 <s a_356, a_356 <s 3329@16, (-3329)@16 <s a_358, a_358 <s 3329@16, 
(-3329)@16 <s a_360, a_360 <s 3329@16, (-3329)@16 <s a_362, a_362 <s 3329@16, 
(-3329)@16 <s a_364, a_364 <s 3329@16, (-3329)@16 <s a_366, a_366 <s 3329@16, 
(-3329)@16 <s a_368, a_368 <s 3329@16, (-3329)@16 <s a_370, a_370 <s 3329@16, 
(-3329)@16 <s a_372, a_372 <s 3329@16, (-3329)@16 <s a_374, a_374 <s 3329@16, 
(-3329)@16 <s a_376, a_376 <s 3329@16, (-3329)@16 <s a_378, a_378 <s 3329@16, 
(-3329)@16 <s a_380, a_380 <s 3329@16, (-3329)@16 <s a_382, a_382 <s 3329@16, 
(-3329)@16 <s a_384, a_384 <s 3329@16, (-3329)@16 <s a_386, a_386 <s 3329@16, 
(-3329)@16 <s a_388, a_388 <s 3329@16, (-3329)@16 <s a_390, a_390 <s 3329@16, 
(-3329)@16 <s a_392, a_392 <s 3329@16, (-3329)@16 <s a_394, a_394 <s 3329@16, 
(-3329)@16 <s a_396, a_396 <s 3329@16, (-3329)@16 <s a_398, a_398 <s 3329@16, 
(-3329)@16 <s a_400, a_400 <s 3329@16, (-3329)@16 <s a_402, a_402 <s 3329@16, 
(-3329)@16 <s a_404, a_404 <s 3329@16, (-3329)@16 <s a_406, a_406 <s 3329@16, 
(-3329)@16 <s a_408, a_408 <s 3329@16, (-3329)@16 <s a_410, a_410 <s 3329@16, 
(-3329)@16 <s a_412, a_412 <s 3329@16, (-3329)@16 <s a_414, a_414 <s 3329@16, 
(-3329)@16 <s a_416, a_416 <s 3329@16, (-3329)@16 <s a_418, a_418 <s 3329@16, 
(-3329)@16 <s a_420, a_420 <s 3329@16, (-3329)@16 <s a_422, a_422 <s 3329@16, 
(-3329)@16 <s a_424, a_424 <s 3329@16, (-3329)@16 <s a_426, a_426 <s 3329@16, 
(-3329)@16 <s a_428, a_428 <s 3329@16, (-3329)@16 <s a_430, a_430 <s 3329@16, 
(-3329)@16 <s a_432, a_432 <s 3329@16, (-3329)@16 <s a_434, a_434 <s 3329@16, 
(-3329)@16 <s a_436, a_436 <s 3329@16, (-3329)@16 <s a_438, a_438 <s 3329@16, 
(-3329)@16 <s a_440, a_440 <s 3329@16, (-3329)@16 <s a_442, a_442 <s 3329@16, 
(-3329)@16 <s a_444, a_444 <s 3329@16, (-3329)@16 <s a_446, a_446 <s 3329@16, 
(-3329)@16 <s a_448, a_448 <s 3329@16, (-3329)@16 <s a_450, a_450 <s 3329@16, 
(-3329)@16 <s a_452, a_452 <s 3329@16, (-3329)@16 <s a_454, a_454 <s 3329@16, 
(-3329)@16 <s a_456, a_456 <s 3329@16, (-3329)@16 <s a_458, a_458 <s 3329@16, 
(-3329)@16 <s a_460, a_460 <s 3329@16, (-3329)@16 <s a_462, a_462 <s 3329@16, 
(-3329)@16 <s a_464, a_464 <s 3329@16, (-3329)@16 <s a_466, a_466 <s 3329@16, 
(-3329)@16 <s a_468, a_468 <s 3329@16, (-3329)@16 <s a_470, a_470 <s 3329@16, 
(-3329)@16 <s a_472, a_472 <s 3329@16, (-3329)@16 <s a_474, a_474 <s 3329@16, 
(-3329)@16 <s a_476, a_476 <s 3329@16, (-3329)@16 <s a_478, a_478 <s 3329@16, 
(-3329)@16 <s a_480, a_480 <s 3329@16, (-3329)@16 <s a_482, a_482 <s 3329@16, 
(-3329)@16 <s a_484, a_484 <s 3329@16, (-3329)@16 <s a_486, a_486 <s 3329@16, 
(-3329)@16 <s a_488, a_488 <s 3329@16, (-3329)@16 <s a_490, a_490 <s 3329@16, 
(-3329)@16 <s a_492, a_492 <s 3329@16, (-3329)@16 <s a_494, a_494 <s 3329@16, 
(-3329)@16 <s a_496, a_496 <s 3329@16, (-3329)@16 <s a_498, a_498 <s 3329@16, 
(-3329)@16 <s a_500, a_500 <s 3329@16, (-3329)@16 <s a_502, a_502 <s 3329@16, 
(-3329)@16 <s a_504, a_504 <s 3329@16, (-3329)@16 <s a_506, a_506 <s 3329@16, 
(-3329)@16 <s a_508, a_508 <s 3329@16, (-3329)@16 <s a_510, a_510 <s 3329@16]

}

ghost input_polynomial@bit, x@bit :
      input_polynomial * input_polynomial =
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
&& true;      

mov mem0_0 a_0; mov mem0_2 a_2; mov mem0_4 a_4; 
mov mem0_6 a_6; mov mem0_8 a_8; mov mem0_10 a_10; 
mov mem0_12 a_12; mov mem0_14 a_14; mov mem0_16 a_16; 
mov mem0_18 a_18; mov mem0_20 a_20; mov mem0_22 a_22; 
mov mem0_24 a_24; mov mem0_26 a_26; mov mem0_28 a_28; 
mov mem0_30 a_30; mov mem0_32 a_32; mov mem0_34 a_34; 
mov mem0_36 a_36; mov mem0_38 a_38; mov mem0_40 a_40; 
mov mem0_42 a_42; mov mem0_44 a_44; mov mem0_46 a_46; 
mov mem0_48 a_48; mov mem0_50 a_50; mov mem0_52 a_52; 
mov mem0_54 a_54; mov mem0_56 a_56; mov mem0_58 a_58; 
mov mem0_60 a_60; mov mem0_62 a_62; mov mem0_64 a_64; 
mov mem0_66 a_66; mov mem0_68 a_68; mov mem0_70 a_70; 
mov mem0_72 a_72; mov mem0_74 a_74; mov mem0_76 a_76; 
mov mem0_78 a_78; mov mem0_80 a_80; mov mem0_82 a_82; 
mov mem0_84 a_84; mov mem0_86 a_86; mov mem0_88 a_88; 
mov mem0_90 a_90; mov mem0_92 a_92; mov mem0_94 a_94; 
mov mem0_96 a_96; mov mem0_98 a_98; mov mem0_100 a_100; 
mov mem0_102 a_102; mov mem0_104 a_104; mov mem0_106 a_106; 
mov mem0_108 a_108; mov mem0_110 a_110; mov mem0_112 a_112; 
mov mem0_114 a_114; mov mem0_116 a_116; mov mem0_118 a_118; 
mov mem0_120 a_120; mov mem0_122 a_122; mov mem0_124 a_124; 
mov mem0_126 a_126; mov mem0_128 a_128; mov mem0_130 a_130; 
mov mem0_132 a_132; mov mem0_134 a_134; mov mem0_136 a_136; 
mov mem0_138 a_138; mov mem0_140 a_140; mov mem0_142 a_142; 
mov mem0_144 a_144; mov mem0_146 a_146; mov mem0_148 a_148; 
mov mem0_150 a_150; mov mem0_152 a_152; mov mem0_154 a_154; 
mov mem0_156 a_156; mov mem0_158 a_158; mov mem0_160 a_160; 
mov mem0_162 a_162; mov mem0_164 a_164; mov mem0_166 a_166; 
mov mem0_168 a_168; mov mem0_170 a_170; mov mem0_172 a_172; 
mov mem0_174 a_174; mov mem0_176 a_176; mov mem0_178 a_178; 
mov mem0_180 a_180; mov mem0_182 a_182; mov mem0_184 a_184; 
mov mem0_186 a_186; mov mem0_188 a_188; mov mem0_190 a_190; 
mov mem0_192 a_192; mov mem0_194 a_194; mov mem0_196 a_196; 
mov mem0_198 a_198; mov mem0_200 a_200; mov mem0_202 a_202; 
mov mem0_204 a_204; mov mem0_206 a_206; mov mem0_208 a_208; 
mov mem0_210 a_210; mov mem0_212 a_212; mov mem0_214 a_214; 
mov mem0_216 a_216; mov mem0_218 a_218; mov mem0_220 a_220; 
mov mem0_222 a_222; mov mem0_224 a_224; mov mem0_226 a_226; 
mov mem0_228 a_228; mov mem0_230 a_230; mov mem0_232 a_232; 
mov mem0_234 a_234; mov mem0_236 a_236; mov mem0_238 a_238; 
mov mem0_240 a_240; mov mem0_242 a_242; mov mem0_244 a_244; 
mov mem0_246 a_246; mov mem0_248 a_248; mov mem0_250 a_250; 
mov mem0_252 a_252; mov mem0_254 a_254; mov mem0_256 a_256; 
mov mem0_258 a_258; mov mem0_260 a_260; mov mem0_262 a_262; 
mov mem0_264 a_264; mov mem0_266 a_266; mov mem0_268 a_268; 
mov mem0_270 a_270; mov mem0_272 a_272; mov mem0_274 a_274; 
mov mem0_276 a_276; mov mem0_278 a_278; mov mem0_280 a_280; 
mov mem0_282 a_282; mov mem0_284 a_284; mov mem0_286 a_286; 
mov mem0_288 a_288; mov mem0_290 a_290; mov mem0_292 a_292; 
mov mem0_294 a_294; mov mem0_296 a_296; mov mem0_298 a_298; 
mov mem0_300 a_300; mov mem0_302 a_302; mov mem0_304 a_304; 
mov mem0_306 a_306; mov mem0_308 a_308; mov mem0_310 a_310; 
mov mem0_312 a_312; mov mem0_314 a_314; mov mem0_316 a_316; 
mov mem0_318 a_318; mov mem0_320 a_320; mov mem0_322 a_322; 
mov mem0_324 a_324; mov mem0_326 a_326; mov mem0_328 a_328; 
mov mem0_330 a_330; mov mem0_332 a_332; mov mem0_334 a_334; 
mov mem0_336 a_336; mov mem0_338 a_338; mov mem0_340 a_340; 
mov mem0_342 a_342; mov mem0_344 a_344; mov mem0_346 a_346; 
mov mem0_348 a_348; mov mem0_350 a_350; mov mem0_352 a_352; 
mov mem0_354 a_354; mov mem0_356 a_356; mov mem0_358 a_358; 
mov mem0_360 a_360; mov mem0_362 a_362; mov mem0_364 a_364; 
mov mem0_366 a_366; mov mem0_368 a_368; mov mem0_370 a_370; 
mov mem0_372 a_372; mov mem0_374 a_374; mov mem0_376 a_376; 
mov mem0_378 a_378; mov mem0_380 a_380; mov mem0_382 a_382; 
mov mem0_384 a_384; mov mem0_386 a_386; mov mem0_388 a_388; 
mov mem0_390 a_390; mov mem0_392 a_392; mov mem0_394 a_394; 
mov mem0_396 a_396; mov mem0_398 a_398; mov mem0_400 a_400; 
mov mem0_402 a_402; mov mem0_404 a_404; mov mem0_406 a_406; 
mov mem0_408 a_408; mov mem0_410 a_410; mov mem0_412 a_412; 
mov mem0_414 a_414; mov mem0_416 a_416; mov mem0_418 a_418; 
mov mem0_420 a_420; mov mem0_422 a_422; mov mem0_424 a_424; 
mov mem0_426 a_426; mov mem0_428 a_428; mov mem0_430 a_430; 
mov mem0_432 a_432; mov mem0_434 a_434; mov mem0_436 a_436; 
mov mem0_438 a_438; mov mem0_440 a_440; mov mem0_442 a_442; 
mov mem0_444 a_444; mov mem0_446 a_446; mov mem0_448 a_448; 
mov mem0_450 a_450; mov mem0_452 a_452; mov mem0_454 a_454; 
mov mem0_456 a_456; mov mem0_458 a_458; mov mem0_460 a_460; 
mov mem0_462 a_462; mov mem0_464 a_464; mov mem0_466 a_466; 
mov mem0_468 a_468; mov mem0_470 a_470; mov mem0_472 a_472; 
mov mem0_474 a_474; mov mem0_476 a_476; mov mem0_478 a_478; 
mov mem0_480 a_480; mov mem0_482 a_482; mov mem0_484 a_484; 
mov mem0_486 a_486; mov mem0_488 a_488; mov mem0_490 a_490; 
mov mem0_492 a_492; mov mem0_494 a_494; mov mem0_496 a_496; 
mov mem0_498 a_498; mov mem0_500 a_500; mov mem0_502 a_502; 
mov mem0_504 a_504; mov mem0_506 a_506; mov mem0_508 a_508; 
mov mem0_510 a_510; 

nondet v_call_i@sint16;
nondet v_call_i_1289@sint16;
nondet v_call_i_2296@sint16;
nondet v_call_i_3303@sint16;
nondet v_call_i_4310@sint16;
nondet v_call_i_5317@sint16;
nondet v_call_i_6324@sint16;
nondet v_call_i_7@sint16;
nondet v_call_i_8@sint16;
nondet v_call_i_9@sint16;
nondet v_call_i_10@sint16;
nondet v_call_i_11@sint16;
nondet v_call_i_12@sint16;
nondet v_call_i_13@sint16;
nondet v_call_i_14@sint16;
nondet v_call_i_15@sint16;
nondet v_call_i_16@sint16;
nondet v_call_i_17@sint16;
nondet v_call_i_18@sint16;
nondet v_call_i_19@sint16;
nondet v_call_i_20@sint16;
nondet v_call_i_21@sint16;
nondet v_call_i_22@sint16;
nondet v_call_i_23@sint16;
nondet v_call_i_24@sint16;
nondet v_call_i_25@sint16;
nondet v_call_i_26@sint16;
nondet v_call_i_27@sint16;
nondet v_call_i_28@sint16;
nondet v_call_i_29@sint16;
nondet v_call_i_30@sint16;
nondet v_call_i_31@sint16;
nondet v_call_i_32@sint16;
nondet v_call_i_33@sint16;
nondet v_call_i_34@sint16;
nondet v_call_i_35@sint16;
nondet v_call_i_36@sint16;
nondet v_call_i_37@sint16;
nondet v_call_i_38@sint16;
nondet v_call_i_39@sint16;
nondet v_call_i_40@sint16;
nondet v_call_i_41@sint16;
nondet v_call_i_42@sint16;
nondet v_call_i_43@sint16;
nondet v_call_i_44@sint16;
nondet v_call_i_45@sint16;
nondet v_call_i_46@sint16;
nondet v_call_i_47@sint16;
nondet v_call_i_48@sint16;
nondet v_call_i_49@sint16;
nondet v_call_i_50@sint16;
nondet v_call_i_51@sint16;
nondet v_call_i_52@sint16;
nondet v_call_i_53@sint16;
nondet v_call_i_54@sint16;
nondet v_call_i_55@sint16;
nondet v_call_i_56@sint16;
nondet v_call_i_57@sint16;
nondet v_call_i_58@sint16;
nondet v_call_i_59@sint16;
nondet v_call_i_60@sint16;
nondet v_call_i_61@sint16;
nondet v_call_i_62@sint16;
nondet v_call_i_63@sint16;
nondet v_call_i_64@sint16;
nondet v_call_i_65@sint16;
nondet v_call_i_66@sint16;
nondet v_call_i_67@sint16;
nondet v_call_i_68@sint16;
nondet v_call_i_69@sint16;
nondet v_call_i_70@sint16;
nondet v_call_i_71@sint16;
nondet v_call_i_72@sint16;
nondet v_call_i_73@sint16;
nondet v_call_i_74@sint16;
nondet v_call_i_75@sint16;
nondet v_call_i_76@sint16;
nondet v_call_i_77@sint16;
nondet v_call_i_78@sint16;
nondet v_call_i_79@sint16;
nondet v_call_i_80@sint16;
nondet v_call_i_81@sint16;
nondet v_call_i_82@sint16;
nondet v_call_i_83@sint16;
nondet v_call_i_84@sint16;
nondet v_call_i_85@sint16;
nondet v_call_i_86@sint16;
nondet v_call_i_87@sint16;
nondet v_call_i_88@sint16;
nondet v_call_i_89@sint16;
nondet v_call_i_90@sint16;
nondet v_call_i_91@sint16;
nondet v_call_i_92@sint16;
nondet v_call_i_93@sint16;
nondet v_call_i_94@sint16;
nondet v_call_i_95@sint16;
nondet v_call_i_96@sint16;
nondet v_call_i_97@sint16;
nondet v_call_i_98@sint16;
nondet v_call_i_99@sint16;
nondet v_call_i_100@sint16;
nondet v_call_i_101@sint16;
nondet v_call_i_102@sint16;
nondet v_call_i_103@sint16;
nondet v_call_i_104@sint16;
nondet v_call_i_105@sint16;
nondet v_call_i_106@sint16;
nondet v_call_i_107@sint16;
nondet v_call_i_108@sint16;
nondet v_call_i_109@sint16;
nondet v_call_i_110@sint16;
nondet v_call_i_111@sint16;
nondet v_call_i_112@sint16;
nondet v_call_i_113@sint16;
nondet v_call_i_114@sint16;
nondet v_call_i_115@sint16;
nondet v_call_i_116@sint16;
nondet v_call_i_117@sint16;
nondet v_call_i_118@sint16;
nondet v_call_i_119@sint16;
nondet v_call_i_120@sint16;
nondet v_call_i_121@sint16;
nondet v_call_i_122@sint16;
nondet v_call_i_123@sint16;
nondet v_call_i_124@sint16;
nondet v_call_i_125@sint16;
nondet v_call_i_126@sint16;
nondet v_call_i_127@sint16;
nondet v_call_i_1@sint16;
nondet v_call_i_1_1@sint16;
nondet v_call_i_1_2@sint16;
nondet v_call_i_1_3@sint16;
nondet v_call_i_1_4@sint16;
nondet v_call_i_1_5@sint16;
nondet v_call_i_1_6@sint16;
nondet v_call_i_1_7@sint16;
nondet v_call_i_1_8@sint16;
nondet v_call_i_1_9@sint16;
nondet v_call_i_1_10@sint16;
nondet v_call_i_1_11@sint16;
nondet v_call_i_1_12@sint16;
nondet v_call_i_1_13@sint16;
nondet v_call_i_1_14@sint16;
nondet v_call_i_1_15@sint16;
nondet v_call_i_1_16@sint16;
nondet v_call_i_1_17@sint16;
nondet v_call_i_1_18@sint16;
nondet v_call_i_1_19@sint16;
nondet v_call_i_1_20@sint16;
nondet v_call_i_1_21@sint16;
nondet v_call_i_1_22@sint16;
nondet v_call_i_1_23@sint16;
nondet v_call_i_1_24@sint16;
nondet v_call_i_1_25@sint16;
nondet v_call_i_1_26@sint16;
nondet v_call_i_1_27@sint16;
nondet v_call_i_1_28@sint16;
nondet v_call_i_1_29@sint16;
nondet v_call_i_1_30@sint16;
nondet v_call_i_1_31@sint16;
nondet v_call_i_1_32@sint16;
nondet v_call_i_1_33@sint16;
nondet v_call_i_1_34@sint16;
nondet v_call_i_1_35@sint16;
nondet v_call_i_1_36@sint16;
nondet v_call_i_1_37@sint16;
nondet v_call_i_1_38@sint16;
nondet v_call_i_1_39@sint16;
nondet v_call_i_1_40@sint16;
nondet v_call_i_1_41@sint16;
nondet v_call_i_1_42@sint16;
nondet v_call_i_1_43@sint16;
nondet v_call_i_1_44@sint16;
nondet v_call_i_1_45@sint16;
nondet v_call_i_1_46@sint16;
nondet v_call_i_1_47@sint16;
nondet v_call_i_1_48@sint16;
nondet v_call_i_1_49@sint16;
nondet v_call_i_1_50@sint16;
nondet v_call_i_1_51@sint16;
nondet v_call_i_1_52@sint16;
nondet v_call_i_1_53@sint16;
nondet v_call_i_1_54@sint16;
nondet v_call_i_1_55@sint16;
nondet v_call_i_1_56@sint16;
nondet v_call_i_1_57@sint16;
nondet v_call_i_1_58@sint16;
nondet v_call_i_1_59@sint16;
nondet v_call_i_1_60@sint16;
nondet v_call_i_1_61@sint16;
nondet v_call_i_1_62@sint16;
nondet v_call_i_1_63@sint16;
nondet v_call_i_1_1281@sint16;
nondet v_call_i_1_1_1@sint16;
nondet v_call_i_1_2_1@sint16;
nondet v_call_i_1_3_1@sint16;
nondet v_call_i_1_4_1@sint16;
nondet v_call_i_1_5_1@sint16;
nondet v_call_i_1_6_1@sint16;
nondet v_call_i_1_7_1@sint16;
nondet v_call_i_1_8_1@sint16;
nondet v_call_i_1_9_1@sint16;
nondet v_call_i_1_10_1@sint16;
nondet v_call_i_1_11_1@sint16;
nondet v_call_i_1_12_1@sint16;
nondet v_call_i_1_13_1@sint16;
nondet v_call_i_1_14_1@sint16;
nondet v_call_i_1_15_1@sint16;
nondet v_call_i_1_16_1@sint16;
nondet v_call_i_1_17_1@sint16;
nondet v_call_i_1_18_1@sint16;
nondet v_call_i_1_19_1@sint16;
nondet v_call_i_1_20_1@sint16;
nondet v_call_i_1_21_1@sint16;
nondet v_call_i_1_22_1@sint16;
nondet v_call_i_1_23_1@sint16;
nondet v_call_i_1_24_1@sint16;
nondet v_call_i_1_25_1@sint16;
nondet v_call_i_1_26_1@sint16;
nondet v_call_i_1_27_1@sint16;
nondet v_call_i_1_28_1@sint16;
nondet v_call_i_1_29_1@sint16;
nondet v_call_i_1_30_1@sint16;
nondet v_call_i_1_31_1@sint16;
nondet v_call_i_1_32_1@sint16;
nondet v_call_i_1_33_1@sint16;
nondet v_call_i_1_34_1@sint16;
nondet v_call_i_1_35_1@sint16;
nondet v_call_i_1_36_1@sint16;
nondet v_call_i_1_37_1@sint16;
nondet v_call_i_1_38_1@sint16;
nondet v_call_i_1_39_1@sint16;
nondet v_call_i_1_40_1@sint16;
nondet v_call_i_1_41_1@sint16;
nondet v_call_i_1_42_1@sint16;
nondet v_call_i_1_43_1@sint16;
nondet v_call_i_1_44_1@sint16;
nondet v_call_i_1_45_1@sint16;
nondet v_call_i_1_46_1@sint16;
nondet v_call_i_1_47_1@sint16;
nondet v_call_i_1_48_1@sint16;
nondet v_call_i_1_49_1@sint16;
nondet v_call_i_1_50_1@sint16;
nondet v_call_i_1_51_1@sint16;
nondet v_call_i_1_52_1@sint16;
nondet v_call_i_1_53_1@sint16;
nondet v_call_i_1_54_1@sint16;
nondet v_call_i_1_55_1@sint16;
nondet v_call_i_1_56_1@sint16;
nondet v_call_i_1_57_1@sint16;
nondet v_call_i_1_58_1@sint16;
nondet v_call_i_1_59_1@sint16;
nondet v_call_i_1_60_1@sint16;
nondet v_call_i_1_61_1@sint16;
nondet v_call_i_1_62_1@sint16;
nondet v_call_i_1_63_1@sint16;
nondet v_call_i_2@sint16;
nondet v_call_i_2_1@sint16;
nondet v_call_i_2_2@sint16;
nondet v_call_i_2_3@sint16;
nondet v_call_i_2_4@sint16;
nondet v_call_i_2_5@sint16;
nondet v_call_i_2_6@sint16;
nondet v_call_i_2_7@sint16;
nondet v_call_i_2_8@sint16;
nondet v_call_i_2_9@sint16;
nondet v_call_i_2_10@sint16;
nondet v_call_i_2_11@sint16;
nondet v_call_i_2_12@sint16;
nondet v_call_i_2_13@sint16;
nondet v_call_i_2_14@sint16;
nondet v_call_i_2_15@sint16;
nondet v_call_i_2_16@sint16;
nondet v_call_i_2_17@sint16;
nondet v_call_i_2_18@sint16;
nondet v_call_i_2_19@sint16;
nondet v_call_i_2_20@sint16;
nondet v_call_i_2_21@sint16;
nondet v_call_i_2_22@sint16;
nondet v_call_i_2_23@sint16;
nondet v_call_i_2_24@sint16;
nondet v_call_i_2_25@sint16;
nondet v_call_i_2_26@sint16;
nondet v_call_i_2_27@sint16;
nondet v_call_i_2_28@sint16;
nondet v_call_i_2_29@sint16;
nondet v_call_i_2_30@sint16;
nondet v_call_i_2_31@sint16;
nondet v_call_i_2_1251@sint16;
nondet v_call_i_2_1_1@sint16;
nondet v_call_i_2_2_1@sint16;
nondet v_call_i_2_3_1@sint16;
nondet v_call_i_2_4_1@sint16;
nondet v_call_i_2_5_1@sint16;
nondet v_call_i_2_6_1@sint16;
nondet v_call_i_2_7_1@sint16;
nondet v_call_i_2_8_1@sint16;
nondet v_call_i_2_9_1@sint16;
nondet v_call_i_2_10_1@sint16;
nondet v_call_i_2_11_1@sint16;
nondet v_call_i_2_12_1@sint16;
nondet v_call_i_2_13_1@sint16;
nondet v_call_i_2_14_1@sint16;
nondet v_call_i_2_15_1@sint16;
nondet v_call_i_2_16_1@sint16;
nondet v_call_i_2_17_1@sint16;
nondet v_call_i_2_18_1@sint16;
nondet v_call_i_2_19_1@sint16;
nondet v_call_i_2_20_1@sint16;
nondet v_call_i_2_21_1@sint16;
nondet v_call_i_2_22_1@sint16;
nondet v_call_i_2_23_1@sint16;
nondet v_call_i_2_24_1@sint16;
nondet v_call_i_2_25_1@sint16;
nondet v_call_i_2_26_1@sint16;
nondet v_call_i_2_27_1@sint16;
nondet v_call_i_2_28_1@sint16;
nondet v_call_i_2_29_1@sint16;
nondet v_call_i_2_30_1@sint16;
nondet v_call_i_2_31_1@sint16;
nondet v_call_i_2_2261@sint16;
nondet v_call_i_2_1_2@sint16;
nondet v_call_i_2_2_2@sint16;
nondet v_call_i_2_3_2@sint16;
nondet v_call_i_2_4_2@sint16;
nondet v_call_i_2_5_2@sint16;
nondet v_call_i_2_6_2@sint16;
nondet v_call_i_2_7_2@sint16;
nondet v_call_i_2_8_2@sint16;
nondet v_call_i_2_9_2@sint16;
nondet v_call_i_2_10_2@sint16;
nondet v_call_i_2_11_2@sint16;
nondet v_call_i_2_12_2@sint16;
nondet v_call_i_2_13_2@sint16;
nondet v_call_i_2_14_2@sint16;
nondet v_call_i_2_15_2@sint16;
nondet v_call_i_2_16_2@sint16;
nondet v_call_i_2_17_2@sint16;
nondet v_call_i_2_18_2@sint16;
nondet v_call_i_2_19_2@sint16;
nondet v_call_i_2_20_2@sint16;
nondet v_call_i_2_21_2@sint16;
nondet v_call_i_2_22_2@sint16;
nondet v_call_i_2_23_2@sint16;
nondet v_call_i_2_24_2@sint16;
nondet v_call_i_2_25_2@sint16;
nondet v_call_i_2_26_2@sint16;
nondet v_call_i_2_27_2@sint16;
nondet v_call_i_2_28_2@sint16;
nondet v_call_i_2_29_2@sint16;
nondet v_call_i_2_30_2@sint16;
nondet v_call_i_2_31_2@sint16;
nondet v_call_i_2_3271@sint16;
nondet v_call_i_2_1_3@sint16;
nondet v_call_i_2_2_3@sint16;
nondet v_call_i_2_3_3@sint16;
nondet v_call_i_2_4_3@sint16;
nondet v_call_i_2_5_3@sint16;
nondet v_call_i_2_6_3@sint16;
nondet v_call_i_2_7_3@sint16;
nondet v_call_i_2_8_3@sint16;
nondet v_call_i_2_9_3@sint16;
nondet v_call_i_2_10_3@sint16;
nondet v_call_i_2_11_3@sint16;
nondet v_call_i_2_12_3@sint16;
nondet v_call_i_2_13_3@sint16;
nondet v_call_i_2_14_3@sint16;
nondet v_call_i_2_15_3@sint16;
nondet v_call_i_2_16_3@sint16;
nondet v_call_i_2_17_3@sint16;
nondet v_call_i_2_18_3@sint16;
nondet v_call_i_2_19_3@sint16;
nondet v_call_i_2_20_3@sint16;
nondet v_call_i_2_21_3@sint16;
nondet v_call_i_2_22_3@sint16;
nondet v_call_i_2_23_3@sint16;
nondet v_call_i_2_24_3@sint16;
nondet v_call_i_2_25_3@sint16;
nondet v_call_i_2_26_3@sint16;
nondet v_call_i_2_27_3@sint16;
nondet v_call_i_2_28_3@sint16;
nondet v_call_i_2_29_3@sint16;
nondet v_call_i_2_30_3@sint16;
nondet v_call_i_2_31_3@sint16;
nondet v_call_i_3@sint16;
nondet v_call_i_3_1@sint16;
nondet v_call_i_3_2@sint16;
nondet v_call_i_3_3@sint16;
nondet v_call_i_3_4@sint16;
nondet v_call_i_3_5@sint16;
nondet v_call_i_3_6@sint16;
nondet v_call_i_3_7@sint16;
nondet v_call_i_3_8@sint16;
nondet v_call_i_3_9@sint16;
nondet v_call_i_3_10@sint16;
nondet v_call_i_3_11@sint16;
nondet v_call_i_3_12@sint16;
nondet v_call_i_3_13@sint16;
nondet v_call_i_3_14@sint16;
nondet v_call_i_3_15@sint16;
nondet v_call_i_3_1181@sint16;
nondet v_call_i_3_1_1@sint16;
nondet v_call_i_3_2_1@sint16;
nondet v_call_i_3_3_1@sint16;
nondet v_call_i_3_4_1@sint16;
nondet v_call_i_3_5_1@sint16;
nondet v_call_i_3_6_1@sint16;
nondet v_call_i_3_7_1@sint16;
nondet v_call_i_3_8_1@sint16;
nondet v_call_i_3_9_1@sint16;
nondet v_call_i_3_10_1@sint16;
nondet v_call_i_3_11_1@sint16;
nondet v_call_i_3_12_1@sint16;
nondet v_call_i_3_13_1@sint16;
nondet v_call_i_3_14_1@sint16;
nondet v_call_i_3_15_1@sint16;
nondet v_call_i_3_2191@sint16;
nondet v_call_i_3_1_2@sint16;
nondet v_call_i_3_2_2@sint16;
nondet v_call_i_3_3_2@sint16;
nondet v_call_i_3_4_2@sint16;
nondet v_call_i_3_5_2@sint16;
nondet v_call_i_3_6_2@sint16;
nondet v_call_i_3_7_2@sint16;
nondet v_call_i_3_8_2@sint16;
nondet v_call_i_3_9_2@sint16;
nondet v_call_i_3_10_2@sint16;
nondet v_call_i_3_11_2@sint16;
nondet v_call_i_3_12_2@sint16;
nondet v_call_i_3_13_2@sint16;
nondet v_call_i_3_14_2@sint16;
nondet v_call_i_3_15_2@sint16;
nondet v_call_i_3_3201@sint16;
nondet v_call_i_3_1_3@sint16;
nondet v_call_i_3_2_3@sint16;
nondet v_call_i_3_3_3@sint16;
nondet v_call_i_3_4_3@sint16;
nondet v_call_i_3_5_3@sint16;
nondet v_call_i_3_6_3@sint16;
nondet v_call_i_3_7_3@sint16;
nondet v_call_i_3_8_3@sint16;
nondet v_call_i_3_9_3@sint16;
nondet v_call_i_3_10_3@sint16;
nondet v_call_i_3_11_3@sint16;
nondet v_call_i_3_12_3@sint16;
nondet v_call_i_3_13_3@sint16;
nondet v_call_i_3_14_3@sint16;
nondet v_call_i_3_15_3@sint16;
nondet v_call_i_3_4211@sint16;
nondet v_call_i_3_1_4@sint16;
nondet v_call_i_3_2_4@sint16;
nondet v_call_i_3_3_4@sint16;
nondet v_call_i_3_4_4@sint16;
nondet v_call_i_3_5_4@sint16;
nondet v_call_i_3_6_4@sint16;
nondet v_call_i_3_7_4@sint16;
nondet v_call_i_3_8_4@sint16;
nondet v_call_i_3_9_4@sint16;
nondet v_call_i_3_10_4@sint16;
nondet v_call_i_3_11_4@sint16;
nondet v_call_i_3_12_4@sint16;
nondet v_call_i_3_13_4@sint16;
nondet v_call_i_3_14_4@sint16;
nondet v_call_i_3_15_4@sint16;
nondet v_call_i_3_5221@sint16;
nondet v_call_i_3_1_5@sint16;
nondet v_call_i_3_2_5@sint16;
nondet v_call_i_3_3_5@sint16;
nondet v_call_i_3_4_5@sint16;
nondet v_call_i_3_5_5@sint16;
nondet v_call_i_3_6_5@sint16;
nondet v_call_i_3_7_5@sint16;
nondet v_call_i_3_8_5@sint16;
nondet v_call_i_3_9_5@sint16;
nondet v_call_i_3_10_5@sint16;
nondet v_call_i_3_11_5@sint16;
nondet v_call_i_3_12_5@sint16;
nondet v_call_i_3_13_5@sint16;
nondet v_call_i_3_14_5@sint16;
nondet v_call_i_3_15_5@sint16;
nondet v_call_i_3_6231@sint16;
nondet v_call_i_3_1_6@sint16;
nondet v_call_i_3_2_6@sint16;
nondet v_call_i_3_3_6@sint16;
nondet v_call_i_3_4_6@sint16;
nondet v_call_i_3_5_6@sint16;
nondet v_call_i_3_6_6@sint16;
nondet v_call_i_3_7_6@sint16;
nondet v_call_i_3_8_6@sint16;
nondet v_call_i_3_9_6@sint16;
nondet v_call_i_3_10_6@sint16;
nondet v_call_i_3_11_6@sint16;
nondet v_call_i_3_12_6@sint16;
nondet v_call_i_3_13_6@sint16;
nondet v_call_i_3_14_6@sint16;
nondet v_call_i_3_15_6@sint16;
nondet v_call_i_3_7241@sint16;
nondet v_call_i_3_1_7@sint16;
nondet v_call_i_3_2_7@sint16;
nondet v_call_i_3_3_7@sint16;
nondet v_call_i_3_4_7@sint16;
nondet v_call_i_3_5_7@sint16;
nondet v_call_i_3_6_7@sint16;
nondet v_call_i_3_7_7@sint16;
nondet v_call_i_3_8_7@sint16;
nondet v_call_i_3_9_7@sint16;
nondet v_call_i_3_10_7@sint16;
nondet v_call_i_3_11_7@sint16;
nondet v_call_i_3_12_7@sint16;
nondet v_call_i_3_13_7@sint16;
nondet v_call_i_3_14_7@sint16;
nondet v_call_i_3_15_7@sint16;
nondet v_call_i_4@sint16;
nondet v_call_i_4_1@sint16;
nondet v_call_i_4_2@sint16;
nondet v_call_i_4_3@sint16;
nondet v_call_i_4_4@sint16;
nondet v_call_i_4_5@sint16;
nondet v_call_i_4_6@sint16;
nondet v_call_i_4_7@sint16;
nondet v_call_i_4_1111@sint16;
nondet v_call_i_4_1_1@sint16;
nondet v_call_i_4_2_1@sint16;
nondet v_call_i_4_3_1@sint16;
nondet v_call_i_4_4_1@sint16;
nondet v_call_i_4_5_1@sint16;
nondet v_call_i_4_6_1@sint16;
nondet v_call_i_4_7_1@sint16;
nondet v_call_i_4_2121@sint16;
nondet v_call_i_4_1_2@sint16;
nondet v_call_i_4_2_2@sint16;
nondet v_call_i_4_3_2@sint16;
nondet v_call_i_4_4_2@sint16;
nondet v_call_i_4_5_2@sint16;
nondet v_call_i_4_6_2@sint16;
nondet v_call_i_4_7_2@sint16;
nondet v_call_i_4_3131@sint16;
nondet v_call_i_4_1_3@sint16;
nondet v_call_i_4_2_3@sint16;
nondet v_call_i_4_3_3@sint16;
nondet v_call_i_4_4_3@sint16;
nondet v_call_i_4_5_3@sint16;
nondet v_call_i_4_6_3@sint16;
nondet v_call_i_4_7_3@sint16;
nondet v_call_i_4_4141@sint16;
nondet v_call_i_4_1_4@sint16;
nondet v_call_i_4_2_4@sint16;
nondet v_call_i_4_3_4@sint16;
nondet v_call_i_4_4_4@sint16;
nondet v_call_i_4_5_4@sint16;
nondet v_call_i_4_6_4@sint16;
nondet v_call_i_4_7_4@sint16;
nondet v_call_i_4_5151@sint16;
nondet v_call_i_4_1_5@sint16;
nondet v_call_i_4_2_5@sint16;
nondet v_call_i_4_3_5@sint16;
nondet v_call_i_4_4_5@sint16;
nondet v_call_i_4_5_5@sint16;
nondet v_call_i_4_6_5@sint16;
nondet v_call_i_4_7_5@sint16;
nondet v_call_i_4_6161@sint16;
nondet v_call_i_4_1_6@sint16;
nondet v_call_i_4_2_6@sint16;
nondet v_call_i_4_3_6@sint16;
nondet v_call_i_4_4_6@sint16;
nondet v_call_i_4_5_6@sint16;
nondet v_call_i_4_6_6@sint16;
nondet v_call_i_4_7_6@sint16;
nondet v_call_i_4_7171@sint16;
nondet v_call_i_4_1_7@sint16;
nondet v_call_i_4_2_7@sint16;
nondet v_call_i_4_3_7@sint16;
nondet v_call_i_4_4_7@sint16;
nondet v_call_i_4_5_7@sint16;
nondet v_call_i_4_6_7@sint16;
nondet v_call_i_4_7_7@sint16;
nondet v_call_i_4_8@sint16;
nondet v_call_i_4_1_8@sint16;
nondet v_call_i_4_2_8@sint16;
nondet v_call_i_4_3_8@sint16;
nondet v_call_i_4_4_8@sint16;
nondet v_call_i_4_5_8@sint16;
nondet v_call_i_4_6_8@sint16;
nondet v_call_i_4_7_8@sint16;
nondet v_call_i_4_9@sint16;
nondet v_call_i_4_1_9@sint16;
nondet v_call_i_4_2_9@sint16;
nondet v_call_i_4_3_9@sint16;
nondet v_call_i_4_4_9@sint16;
nondet v_call_i_4_5_9@sint16;
nondet v_call_i_4_6_9@sint16;
nondet v_call_i_4_7_9@sint16;
nondet v_call_i_4_10@sint16;
nondet v_call_i_4_1_10@sint16;
nondet v_call_i_4_2_10@sint16;
nondet v_call_i_4_3_10@sint16;
nondet v_call_i_4_4_10@sint16;
nondet v_call_i_4_5_10@sint16;
nondet v_call_i_4_6_10@sint16;
nondet v_call_i_4_7_10@sint16;
nondet v_call_i_4_11@sint16;
nondet v_call_i_4_1_11@sint16;
nondet v_call_i_4_2_11@sint16;
nondet v_call_i_4_3_11@sint16;
nondet v_call_i_4_4_11@sint16;
nondet v_call_i_4_5_11@sint16;
nondet v_call_i_4_6_11@sint16;
nondet v_call_i_4_7_11@sint16;
nondet v_call_i_4_12@sint16;
nondet v_call_i_4_1_12@sint16;
nondet v_call_i_4_2_12@sint16;
nondet v_call_i_4_3_12@sint16;
nondet v_call_i_4_4_12@sint16;
nondet v_call_i_4_5_12@sint16;
nondet v_call_i_4_6_12@sint16;
nondet v_call_i_4_7_12@sint16;
nondet v_call_i_4_13@sint16;
nondet v_call_i_4_1_13@sint16;
nondet v_call_i_4_2_13@sint16;
nondet v_call_i_4_3_13@sint16;
nondet v_call_i_4_4_13@sint16;
nondet v_call_i_4_5_13@sint16;
nondet v_call_i_4_6_13@sint16;
nondet v_call_i_4_7_13@sint16;
nondet v_call_i_4_14@sint16;
nondet v_call_i_4_1_14@sint16;
nondet v_call_i_4_2_14@sint16;
nondet v_call_i_4_3_14@sint16;
nondet v_call_i_4_4_14@sint16;
nondet v_call_i_4_5_14@sint16;
nondet v_call_i_4_6_14@sint16;
nondet v_call_i_4_7_14@sint16;
nondet v_call_i_4_15@sint16;
nondet v_call_i_4_1_15@sint16;
nondet v_call_i_4_2_15@sint16;
nondet v_call_i_4_3_15@sint16;
nondet v_call_i_4_4_15@sint16;
nondet v_call_i_4_5_15@sint16;
nondet v_call_i_4_6_15@sint16;
nondet v_call_i_4_7_15@sint16;
nondet v_call_i_5@sint16;
nondet v_call_i_5_1@sint16;
nondet v_call_i_5_2@sint16;
nondet v_call_i_5_3@sint16;
nondet v_call_i_5_181@sint16;
nondet v_call_i_5_1_1@sint16;
nondet v_call_i_5_2_1@sint16;
nondet v_call_i_5_3_1@sint16;
nondet v_call_i_5_291@sint16;
nondet v_call_i_5_1_2@sint16;
nondet v_call_i_5_2_2@sint16;
nondet v_call_i_5_3_2@sint16;
nondet v_call_i_5_3101@sint16;
nondet v_call_i_5_1_3@sint16;
nondet v_call_i_5_2_3@sint16;
nondet v_call_i_5_3_3@sint16;
nondet v_call_i_5_4@sint16;
nondet v_call_i_5_1_4@sint16;
nondet v_call_i_5_2_4@sint16;
nondet v_call_i_5_3_4@sint16;
nondet v_call_i_5_5@sint16;
nondet v_call_i_5_1_5@sint16;
nondet v_call_i_5_2_5@sint16;
nondet v_call_i_5_3_5@sint16;
nondet v_call_i_5_6@sint16;
nondet v_call_i_5_1_6@sint16;
nondet v_call_i_5_2_6@sint16;
nondet v_call_i_5_3_6@sint16;
nondet v_call_i_5_7@sint16;
nondet v_call_i_5_1_7@sint16;
nondet v_call_i_5_2_7@sint16;
nondet v_call_i_5_3_7@sint16;
nondet v_call_i_5_8@sint16;
nondet v_call_i_5_1_8@sint16;
nondet v_call_i_5_2_8@sint16;
nondet v_call_i_5_3_8@sint16;
nondet v_call_i_5_9@sint16;
nondet v_call_i_5_1_9@sint16;
nondet v_call_i_5_2_9@sint16;
nondet v_call_i_5_3_9@sint16;
nondet v_call_i_5_10@sint16;
nondet v_call_i_5_1_10@sint16;
nondet v_call_i_5_2_10@sint16;
nondet v_call_i_5_3_10@sint16;
nondet v_call_i_5_11@sint16;
nondet v_call_i_5_1_11@sint16;
nondet v_call_i_5_2_11@sint16;
nondet v_call_i_5_3_11@sint16;
nondet v_call_i_5_12@sint16;
nondet v_call_i_5_1_12@sint16;
nondet v_call_i_5_2_12@sint16;
nondet v_call_i_5_3_12@sint16;
nondet v_call_i_5_13@sint16;
nondet v_call_i_5_1_13@sint16;
nondet v_call_i_5_2_13@sint16;
nondet v_call_i_5_3_13@sint16;
nondet v_call_i_5_14@sint16;
nondet v_call_i_5_1_14@sint16;
nondet v_call_i_5_2_14@sint16;
nondet v_call_i_5_3_14@sint16;
nondet v_call_i_5_15@sint16;
nondet v_call_i_5_1_15@sint16;
nondet v_call_i_5_2_15@sint16;
nondet v_call_i_5_3_15@sint16;
nondet v_call_i_5_16@sint16;
nondet v_call_i_5_1_16@sint16;
nondet v_call_i_5_2_16@sint16;
nondet v_call_i_5_3_16@sint16;
nondet v_call_i_5_17@sint16;
nondet v_call_i_5_1_17@sint16;
nondet v_call_i_5_2_17@sint16;
nondet v_call_i_5_3_17@sint16;
nondet v_call_i_5_18@sint16;
nondet v_call_i_5_1_18@sint16;
nondet v_call_i_5_2_18@sint16;
nondet v_call_i_5_3_18@sint16;
nondet v_call_i_5_19@sint16;
nondet v_call_i_5_1_19@sint16;
nondet v_call_i_5_2_19@sint16;
nondet v_call_i_5_3_19@sint16;
nondet v_call_i_5_20@sint16;
nondet v_call_i_5_1_20@sint16;
nondet v_call_i_5_2_20@sint16;
nondet v_call_i_5_3_20@sint16;
nondet v_call_i_5_21@sint16;
nondet v_call_i_5_1_21@sint16;
nondet v_call_i_5_2_21@sint16;
nondet v_call_i_5_3_21@sint16;
nondet v_call_i_5_22@sint16;
nondet v_call_i_5_1_22@sint16;
nondet v_call_i_5_2_22@sint16;
nondet v_call_i_5_3_22@sint16;
nondet v_call_i_5_23@sint16;
nondet v_call_i_5_1_23@sint16;
nondet v_call_i_5_2_23@sint16;
nondet v_call_i_5_3_23@sint16;
nondet v_call_i_5_24@sint16;
nondet v_call_i_5_1_24@sint16;
nondet v_call_i_5_2_24@sint16;
nondet v_call_i_5_3_24@sint16;
nondet v_call_i_5_25@sint16;
nondet v_call_i_5_1_25@sint16;
nondet v_call_i_5_2_25@sint16;
nondet v_call_i_5_3_25@sint16;
nondet v_call_i_5_26@sint16;
nondet v_call_i_5_1_26@sint16;
nondet v_call_i_5_2_26@sint16;
nondet v_call_i_5_3_26@sint16;
nondet v_call_i_5_27@sint16;
nondet v_call_i_5_1_27@sint16;
nondet v_call_i_5_2_27@sint16;
nondet v_call_i_5_3_27@sint16;
nondet v_call_i_5_28@sint16;
nondet v_call_i_5_1_28@sint16;
nondet v_call_i_5_2_28@sint16;
nondet v_call_i_5_3_28@sint16;
nondet v_call_i_5_29@sint16;
nondet v_call_i_5_1_29@sint16;
nondet v_call_i_5_2_29@sint16;
nondet v_call_i_5_3_29@sint16;
nondet v_call_i_5_30@sint16;
nondet v_call_i_5_1_30@sint16;
nondet v_call_i_5_2_30@sint16;
nondet v_call_i_5_3_30@sint16;
nondet v_call_i_5_31@sint16;
nondet v_call_i_5_1_31@sint16;
nondet v_call_i_5_2_31@sint16;
nondet v_call_i_5_3_31@sint16;
nondet v_call_i_6@sint16;
nondet v_call_i_6_1@sint16;
nondet v_call_i_6_171@sint16;
nondet v_call_i_6_1_1@sint16;
nondet v_call_i_6_2@sint16;
nondet v_call_i_6_1_2@sint16;
nondet v_call_i_6_3@sint16;
nondet v_call_i_6_1_3@sint16;
nondet v_call_i_6_4@sint16;
nondet v_call_i_6_1_4@sint16;
nondet v_call_i_6_5@sint16;
nondet v_call_i_6_1_5@sint16;
nondet v_call_i_6_6@sint16;
nondet v_call_i_6_1_6@sint16;
nondet v_call_i_6_7@sint16;
nondet v_call_i_6_1_7@sint16;
nondet v_call_i_6_8@sint16;
nondet v_call_i_6_1_8@sint16;
nondet v_call_i_6_9@sint16;
nondet v_call_i_6_1_9@sint16;
nondet v_call_i_6_10@sint16;
nondet v_call_i_6_1_10@sint16;
nondet v_call_i_6_11@sint16;
nondet v_call_i_6_1_11@sint16;
nondet v_call_i_6_12@sint16;
nondet v_call_i_6_1_12@sint16;
nondet v_call_i_6_13@sint16;
nondet v_call_i_6_1_13@sint16;
nondet v_call_i_6_14@sint16;
nondet v_call_i_6_1_14@sint16;
nondet v_call_i_6_15@sint16;
nondet v_call_i_6_1_15@sint16;
nondet v_call_i_6_16@sint16;
nondet v_call_i_6_1_16@sint16;
nondet v_call_i_6_17@sint16;
nondet v_call_i_6_1_17@sint16;
nondet v_call_i_6_18@sint16;
nondet v_call_i_6_1_18@sint16;
nondet v_call_i_6_19@sint16;
nondet v_call_i_6_1_19@sint16;
nondet v_call_i_6_20@sint16;
nondet v_call_i_6_1_20@sint16;
nondet v_call_i_6_21@sint16;
nondet v_call_i_6_1_21@sint16;
nondet v_call_i_6_22@sint16;
nondet v_call_i_6_1_22@sint16;
nondet v_call_i_6_23@sint16;
nondet v_call_i_6_1_23@sint16;
nondet v_call_i_6_24@sint16;
nondet v_call_i_6_1_24@sint16;
nondet v_call_i_6_25@sint16;
nondet v_call_i_6_1_25@sint16;
nondet v_call_i_6_26@sint16;
nondet v_call_i_6_1_26@sint16;
nondet v_call_i_6_27@sint16;
nondet v_call_i_6_1_27@sint16;
nondet v_call_i_6_28@sint16;
nondet v_call_i_6_1_28@sint16;
nondet v_call_i_6_29@sint16;
nondet v_call_i_6_1_29@sint16;
nondet v_call_i_6_30@sint16;
nondet v_call_i_6_1_30@sint16;
nondet v_call_i_6_31@sint16;
nondet v_call_i_6_1_31@sint16;
nondet v_call_i_6_32@sint16;
nondet v_call_i_6_1_32@sint16;
nondet v_call_i_6_33@sint16;
nondet v_call_i_6_1_33@sint16;
nondet v_call_i_6_34@sint16;
nondet v_call_i_6_1_34@sint16;
nondet v_call_i_6_35@sint16;
nondet v_call_i_6_1_35@sint16;
nondet v_call_i_6_36@sint16;
nondet v_call_i_6_1_36@sint16;
nondet v_call_i_6_37@sint16;
nondet v_call_i_6_1_37@sint16;
nondet v_call_i_6_38@sint16;
nondet v_call_i_6_1_38@sint16;
nondet v_call_i_6_39@sint16;
nondet v_call_i_6_1_39@sint16;
nondet v_call_i_6_40@sint16;
nondet v_call_i_6_1_40@sint16;
nondet v_call_i_6_41@sint16;
nondet v_call_i_6_1_41@sint16;
nondet v_call_i_6_42@sint16;
nondet v_call_i_6_1_42@sint16;
nondet v_call_i_6_43@sint16;
nondet v_call_i_6_1_43@sint16;
nondet v_call_i_6_44@sint16;
nondet v_call_i_6_1_44@sint16;
nondet v_call_i_6_45@sint16;
nondet v_call_i_6_1_45@sint16;
nondet v_call_i_6_46@sint16;
nondet v_call_i_6_1_46@sint16;
nondet v_call_i_6_47@sint16;
nondet v_call_i_6_1_47@sint16;
nondet v_call_i_6_48@sint16;
nondet v_call_i_6_1_48@sint16;
nondet v_call_i_6_49@sint16;
nondet v_call_i_6_1_49@sint16;
nondet v_call_i_6_50@sint16;
nondet v_call_i_6_1_50@sint16;
nondet v_call_i_6_51@sint16;
nondet v_call_i_6_1_51@sint16;
nondet v_call_i_6_52@sint16;
nondet v_call_i_6_1_52@sint16;
nondet v_call_i_6_53@sint16;
nondet v_call_i_6_1_53@sint16;
nondet v_call_i_6_54@sint16;
nondet v_call_i_6_1_54@sint16;
nondet v_call_i_6_55@sint16;
nondet v_call_i_6_1_55@sint16;
nondet v_call_i_6_56@sint16;
nondet v_call_i_6_1_56@sint16;
nondet v_call_i_6_57@sint16;
nondet v_call_i_6_1_57@sint16;
nondet v_call_i_6_58@sint16;
nondet v_call_i_6_1_58@sint16;
nondet v_call_i_6_59@sint16;
nondet v_call_i_6_1_59@sint16;
nondet v_call_i_6_60@sint16;
nondet v_call_i_6_1_60@sint16;
nondet v_call_i_6_61@sint16;
nondet v_call_i_6_1_61@sint16;
nondet v_call_i_6_62@sint16;
nondet v_call_i_6_1_62@sint16;
nondet v_call_i_6_63@sint16;
nondet v_call_i_6_1_63@sint16;

(*   %arrayidx9 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %0 = load i16, i16* %arrayidx9, align 2, !tbaa !3 *)
mov v0 mem0_256;
(*   %conv1.i = sext i16 %0 to i32 *)
cast v_conv1_i@sint32 v0@sint16;
(*   %mul.i = mul nsw i32 %conv1.i, -758 *)
mul v_mul_i v_conv1_i (-758)@sint32;
(*   %call.i = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i, v_call_i);
(*   %1 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v1 mem0_0;
(*   %sub = sub i16 %1, %call.i *)
sub v_sub v1 v_call_i;
(*   store i16 %sub, i16* %arrayidx9, align 2, !tbaa !3 *)
mov mem0_256 v_sub;
(*   %add21 = add i16 %1, %call.i *)
add v_add21 v1 v_call_i;
(*   store i16 %add21, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21;
(*   %arrayidx9.1286 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %2 = load i16, i16* %arrayidx9.1286, align 2, !tbaa !3 *)
mov v2 mem0_258;
(*   %conv1.i.1287 = sext i16 %2 to i32 *)
cast v_conv1_i_1287@sint32 v2@sint16;
(*   %mul.i.1288 = mul nsw i32 %conv1.i.1287, -758 *)
mul v_mul_i_1288 v_conv1_i_1287 (-758)@sint32;
(*   %call.i.1289 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1288) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1288, v_call_i_1289);
(*   %arrayidx11.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %3 = load i16, i16* %arrayidx11.1, align 2, !tbaa !3 *)
mov v3 mem0_2;
(*   %sub.1290 = sub i16 %3, %call.i.1289 *)
sub v_sub_1290 v3 v_call_i_1289;
(*   store i16 %sub.1290, i16* %arrayidx9.1286, align 2, !tbaa !3 *)
mov mem0_258 v_sub_1290;
(*   %add21.1291 = add i16 %3, %call.i.1289 *)
add v_add21_1291 v3 v_call_i_1289;
(*   store i16 %add21.1291, i16* %arrayidx11.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_1291;
(*   %arrayidx9.2293 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %4 = load i16, i16* %arrayidx9.2293, align 2, !tbaa !3 *)
mov v4 mem0_260;
(*   %conv1.i.2294 = sext i16 %4 to i32 *)
cast v_conv1_i_2294@sint32 v4@sint16;
(*   %mul.i.2295 = mul nsw i32 %conv1.i.2294, -758 *)
mul v_mul_i_2295 v_conv1_i_2294 (-758)@sint32;
(*   %call.i.2296 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2295) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2295, v_call_i_2296);
(*   %arrayidx11.2 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %5 = load i16, i16* %arrayidx11.2, align 2, !tbaa !3 *)
mov v5 mem0_4;
(*   %sub.2297 = sub i16 %5, %call.i.2296 *)
sub v_sub_2297 v5 v_call_i_2296;
(*   store i16 %sub.2297, i16* %arrayidx9.2293, align 2, !tbaa !3 *)
mov mem0_260 v_sub_2297;
(*   %add21.2298 = add i16 %5, %call.i.2296 *)
add v_add21_2298 v5 v_call_i_2296;
(*   store i16 %add21.2298, i16* %arrayidx11.2, align 2, !tbaa !3 *)
mov mem0_4 v_add21_2298;
(*   %arrayidx9.3300 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %6 = load i16, i16* %arrayidx9.3300, align 2, !tbaa !3 *)
mov v6 mem0_262;
(*   %conv1.i.3301 = sext i16 %6 to i32 *)
cast v_conv1_i_3301@sint32 v6@sint16;
(*   %mul.i.3302 = mul nsw i32 %conv1.i.3301, -758 *)
mul v_mul_i_3302 v_conv1_i_3301 (-758)@sint32;
(*   %call.i.3303 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3302) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3302, v_call_i_3303);
(*   %arrayidx11.3 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %7 = load i16, i16* %arrayidx11.3, align 2, !tbaa !3 *)
mov v7 mem0_6;
(*   %sub.3304 = sub i16 %7, %call.i.3303 *)
sub v_sub_3304 v7 v_call_i_3303;
(*   store i16 %sub.3304, i16* %arrayidx9.3300, align 2, !tbaa !3 *)
mov mem0_262 v_sub_3304;
(*   %add21.3305 = add i16 %7, %call.i.3303 *)
add v_add21_3305 v7 v_call_i_3303;
(*   store i16 %add21.3305, i16* %arrayidx11.3, align 2, !tbaa !3 *)
mov mem0_6 v_add21_3305;
(*   %arrayidx9.4307 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %8 = load i16, i16* %arrayidx9.4307, align 2, !tbaa !3 *)
mov v8 mem0_264;
(*   %conv1.i.4308 = sext i16 %8 to i32 *)
cast v_conv1_i_4308@sint32 v8@sint16;
(*   %mul.i.4309 = mul nsw i32 %conv1.i.4308, -758 *)
mul v_mul_i_4309 v_conv1_i_4308 (-758)@sint32;
(*   %call.i.4310 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4309) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4309, v_call_i_4310);
(*   %arrayidx11.4 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %9 = load i16, i16* %arrayidx11.4, align 2, !tbaa !3 *)
mov v9 mem0_8;
(*   %sub.4311 = sub i16 %9, %call.i.4310 *)
sub v_sub_4311 v9 v_call_i_4310;
(*   store i16 %sub.4311, i16* %arrayidx9.4307, align 2, !tbaa !3 *)
mov mem0_264 v_sub_4311;
(*   %add21.4312 = add i16 %9, %call.i.4310 *)
add v_add21_4312 v9 v_call_i_4310;
(*   store i16 %add21.4312, i16* %arrayidx11.4, align 2, !tbaa !3 *)
mov mem0_8 v_add21_4312;
(*   %arrayidx9.5314 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %10 = load i16, i16* %arrayidx9.5314, align 2, !tbaa !3 *)
mov v10 mem0_266;
(*   %conv1.i.5315 = sext i16 %10 to i32 *)
cast v_conv1_i_5315@sint32 v10@sint16;
(*   %mul.i.5316 = mul nsw i32 %conv1.i.5315, -758 *)
mul v_mul_i_5316 v_conv1_i_5315 (-758)@sint32;
(*   %call.i.5317 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5316) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5316, v_call_i_5317);
(*   %arrayidx11.5 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %11 = load i16, i16* %arrayidx11.5, align 2, !tbaa !3 *)
mov v11 mem0_10;
(*   %sub.5318 = sub i16 %11, %call.i.5317 *)
sub v_sub_5318 v11 v_call_i_5317;
(*   store i16 %sub.5318, i16* %arrayidx9.5314, align 2, !tbaa !3 *)
mov mem0_266 v_sub_5318;
(*   %add21.5319 = add i16 %11, %call.i.5317 *)
add v_add21_5319 v11 v_call_i_5317;
(*   store i16 %add21.5319, i16* %arrayidx11.5, align 2, !tbaa !3 *)
mov mem0_10 v_add21_5319;
(*   %arrayidx9.6321 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %12 = load i16, i16* %arrayidx9.6321, align 2, !tbaa !3 *)
mov v12 mem0_268;
(*   %conv1.i.6322 = sext i16 %12 to i32 *)
cast v_conv1_i_6322@sint32 v12@sint16;
(*   %mul.i.6323 = mul nsw i32 %conv1.i.6322, -758 *)
mul v_mul_i_6323 v_conv1_i_6322 (-758)@sint32;
(*   %call.i.6324 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6323) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6323, v_call_i_6324);
(*   %arrayidx11.6 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %13 = load i16, i16* %arrayidx11.6, align 2, !tbaa !3 *)
mov v13 mem0_12;
(*   %sub.6325 = sub i16 %13, %call.i.6324 *)
sub v_sub_6325 v13 v_call_i_6324;
(*   store i16 %sub.6325, i16* %arrayidx9.6321, align 2, !tbaa !3 *)
mov mem0_268 v_sub_6325;
(*   %add21.6326 = add i16 %13, %call.i.6324 *)
add v_add21_6326 v13 v_call_i_6324;
(*   store i16 %add21.6326, i16* %arrayidx11.6, align 2, !tbaa !3 *)
mov mem0_12 v_add21_6326;
(*   %arrayidx9.7 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %14 = load i16, i16* %arrayidx9.7, align 2, !tbaa !3 *)
mov v14 mem0_270;
(*   %conv1.i.7 = sext i16 %14 to i32 *)
cast v_conv1_i_7@sint32 v14@sint16;
(*   %mul.i.7 = mul nsw i32 %conv1.i.7, -758 *)
mul v_mul_i_7 v_conv1_i_7 (-758)@sint32;
(*   %call.i.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_7, v_call_i_7);
(*   %arrayidx11.7 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %15 = load i16, i16* %arrayidx11.7, align 2, !tbaa !3 *)
mov v15 mem0_14;
(*   %sub.7 = sub i16 %15, %call.i.7 *)
sub v_sub_7 v15 v_call_i_7;
(*   store i16 %sub.7, i16* %arrayidx9.7, align 2, !tbaa !3 *)
mov mem0_270 v_sub_7;
(*   %add21.7 = add i16 %15, %call.i.7 *)
add v_add21_7 v15 v_call_i_7;
(*   store i16 %add21.7, i16* %arrayidx11.7, align 2, !tbaa !3 *)
mov mem0_14 v_add21_7;
(*   %arrayidx9.8 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %16 = load i16, i16* %arrayidx9.8, align 2, !tbaa !3 *)
mov v16 mem0_272;
(*   %conv1.i.8 = sext i16 %16 to i32 *)
cast v_conv1_i_8@sint32 v16@sint16;
(*   %mul.i.8 = mul nsw i32 %conv1.i.8, -758 *)
mul v_mul_i_8 v_conv1_i_8 (-758)@sint32;
(*   %call.i.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_8, v_call_i_8);
(*   %arrayidx11.8 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %17 = load i16, i16* %arrayidx11.8, align 2, !tbaa !3 *)
mov v17 mem0_16;
(*   %sub.8 = sub i16 %17, %call.i.8 *)
sub v_sub_8 v17 v_call_i_8;
(*   store i16 %sub.8, i16* %arrayidx9.8, align 2, !tbaa !3 *)
mov mem0_272 v_sub_8;
(*   %add21.8 = add i16 %17, %call.i.8 *)
add v_add21_8 v17 v_call_i_8;
(*   store i16 %add21.8, i16* %arrayidx11.8, align 2, !tbaa !3 *)
mov mem0_16 v_add21_8;
(*   %arrayidx9.9 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %18 = load i16, i16* %arrayidx9.9, align 2, !tbaa !3 *)
mov v18 mem0_274;
(*   %conv1.i.9 = sext i16 %18 to i32 *)
cast v_conv1_i_9@sint32 v18@sint16;
(*   %mul.i.9 = mul nsw i32 %conv1.i.9, -758 *)
mul v_mul_i_9 v_conv1_i_9 (-758)@sint32;
(*   %call.i.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_9, v_call_i_9);
(*   %arrayidx11.9 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %19 = load i16, i16* %arrayidx11.9, align 2, !tbaa !3 *)
mov v19 mem0_18;
(*   %sub.9 = sub i16 %19, %call.i.9 *)
sub v_sub_9 v19 v_call_i_9;
(*   store i16 %sub.9, i16* %arrayidx9.9, align 2, !tbaa !3 *)
mov mem0_274 v_sub_9;
(*   %add21.9 = add i16 %19, %call.i.9 *)
add v_add21_9 v19 v_call_i_9;
(*   store i16 %add21.9, i16* %arrayidx11.9, align 2, !tbaa !3 *)
mov mem0_18 v_add21_9;
(*   %arrayidx9.10 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %20 = load i16, i16* %arrayidx9.10, align 2, !tbaa !3 *)
mov v20 mem0_276;
(*   %conv1.i.10 = sext i16 %20 to i32 *)
cast v_conv1_i_10@sint32 v20@sint16;
(*   %mul.i.10 = mul nsw i32 %conv1.i.10, -758 *)
mul v_mul_i_10 v_conv1_i_10 (-758)@sint32;
(*   %call.i.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_10, v_call_i_10);
(*   %arrayidx11.10 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %21 = load i16, i16* %arrayidx11.10, align 2, !tbaa !3 *)
mov v21 mem0_20;
(*   %sub.10 = sub i16 %21, %call.i.10 *)
sub v_sub_10 v21 v_call_i_10;
(*   store i16 %sub.10, i16* %arrayidx9.10, align 2, !tbaa !3 *)
mov mem0_276 v_sub_10;
(*   %add21.10 = add i16 %21, %call.i.10 *)
add v_add21_10 v21 v_call_i_10;
(*   store i16 %add21.10, i16* %arrayidx11.10, align 2, !tbaa !3 *)
mov mem0_20 v_add21_10;
(*   %arrayidx9.11 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %22 = load i16, i16* %arrayidx9.11, align 2, !tbaa !3 *)
mov v22 mem0_278;
(*   %conv1.i.11 = sext i16 %22 to i32 *)
cast v_conv1_i_11@sint32 v22@sint16;
(*   %mul.i.11 = mul nsw i32 %conv1.i.11, -758 *)
mul v_mul_i_11 v_conv1_i_11 (-758)@sint32;
(*   %call.i.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_11, v_call_i_11);
(*   %arrayidx11.11 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %23 = load i16, i16* %arrayidx11.11, align 2, !tbaa !3 *)
mov v23 mem0_22;
(*   %sub.11 = sub i16 %23, %call.i.11 *)
sub v_sub_11 v23 v_call_i_11;
(*   store i16 %sub.11, i16* %arrayidx9.11, align 2, !tbaa !3 *)
mov mem0_278 v_sub_11;
(*   %add21.11 = add i16 %23, %call.i.11 *)
add v_add21_11 v23 v_call_i_11;
(*   store i16 %add21.11, i16* %arrayidx11.11, align 2, !tbaa !3 *)
mov mem0_22 v_add21_11;
(*   %arrayidx9.12 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %24 = load i16, i16* %arrayidx9.12, align 2, !tbaa !3 *)
mov v24 mem0_280;
(*   %conv1.i.12 = sext i16 %24 to i32 *)
cast v_conv1_i_12@sint32 v24@sint16;
(*   %mul.i.12 = mul nsw i32 %conv1.i.12, -758 *)
mul v_mul_i_12 v_conv1_i_12 (-758)@sint32;
(*   %call.i.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_12, v_call_i_12);
(*   %arrayidx11.12 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %25 = load i16, i16* %arrayidx11.12, align 2, !tbaa !3 *)
mov v25 mem0_24;
(*   %sub.12 = sub i16 %25, %call.i.12 *)
sub v_sub_12 v25 v_call_i_12;
(*   store i16 %sub.12, i16* %arrayidx9.12, align 2, !tbaa !3 *)
mov mem0_280 v_sub_12;
(*   %add21.12 = add i16 %25, %call.i.12 *)
add v_add21_12 v25 v_call_i_12;
(*   store i16 %add21.12, i16* %arrayidx11.12, align 2, !tbaa !3 *)
mov mem0_24 v_add21_12;
(*   %arrayidx9.13 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %26 = load i16, i16* %arrayidx9.13, align 2, !tbaa !3 *)
mov v26 mem0_282;
(*   %conv1.i.13 = sext i16 %26 to i32 *)
cast v_conv1_i_13@sint32 v26@sint16;
(*   %mul.i.13 = mul nsw i32 %conv1.i.13, -758 *)
mul v_mul_i_13 v_conv1_i_13 (-758)@sint32;
(*   %call.i.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_13, v_call_i_13);
(*   %arrayidx11.13 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %27 = load i16, i16* %arrayidx11.13, align 2, !tbaa !3 *)
mov v27 mem0_26;
(*   %sub.13 = sub i16 %27, %call.i.13 *)
sub v_sub_13 v27 v_call_i_13;
(*   store i16 %sub.13, i16* %arrayidx9.13, align 2, !tbaa !3 *)
mov mem0_282 v_sub_13;
(*   %add21.13 = add i16 %27, %call.i.13 *)
add v_add21_13 v27 v_call_i_13;
(*   store i16 %add21.13, i16* %arrayidx11.13, align 2, !tbaa !3 *)
mov mem0_26 v_add21_13;
(*   %arrayidx9.14 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %28 = load i16, i16* %arrayidx9.14, align 2, !tbaa !3 *)
mov v28 mem0_284;
(*   %conv1.i.14 = sext i16 %28 to i32 *)
cast v_conv1_i_14@sint32 v28@sint16;
(*   %mul.i.14 = mul nsw i32 %conv1.i.14, -758 *)
mul v_mul_i_14 v_conv1_i_14 (-758)@sint32;
(*   %call.i.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_14, v_call_i_14);
(*   %arrayidx11.14 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %29 = load i16, i16* %arrayidx11.14, align 2, !tbaa !3 *)
mov v29 mem0_28;
(*   %sub.14 = sub i16 %29, %call.i.14 *)
sub v_sub_14 v29 v_call_i_14;
(*   store i16 %sub.14, i16* %arrayidx9.14, align 2, !tbaa !3 *)
mov mem0_284 v_sub_14;
(*   %add21.14 = add i16 %29, %call.i.14 *)
add v_add21_14 v29 v_call_i_14;
(*   store i16 %add21.14, i16* %arrayidx11.14, align 2, !tbaa !3 *)
mov mem0_28 v_add21_14;
(*   %arrayidx9.15 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %30 = load i16, i16* %arrayidx9.15, align 2, !tbaa !3 *)
mov v30 mem0_286;
(*   %conv1.i.15 = sext i16 %30 to i32 *)
cast v_conv1_i_15@sint32 v30@sint16;
(*   %mul.i.15 = mul nsw i32 %conv1.i.15, -758 *)
mul v_mul_i_15 v_conv1_i_15 (-758)@sint32;
(*   %call.i.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_15, v_call_i_15);
(*   %arrayidx11.15 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %31 = load i16, i16* %arrayidx11.15, align 2, !tbaa !3 *)
mov v31 mem0_30;
(*   %sub.15 = sub i16 %31, %call.i.15 *)
sub v_sub_15 v31 v_call_i_15;
(*   store i16 %sub.15, i16* %arrayidx9.15, align 2, !tbaa !3 *)
mov mem0_286 v_sub_15;
(*   %add21.15 = add i16 %31, %call.i.15 *)
add v_add21_15 v31 v_call_i_15;
(*   store i16 %add21.15, i16* %arrayidx11.15, align 2, !tbaa !3 *)
mov mem0_30 v_add21_15;
(*   %arrayidx9.16 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %32 = load i16, i16* %arrayidx9.16, align 2, !tbaa !3 *)
mov v32 mem0_288;
(*   %conv1.i.16 = sext i16 %32 to i32 *)
cast v_conv1_i_16@sint32 v32@sint16;
(*   %mul.i.16 = mul nsw i32 %conv1.i.16, -758 *)
mul v_mul_i_16 v_conv1_i_16 (-758)@sint32;
(*   %call.i.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_16, v_call_i_16);
(*   %arrayidx11.16 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %33 = load i16, i16* %arrayidx11.16, align 2, !tbaa !3 *)
mov v33 mem0_32;
(*   %sub.16 = sub i16 %33, %call.i.16 *)
sub v_sub_16 v33 v_call_i_16;
(*   store i16 %sub.16, i16* %arrayidx9.16, align 2, !tbaa !3 *)
mov mem0_288 v_sub_16;
(*   %add21.16 = add i16 %33, %call.i.16 *)
add v_add21_16 v33 v_call_i_16;
(*   store i16 %add21.16, i16* %arrayidx11.16, align 2, !tbaa !3 *)
mov mem0_32 v_add21_16;
(*   %arrayidx9.17 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %34 = load i16, i16* %arrayidx9.17, align 2, !tbaa !3 *)
mov v34 mem0_290;
(*   %conv1.i.17 = sext i16 %34 to i32 *)
cast v_conv1_i_17@sint32 v34@sint16;
(*   %mul.i.17 = mul nsw i32 %conv1.i.17, -758 *)
mul v_mul_i_17 v_conv1_i_17 (-758)@sint32;
(*   %call.i.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_17, v_call_i_17);
(*   %arrayidx11.17 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %35 = load i16, i16* %arrayidx11.17, align 2, !tbaa !3 *)
mov v35 mem0_34;
(*   %sub.17 = sub i16 %35, %call.i.17 *)
sub v_sub_17 v35 v_call_i_17;
(*   store i16 %sub.17, i16* %arrayidx9.17, align 2, !tbaa !3 *)
mov mem0_290 v_sub_17;
(*   %add21.17 = add i16 %35, %call.i.17 *)
add v_add21_17 v35 v_call_i_17;
(*   store i16 %add21.17, i16* %arrayidx11.17, align 2, !tbaa !3 *)
mov mem0_34 v_add21_17;
(*   %arrayidx9.18 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %36 = load i16, i16* %arrayidx9.18, align 2, !tbaa !3 *)
mov v36 mem0_292;
(*   %conv1.i.18 = sext i16 %36 to i32 *)
cast v_conv1_i_18@sint32 v36@sint16;
(*   %mul.i.18 = mul nsw i32 %conv1.i.18, -758 *)
mul v_mul_i_18 v_conv1_i_18 (-758)@sint32;
(*   %call.i.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_18, v_call_i_18);
(*   %arrayidx11.18 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %37 = load i16, i16* %arrayidx11.18, align 2, !tbaa !3 *)
mov v37 mem0_36;
(*   %sub.18 = sub i16 %37, %call.i.18 *)
sub v_sub_18 v37 v_call_i_18;
(*   store i16 %sub.18, i16* %arrayidx9.18, align 2, !tbaa !3 *)
mov mem0_292 v_sub_18;
(*   %add21.18 = add i16 %37, %call.i.18 *)
add v_add21_18 v37 v_call_i_18;
(*   store i16 %add21.18, i16* %arrayidx11.18, align 2, !tbaa !3 *)
mov mem0_36 v_add21_18;
(*   %arrayidx9.19 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %38 = load i16, i16* %arrayidx9.19, align 2, !tbaa !3 *)
mov v38 mem0_294;
(*   %conv1.i.19 = sext i16 %38 to i32 *)
cast v_conv1_i_19@sint32 v38@sint16;
(*   %mul.i.19 = mul nsw i32 %conv1.i.19, -758 *)
mul v_mul_i_19 v_conv1_i_19 (-758)@sint32;
(*   %call.i.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_19, v_call_i_19);
(*   %arrayidx11.19 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %39 = load i16, i16* %arrayidx11.19, align 2, !tbaa !3 *)
mov v39 mem0_38;
(*   %sub.19 = sub i16 %39, %call.i.19 *)
sub v_sub_19 v39 v_call_i_19;
(*   store i16 %sub.19, i16* %arrayidx9.19, align 2, !tbaa !3 *)
mov mem0_294 v_sub_19;
(*   %add21.19 = add i16 %39, %call.i.19 *)
add v_add21_19 v39 v_call_i_19;
(*   store i16 %add21.19, i16* %arrayidx11.19, align 2, !tbaa !3 *)
mov mem0_38 v_add21_19;
(*   %arrayidx9.20 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %40 = load i16, i16* %arrayidx9.20, align 2, !tbaa !3 *)
mov v40 mem0_296;
(*   %conv1.i.20 = sext i16 %40 to i32 *)
cast v_conv1_i_20@sint32 v40@sint16;
(*   %mul.i.20 = mul nsw i32 %conv1.i.20, -758 *)
mul v_mul_i_20 v_conv1_i_20 (-758)@sint32;
(*   %call.i.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_20, v_call_i_20);
(*   %arrayidx11.20 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %41 = load i16, i16* %arrayidx11.20, align 2, !tbaa !3 *)
mov v41 mem0_40;
(*   %sub.20 = sub i16 %41, %call.i.20 *)
sub v_sub_20 v41 v_call_i_20;
(*   store i16 %sub.20, i16* %arrayidx9.20, align 2, !tbaa !3 *)
mov mem0_296 v_sub_20;
(*   %add21.20 = add i16 %41, %call.i.20 *)
add v_add21_20 v41 v_call_i_20;
(*   store i16 %add21.20, i16* %arrayidx11.20, align 2, !tbaa !3 *)
mov mem0_40 v_add21_20;
(*   %arrayidx9.21 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %42 = load i16, i16* %arrayidx9.21, align 2, !tbaa !3 *)
mov v42 mem0_298;
(*   %conv1.i.21 = sext i16 %42 to i32 *)
cast v_conv1_i_21@sint32 v42@sint16;
(*   %mul.i.21 = mul nsw i32 %conv1.i.21, -758 *)
mul v_mul_i_21 v_conv1_i_21 (-758)@sint32;
(*   %call.i.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_21, v_call_i_21);
(*   %arrayidx11.21 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %43 = load i16, i16* %arrayidx11.21, align 2, !tbaa !3 *)
mov v43 mem0_42;
(*   %sub.21 = sub i16 %43, %call.i.21 *)
sub v_sub_21 v43 v_call_i_21;
(*   store i16 %sub.21, i16* %arrayidx9.21, align 2, !tbaa !3 *)
mov mem0_298 v_sub_21;
(*   %add21.21 = add i16 %43, %call.i.21 *)
add v_add21_21 v43 v_call_i_21;
(*   store i16 %add21.21, i16* %arrayidx11.21, align 2, !tbaa !3 *)
mov mem0_42 v_add21_21;
(*   %arrayidx9.22 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %44 = load i16, i16* %arrayidx9.22, align 2, !tbaa !3 *)
mov v44 mem0_300;
(*   %conv1.i.22 = sext i16 %44 to i32 *)
cast v_conv1_i_22@sint32 v44@sint16;
(*   %mul.i.22 = mul nsw i32 %conv1.i.22, -758 *)
mul v_mul_i_22 v_conv1_i_22 (-758)@sint32;
(*   %call.i.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_22, v_call_i_22);
(*   %arrayidx11.22 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %45 = load i16, i16* %arrayidx11.22, align 2, !tbaa !3 *)
mov v45 mem0_44;
(*   %sub.22 = sub i16 %45, %call.i.22 *)
sub v_sub_22 v45 v_call_i_22;
(*   store i16 %sub.22, i16* %arrayidx9.22, align 2, !tbaa !3 *)
mov mem0_300 v_sub_22;
(*   %add21.22 = add i16 %45, %call.i.22 *)
add v_add21_22 v45 v_call_i_22;
(*   store i16 %add21.22, i16* %arrayidx11.22, align 2, !tbaa !3 *)
mov mem0_44 v_add21_22;
(*   %arrayidx9.23 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %46 = load i16, i16* %arrayidx9.23, align 2, !tbaa !3 *)
mov v46 mem0_302;
(*   %conv1.i.23 = sext i16 %46 to i32 *)
cast v_conv1_i_23@sint32 v46@sint16;
(*   %mul.i.23 = mul nsw i32 %conv1.i.23, -758 *)
mul v_mul_i_23 v_conv1_i_23 (-758)@sint32;
(*   %call.i.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_23, v_call_i_23);
(*   %arrayidx11.23 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %47 = load i16, i16* %arrayidx11.23, align 2, !tbaa !3 *)
mov v47 mem0_46;
(*   %sub.23 = sub i16 %47, %call.i.23 *)
sub v_sub_23 v47 v_call_i_23;
(*   store i16 %sub.23, i16* %arrayidx9.23, align 2, !tbaa !3 *)
mov mem0_302 v_sub_23;
(*   %add21.23 = add i16 %47, %call.i.23 *)
add v_add21_23 v47 v_call_i_23;
(*   store i16 %add21.23, i16* %arrayidx11.23, align 2, !tbaa !3 *)
mov mem0_46 v_add21_23;
(*   %arrayidx9.24 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %48 = load i16, i16* %arrayidx9.24, align 2, !tbaa !3 *)
mov v48 mem0_304;
(*   %conv1.i.24 = sext i16 %48 to i32 *)
cast v_conv1_i_24@sint32 v48@sint16;
(*   %mul.i.24 = mul nsw i32 %conv1.i.24, -758 *)
mul v_mul_i_24 v_conv1_i_24 (-758)@sint32;
(*   %call.i.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_24, v_call_i_24);
(*   %arrayidx11.24 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %49 = load i16, i16* %arrayidx11.24, align 2, !tbaa !3 *)
mov v49 mem0_48;
(*   %sub.24 = sub i16 %49, %call.i.24 *)
sub v_sub_24 v49 v_call_i_24;
(*   store i16 %sub.24, i16* %arrayidx9.24, align 2, !tbaa !3 *)
mov mem0_304 v_sub_24;
(*   %add21.24 = add i16 %49, %call.i.24 *)
add v_add21_24 v49 v_call_i_24;
(*   store i16 %add21.24, i16* %arrayidx11.24, align 2, !tbaa !3 *)
mov mem0_48 v_add21_24;
(*   %arrayidx9.25 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %50 = load i16, i16* %arrayidx9.25, align 2, !tbaa !3 *)
mov v50 mem0_306;
(*   %conv1.i.25 = sext i16 %50 to i32 *)
cast v_conv1_i_25@sint32 v50@sint16;
(*   %mul.i.25 = mul nsw i32 %conv1.i.25, -758 *)
mul v_mul_i_25 v_conv1_i_25 (-758)@sint32;
(*   %call.i.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_25, v_call_i_25);
(*   %arrayidx11.25 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %51 = load i16, i16* %arrayidx11.25, align 2, !tbaa !3 *)
mov v51 mem0_50;
(*   %sub.25 = sub i16 %51, %call.i.25 *)
sub v_sub_25 v51 v_call_i_25;
(*   store i16 %sub.25, i16* %arrayidx9.25, align 2, !tbaa !3 *)
mov mem0_306 v_sub_25;
(*   %add21.25 = add i16 %51, %call.i.25 *)
add v_add21_25 v51 v_call_i_25;
(*   store i16 %add21.25, i16* %arrayidx11.25, align 2, !tbaa !3 *)
mov mem0_50 v_add21_25;
(*   %arrayidx9.26 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %52 = load i16, i16* %arrayidx9.26, align 2, !tbaa !3 *)
mov v52 mem0_308;
(*   %conv1.i.26 = sext i16 %52 to i32 *)
cast v_conv1_i_26@sint32 v52@sint16;
(*   %mul.i.26 = mul nsw i32 %conv1.i.26, -758 *)
mul v_mul_i_26 v_conv1_i_26 (-758)@sint32;
(*   %call.i.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_26, v_call_i_26);
(*   %arrayidx11.26 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %53 = load i16, i16* %arrayidx11.26, align 2, !tbaa !3 *)
mov v53 mem0_52;
(*   %sub.26 = sub i16 %53, %call.i.26 *)
sub v_sub_26 v53 v_call_i_26;
(*   store i16 %sub.26, i16* %arrayidx9.26, align 2, !tbaa !3 *)
mov mem0_308 v_sub_26;
(*   %add21.26 = add i16 %53, %call.i.26 *)
add v_add21_26 v53 v_call_i_26;
(*   store i16 %add21.26, i16* %arrayidx11.26, align 2, !tbaa !3 *)
mov mem0_52 v_add21_26;
(*   %arrayidx9.27 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %54 = load i16, i16* %arrayidx9.27, align 2, !tbaa !3 *)
mov v54 mem0_310;
(*   %conv1.i.27 = sext i16 %54 to i32 *)
cast v_conv1_i_27@sint32 v54@sint16;
(*   %mul.i.27 = mul nsw i32 %conv1.i.27, -758 *)
mul v_mul_i_27 v_conv1_i_27 (-758)@sint32;
(*   %call.i.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_27, v_call_i_27);
(*   %arrayidx11.27 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %55 = load i16, i16* %arrayidx11.27, align 2, !tbaa !3 *)
mov v55 mem0_54;
(*   %sub.27 = sub i16 %55, %call.i.27 *)
sub v_sub_27 v55 v_call_i_27;
(*   store i16 %sub.27, i16* %arrayidx9.27, align 2, !tbaa !3 *)
mov mem0_310 v_sub_27;
(*   %add21.27 = add i16 %55, %call.i.27 *)
add v_add21_27 v55 v_call_i_27;
(*   store i16 %add21.27, i16* %arrayidx11.27, align 2, !tbaa !3 *)
mov mem0_54 v_add21_27;
(*   %arrayidx9.28 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %56 = load i16, i16* %arrayidx9.28, align 2, !tbaa !3 *)
mov v56 mem0_312;
(*   %conv1.i.28 = sext i16 %56 to i32 *)
cast v_conv1_i_28@sint32 v56@sint16;
(*   %mul.i.28 = mul nsw i32 %conv1.i.28, -758 *)
mul v_mul_i_28 v_conv1_i_28 (-758)@sint32;
(*   %call.i.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_28, v_call_i_28);
(*   %arrayidx11.28 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %57 = load i16, i16* %arrayidx11.28, align 2, !tbaa !3 *)
mov v57 mem0_56;
(*   %sub.28 = sub i16 %57, %call.i.28 *)
sub v_sub_28 v57 v_call_i_28;
(*   store i16 %sub.28, i16* %arrayidx9.28, align 2, !tbaa !3 *)
mov mem0_312 v_sub_28;
(*   %add21.28 = add i16 %57, %call.i.28 *)
add v_add21_28 v57 v_call_i_28;
(*   store i16 %add21.28, i16* %arrayidx11.28, align 2, !tbaa !3 *)
mov mem0_56 v_add21_28;
(*   %arrayidx9.29 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %58 = load i16, i16* %arrayidx9.29, align 2, !tbaa !3 *)
mov v58 mem0_314;
(*   %conv1.i.29 = sext i16 %58 to i32 *)
cast v_conv1_i_29@sint32 v58@sint16;
(*   %mul.i.29 = mul nsw i32 %conv1.i.29, -758 *)
mul v_mul_i_29 v_conv1_i_29 (-758)@sint32;
(*   %call.i.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_29, v_call_i_29);
(*   %arrayidx11.29 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %59 = load i16, i16* %arrayidx11.29, align 2, !tbaa !3 *)
mov v59 mem0_58;
(*   %sub.29 = sub i16 %59, %call.i.29 *)
sub v_sub_29 v59 v_call_i_29;
(*   store i16 %sub.29, i16* %arrayidx9.29, align 2, !tbaa !3 *)
mov mem0_314 v_sub_29;
(*   %add21.29 = add i16 %59, %call.i.29 *)
add v_add21_29 v59 v_call_i_29;
(*   store i16 %add21.29, i16* %arrayidx11.29, align 2, !tbaa !3 *)
mov mem0_58 v_add21_29;
(*   %arrayidx9.30 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %60 = load i16, i16* %arrayidx9.30, align 2, !tbaa !3 *)
mov v60 mem0_316;
(*   %conv1.i.30 = sext i16 %60 to i32 *)
cast v_conv1_i_30@sint32 v60@sint16;
(*   %mul.i.30 = mul nsw i32 %conv1.i.30, -758 *)
mul v_mul_i_30 v_conv1_i_30 (-758)@sint32;
(*   %call.i.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_30, v_call_i_30);
(*   %arrayidx11.30 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %61 = load i16, i16* %arrayidx11.30, align 2, !tbaa !3 *)
mov v61 mem0_60;
(*   %sub.30 = sub i16 %61, %call.i.30 *)
sub v_sub_30 v61 v_call_i_30;
(*   store i16 %sub.30, i16* %arrayidx9.30, align 2, !tbaa !3 *)
mov mem0_316 v_sub_30;
(*   %add21.30 = add i16 %61, %call.i.30 *)
add v_add21_30 v61 v_call_i_30;
(*   store i16 %add21.30, i16* %arrayidx11.30, align 2, !tbaa !3 *)
mov mem0_60 v_add21_30;
(*   %arrayidx9.31 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %62 = load i16, i16* %arrayidx9.31, align 2, !tbaa !3 *)
mov v62 mem0_318;
(*   %conv1.i.31 = sext i16 %62 to i32 *)
cast v_conv1_i_31@sint32 v62@sint16;
(*   %mul.i.31 = mul nsw i32 %conv1.i.31, -758 *)
mul v_mul_i_31 v_conv1_i_31 (-758)@sint32;
(*   %call.i.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_31, v_call_i_31);
(*   %arrayidx11.31 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %63 = load i16, i16* %arrayidx11.31, align 2, !tbaa !3 *)
mov v63 mem0_62;
(*   %sub.31 = sub i16 %63, %call.i.31 *)
sub v_sub_31 v63 v_call_i_31;
(*   store i16 %sub.31, i16* %arrayidx9.31, align 2, !tbaa !3 *)
mov mem0_318 v_sub_31;
(*   %add21.31 = add i16 %63, %call.i.31 *)
add v_add21_31 v63 v_call_i_31;
(*   store i16 %add21.31, i16* %arrayidx11.31, align 2, !tbaa !3 *)
mov mem0_62 v_add21_31;
(*   %arrayidx9.32 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %64 = load i16, i16* %arrayidx9.32, align 2, !tbaa !3 *)
mov v64 mem0_320;
(*   %conv1.i.32 = sext i16 %64 to i32 *)
cast v_conv1_i_32@sint32 v64@sint16;
(*   %mul.i.32 = mul nsw i32 %conv1.i.32, -758 *)
mul v_mul_i_32 v_conv1_i_32 (-758)@sint32;
(*   %call.i.32 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.32) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_32, v_call_i_32);
(*   %arrayidx11.32 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %65 = load i16, i16* %arrayidx11.32, align 2, !tbaa !3 *)
mov v65 mem0_64;
(*   %sub.32 = sub i16 %65, %call.i.32 *)
sub v_sub_32 v65 v_call_i_32;
(*   store i16 %sub.32, i16* %arrayidx9.32, align 2, !tbaa !3 *)
mov mem0_320 v_sub_32;
(*   %add21.32 = add i16 %65, %call.i.32 *)
add v_add21_32 v65 v_call_i_32;
(*   store i16 %add21.32, i16* %arrayidx11.32, align 2, !tbaa !3 *)
mov mem0_64 v_add21_32;
(*   %arrayidx9.33 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %66 = load i16, i16* %arrayidx9.33, align 2, !tbaa !3 *)
mov v66 mem0_322;
(*   %conv1.i.33 = sext i16 %66 to i32 *)
cast v_conv1_i_33@sint32 v66@sint16;
(*   %mul.i.33 = mul nsw i32 %conv1.i.33, -758 *)
mul v_mul_i_33 v_conv1_i_33 (-758)@sint32;
(*   %call.i.33 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.33) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_33, v_call_i_33);
(*   %arrayidx11.33 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %67 = load i16, i16* %arrayidx11.33, align 2, !tbaa !3 *)
mov v67 mem0_66;
(*   %sub.33 = sub i16 %67, %call.i.33 *)
sub v_sub_33 v67 v_call_i_33;
(*   store i16 %sub.33, i16* %arrayidx9.33, align 2, !tbaa !3 *)
mov mem0_322 v_sub_33;
(*   %add21.33 = add i16 %67, %call.i.33 *)
add v_add21_33 v67 v_call_i_33;
(*   store i16 %add21.33, i16* %arrayidx11.33, align 2, !tbaa !3 *)
mov mem0_66 v_add21_33;
(*   %arrayidx9.34 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %68 = load i16, i16* %arrayidx9.34, align 2, !tbaa !3 *)
mov v68 mem0_324;
(*   %conv1.i.34 = sext i16 %68 to i32 *)
cast v_conv1_i_34@sint32 v68@sint16;
(*   %mul.i.34 = mul nsw i32 %conv1.i.34, -758 *)
mul v_mul_i_34 v_conv1_i_34 (-758)@sint32;
(*   %call.i.34 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.34) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_34, v_call_i_34);
(*   %arrayidx11.34 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %69 = load i16, i16* %arrayidx11.34, align 2, !tbaa !3 *)
mov v69 mem0_68;
(*   %sub.34 = sub i16 %69, %call.i.34 *)
sub v_sub_34 v69 v_call_i_34;
(*   store i16 %sub.34, i16* %arrayidx9.34, align 2, !tbaa !3 *)
mov mem0_324 v_sub_34;
(*   %add21.34 = add i16 %69, %call.i.34 *)
add v_add21_34 v69 v_call_i_34;
(*   store i16 %add21.34, i16* %arrayidx11.34, align 2, !tbaa !3 *)
mov mem0_68 v_add21_34;
(*   %arrayidx9.35 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %70 = load i16, i16* %arrayidx9.35, align 2, !tbaa !3 *)
mov v70 mem0_326;
(*   %conv1.i.35 = sext i16 %70 to i32 *)
cast v_conv1_i_35@sint32 v70@sint16;
(*   %mul.i.35 = mul nsw i32 %conv1.i.35, -758 *)
mul v_mul_i_35 v_conv1_i_35 (-758)@sint32;
(*   %call.i.35 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.35) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_35, v_call_i_35);
(*   %arrayidx11.35 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %71 = load i16, i16* %arrayidx11.35, align 2, !tbaa !3 *)
mov v71 mem0_70;
(*   %sub.35 = sub i16 %71, %call.i.35 *)
sub v_sub_35 v71 v_call_i_35;
(*   store i16 %sub.35, i16* %arrayidx9.35, align 2, !tbaa !3 *)
mov mem0_326 v_sub_35;
(*   %add21.35 = add i16 %71, %call.i.35 *)
add v_add21_35 v71 v_call_i_35;
(*   store i16 %add21.35, i16* %arrayidx11.35, align 2, !tbaa !3 *)
mov mem0_70 v_add21_35;
(*   %arrayidx9.36 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %72 = load i16, i16* %arrayidx9.36, align 2, !tbaa !3 *)
mov v72 mem0_328;
(*   %conv1.i.36 = sext i16 %72 to i32 *)
cast v_conv1_i_36@sint32 v72@sint16;
(*   %mul.i.36 = mul nsw i32 %conv1.i.36, -758 *)
mul v_mul_i_36 v_conv1_i_36 (-758)@sint32;
(*   %call.i.36 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.36) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_36, v_call_i_36);
(*   %arrayidx11.36 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %73 = load i16, i16* %arrayidx11.36, align 2, !tbaa !3 *)
mov v73 mem0_72;
(*   %sub.36 = sub i16 %73, %call.i.36 *)
sub v_sub_36 v73 v_call_i_36;
(*   store i16 %sub.36, i16* %arrayidx9.36, align 2, !tbaa !3 *)
mov mem0_328 v_sub_36;
(*   %add21.36 = add i16 %73, %call.i.36 *)
add v_add21_36 v73 v_call_i_36;
(*   store i16 %add21.36, i16* %arrayidx11.36, align 2, !tbaa !3 *)
mov mem0_72 v_add21_36;
(*   %arrayidx9.37 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %74 = load i16, i16* %arrayidx9.37, align 2, !tbaa !3 *)
mov v74 mem0_330;
(*   %conv1.i.37 = sext i16 %74 to i32 *)
cast v_conv1_i_37@sint32 v74@sint16;
(*   %mul.i.37 = mul nsw i32 %conv1.i.37, -758 *)
mul v_mul_i_37 v_conv1_i_37 (-758)@sint32;
(*   %call.i.37 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.37) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_37, v_call_i_37);
(*   %arrayidx11.37 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %75 = load i16, i16* %arrayidx11.37, align 2, !tbaa !3 *)
mov v75 mem0_74;
(*   %sub.37 = sub i16 %75, %call.i.37 *)
sub v_sub_37 v75 v_call_i_37;
(*   store i16 %sub.37, i16* %arrayidx9.37, align 2, !tbaa !3 *)
mov mem0_330 v_sub_37;
(*   %add21.37 = add i16 %75, %call.i.37 *)
add v_add21_37 v75 v_call_i_37;
(*   store i16 %add21.37, i16* %arrayidx11.37, align 2, !tbaa !3 *)
mov mem0_74 v_add21_37;
(*   %arrayidx9.38 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %76 = load i16, i16* %arrayidx9.38, align 2, !tbaa !3 *)
mov v76 mem0_332;
(*   %conv1.i.38 = sext i16 %76 to i32 *)
cast v_conv1_i_38@sint32 v76@sint16;
(*   %mul.i.38 = mul nsw i32 %conv1.i.38, -758 *)
mul v_mul_i_38 v_conv1_i_38 (-758)@sint32;
(*   %call.i.38 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.38) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_38, v_call_i_38);
(*   %arrayidx11.38 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %77 = load i16, i16* %arrayidx11.38, align 2, !tbaa !3 *)
mov v77 mem0_76;
(*   %sub.38 = sub i16 %77, %call.i.38 *)
sub v_sub_38 v77 v_call_i_38;
(*   store i16 %sub.38, i16* %arrayidx9.38, align 2, !tbaa !3 *)
mov mem0_332 v_sub_38;
(*   %add21.38 = add i16 %77, %call.i.38 *)
add v_add21_38 v77 v_call_i_38;
(*   store i16 %add21.38, i16* %arrayidx11.38, align 2, !tbaa !3 *)
mov mem0_76 v_add21_38;
(*   %arrayidx9.39 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %78 = load i16, i16* %arrayidx9.39, align 2, !tbaa !3 *)
mov v78 mem0_334;
(*   %conv1.i.39 = sext i16 %78 to i32 *)
cast v_conv1_i_39@sint32 v78@sint16;
(*   %mul.i.39 = mul nsw i32 %conv1.i.39, -758 *)
mul v_mul_i_39 v_conv1_i_39 (-758)@sint32;
(*   %call.i.39 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.39) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_39, v_call_i_39);
(*   %arrayidx11.39 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %79 = load i16, i16* %arrayidx11.39, align 2, !tbaa !3 *)
mov v79 mem0_78;
(*   %sub.39 = sub i16 %79, %call.i.39 *)
sub v_sub_39 v79 v_call_i_39;
(*   store i16 %sub.39, i16* %arrayidx9.39, align 2, !tbaa !3 *)
mov mem0_334 v_sub_39;
(*   %add21.39 = add i16 %79, %call.i.39 *)
add v_add21_39 v79 v_call_i_39;
(*   store i16 %add21.39, i16* %arrayidx11.39, align 2, !tbaa !3 *)
mov mem0_78 v_add21_39;
(*   %arrayidx9.40 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %80 = load i16, i16* %arrayidx9.40, align 2, !tbaa !3 *)
mov v80 mem0_336;
(*   %conv1.i.40 = sext i16 %80 to i32 *)
cast v_conv1_i_40@sint32 v80@sint16;
(*   %mul.i.40 = mul nsw i32 %conv1.i.40, -758 *)
mul v_mul_i_40 v_conv1_i_40 (-758)@sint32;
(*   %call.i.40 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.40) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_40, v_call_i_40);
(*   %arrayidx11.40 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %81 = load i16, i16* %arrayidx11.40, align 2, !tbaa !3 *)
mov v81 mem0_80;
(*   %sub.40 = sub i16 %81, %call.i.40 *)
sub v_sub_40 v81 v_call_i_40;
(*   store i16 %sub.40, i16* %arrayidx9.40, align 2, !tbaa !3 *)
mov mem0_336 v_sub_40;
(*   %add21.40 = add i16 %81, %call.i.40 *)
add v_add21_40 v81 v_call_i_40;
(*   store i16 %add21.40, i16* %arrayidx11.40, align 2, !tbaa !3 *)
mov mem0_80 v_add21_40;
(*   %arrayidx9.41 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %82 = load i16, i16* %arrayidx9.41, align 2, !tbaa !3 *)
mov v82 mem0_338;
(*   %conv1.i.41 = sext i16 %82 to i32 *)
cast v_conv1_i_41@sint32 v82@sint16;
(*   %mul.i.41 = mul nsw i32 %conv1.i.41, -758 *)
mul v_mul_i_41 v_conv1_i_41 (-758)@sint32;
(*   %call.i.41 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.41) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_41, v_call_i_41);
(*   %arrayidx11.41 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %83 = load i16, i16* %arrayidx11.41, align 2, !tbaa !3 *)
mov v83 mem0_82;
(*   %sub.41 = sub i16 %83, %call.i.41 *)
sub v_sub_41 v83 v_call_i_41;
(*   store i16 %sub.41, i16* %arrayidx9.41, align 2, !tbaa !3 *)
mov mem0_338 v_sub_41;
(*   %add21.41 = add i16 %83, %call.i.41 *)
add v_add21_41 v83 v_call_i_41;
(*   store i16 %add21.41, i16* %arrayidx11.41, align 2, !tbaa !3 *)
mov mem0_82 v_add21_41;
(*   %arrayidx9.42 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %84 = load i16, i16* %arrayidx9.42, align 2, !tbaa !3 *)
mov v84 mem0_340;
(*   %conv1.i.42 = sext i16 %84 to i32 *)
cast v_conv1_i_42@sint32 v84@sint16;
(*   %mul.i.42 = mul nsw i32 %conv1.i.42, -758 *)
mul v_mul_i_42 v_conv1_i_42 (-758)@sint32;
(*   %call.i.42 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.42) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_42, v_call_i_42);
(*   %arrayidx11.42 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %85 = load i16, i16* %arrayidx11.42, align 2, !tbaa !3 *)
mov v85 mem0_84;
(*   %sub.42 = sub i16 %85, %call.i.42 *)
sub v_sub_42 v85 v_call_i_42;
(*   store i16 %sub.42, i16* %arrayidx9.42, align 2, !tbaa !3 *)
mov mem0_340 v_sub_42;
(*   %add21.42 = add i16 %85, %call.i.42 *)
add v_add21_42 v85 v_call_i_42;
(*   store i16 %add21.42, i16* %arrayidx11.42, align 2, !tbaa !3 *)
mov mem0_84 v_add21_42;
(*   %arrayidx9.43 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %86 = load i16, i16* %arrayidx9.43, align 2, !tbaa !3 *)
mov v86 mem0_342;
(*   %conv1.i.43 = sext i16 %86 to i32 *)
cast v_conv1_i_43@sint32 v86@sint16;
(*   %mul.i.43 = mul nsw i32 %conv1.i.43, -758 *)
mul v_mul_i_43 v_conv1_i_43 (-758)@sint32;
(*   %call.i.43 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.43) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_43, v_call_i_43);
(*   %arrayidx11.43 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %87 = load i16, i16* %arrayidx11.43, align 2, !tbaa !3 *)
mov v87 mem0_86;
(*   %sub.43 = sub i16 %87, %call.i.43 *)
sub v_sub_43 v87 v_call_i_43;
(*   store i16 %sub.43, i16* %arrayidx9.43, align 2, !tbaa !3 *)
mov mem0_342 v_sub_43;
(*   %add21.43 = add i16 %87, %call.i.43 *)
add v_add21_43 v87 v_call_i_43;
(*   store i16 %add21.43, i16* %arrayidx11.43, align 2, !tbaa !3 *)
mov mem0_86 v_add21_43;
(*   %arrayidx9.44 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %88 = load i16, i16* %arrayidx9.44, align 2, !tbaa !3 *)
mov v88 mem0_344;
(*   %conv1.i.44 = sext i16 %88 to i32 *)
cast v_conv1_i_44@sint32 v88@sint16;
(*   %mul.i.44 = mul nsw i32 %conv1.i.44, -758 *)
mul v_mul_i_44 v_conv1_i_44 (-758)@sint32;
(*   %call.i.44 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.44) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_44, v_call_i_44);
(*   %arrayidx11.44 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %89 = load i16, i16* %arrayidx11.44, align 2, !tbaa !3 *)
mov v89 mem0_88;
(*   %sub.44 = sub i16 %89, %call.i.44 *)
sub v_sub_44 v89 v_call_i_44;
(*   store i16 %sub.44, i16* %arrayidx9.44, align 2, !tbaa !3 *)
mov mem0_344 v_sub_44;
(*   %add21.44 = add i16 %89, %call.i.44 *)
add v_add21_44 v89 v_call_i_44;
(*   store i16 %add21.44, i16* %arrayidx11.44, align 2, !tbaa !3 *)
mov mem0_88 v_add21_44;
(*   %arrayidx9.45 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %90 = load i16, i16* %arrayidx9.45, align 2, !tbaa !3 *)
mov v90 mem0_346;
(*   %conv1.i.45 = sext i16 %90 to i32 *)
cast v_conv1_i_45@sint32 v90@sint16;
(*   %mul.i.45 = mul nsw i32 %conv1.i.45, -758 *)
mul v_mul_i_45 v_conv1_i_45 (-758)@sint32;
(*   %call.i.45 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.45) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_45, v_call_i_45);
(*   %arrayidx11.45 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %91 = load i16, i16* %arrayidx11.45, align 2, !tbaa !3 *)
mov v91 mem0_90;
(*   %sub.45 = sub i16 %91, %call.i.45 *)
sub v_sub_45 v91 v_call_i_45;
(*   store i16 %sub.45, i16* %arrayidx9.45, align 2, !tbaa !3 *)
mov mem0_346 v_sub_45;
(*   %add21.45 = add i16 %91, %call.i.45 *)
add v_add21_45 v91 v_call_i_45;
(*   store i16 %add21.45, i16* %arrayidx11.45, align 2, !tbaa !3 *)
mov mem0_90 v_add21_45;
(*   %arrayidx9.46 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %92 = load i16, i16* %arrayidx9.46, align 2, !tbaa !3 *)
mov v92 mem0_348;
(*   %conv1.i.46 = sext i16 %92 to i32 *)
cast v_conv1_i_46@sint32 v92@sint16;
(*   %mul.i.46 = mul nsw i32 %conv1.i.46, -758 *)
mul v_mul_i_46 v_conv1_i_46 (-758)@sint32;
(*   %call.i.46 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.46) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_46, v_call_i_46);
(*   %arrayidx11.46 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %93 = load i16, i16* %arrayidx11.46, align 2, !tbaa !3 *)
mov v93 mem0_92;
(*   %sub.46 = sub i16 %93, %call.i.46 *)
sub v_sub_46 v93 v_call_i_46;
(*   store i16 %sub.46, i16* %arrayidx9.46, align 2, !tbaa !3 *)
mov mem0_348 v_sub_46;
(*   %add21.46 = add i16 %93, %call.i.46 *)
add v_add21_46 v93 v_call_i_46;
(*   store i16 %add21.46, i16* %arrayidx11.46, align 2, !tbaa !3 *)
mov mem0_92 v_add21_46;
(*   %arrayidx9.47 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %94 = load i16, i16* %arrayidx9.47, align 2, !tbaa !3 *)
mov v94 mem0_350;
(*   %conv1.i.47 = sext i16 %94 to i32 *)
cast v_conv1_i_47@sint32 v94@sint16;
(*   %mul.i.47 = mul nsw i32 %conv1.i.47, -758 *)
mul v_mul_i_47 v_conv1_i_47 (-758)@sint32;
(*   %call.i.47 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.47) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_47, v_call_i_47);
(*   %arrayidx11.47 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %95 = load i16, i16* %arrayidx11.47, align 2, !tbaa !3 *)
mov v95 mem0_94;
(*   %sub.47 = sub i16 %95, %call.i.47 *)
sub v_sub_47 v95 v_call_i_47;
(*   store i16 %sub.47, i16* %arrayidx9.47, align 2, !tbaa !3 *)
mov mem0_350 v_sub_47;
(*   %add21.47 = add i16 %95, %call.i.47 *)
add v_add21_47 v95 v_call_i_47;
(*   store i16 %add21.47, i16* %arrayidx11.47, align 2, !tbaa !3 *)
mov mem0_94 v_add21_47;
(*   %arrayidx9.48 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %96 = load i16, i16* %arrayidx9.48, align 2, !tbaa !3 *)
mov v96 mem0_352;
(*   %conv1.i.48 = sext i16 %96 to i32 *)
cast v_conv1_i_48@sint32 v96@sint16;
(*   %mul.i.48 = mul nsw i32 %conv1.i.48, -758 *)
mul v_mul_i_48 v_conv1_i_48 (-758)@sint32;
(*   %call.i.48 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.48) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_48, v_call_i_48);
(*   %arrayidx11.48 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %97 = load i16, i16* %arrayidx11.48, align 2, !tbaa !3 *)
mov v97 mem0_96;
(*   %sub.48 = sub i16 %97, %call.i.48 *)
sub v_sub_48 v97 v_call_i_48;
(*   store i16 %sub.48, i16* %arrayidx9.48, align 2, !tbaa !3 *)
mov mem0_352 v_sub_48;
(*   %add21.48 = add i16 %97, %call.i.48 *)
add v_add21_48 v97 v_call_i_48;
(*   store i16 %add21.48, i16* %arrayidx11.48, align 2, !tbaa !3 *)
mov mem0_96 v_add21_48;
(*   %arrayidx9.49 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %98 = load i16, i16* %arrayidx9.49, align 2, !tbaa !3 *)
mov v98 mem0_354;
(*   %conv1.i.49 = sext i16 %98 to i32 *)
cast v_conv1_i_49@sint32 v98@sint16;
(*   %mul.i.49 = mul nsw i32 %conv1.i.49, -758 *)
mul v_mul_i_49 v_conv1_i_49 (-758)@sint32;
(*   %call.i.49 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.49) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_49, v_call_i_49);
(*   %arrayidx11.49 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %99 = load i16, i16* %arrayidx11.49, align 2, !tbaa !3 *)
mov v99 mem0_98;
(*   %sub.49 = sub i16 %99, %call.i.49 *)
sub v_sub_49 v99 v_call_i_49;
(*   store i16 %sub.49, i16* %arrayidx9.49, align 2, !tbaa !3 *)
mov mem0_354 v_sub_49;
(*   %add21.49 = add i16 %99, %call.i.49 *)
add v_add21_49 v99 v_call_i_49;
(*   store i16 %add21.49, i16* %arrayidx11.49, align 2, !tbaa !3 *)
mov mem0_98 v_add21_49;
(*   %arrayidx9.50 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %100 = load i16, i16* %arrayidx9.50, align 2, !tbaa !3 *)
mov v100 mem0_356;
(*   %conv1.i.50 = sext i16 %100 to i32 *)
cast v_conv1_i_50@sint32 v100@sint16;
(*   %mul.i.50 = mul nsw i32 %conv1.i.50, -758 *)
mul v_mul_i_50 v_conv1_i_50 (-758)@sint32;
(*   %call.i.50 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.50) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_50, v_call_i_50);
(*   %arrayidx11.50 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %101 = load i16, i16* %arrayidx11.50, align 2, !tbaa !3 *)
mov v101 mem0_100;
(*   %sub.50 = sub i16 %101, %call.i.50 *)
sub v_sub_50 v101 v_call_i_50;
(*   store i16 %sub.50, i16* %arrayidx9.50, align 2, !tbaa !3 *)
mov mem0_356 v_sub_50;
(*   %add21.50 = add i16 %101, %call.i.50 *)
add v_add21_50 v101 v_call_i_50;
(*   store i16 %add21.50, i16* %arrayidx11.50, align 2, !tbaa !3 *)
mov mem0_100 v_add21_50;
(*   %arrayidx9.51 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %102 = load i16, i16* %arrayidx9.51, align 2, !tbaa !3 *)
mov v102 mem0_358;
(*   %conv1.i.51 = sext i16 %102 to i32 *)
cast v_conv1_i_51@sint32 v102@sint16;
(*   %mul.i.51 = mul nsw i32 %conv1.i.51, -758 *)
mul v_mul_i_51 v_conv1_i_51 (-758)@sint32;
(*   %call.i.51 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.51) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_51, v_call_i_51);
(*   %arrayidx11.51 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %103 = load i16, i16* %arrayidx11.51, align 2, !tbaa !3 *)
mov v103 mem0_102;
(*   %sub.51 = sub i16 %103, %call.i.51 *)
sub v_sub_51 v103 v_call_i_51;
(*   store i16 %sub.51, i16* %arrayidx9.51, align 2, !tbaa !3 *)
mov mem0_358 v_sub_51;
(*   %add21.51 = add i16 %103, %call.i.51 *)
add v_add21_51 v103 v_call_i_51;
(*   store i16 %add21.51, i16* %arrayidx11.51, align 2, !tbaa !3 *)
mov mem0_102 v_add21_51;
(*   %arrayidx9.52 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %104 = load i16, i16* %arrayidx9.52, align 2, !tbaa !3 *)
mov v104 mem0_360;
(*   %conv1.i.52 = sext i16 %104 to i32 *)
cast v_conv1_i_52@sint32 v104@sint16;
(*   %mul.i.52 = mul nsw i32 %conv1.i.52, -758 *)
mul v_mul_i_52 v_conv1_i_52 (-758)@sint32;
(*   %call.i.52 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.52) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_52, v_call_i_52);
(*   %arrayidx11.52 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %105 = load i16, i16* %arrayidx11.52, align 2, !tbaa !3 *)
mov v105 mem0_104;
(*   %sub.52 = sub i16 %105, %call.i.52 *)
sub v_sub_52 v105 v_call_i_52;
(*   store i16 %sub.52, i16* %arrayidx9.52, align 2, !tbaa !3 *)
mov mem0_360 v_sub_52;
(*   %add21.52 = add i16 %105, %call.i.52 *)
add v_add21_52 v105 v_call_i_52;
(*   store i16 %add21.52, i16* %arrayidx11.52, align 2, !tbaa !3 *)
mov mem0_104 v_add21_52;
(*   %arrayidx9.53 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %106 = load i16, i16* %arrayidx9.53, align 2, !tbaa !3 *)
mov v106 mem0_362;
(*   %conv1.i.53 = sext i16 %106 to i32 *)
cast v_conv1_i_53@sint32 v106@sint16;
(*   %mul.i.53 = mul nsw i32 %conv1.i.53, -758 *)
mul v_mul_i_53 v_conv1_i_53 (-758)@sint32;
(*   %call.i.53 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.53) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_53, v_call_i_53);
(*   %arrayidx11.53 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %107 = load i16, i16* %arrayidx11.53, align 2, !tbaa !3 *)
mov v107 mem0_106;
(*   %sub.53 = sub i16 %107, %call.i.53 *)
sub v_sub_53 v107 v_call_i_53;
(*   store i16 %sub.53, i16* %arrayidx9.53, align 2, !tbaa !3 *)
mov mem0_362 v_sub_53;
(*   %add21.53 = add i16 %107, %call.i.53 *)
add v_add21_53 v107 v_call_i_53;
(*   store i16 %add21.53, i16* %arrayidx11.53, align 2, !tbaa !3 *)
mov mem0_106 v_add21_53;
(*   %arrayidx9.54 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %108 = load i16, i16* %arrayidx9.54, align 2, !tbaa !3 *)
mov v108 mem0_364;
(*   %conv1.i.54 = sext i16 %108 to i32 *)
cast v_conv1_i_54@sint32 v108@sint16;
(*   %mul.i.54 = mul nsw i32 %conv1.i.54, -758 *)
mul v_mul_i_54 v_conv1_i_54 (-758)@sint32;
(*   %call.i.54 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.54) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_54, v_call_i_54);
(*   %arrayidx11.54 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %109 = load i16, i16* %arrayidx11.54, align 2, !tbaa !3 *)
mov v109 mem0_108;
(*   %sub.54 = sub i16 %109, %call.i.54 *)
sub v_sub_54 v109 v_call_i_54;
(*   store i16 %sub.54, i16* %arrayidx9.54, align 2, !tbaa !3 *)
mov mem0_364 v_sub_54;
(*   %add21.54 = add i16 %109, %call.i.54 *)
add v_add21_54 v109 v_call_i_54;
(*   store i16 %add21.54, i16* %arrayidx11.54, align 2, !tbaa !3 *)
mov mem0_108 v_add21_54;
(*   %arrayidx9.55 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %110 = load i16, i16* %arrayidx9.55, align 2, !tbaa !3 *)
mov v110 mem0_366;
(*   %conv1.i.55 = sext i16 %110 to i32 *)
cast v_conv1_i_55@sint32 v110@sint16;
(*   %mul.i.55 = mul nsw i32 %conv1.i.55, -758 *)
mul v_mul_i_55 v_conv1_i_55 (-758)@sint32;
(*   %call.i.55 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.55) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_55, v_call_i_55);
(*   %arrayidx11.55 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %111 = load i16, i16* %arrayidx11.55, align 2, !tbaa !3 *)
mov v111 mem0_110;
(*   %sub.55 = sub i16 %111, %call.i.55 *)
sub v_sub_55 v111 v_call_i_55;
(*   store i16 %sub.55, i16* %arrayidx9.55, align 2, !tbaa !3 *)
mov mem0_366 v_sub_55;
(*   %add21.55 = add i16 %111, %call.i.55 *)
add v_add21_55 v111 v_call_i_55;
(*   store i16 %add21.55, i16* %arrayidx11.55, align 2, !tbaa !3 *)
mov mem0_110 v_add21_55;
(*   %arrayidx9.56 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %112 = load i16, i16* %arrayidx9.56, align 2, !tbaa !3 *)
mov v112 mem0_368;
(*   %conv1.i.56 = sext i16 %112 to i32 *)
cast v_conv1_i_56@sint32 v112@sint16;
(*   %mul.i.56 = mul nsw i32 %conv1.i.56, -758 *)
mul v_mul_i_56 v_conv1_i_56 (-758)@sint32;
(*   %call.i.56 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.56) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_56, v_call_i_56);
(*   %arrayidx11.56 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %113 = load i16, i16* %arrayidx11.56, align 2, !tbaa !3 *)
mov v113 mem0_112;
(*   %sub.56 = sub i16 %113, %call.i.56 *)
sub v_sub_56 v113 v_call_i_56;
(*   store i16 %sub.56, i16* %arrayidx9.56, align 2, !tbaa !3 *)
mov mem0_368 v_sub_56;
(*   %add21.56 = add i16 %113, %call.i.56 *)
add v_add21_56 v113 v_call_i_56;
(*   store i16 %add21.56, i16* %arrayidx11.56, align 2, !tbaa !3 *)
mov mem0_112 v_add21_56;
(*   %arrayidx9.57 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %114 = load i16, i16* %arrayidx9.57, align 2, !tbaa !3 *)
mov v114 mem0_370;
(*   %conv1.i.57 = sext i16 %114 to i32 *)
cast v_conv1_i_57@sint32 v114@sint16;
(*   %mul.i.57 = mul nsw i32 %conv1.i.57, -758 *)
mul v_mul_i_57 v_conv1_i_57 (-758)@sint32;
(*   %call.i.57 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.57) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_57, v_call_i_57);
(*   %arrayidx11.57 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %115 = load i16, i16* %arrayidx11.57, align 2, !tbaa !3 *)
mov v115 mem0_114;
(*   %sub.57 = sub i16 %115, %call.i.57 *)
sub v_sub_57 v115 v_call_i_57;
(*   store i16 %sub.57, i16* %arrayidx9.57, align 2, !tbaa !3 *)
mov mem0_370 v_sub_57;
(*   %add21.57 = add i16 %115, %call.i.57 *)
add v_add21_57 v115 v_call_i_57;
(*   store i16 %add21.57, i16* %arrayidx11.57, align 2, !tbaa !3 *)
mov mem0_114 v_add21_57;
(*   %arrayidx9.58 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %116 = load i16, i16* %arrayidx9.58, align 2, !tbaa !3 *)
mov v116 mem0_372;
(*   %conv1.i.58 = sext i16 %116 to i32 *)
cast v_conv1_i_58@sint32 v116@sint16;
(*   %mul.i.58 = mul nsw i32 %conv1.i.58, -758 *)
mul v_mul_i_58 v_conv1_i_58 (-758)@sint32;
(*   %call.i.58 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.58) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_58, v_call_i_58);
(*   %arrayidx11.58 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %117 = load i16, i16* %arrayidx11.58, align 2, !tbaa !3 *)
mov v117 mem0_116;
(*   %sub.58 = sub i16 %117, %call.i.58 *)
sub v_sub_58 v117 v_call_i_58;
(*   store i16 %sub.58, i16* %arrayidx9.58, align 2, !tbaa !3 *)
mov mem0_372 v_sub_58;
(*   %add21.58 = add i16 %117, %call.i.58 *)
add v_add21_58 v117 v_call_i_58;
(*   store i16 %add21.58, i16* %arrayidx11.58, align 2, !tbaa !3 *)
mov mem0_116 v_add21_58;
(*   %arrayidx9.59 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %118 = load i16, i16* %arrayidx9.59, align 2, !tbaa !3 *)
mov v118 mem0_374;
(*   %conv1.i.59 = sext i16 %118 to i32 *)
cast v_conv1_i_59@sint32 v118@sint16;
(*   %mul.i.59 = mul nsw i32 %conv1.i.59, -758 *)
mul v_mul_i_59 v_conv1_i_59 (-758)@sint32;
(*   %call.i.59 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.59) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_59, v_call_i_59);
(*   %arrayidx11.59 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %119 = load i16, i16* %arrayidx11.59, align 2, !tbaa !3 *)
mov v119 mem0_118;
(*   %sub.59 = sub i16 %119, %call.i.59 *)
sub v_sub_59 v119 v_call_i_59;
(*   store i16 %sub.59, i16* %arrayidx9.59, align 2, !tbaa !3 *)
mov mem0_374 v_sub_59;
(*   %add21.59 = add i16 %119, %call.i.59 *)
add v_add21_59 v119 v_call_i_59;
(*   store i16 %add21.59, i16* %arrayidx11.59, align 2, !tbaa !3 *)
mov mem0_118 v_add21_59;
(*   %arrayidx9.60 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %120 = load i16, i16* %arrayidx9.60, align 2, !tbaa !3 *)
mov v120 mem0_376;
(*   %conv1.i.60 = sext i16 %120 to i32 *)
cast v_conv1_i_60@sint32 v120@sint16;
(*   %mul.i.60 = mul nsw i32 %conv1.i.60, -758 *)
mul v_mul_i_60 v_conv1_i_60 (-758)@sint32;
(*   %call.i.60 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.60) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_60, v_call_i_60);
(*   %arrayidx11.60 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %121 = load i16, i16* %arrayidx11.60, align 2, !tbaa !3 *)
mov v121 mem0_120;
(*   %sub.60 = sub i16 %121, %call.i.60 *)
sub v_sub_60 v121 v_call_i_60;
(*   store i16 %sub.60, i16* %arrayidx9.60, align 2, !tbaa !3 *)
mov mem0_376 v_sub_60;
(*   %add21.60 = add i16 %121, %call.i.60 *)
add v_add21_60 v121 v_call_i_60;
(*   store i16 %add21.60, i16* %arrayidx11.60, align 2, !tbaa !3 *)
mov mem0_120 v_add21_60;
(*   %arrayidx9.61 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %122 = load i16, i16* %arrayidx9.61, align 2, !tbaa !3 *)
mov v122 mem0_378;
(*   %conv1.i.61 = sext i16 %122 to i32 *)
cast v_conv1_i_61@sint32 v122@sint16;
(*   %mul.i.61 = mul nsw i32 %conv1.i.61, -758 *)
mul v_mul_i_61 v_conv1_i_61 (-758)@sint32;
(*   %call.i.61 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.61) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_61, v_call_i_61);
(*   %arrayidx11.61 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %123 = load i16, i16* %arrayidx11.61, align 2, !tbaa !3 *)
mov v123 mem0_122;
(*   %sub.61 = sub i16 %123, %call.i.61 *)
sub v_sub_61 v123 v_call_i_61;
(*   store i16 %sub.61, i16* %arrayidx9.61, align 2, !tbaa !3 *)
mov mem0_378 v_sub_61;
(*   %add21.61 = add i16 %123, %call.i.61 *)
add v_add21_61 v123 v_call_i_61;
(*   store i16 %add21.61, i16* %arrayidx11.61, align 2, !tbaa !3 *)
mov mem0_122 v_add21_61;
(*   %arrayidx9.62 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %124 = load i16, i16* %arrayidx9.62, align 2, !tbaa !3 *)
mov v124 mem0_380;
(*   %conv1.i.62 = sext i16 %124 to i32 *)
cast v_conv1_i_62@sint32 v124@sint16;
(*   %mul.i.62 = mul nsw i32 %conv1.i.62, -758 *)
mul v_mul_i_62 v_conv1_i_62 (-758)@sint32;
(*   %call.i.62 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.62) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_62, v_call_i_62);
(*   %arrayidx11.62 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %125 = load i16, i16* %arrayidx11.62, align 2, !tbaa !3 *)
mov v125 mem0_124;
(*   %sub.62 = sub i16 %125, %call.i.62 *)
sub v_sub_62 v125 v_call_i_62;
(*   store i16 %sub.62, i16* %arrayidx9.62, align 2, !tbaa !3 *)
mov mem0_380 v_sub_62;
(*   %add21.62 = add i16 %125, %call.i.62 *)
add v_add21_62 v125 v_call_i_62;
(*   store i16 %add21.62, i16* %arrayidx11.62, align 2, !tbaa !3 *)
mov mem0_124 v_add21_62;
(*   %arrayidx9.63 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %126 = load i16, i16* %arrayidx9.63, align 2, !tbaa !3 *)
mov v126 mem0_382;
(*   %conv1.i.63 = sext i16 %126 to i32 *)
cast v_conv1_i_63@sint32 v126@sint16;
(*   %mul.i.63 = mul nsw i32 %conv1.i.63, -758 *)
mul v_mul_i_63 v_conv1_i_63 (-758)@sint32;
(*   %call.i.63 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.63) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_63, v_call_i_63);
(*   %arrayidx11.63 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %127 = load i16, i16* %arrayidx11.63, align 2, !tbaa !3 *)
mov v127 mem0_126;
(*   %sub.63 = sub i16 %127, %call.i.63 *)
sub v_sub_63 v127 v_call_i_63;
(*   store i16 %sub.63, i16* %arrayidx9.63, align 2, !tbaa !3 *)
mov mem0_382 v_sub_63;
(*   %add21.63 = add i16 %127, %call.i.63 *)
add v_add21_63 v127 v_call_i_63;
(*   store i16 %add21.63, i16* %arrayidx11.63, align 2, !tbaa !3 *)
mov mem0_126 v_add21_63;
(*   %arrayidx9.64 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %128 = load i16, i16* %arrayidx9.64, align 2, !tbaa !3 *)
mov v128 mem0_384;
(*   %conv1.i.64 = sext i16 %128 to i32 *)
cast v_conv1_i_64@sint32 v128@sint16;
(*   %mul.i.64 = mul nsw i32 %conv1.i.64, -758 *)
mul v_mul_i_64 v_conv1_i_64 (-758)@sint32;
(*   %call.i.64 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.64) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_64, v_call_i_64);
(*   %arrayidx11.64 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %129 = load i16, i16* %arrayidx11.64, align 2, !tbaa !3 *)
mov v129 mem0_128;
(*   %sub.64 = sub i16 %129, %call.i.64 *)
sub v_sub_64 v129 v_call_i_64;
(*   store i16 %sub.64, i16* %arrayidx9.64, align 2, !tbaa !3 *)
mov mem0_384 v_sub_64;
(*   %add21.64 = add i16 %129, %call.i.64 *)
add v_add21_64 v129 v_call_i_64;
(*   store i16 %add21.64, i16* %arrayidx11.64, align 2, !tbaa !3 *)
mov mem0_128 v_add21_64;
(*   %arrayidx9.65 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %130 = load i16, i16* %arrayidx9.65, align 2, !tbaa !3 *)
mov v130 mem0_386;
(*   %conv1.i.65 = sext i16 %130 to i32 *)
cast v_conv1_i_65@sint32 v130@sint16;
(*   %mul.i.65 = mul nsw i32 %conv1.i.65, -758 *)
mul v_mul_i_65 v_conv1_i_65 (-758)@sint32;
(*   %call.i.65 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.65) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_65, v_call_i_65);
(*   %arrayidx11.65 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %131 = load i16, i16* %arrayidx11.65, align 2, !tbaa !3 *)
mov v131 mem0_130;
(*   %sub.65 = sub i16 %131, %call.i.65 *)
sub v_sub_65 v131 v_call_i_65;
(*   store i16 %sub.65, i16* %arrayidx9.65, align 2, !tbaa !3 *)
mov mem0_386 v_sub_65;
(*   %add21.65 = add i16 %131, %call.i.65 *)
add v_add21_65 v131 v_call_i_65;
(*   store i16 %add21.65, i16* %arrayidx11.65, align 2, !tbaa !3 *)
mov mem0_130 v_add21_65;
(*   %arrayidx9.66 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %132 = load i16, i16* %arrayidx9.66, align 2, !tbaa !3 *)
mov v132 mem0_388;
(*   %conv1.i.66 = sext i16 %132 to i32 *)
cast v_conv1_i_66@sint32 v132@sint16;
(*   %mul.i.66 = mul nsw i32 %conv1.i.66, -758 *)
mul v_mul_i_66 v_conv1_i_66 (-758)@sint32;
(*   %call.i.66 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.66) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_66, v_call_i_66);
(*   %arrayidx11.66 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %133 = load i16, i16* %arrayidx11.66, align 2, !tbaa !3 *)
mov v133 mem0_132;
(*   %sub.66 = sub i16 %133, %call.i.66 *)
sub v_sub_66 v133 v_call_i_66;
(*   store i16 %sub.66, i16* %arrayidx9.66, align 2, !tbaa !3 *)
mov mem0_388 v_sub_66;
(*   %add21.66 = add i16 %133, %call.i.66 *)
add v_add21_66 v133 v_call_i_66;
(*   store i16 %add21.66, i16* %arrayidx11.66, align 2, !tbaa !3 *)
mov mem0_132 v_add21_66;
(*   %arrayidx9.67 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %134 = load i16, i16* %arrayidx9.67, align 2, !tbaa !3 *)
mov v134 mem0_390;
(*   %conv1.i.67 = sext i16 %134 to i32 *)
cast v_conv1_i_67@sint32 v134@sint16;
(*   %mul.i.67 = mul nsw i32 %conv1.i.67, -758 *)
mul v_mul_i_67 v_conv1_i_67 (-758)@sint32;
(*   %call.i.67 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.67) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_67, v_call_i_67);
(*   %arrayidx11.67 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %135 = load i16, i16* %arrayidx11.67, align 2, !tbaa !3 *)
mov v135 mem0_134;
(*   %sub.67 = sub i16 %135, %call.i.67 *)
sub v_sub_67 v135 v_call_i_67;
(*   store i16 %sub.67, i16* %arrayidx9.67, align 2, !tbaa !3 *)
mov mem0_390 v_sub_67;
(*   %add21.67 = add i16 %135, %call.i.67 *)
add v_add21_67 v135 v_call_i_67;
(*   store i16 %add21.67, i16* %arrayidx11.67, align 2, !tbaa !3 *)
mov mem0_134 v_add21_67;
(*   %arrayidx9.68 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %136 = load i16, i16* %arrayidx9.68, align 2, !tbaa !3 *)
mov v136 mem0_392;
(*   %conv1.i.68 = sext i16 %136 to i32 *)
cast v_conv1_i_68@sint32 v136@sint16;
(*   %mul.i.68 = mul nsw i32 %conv1.i.68, -758 *)
mul v_mul_i_68 v_conv1_i_68 (-758)@sint32;
(*   %call.i.68 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.68) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_68, v_call_i_68);
(*   %arrayidx11.68 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %137 = load i16, i16* %arrayidx11.68, align 2, !tbaa !3 *)
mov v137 mem0_136;
(*   %sub.68 = sub i16 %137, %call.i.68 *)
sub v_sub_68 v137 v_call_i_68;
(*   store i16 %sub.68, i16* %arrayidx9.68, align 2, !tbaa !3 *)
mov mem0_392 v_sub_68;
(*   %add21.68 = add i16 %137, %call.i.68 *)
add v_add21_68 v137 v_call_i_68;
(*   store i16 %add21.68, i16* %arrayidx11.68, align 2, !tbaa !3 *)
mov mem0_136 v_add21_68;
(*   %arrayidx9.69 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %138 = load i16, i16* %arrayidx9.69, align 2, !tbaa !3 *)
mov v138 mem0_394;
(*   %conv1.i.69 = sext i16 %138 to i32 *)
cast v_conv1_i_69@sint32 v138@sint16;
(*   %mul.i.69 = mul nsw i32 %conv1.i.69, -758 *)
mul v_mul_i_69 v_conv1_i_69 (-758)@sint32;
(*   %call.i.69 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.69) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_69, v_call_i_69);
(*   %arrayidx11.69 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %139 = load i16, i16* %arrayidx11.69, align 2, !tbaa !3 *)
mov v139 mem0_138;
(*   %sub.69 = sub i16 %139, %call.i.69 *)
sub v_sub_69 v139 v_call_i_69;
(*   store i16 %sub.69, i16* %arrayidx9.69, align 2, !tbaa !3 *)
mov mem0_394 v_sub_69;
(*   %add21.69 = add i16 %139, %call.i.69 *)
add v_add21_69 v139 v_call_i_69;
(*   store i16 %add21.69, i16* %arrayidx11.69, align 2, !tbaa !3 *)
mov mem0_138 v_add21_69;
(*   %arrayidx9.70 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %140 = load i16, i16* %arrayidx9.70, align 2, !tbaa !3 *)
mov v140 mem0_396;
(*   %conv1.i.70 = sext i16 %140 to i32 *)
cast v_conv1_i_70@sint32 v140@sint16;
(*   %mul.i.70 = mul nsw i32 %conv1.i.70, -758 *)
mul v_mul_i_70 v_conv1_i_70 (-758)@sint32;
(*   %call.i.70 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.70) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_70, v_call_i_70);
(*   %arrayidx11.70 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %141 = load i16, i16* %arrayidx11.70, align 2, !tbaa !3 *)
mov v141 mem0_140;
(*   %sub.70 = sub i16 %141, %call.i.70 *)
sub v_sub_70 v141 v_call_i_70;
(*   store i16 %sub.70, i16* %arrayidx9.70, align 2, !tbaa !3 *)
mov mem0_396 v_sub_70;
(*   %add21.70 = add i16 %141, %call.i.70 *)
add v_add21_70 v141 v_call_i_70;
(*   store i16 %add21.70, i16* %arrayidx11.70, align 2, !tbaa !3 *)
mov mem0_140 v_add21_70;
(*   %arrayidx9.71 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %142 = load i16, i16* %arrayidx9.71, align 2, !tbaa !3 *)
mov v142 mem0_398;
(*   %conv1.i.71 = sext i16 %142 to i32 *)
cast v_conv1_i_71@sint32 v142@sint16;
(*   %mul.i.71 = mul nsw i32 %conv1.i.71, -758 *)
mul v_mul_i_71 v_conv1_i_71 (-758)@sint32;
(*   %call.i.71 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.71) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_71, v_call_i_71);
(*   %arrayidx11.71 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %143 = load i16, i16* %arrayidx11.71, align 2, !tbaa !3 *)
mov v143 mem0_142;
(*   %sub.71 = sub i16 %143, %call.i.71 *)
sub v_sub_71 v143 v_call_i_71;
(*   store i16 %sub.71, i16* %arrayidx9.71, align 2, !tbaa !3 *)
mov mem0_398 v_sub_71;
(*   %add21.71 = add i16 %143, %call.i.71 *)
add v_add21_71 v143 v_call_i_71;
(*   store i16 %add21.71, i16* %arrayidx11.71, align 2, !tbaa !3 *)
mov mem0_142 v_add21_71;
(*   %arrayidx9.72 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %144 = load i16, i16* %arrayidx9.72, align 2, !tbaa !3 *)
mov v144 mem0_400;
(*   %conv1.i.72 = sext i16 %144 to i32 *)
cast v_conv1_i_72@sint32 v144@sint16;
(*   %mul.i.72 = mul nsw i32 %conv1.i.72, -758 *)
mul v_mul_i_72 v_conv1_i_72 (-758)@sint32;
(*   %call.i.72 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.72) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_72, v_call_i_72);
(*   %arrayidx11.72 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %145 = load i16, i16* %arrayidx11.72, align 2, !tbaa !3 *)
mov v145 mem0_144;
(*   %sub.72 = sub i16 %145, %call.i.72 *)
sub v_sub_72 v145 v_call_i_72;
(*   store i16 %sub.72, i16* %arrayidx9.72, align 2, !tbaa !3 *)
mov mem0_400 v_sub_72;
(*   %add21.72 = add i16 %145, %call.i.72 *)
add v_add21_72 v145 v_call_i_72;
(*   store i16 %add21.72, i16* %arrayidx11.72, align 2, !tbaa !3 *)
mov mem0_144 v_add21_72;
(*   %arrayidx9.73 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %146 = load i16, i16* %arrayidx9.73, align 2, !tbaa !3 *)
mov v146 mem0_402;
(*   %conv1.i.73 = sext i16 %146 to i32 *)
cast v_conv1_i_73@sint32 v146@sint16;
(*   %mul.i.73 = mul nsw i32 %conv1.i.73, -758 *)
mul v_mul_i_73 v_conv1_i_73 (-758)@sint32;
(*   %call.i.73 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.73) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_73, v_call_i_73);
(*   %arrayidx11.73 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %147 = load i16, i16* %arrayidx11.73, align 2, !tbaa !3 *)
mov v147 mem0_146;
(*   %sub.73 = sub i16 %147, %call.i.73 *)
sub v_sub_73 v147 v_call_i_73;
(*   store i16 %sub.73, i16* %arrayidx9.73, align 2, !tbaa !3 *)
mov mem0_402 v_sub_73;
(*   %add21.73 = add i16 %147, %call.i.73 *)
add v_add21_73 v147 v_call_i_73;
(*   store i16 %add21.73, i16* %arrayidx11.73, align 2, !tbaa !3 *)
mov mem0_146 v_add21_73;
(*   %arrayidx9.74 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %148 = load i16, i16* %arrayidx9.74, align 2, !tbaa !3 *)
mov v148 mem0_404;
(*   %conv1.i.74 = sext i16 %148 to i32 *)
cast v_conv1_i_74@sint32 v148@sint16;
(*   %mul.i.74 = mul nsw i32 %conv1.i.74, -758 *)
mul v_mul_i_74 v_conv1_i_74 (-758)@sint32;
(*   %call.i.74 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.74) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_74, v_call_i_74);
(*   %arrayidx11.74 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %149 = load i16, i16* %arrayidx11.74, align 2, !tbaa !3 *)
mov v149 mem0_148;
(*   %sub.74 = sub i16 %149, %call.i.74 *)
sub v_sub_74 v149 v_call_i_74;
(*   store i16 %sub.74, i16* %arrayidx9.74, align 2, !tbaa !3 *)
mov mem0_404 v_sub_74;
(*   %add21.74 = add i16 %149, %call.i.74 *)
add v_add21_74 v149 v_call_i_74;
(*   store i16 %add21.74, i16* %arrayidx11.74, align 2, !tbaa !3 *)
mov mem0_148 v_add21_74;
(*   %arrayidx9.75 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %150 = load i16, i16* %arrayidx9.75, align 2, !tbaa !3 *)
mov v150 mem0_406;
(*   %conv1.i.75 = sext i16 %150 to i32 *)
cast v_conv1_i_75@sint32 v150@sint16;
(*   %mul.i.75 = mul nsw i32 %conv1.i.75, -758 *)
mul v_mul_i_75 v_conv1_i_75 (-758)@sint32;
(*   %call.i.75 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.75) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_75, v_call_i_75);
(*   %arrayidx11.75 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %151 = load i16, i16* %arrayidx11.75, align 2, !tbaa !3 *)
mov v151 mem0_150;
(*   %sub.75 = sub i16 %151, %call.i.75 *)
sub v_sub_75 v151 v_call_i_75;
(*   store i16 %sub.75, i16* %arrayidx9.75, align 2, !tbaa !3 *)
mov mem0_406 v_sub_75;
(*   %add21.75 = add i16 %151, %call.i.75 *)
add v_add21_75 v151 v_call_i_75;
(*   store i16 %add21.75, i16* %arrayidx11.75, align 2, !tbaa !3 *)
mov mem0_150 v_add21_75;
(*   %arrayidx9.76 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %152 = load i16, i16* %arrayidx9.76, align 2, !tbaa !3 *)
mov v152 mem0_408;
(*   %conv1.i.76 = sext i16 %152 to i32 *)
cast v_conv1_i_76@sint32 v152@sint16;
(*   %mul.i.76 = mul nsw i32 %conv1.i.76, -758 *)
mul v_mul_i_76 v_conv1_i_76 (-758)@sint32;
(*   %call.i.76 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.76) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_76, v_call_i_76);
(*   %arrayidx11.76 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %153 = load i16, i16* %arrayidx11.76, align 2, !tbaa !3 *)
mov v153 mem0_152;
(*   %sub.76 = sub i16 %153, %call.i.76 *)
sub v_sub_76 v153 v_call_i_76;
(*   store i16 %sub.76, i16* %arrayidx9.76, align 2, !tbaa !3 *)
mov mem0_408 v_sub_76;
(*   %add21.76 = add i16 %153, %call.i.76 *)
add v_add21_76 v153 v_call_i_76;
(*   store i16 %add21.76, i16* %arrayidx11.76, align 2, !tbaa !3 *)
mov mem0_152 v_add21_76;
(*   %arrayidx9.77 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %154 = load i16, i16* %arrayidx9.77, align 2, !tbaa !3 *)
mov v154 mem0_410;
(*   %conv1.i.77 = sext i16 %154 to i32 *)
cast v_conv1_i_77@sint32 v154@sint16;
(*   %mul.i.77 = mul nsw i32 %conv1.i.77, -758 *)
mul v_mul_i_77 v_conv1_i_77 (-758)@sint32;
(*   %call.i.77 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.77) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_77, v_call_i_77);
(*   %arrayidx11.77 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %155 = load i16, i16* %arrayidx11.77, align 2, !tbaa !3 *)
mov v155 mem0_154;
(*   %sub.77 = sub i16 %155, %call.i.77 *)
sub v_sub_77 v155 v_call_i_77;
(*   store i16 %sub.77, i16* %arrayidx9.77, align 2, !tbaa !3 *)
mov mem0_410 v_sub_77;
(*   %add21.77 = add i16 %155, %call.i.77 *)
add v_add21_77 v155 v_call_i_77;
(*   store i16 %add21.77, i16* %arrayidx11.77, align 2, !tbaa !3 *)
mov mem0_154 v_add21_77;
(*   %arrayidx9.78 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %156 = load i16, i16* %arrayidx9.78, align 2, !tbaa !3 *)
mov v156 mem0_412;
(*   %conv1.i.78 = sext i16 %156 to i32 *)
cast v_conv1_i_78@sint32 v156@sint16;
(*   %mul.i.78 = mul nsw i32 %conv1.i.78, -758 *)
mul v_mul_i_78 v_conv1_i_78 (-758)@sint32;
(*   %call.i.78 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.78) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_78, v_call_i_78);
(*   %arrayidx11.78 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %157 = load i16, i16* %arrayidx11.78, align 2, !tbaa !3 *)
mov v157 mem0_156;
(*   %sub.78 = sub i16 %157, %call.i.78 *)
sub v_sub_78 v157 v_call_i_78;
(*   store i16 %sub.78, i16* %arrayidx9.78, align 2, !tbaa !3 *)
mov mem0_412 v_sub_78;
(*   %add21.78 = add i16 %157, %call.i.78 *)
add v_add21_78 v157 v_call_i_78;
(*   store i16 %add21.78, i16* %arrayidx11.78, align 2, !tbaa !3 *)
mov mem0_156 v_add21_78;
(*   %arrayidx9.79 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %158 = load i16, i16* %arrayidx9.79, align 2, !tbaa !3 *)
mov v158 mem0_414;
(*   %conv1.i.79 = sext i16 %158 to i32 *)
cast v_conv1_i_79@sint32 v158@sint16;
(*   %mul.i.79 = mul nsw i32 %conv1.i.79, -758 *)
mul v_mul_i_79 v_conv1_i_79 (-758)@sint32;
(*   %call.i.79 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.79) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_79, v_call_i_79);
(*   %arrayidx11.79 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %159 = load i16, i16* %arrayidx11.79, align 2, !tbaa !3 *)
mov v159 mem0_158;
(*   %sub.79 = sub i16 %159, %call.i.79 *)
sub v_sub_79 v159 v_call_i_79;
(*   store i16 %sub.79, i16* %arrayidx9.79, align 2, !tbaa !3 *)
mov mem0_414 v_sub_79;
(*   %add21.79 = add i16 %159, %call.i.79 *)
add v_add21_79 v159 v_call_i_79;
(*   store i16 %add21.79, i16* %arrayidx11.79, align 2, !tbaa !3 *)
mov mem0_158 v_add21_79;
(*   %arrayidx9.80 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %160 = load i16, i16* %arrayidx9.80, align 2, !tbaa !3 *)
mov v160 mem0_416;
(*   %conv1.i.80 = sext i16 %160 to i32 *)
cast v_conv1_i_80@sint32 v160@sint16;
(*   %mul.i.80 = mul nsw i32 %conv1.i.80, -758 *)
mul v_mul_i_80 v_conv1_i_80 (-758)@sint32;
(*   %call.i.80 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.80) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_80, v_call_i_80);
(*   %arrayidx11.80 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %161 = load i16, i16* %arrayidx11.80, align 2, !tbaa !3 *)
mov v161 mem0_160;
(*   %sub.80 = sub i16 %161, %call.i.80 *)
sub v_sub_80 v161 v_call_i_80;
(*   store i16 %sub.80, i16* %arrayidx9.80, align 2, !tbaa !3 *)
mov mem0_416 v_sub_80;
(*   %add21.80 = add i16 %161, %call.i.80 *)
add v_add21_80 v161 v_call_i_80;
(*   store i16 %add21.80, i16* %arrayidx11.80, align 2, !tbaa !3 *)
mov mem0_160 v_add21_80;
(*   %arrayidx9.81 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %162 = load i16, i16* %arrayidx9.81, align 2, !tbaa !3 *)
mov v162 mem0_418;
(*   %conv1.i.81 = sext i16 %162 to i32 *)
cast v_conv1_i_81@sint32 v162@sint16;
(*   %mul.i.81 = mul nsw i32 %conv1.i.81, -758 *)
mul v_mul_i_81 v_conv1_i_81 (-758)@sint32;
(*   %call.i.81 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.81) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_81, v_call_i_81);
(*   %arrayidx11.81 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %163 = load i16, i16* %arrayidx11.81, align 2, !tbaa !3 *)
mov v163 mem0_162;
(*   %sub.81 = sub i16 %163, %call.i.81 *)
sub v_sub_81 v163 v_call_i_81;
(*   store i16 %sub.81, i16* %arrayidx9.81, align 2, !tbaa !3 *)
mov mem0_418 v_sub_81;
(*   %add21.81 = add i16 %163, %call.i.81 *)
add v_add21_81 v163 v_call_i_81;
(*   store i16 %add21.81, i16* %arrayidx11.81, align 2, !tbaa !3 *)
mov mem0_162 v_add21_81;
(*   %arrayidx9.82 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %164 = load i16, i16* %arrayidx9.82, align 2, !tbaa !3 *)
mov v164 mem0_420;
(*   %conv1.i.82 = sext i16 %164 to i32 *)
cast v_conv1_i_82@sint32 v164@sint16;
(*   %mul.i.82 = mul nsw i32 %conv1.i.82, -758 *)
mul v_mul_i_82 v_conv1_i_82 (-758)@sint32;
(*   %call.i.82 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.82) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_82, v_call_i_82);
(*   %arrayidx11.82 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %165 = load i16, i16* %arrayidx11.82, align 2, !tbaa !3 *)
mov v165 mem0_164;
(*   %sub.82 = sub i16 %165, %call.i.82 *)
sub v_sub_82 v165 v_call_i_82;
(*   store i16 %sub.82, i16* %arrayidx9.82, align 2, !tbaa !3 *)
mov mem0_420 v_sub_82;
(*   %add21.82 = add i16 %165, %call.i.82 *)
add v_add21_82 v165 v_call_i_82;
(*   store i16 %add21.82, i16* %arrayidx11.82, align 2, !tbaa !3 *)
mov mem0_164 v_add21_82;
(*   %arrayidx9.83 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %166 = load i16, i16* %arrayidx9.83, align 2, !tbaa !3 *)
mov v166 mem0_422;
(*   %conv1.i.83 = sext i16 %166 to i32 *)
cast v_conv1_i_83@sint32 v166@sint16;
(*   %mul.i.83 = mul nsw i32 %conv1.i.83, -758 *)
mul v_mul_i_83 v_conv1_i_83 (-758)@sint32;
(*   %call.i.83 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.83) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_83, v_call_i_83);
(*   %arrayidx11.83 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %167 = load i16, i16* %arrayidx11.83, align 2, !tbaa !3 *)
mov v167 mem0_166;
(*   %sub.83 = sub i16 %167, %call.i.83 *)
sub v_sub_83 v167 v_call_i_83;
(*   store i16 %sub.83, i16* %arrayidx9.83, align 2, !tbaa !3 *)
mov mem0_422 v_sub_83;
(*   %add21.83 = add i16 %167, %call.i.83 *)
add v_add21_83 v167 v_call_i_83;
(*   store i16 %add21.83, i16* %arrayidx11.83, align 2, !tbaa !3 *)
mov mem0_166 v_add21_83;
(*   %arrayidx9.84 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %168 = load i16, i16* %arrayidx9.84, align 2, !tbaa !3 *)
mov v168 mem0_424;
(*   %conv1.i.84 = sext i16 %168 to i32 *)
cast v_conv1_i_84@sint32 v168@sint16;
(*   %mul.i.84 = mul nsw i32 %conv1.i.84, -758 *)
mul v_mul_i_84 v_conv1_i_84 (-758)@sint32;
(*   %call.i.84 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.84) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_84, v_call_i_84);
(*   %arrayidx11.84 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %169 = load i16, i16* %arrayidx11.84, align 2, !tbaa !3 *)
mov v169 mem0_168;
(*   %sub.84 = sub i16 %169, %call.i.84 *)
sub v_sub_84 v169 v_call_i_84;
(*   store i16 %sub.84, i16* %arrayidx9.84, align 2, !tbaa !3 *)
mov mem0_424 v_sub_84;
(*   %add21.84 = add i16 %169, %call.i.84 *)
add v_add21_84 v169 v_call_i_84;
(*   store i16 %add21.84, i16* %arrayidx11.84, align 2, !tbaa !3 *)
mov mem0_168 v_add21_84;
(*   %arrayidx9.85 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %170 = load i16, i16* %arrayidx9.85, align 2, !tbaa !3 *)
mov v170 mem0_426;
(*   %conv1.i.85 = sext i16 %170 to i32 *)
cast v_conv1_i_85@sint32 v170@sint16;
(*   %mul.i.85 = mul nsw i32 %conv1.i.85, -758 *)
mul v_mul_i_85 v_conv1_i_85 (-758)@sint32;
(*   %call.i.85 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.85) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_85, v_call_i_85);
(*   %arrayidx11.85 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %171 = load i16, i16* %arrayidx11.85, align 2, !tbaa !3 *)
mov v171 mem0_170;
(*   %sub.85 = sub i16 %171, %call.i.85 *)
sub v_sub_85 v171 v_call_i_85;
(*   store i16 %sub.85, i16* %arrayidx9.85, align 2, !tbaa !3 *)
mov mem0_426 v_sub_85;
(*   %add21.85 = add i16 %171, %call.i.85 *)
add v_add21_85 v171 v_call_i_85;
(*   store i16 %add21.85, i16* %arrayidx11.85, align 2, !tbaa !3 *)
mov mem0_170 v_add21_85;
(*   %arrayidx9.86 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %172 = load i16, i16* %arrayidx9.86, align 2, !tbaa !3 *)
mov v172 mem0_428;
(*   %conv1.i.86 = sext i16 %172 to i32 *)
cast v_conv1_i_86@sint32 v172@sint16;
(*   %mul.i.86 = mul nsw i32 %conv1.i.86, -758 *)
mul v_mul_i_86 v_conv1_i_86 (-758)@sint32;
(*   %call.i.86 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.86) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_86, v_call_i_86);
(*   %arrayidx11.86 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %173 = load i16, i16* %arrayidx11.86, align 2, !tbaa !3 *)
mov v173 mem0_172;
(*   %sub.86 = sub i16 %173, %call.i.86 *)
sub v_sub_86 v173 v_call_i_86;
(*   store i16 %sub.86, i16* %arrayidx9.86, align 2, !tbaa !3 *)
mov mem0_428 v_sub_86;
(*   %add21.86 = add i16 %173, %call.i.86 *)
add v_add21_86 v173 v_call_i_86;
(*   store i16 %add21.86, i16* %arrayidx11.86, align 2, !tbaa !3 *)
mov mem0_172 v_add21_86;
(*   %arrayidx9.87 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %174 = load i16, i16* %arrayidx9.87, align 2, !tbaa !3 *)
mov v174 mem0_430;
(*   %conv1.i.87 = sext i16 %174 to i32 *)
cast v_conv1_i_87@sint32 v174@sint16;
(*   %mul.i.87 = mul nsw i32 %conv1.i.87, -758 *)
mul v_mul_i_87 v_conv1_i_87 (-758)@sint32;
(*   %call.i.87 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.87) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_87, v_call_i_87);
(*   %arrayidx11.87 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %175 = load i16, i16* %arrayidx11.87, align 2, !tbaa !3 *)
mov v175 mem0_174;
(*   %sub.87 = sub i16 %175, %call.i.87 *)
sub v_sub_87 v175 v_call_i_87;
(*   store i16 %sub.87, i16* %arrayidx9.87, align 2, !tbaa !3 *)
mov mem0_430 v_sub_87;
(*   %add21.87 = add i16 %175, %call.i.87 *)
add v_add21_87 v175 v_call_i_87;
(*   store i16 %add21.87, i16* %arrayidx11.87, align 2, !tbaa !3 *)
mov mem0_174 v_add21_87;
(*   %arrayidx9.88 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %176 = load i16, i16* %arrayidx9.88, align 2, !tbaa !3 *)
mov v176 mem0_432;
(*   %conv1.i.88 = sext i16 %176 to i32 *)
cast v_conv1_i_88@sint32 v176@sint16;
(*   %mul.i.88 = mul nsw i32 %conv1.i.88, -758 *)
mul v_mul_i_88 v_conv1_i_88 (-758)@sint32;
(*   %call.i.88 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.88) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_88, v_call_i_88);
(*   %arrayidx11.88 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %177 = load i16, i16* %arrayidx11.88, align 2, !tbaa !3 *)
mov v177 mem0_176;
(*   %sub.88 = sub i16 %177, %call.i.88 *)
sub v_sub_88 v177 v_call_i_88;
(*   store i16 %sub.88, i16* %arrayidx9.88, align 2, !tbaa !3 *)
mov mem0_432 v_sub_88;
(*   %add21.88 = add i16 %177, %call.i.88 *)
add v_add21_88 v177 v_call_i_88;
(*   store i16 %add21.88, i16* %arrayidx11.88, align 2, !tbaa !3 *)
mov mem0_176 v_add21_88;
(*   %arrayidx9.89 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %178 = load i16, i16* %arrayidx9.89, align 2, !tbaa !3 *)
mov v178 mem0_434;
(*   %conv1.i.89 = sext i16 %178 to i32 *)
cast v_conv1_i_89@sint32 v178@sint16;
(*   %mul.i.89 = mul nsw i32 %conv1.i.89, -758 *)
mul v_mul_i_89 v_conv1_i_89 (-758)@sint32;
(*   %call.i.89 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.89) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_89, v_call_i_89);
(*   %arrayidx11.89 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %179 = load i16, i16* %arrayidx11.89, align 2, !tbaa !3 *)
mov v179 mem0_178;
(*   %sub.89 = sub i16 %179, %call.i.89 *)
sub v_sub_89 v179 v_call_i_89;
(*   store i16 %sub.89, i16* %arrayidx9.89, align 2, !tbaa !3 *)
mov mem0_434 v_sub_89;
(*   %add21.89 = add i16 %179, %call.i.89 *)
add v_add21_89 v179 v_call_i_89;
(*   store i16 %add21.89, i16* %arrayidx11.89, align 2, !tbaa !3 *)
mov mem0_178 v_add21_89;
(*   %arrayidx9.90 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %180 = load i16, i16* %arrayidx9.90, align 2, !tbaa !3 *)
mov v180 mem0_436;
(*   %conv1.i.90 = sext i16 %180 to i32 *)
cast v_conv1_i_90@sint32 v180@sint16;
(*   %mul.i.90 = mul nsw i32 %conv1.i.90, -758 *)
mul v_mul_i_90 v_conv1_i_90 (-758)@sint32;
(*   %call.i.90 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.90) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_90, v_call_i_90);
(*   %arrayidx11.90 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %181 = load i16, i16* %arrayidx11.90, align 2, !tbaa !3 *)
mov v181 mem0_180;
(*   %sub.90 = sub i16 %181, %call.i.90 *)
sub v_sub_90 v181 v_call_i_90;
(*   store i16 %sub.90, i16* %arrayidx9.90, align 2, !tbaa !3 *)
mov mem0_436 v_sub_90;
(*   %add21.90 = add i16 %181, %call.i.90 *)
add v_add21_90 v181 v_call_i_90;
(*   store i16 %add21.90, i16* %arrayidx11.90, align 2, !tbaa !3 *)
mov mem0_180 v_add21_90;
(*   %arrayidx9.91 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %182 = load i16, i16* %arrayidx9.91, align 2, !tbaa !3 *)
mov v182 mem0_438;
(*   %conv1.i.91 = sext i16 %182 to i32 *)
cast v_conv1_i_91@sint32 v182@sint16;
(*   %mul.i.91 = mul nsw i32 %conv1.i.91, -758 *)
mul v_mul_i_91 v_conv1_i_91 (-758)@sint32;
(*   %call.i.91 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.91) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_91, v_call_i_91);
(*   %arrayidx11.91 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %183 = load i16, i16* %arrayidx11.91, align 2, !tbaa !3 *)
mov v183 mem0_182;
(*   %sub.91 = sub i16 %183, %call.i.91 *)
sub v_sub_91 v183 v_call_i_91;
(*   store i16 %sub.91, i16* %arrayidx9.91, align 2, !tbaa !3 *)
mov mem0_438 v_sub_91;
(*   %add21.91 = add i16 %183, %call.i.91 *)
add v_add21_91 v183 v_call_i_91;
(*   store i16 %add21.91, i16* %arrayidx11.91, align 2, !tbaa !3 *)
mov mem0_182 v_add21_91;
(*   %arrayidx9.92 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %184 = load i16, i16* %arrayidx9.92, align 2, !tbaa !3 *)
mov v184 mem0_440;
(*   %conv1.i.92 = sext i16 %184 to i32 *)
cast v_conv1_i_92@sint32 v184@sint16;
(*   %mul.i.92 = mul nsw i32 %conv1.i.92, -758 *)
mul v_mul_i_92 v_conv1_i_92 (-758)@sint32;
(*   %call.i.92 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.92) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_92, v_call_i_92);
(*   %arrayidx11.92 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %185 = load i16, i16* %arrayidx11.92, align 2, !tbaa !3 *)
mov v185 mem0_184;
(*   %sub.92 = sub i16 %185, %call.i.92 *)
sub v_sub_92 v185 v_call_i_92;
(*   store i16 %sub.92, i16* %arrayidx9.92, align 2, !tbaa !3 *)
mov mem0_440 v_sub_92;
(*   %add21.92 = add i16 %185, %call.i.92 *)
add v_add21_92 v185 v_call_i_92;
(*   store i16 %add21.92, i16* %arrayidx11.92, align 2, !tbaa !3 *)
mov mem0_184 v_add21_92;
(*   %arrayidx9.93 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %186 = load i16, i16* %arrayidx9.93, align 2, !tbaa !3 *)
mov v186 mem0_442;
(*   %conv1.i.93 = sext i16 %186 to i32 *)
cast v_conv1_i_93@sint32 v186@sint16;
(*   %mul.i.93 = mul nsw i32 %conv1.i.93, -758 *)
mul v_mul_i_93 v_conv1_i_93 (-758)@sint32;
(*   %call.i.93 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.93) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_93, v_call_i_93);
(*   %arrayidx11.93 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %187 = load i16, i16* %arrayidx11.93, align 2, !tbaa !3 *)
mov v187 mem0_186;
(*   %sub.93 = sub i16 %187, %call.i.93 *)
sub v_sub_93 v187 v_call_i_93;
(*   store i16 %sub.93, i16* %arrayidx9.93, align 2, !tbaa !3 *)
mov mem0_442 v_sub_93;
(*   %add21.93 = add i16 %187, %call.i.93 *)
add v_add21_93 v187 v_call_i_93;
(*   store i16 %add21.93, i16* %arrayidx11.93, align 2, !tbaa !3 *)
mov mem0_186 v_add21_93;
(*   %arrayidx9.94 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %188 = load i16, i16* %arrayidx9.94, align 2, !tbaa !3 *)
mov v188 mem0_444;
(*   %conv1.i.94 = sext i16 %188 to i32 *)
cast v_conv1_i_94@sint32 v188@sint16;
(*   %mul.i.94 = mul nsw i32 %conv1.i.94, -758 *)
mul v_mul_i_94 v_conv1_i_94 (-758)@sint32;
(*   %call.i.94 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.94) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_94, v_call_i_94);
(*   %arrayidx11.94 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %189 = load i16, i16* %arrayidx11.94, align 2, !tbaa !3 *)
mov v189 mem0_188;
(*   %sub.94 = sub i16 %189, %call.i.94 *)
sub v_sub_94 v189 v_call_i_94;
(*   store i16 %sub.94, i16* %arrayidx9.94, align 2, !tbaa !3 *)
mov mem0_444 v_sub_94;
(*   %add21.94 = add i16 %189, %call.i.94 *)
add v_add21_94 v189 v_call_i_94;
(*   store i16 %add21.94, i16* %arrayidx11.94, align 2, !tbaa !3 *)
mov mem0_188 v_add21_94;
(*   %arrayidx9.95 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %190 = load i16, i16* %arrayidx9.95, align 2, !tbaa !3 *)
mov v190 mem0_446;
(*   %conv1.i.95 = sext i16 %190 to i32 *)
cast v_conv1_i_95@sint32 v190@sint16;
(*   %mul.i.95 = mul nsw i32 %conv1.i.95, -758 *)
mul v_mul_i_95 v_conv1_i_95 (-758)@sint32;
(*   %call.i.95 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.95) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_95, v_call_i_95);
(*   %arrayidx11.95 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %191 = load i16, i16* %arrayidx11.95, align 2, !tbaa !3 *)
mov v191 mem0_190;
(*   %sub.95 = sub i16 %191, %call.i.95 *)
sub v_sub_95 v191 v_call_i_95;
(*   store i16 %sub.95, i16* %arrayidx9.95, align 2, !tbaa !3 *)
mov mem0_446 v_sub_95;
(*   %add21.95 = add i16 %191, %call.i.95 *)
add v_add21_95 v191 v_call_i_95;
(*   store i16 %add21.95, i16* %arrayidx11.95, align 2, !tbaa !3 *)
mov mem0_190 v_add21_95;
(*   %arrayidx9.96 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %192 = load i16, i16* %arrayidx9.96, align 2, !tbaa !3 *)
mov v192 mem0_448;
(*   %conv1.i.96 = sext i16 %192 to i32 *)
cast v_conv1_i_96@sint32 v192@sint16;
(*   %mul.i.96 = mul nsw i32 %conv1.i.96, -758 *)
mul v_mul_i_96 v_conv1_i_96 (-758)@sint32;
(*   %call.i.96 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.96) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_96, v_call_i_96);
(*   %arrayidx11.96 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %193 = load i16, i16* %arrayidx11.96, align 2, !tbaa !3 *)
mov v193 mem0_192;
(*   %sub.96 = sub i16 %193, %call.i.96 *)
sub v_sub_96 v193 v_call_i_96;
(*   store i16 %sub.96, i16* %arrayidx9.96, align 2, !tbaa !3 *)
mov mem0_448 v_sub_96;
(*   %add21.96 = add i16 %193, %call.i.96 *)
add v_add21_96 v193 v_call_i_96;
(*   store i16 %add21.96, i16* %arrayidx11.96, align 2, !tbaa !3 *)
mov mem0_192 v_add21_96;
(*   %arrayidx9.97 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %194 = load i16, i16* %arrayidx9.97, align 2, !tbaa !3 *)
mov v194 mem0_450;
(*   %conv1.i.97 = sext i16 %194 to i32 *)
cast v_conv1_i_97@sint32 v194@sint16;
(*   %mul.i.97 = mul nsw i32 %conv1.i.97, -758 *)
mul v_mul_i_97 v_conv1_i_97 (-758)@sint32;
(*   %call.i.97 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.97) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_97, v_call_i_97);
(*   %arrayidx11.97 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %195 = load i16, i16* %arrayidx11.97, align 2, !tbaa !3 *)
mov v195 mem0_194;
(*   %sub.97 = sub i16 %195, %call.i.97 *)
sub v_sub_97 v195 v_call_i_97;
(*   store i16 %sub.97, i16* %arrayidx9.97, align 2, !tbaa !3 *)
mov mem0_450 v_sub_97;
(*   %add21.97 = add i16 %195, %call.i.97 *)
add v_add21_97 v195 v_call_i_97;
(*   store i16 %add21.97, i16* %arrayidx11.97, align 2, !tbaa !3 *)
mov mem0_194 v_add21_97;
(*   %arrayidx9.98 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %196 = load i16, i16* %arrayidx9.98, align 2, !tbaa !3 *)
mov v196 mem0_452;
(*   %conv1.i.98 = sext i16 %196 to i32 *)
cast v_conv1_i_98@sint32 v196@sint16;
(*   %mul.i.98 = mul nsw i32 %conv1.i.98, -758 *)
mul v_mul_i_98 v_conv1_i_98 (-758)@sint32;
(*   %call.i.98 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.98) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_98, v_call_i_98);
(*   %arrayidx11.98 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %197 = load i16, i16* %arrayidx11.98, align 2, !tbaa !3 *)
mov v197 mem0_196;
(*   %sub.98 = sub i16 %197, %call.i.98 *)
sub v_sub_98 v197 v_call_i_98;
(*   store i16 %sub.98, i16* %arrayidx9.98, align 2, !tbaa !3 *)
mov mem0_452 v_sub_98;
(*   %add21.98 = add i16 %197, %call.i.98 *)
add v_add21_98 v197 v_call_i_98;
(*   store i16 %add21.98, i16* %arrayidx11.98, align 2, !tbaa !3 *)
mov mem0_196 v_add21_98;
(*   %arrayidx9.99 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %198 = load i16, i16* %arrayidx9.99, align 2, !tbaa !3 *)
mov v198 mem0_454;
(*   %conv1.i.99 = sext i16 %198 to i32 *)
cast v_conv1_i_99@sint32 v198@sint16;
(*   %mul.i.99 = mul nsw i32 %conv1.i.99, -758 *)
mul v_mul_i_99 v_conv1_i_99 (-758)@sint32;
(*   %call.i.99 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.99) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_99, v_call_i_99);
(*   %arrayidx11.99 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %199 = load i16, i16* %arrayidx11.99, align 2, !tbaa !3 *)
mov v199 mem0_198;
(*   %sub.99 = sub i16 %199, %call.i.99 *)
sub v_sub_99 v199 v_call_i_99;
(*   store i16 %sub.99, i16* %arrayidx9.99, align 2, !tbaa !3 *)
mov mem0_454 v_sub_99;
(*   %add21.99 = add i16 %199, %call.i.99 *)
add v_add21_99 v199 v_call_i_99;
(*   store i16 %add21.99, i16* %arrayidx11.99, align 2, !tbaa !3 *)
mov mem0_198 v_add21_99;
(*   %arrayidx9.100 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %200 = load i16, i16* %arrayidx9.100, align 2, !tbaa !3 *)
mov v200 mem0_456;
(*   %conv1.i.100 = sext i16 %200 to i32 *)
cast v_conv1_i_100@sint32 v200@sint16;
(*   %mul.i.100 = mul nsw i32 %conv1.i.100, -758 *)
mul v_mul_i_100 v_conv1_i_100 (-758)@sint32;
(*   %call.i.100 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.100) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_100, v_call_i_100);
(*   %arrayidx11.100 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %201 = load i16, i16* %arrayidx11.100, align 2, !tbaa !3 *)
mov v201 mem0_200;
(*   %sub.100 = sub i16 %201, %call.i.100 *)
sub v_sub_100 v201 v_call_i_100;
(*   store i16 %sub.100, i16* %arrayidx9.100, align 2, !tbaa !3 *)
mov mem0_456 v_sub_100;
(*   %add21.100 = add i16 %201, %call.i.100 *)
add v_add21_100 v201 v_call_i_100;
(*   store i16 %add21.100, i16* %arrayidx11.100, align 2, !tbaa !3 *)
mov mem0_200 v_add21_100;
(*   %arrayidx9.101 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %202 = load i16, i16* %arrayidx9.101, align 2, !tbaa !3 *)
mov v202 mem0_458;
(*   %conv1.i.101 = sext i16 %202 to i32 *)
cast v_conv1_i_101@sint32 v202@sint16;
(*   %mul.i.101 = mul nsw i32 %conv1.i.101, -758 *)
mul v_mul_i_101 v_conv1_i_101 (-758)@sint32;
(*   %call.i.101 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.101) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_101, v_call_i_101);
(*   %arrayidx11.101 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %203 = load i16, i16* %arrayidx11.101, align 2, !tbaa !3 *)
mov v203 mem0_202;
(*   %sub.101 = sub i16 %203, %call.i.101 *)
sub v_sub_101 v203 v_call_i_101;
(*   store i16 %sub.101, i16* %arrayidx9.101, align 2, !tbaa !3 *)
mov mem0_458 v_sub_101;
(*   %add21.101 = add i16 %203, %call.i.101 *)
add v_add21_101 v203 v_call_i_101;
(*   store i16 %add21.101, i16* %arrayidx11.101, align 2, !tbaa !3 *)
mov mem0_202 v_add21_101;
(*   %arrayidx9.102 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %204 = load i16, i16* %arrayidx9.102, align 2, !tbaa !3 *)
mov v204 mem0_460;
(*   %conv1.i.102 = sext i16 %204 to i32 *)
cast v_conv1_i_102@sint32 v204@sint16;
(*   %mul.i.102 = mul nsw i32 %conv1.i.102, -758 *)
mul v_mul_i_102 v_conv1_i_102 (-758)@sint32;
(*   %call.i.102 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.102) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_102, v_call_i_102);
(*   %arrayidx11.102 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %205 = load i16, i16* %arrayidx11.102, align 2, !tbaa !3 *)
mov v205 mem0_204;
(*   %sub.102 = sub i16 %205, %call.i.102 *)
sub v_sub_102 v205 v_call_i_102;
(*   store i16 %sub.102, i16* %arrayidx9.102, align 2, !tbaa !3 *)
mov mem0_460 v_sub_102;
(*   %add21.102 = add i16 %205, %call.i.102 *)
add v_add21_102 v205 v_call_i_102;
(*   store i16 %add21.102, i16* %arrayidx11.102, align 2, !tbaa !3 *)
mov mem0_204 v_add21_102;
(*   %arrayidx9.103 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %206 = load i16, i16* %arrayidx9.103, align 2, !tbaa !3 *)
mov v206 mem0_462;
(*   %conv1.i.103 = sext i16 %206 to i32 *)
cast v_conv1_i_103@sint32 v206@sint16;
(*   %mul.i.103 = mul nsw i32 %conv1.i.103, -758 *)
mul v_mul_i_103 v_conv1_i_103 (-758)@sint32;
(*   %call.i.103 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.103) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_103, v_call_i_103);
(*   %arrayidx11.103 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %207 = load i16, i16* %arrayidx11.103, align 2, !tbaa !3 *)
mov v207 mem0_206;
(*   %sub.103 = sub i16 %207, %call.i.103 *)
sub v_sub_103 v207 v_call_i_103;
(*   store i16 %sub.103, i16* %arrayidx9.103, align 2, !tbaa !3 *)
mov mem0_462 v_sub_103;
(*   %add21.103 = add i16 %207, %call.i.103 *)
add v_add21_103 v207 v_call_i_103;
(*   store i16 %add21.103, i16* %arrayidx11.103, align 2, !tbaa !3 *)
mov mem0_206 v_add21_103;
(*   %arrayidx9.104 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %208 = load i16, i16* %arrayidx9.104, align 2, !tbaa !3 *)
mov v208 mem0_464;
(*   %conv1.i.104 = sext i16 %208 to i32 *)
cast v_conv1_i_104@sint32 v208@sint16;
(*   %mul.i.104 = mul nsw i32 %conv1.i.104, -758 *)
mul v_mul_i_104 v_conv1_i_104 (-758)@sint32;
(*   %call.i.104 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.104) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_104, v_call_i_104);
(*   %arrayidx11.104 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %209 = load i16, i16* %arrayidx11.104, align 2, !tbaa !3 *)
mov v209 mem0_208;
(*   %sub.104 = sub i16 %209, %call.i.104 *)
sub v_sub_104 v209 v_call_i_104;
(*   store i16 %sub.104, i16* %arrayidx9.104, align 2, !tbaa !3 *)
mov mem0_464 v_sub_104;
(*   %add21.104 = add i16 %209, %call.i.104 *)
add v_add21_104 v209 v_call_i_104;
(*   store i16 %add21.104, i16* %arrayidx11.104, align 2, !tbaa !3 *)
mov mem0_208 v_add21_104;
(*   %arrayidx9.105 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %210 = load i16, i16* %arrayidx9.105, align 2, !tbaa !3 *)
mov v210 mem0_466;
(*   %conv1.i.105 = sext i16 %210 to i32 *)
cast v_conv1_i_105@sint32 v210@sint16;
(*   %mul.i.105 = mul nsw i32 %conv1.i.105, -758 *)
mul v_mul_i_105 v_conv1_i_105 (-758)@sint32;
(*   %call.i.105 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.105) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_105, v_call_i_105);
(*   %arrayidx11.105 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %211 = load i16, i16* %arrayidx11.105, align 2, !tbaa !3 *)
mov v211 mem0_210;
(*   %sub.105 = sub i16 %211, %call.i.105 *)
sub v_sub_105 v211 v_call_i_105;
(*   store i16 %sub.105, i16* %arrayidx9.105, align 2, !tbaa !3 *)
mov mem0_466 v_sub_105;
(*   %add21.105 = add i16 %211, %call.i.105 *)
add v_add21_105 v211 v_call_i_105;
(*   store i16 %add21.105, i16* %arrayidx11.105, align 2, !tbaa !3 *)
mov mem0_210 v_add21_105;
(*   %arrayidx9.106 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %212 = load i16, i16* %arrayidx9.106, align 2, !tbaa !3 *)
mov v212 mem0_468;
(*   %conv1.i.106 = sext i16 %212 to i32 *)
cast v_conv1_i_106@sint32 v212@sint16;
(*   %mul.i.106 = mul nsw i32 %conv1.i.106, -758 *)
mul v_mul_i_106 v_conv1_i_106 (-758)@sint32;
(*   %call.i.106 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.106) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_106, v_call_i_106);
(*   %arrayidx11.106 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %213 = load i16, i16* %arrayidx11.106, align 2, !tbaa !3 *)
mov v213 mem0_212;
(*   %sub.106 = sub i16 %213, %call.i.106 *)
sub v_sub_106 v213 v_call_i_106;
(*   store i16 %sub.106, i16* %arrayidx9.106, align 2, !tbaa !3 *)
mov mem0_468 v_sub_106;
(*   %add21.106 = add i16 %213, %call.i.106 *)
add v_add21_106 v213 v_call_i_106;
(*   store i16 %add21.106, i16* %arrayidx11.106, align 2, !tbaa !3 *)
mov mem0_212 v_add21_106;
(*   %arrayidx9.107 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %214 = load i16, i16* %arrayidx9.107, align 2, !tbaa !3 *)
mov v214 mem0_470;
(*   %conv1.i.107 = sext i16 %214 to i32 *)
cast v_conv1_i_107@sint32 v214@sint16;
(*   %mul.i.107 = mul nsw i32 %conv1.i.107, -758 *)
mul v_mul_i_107 v_conv1_i_107 (-758)@sint32;
(*   %call.i.107 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.107) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_107, v_call_i_107);
(*   %arrayidx11.107 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %215 = load i16, i16* %arrayidx11.107, align 2, !tbaa !3 *)
mov v215 mem0_214;
(*   %sub.107 = sub i16 %215, %call.i.107 *)
sub v_sub_107 v215 v_call_i_107;
(*   store i16 %sub.107, i16* %arrayidx9.107, align 2, !tbaa !3 *)
mov mem0_470 v_sub_107;
(*   %add21.107 = add i16 %215, %call.i.107 *)
add v_add21_107 v215 v_call_i_107;
(*   store i16 %add21.107, i16* %arrayidx11.107, align 2, !tbaa !3 *)
mov mem0_214 v_add21_107;
(*   %arrayidx9.108 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %216 = load i16, i16* %arrayidx9.108, align 2, !tbaa !3 *)
mov v216 mem0_472;
(*   %conv1.i.108 = sext i16 %216 to i32 *)
cast v_conv1_i_108@sint32 v216@sint16;
(*   %mul.i.108 = mul nsw i32 %conv1.i.108, -758 *)
mul v_mul_i_108 v_conv1_i_108 (-758)@sint32;
(*   %call.i.108 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.108) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_108, v_call_i_108);
(*   %arrayidx11.108 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %217 = load i16, i16* %arrayidx11.108, align 2, !tbaa !3 *)
mov v217 mem0_216;
(*   %sub.108 = sub i16 %217, %call.i.108 *)
sub v_sub_108 v217 v_call_i_108;
(*   store i16 %sub.108, i16* %arrayidx9.108, align 2, !tbaa !3 *)
mov mem0_472 v_sub_108;
(*   %add21.108 = add i16 %217, %call.i.108 *)
add v_add21_108 v217 v_call_i_108;
(*   store i16 %add21.108, i16* %arrayidx11.108, align 2, !tbaa !3 *)
mov mem0_216 v_add21_108;
(*   %arrayidx9.109 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %218 = load i16, i16* %arrayidx9.109, align 2, !tbaa !3 *)
mov v218 mem0_474;
(*   %conv1.i.109 = sext i16 %218 to i32 *)
cast v_conv1_i_109@sint32 v218@sint16;
(*   %mul.i.109 = mul nsw i32 %conv1.i.109, -758 *)
mul v_mul_i_109 v_conv1_i_109 (-758)@sint32;
(*   %call.i.109 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.109) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_109, v_call_i_109);
(*   %arrayidx11.109 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %219 = load i16, i16* %arrayidx11.109, align 2, !tbaa !3 *)
mov v219 mem0_218;
(*   %sub.109 = sub i16 %219, %call.i.109 *)
sub v_sub_109 v219 v_call_i_109;
(*   store i16 %sub.109, i16* %arrayidx9.109, align 2, !tbaa !3 *)
mov mem0_474 v_sub_109;
(*   %add21.109 = add i16 %219, %call.i.109 *)
add v_add21_109 v219 v_call_i_109;
(*   store i16 %add21.109, i16* %arrayidx11.109, align 2, !tbaa !3 *)
mov mem0_218 v_add21_109;
(*   %arrayidx9.110 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %220 = load i16, i16* %arrayidx9.110, align 2, !tbaa !3 *)
mov v220 mem0_476;
(*   %conv1.i.110 = sext i16 %220 to i32 *)
cast v_conv1_i_110@sint32 v220@sint16;
(*   %mul.i.110 = mul nsw i32 %conv1.i.110, -758 *)
mul v_mul_i_110 v_conv1_i_110 (-758)@sint32;
(*   %call.i.110 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.110) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_110, v_call_i_110);
(*   %arrayidx11.110 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %221 = load i16, i16* %arrayidx11.110, align 2, !tbaa !3 *)
mov v221 mem0_220;
(*   %sub.110 = sub i16 %221, %call.i.110 *)
sub v_sub_110 v221 v_call_i_110;
(*   store i16 %sub.110, i16* %arrayidx9.110, align 2, !tbaa !3 *)
mov mem0_476 v_sub_110;
(*   %add21.110 = add i16 %221, %call.i.110 *)
add v_add21_110 v221 v_call_i_110;
(*   store i16 %add21.110, i16* %arrayidx11.110, align 2, !tbaa !3 *)
mov mem0_220 v_add21_110;
(*   %arrayidx9.111 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %222 = load i16, i16* %arrayidx9.111, align 2, !tbaa !3 *)
mov v222 mem0_478;
(*   %conv1.i.111 = sext i16 %222 to i32 *)
cast v_conv1_i_111@sint32 v222@sint16;
(*   %mul.i.111 = mul nsw i32 %conv1.i.111, -758 *)
mul v_mul_i_111 v_conv1_i_111 (-758)@sint32;
(*   %call.i.111 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.111) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_111, v_call_i_111);
(*   %arrayidx11.111 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %223 = load i16, i16* %arrayidx11.111, align 2, !tbaa !3 *)
mov v223 mem0_222;
(*   %sub.111 = sub i16 %223, %call.i.111 *)
sub v_sub_111 v223 v_call_i_111;
(*   store i16 %sub.111, i16* %arrayidx9.111, align 2, !tbaa !3 *)
mov mem0_478 v_sub_111;
(*   %add21.111 = add i16 %223, %call.i.111 *)
add v_add21_111 v223 v_call_i_111;
(*   store i16 %add21.111, i16* %arrayidx11.111, align 2, !tbaa !3 *)
mov mem0_222 v_add21_111;
(*   %arrayidx9.112 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %224 = load i16, i16* %arrayidx9.112, align 2, !tbaa !3 *)
mov v224 mem0_480;
(*   %conv1.i.112 = sext i16 %224 to i32 *)
cast v_conv1_i_112@sint32 v224@sint16;
(*   %mul.i.112 = mul nsw i32 %conv1.i.112, -758 *)
mul v_mul_i_112 v_conv1_i_112 (-758)@sint32;
(*   %call.i.112 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.112) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_112, v_call_i_112);
(*   %arrayidx11.112 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %225 = load i16, i16* %arrayidx11.112, align 2, !tbaa !3 *)
mov v225 mem0_224;
(*   %sub.112 = sub i16 %225, %call.i.112 *)
sub v_sub_112 v225 v_call_i_112;
(*   store i16 %sub.112, i16* %arrayidx9.112, align 2, !tbaa !3 *)
mov mem0_480 v_sub_112;
(*   %add21.112 = add i16 %225, %call.i.112 *)
add v_add21_112 v225 v_call_i_112;
(*   store i16 %add21.112, i16* %arrayidx11.112, align 2, !tbaa !3 *)
mov mem0_224 v_add21_112;
(*   %arrayidx9.113 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %226 = load i16, i16* %arrayidx9.113, align 2, !tbaa !3 *)
mov v226 mem0_482;
(*   %conv1.i.113 = sext i16 %226 to i32 *)
cast v_conv1_i_113@sint32 v226@sint16;
(*   %mul.i.113 = mul nsw i32 %conv1.i.113, -758 *)
mul v_mul_i_113 v_conv1_i_113 (-758)@sint32;
(*   %call.i.113 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.113) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_113, v_call_i_113);
(*   %arrayidx11.113 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %227 = load i16, i16* %arrayidx11.113, align 2, !tbaa !3 *)
mov v227 mem0_226;
(*   %sub.113 = sub i16 %227, %call.i.113 *)
sub v_sub_113 v227 v_call_i_113;
(*   store i16 %sub.113, i16* %arrayidx9.113, align 2, !tbaa !3 *)
mov mem0_482 v_sub_113;
(*   %add21.113 = add i16 %227, %call.i.113 *)
add v_add21_113 v227 v_call_i_113;
(*   store i16 %add21.113, i16* %arrayidx11.113, align 2, !tbaa !3 *)
mov mem0_226 v_add21_113;
(*   %arrayidx9.114 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %228 = load i16, i16* %arrayidx9.114, align 2, !tbaa !3 *)
mov v228 mem0_484;
(*   %conv1.i.114 = sext i16 %228 to i32 *)
cast v_conv1_i_114@sint32 v228@sint16;
(*   %mul.i.114 = mul nsw i32 %conv1.i.114, -758 *)
mul v_mul_i_114 v_conv1_i_114 (-758)@sint32;
(*   %call.i.114 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.114) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_114, v_call_i_114);
(*   %arrayidx11.114 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %229 = load i16, i16* %arrayidx11.114, align 2, !tbaa !3 *)
mov v229 mem0_228;
(*   %sub.114 = sub i16 %229, %call.i.114 *)
sub v_sub_114 v229 v_call_i_114;
(*   store i16 %sub.114, i16* %arrayidx9.114, align 2, !tbaa !3 *)
mov mem0_484 v_sub_114;
(*   %add21.114 = add i16 %229, %call.i.114 *)
add v_add21_114 v229 v_call_i_114;
(*   store i16 %add21.114, i16* %arrayidx11.114, align 2, !tbaa !3 *)
mov mem0_228 v_add21_114;
(*   %arrayidx9.115 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %230 = load i16, i16* %arrayidx9.115, align 2, !tbaa !3 *)
mov v230 mem0_486;
(*   %conv1.i.115 = sext i16 %230 to i32 *)
cast v_conv1_i_115@sint32 v230@sint16;
(*   %mul.i.115 = mul nsw i32 %conv1.i.115, -758 *)
mul v_mul_i_115 v_conv1_i_115 (-758)@sint32;
(*   %call.i.115 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.115) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_115, v_call_i_115);
(*   %arrayidx11.115 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %231 = load i16, i16* %arrayidx11.115, align 2, !tbaa !3 *)
mov v231 mem0_230;
(*   %sub.115 = sub i16 %231, %call.i.115 *)
sub v_sub_115 v231 v_call_i_115;
(*   store i16 %sub.115, i16* %arrayidx9.115, align 2, !tbaa !3 *)
mov mem0_486 v_sub_115;
(*   %add21.115 = add i16 %231, %call.i.115 *)
add v_add21_115 v231 v_call_i_115;
(*   store i16 %add21.115, i16* %arrayidx11.115, align 2, !tbaa !3 *)
mov mem0_230 v_add21_115;
(*   %arrayidx9.116 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %232 = load i16, i16* %arrayidx9.116, align 2, !tbaa !3 *)
mov v232 mem0_488;
(*   %conv1.i.116 = sext i16 %232 to i32 *)
cast v_conv1_i_116@sint32 v232@sint16;
(*   %mul.i.116 = mul nsw i32 %conv1.i.116, -758 *)
mul v_mul_i_116 v_conv1_i_116 (-758)@sint32;
(*   %call.i.116 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.116) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_116, v_call_i_116);
(*   %arrayidx11.116 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %233 = load i16, i16* %arrayidx11.116, align 2, !tbaa !3 *)
mov v233 mem0_232;
(*   %sub.116 = sub i16 %233, %call.i.116 *)
sub v_sub_116 v233 v_call_i_116;
(*   store i16 %sub.116, i16* %arrayidx9.116, align 2, !tbaa !3 *)
mov mem0_488 v_sub_116;
(*   %add21.116 = add i16 %233, %call.i.116 *)
add v_add21_116 v233 v_call_i_116;
(*   store i16 %add21.116, i16* %arrayidx11.116, align 2, !tbaa !3 *)
mov mem0_232 v_add21_116;
(*   %arrayidx9.117 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %234 = load i16, i16* %arrayidx9.117, align 2, !tbaa !3 *)
mov v234 mem0_490;
(*   %conv1.i.117 = sext i16 %234 to i32 *)
cast v_conv1_i_117@sint32 v234@sint16;
(*   %mul.i.117 = mul nsw i32 %conv1.i.117, -758 *)
mul v_mul_i_117 v_conv1_i_117 (-758)@sint32;
(*   %call.i.117 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.117) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_117, v_call_i_117);
(*   %arrayidx11.117 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %235 = load i16, i16* %arrayidx11.117, align 2, !tbaa !3 *)
mov v235 mem0_234;
(*   %sub.117 = sub i16 %235, %call.i.117 *)
sub v_sub_117 v235 v_call_i_117;
(*   store i16 %sub.117, i16* %arrayidx9.117, align 2, !tbaa !3 *)
mov mem0_490 v_sub_117;
(*   %add21.117 = add i16 %235, %call.i.117 *)
add v_add21_117 v235 v_call_i_117;
(*   store i16 %add21.117, i16* %arrayidx11.117, align 2, !tbaa !3 *)
mov mem0_234 v_add21_117;
(*   %arrayidx9.118 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %236 = load i16, i16* %arrayidx9.118, align 2, !tbaa !3 *)
mov v236 mem0_492;
(*   %conv1.i.118 = sext i16 %236 to i32 *)
cast v_conv1_i_118@sint32 v236@sint16;
(*   %mul.i.118 = mul nsw i32 %conv1.i.118, -758 *)
mul v_mul_i_118 v_conv1_i_118 (-758)@sint32;
(*   %call.i.118 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.118) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_118, v_call_i_118);
(*   %arrayidx11.118 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %237 = load i16, i16* %arrayidx11.118, align 2, !tbaa !3 *)
mov v237 mem0_236;
(*   %sub.118 = sub i16 %237, %call.i.118 *)
sub v_sub_118 v237 v_call_i_118;
(*   store i16 %sub.118, i16* %arrayidx9.118, align 2, !tbaa !3 *)
mov mem0_492 v_sub_118;
(*   %add21.118 = add i16 %237, %call.i.118 *)
add v_add21_118 v237 v_call_i_118;
(*   store i16 %add21.118, i16* %arrayidx11.118, align 2, !tbaa !3 *)
mov mem0_236 v_add21_118;
(*   %arrayidx9.119 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %238 = load i16, i16* %arrayidx9.119, align 2, !tbaa !3 *)
mov v238 mem0_494;
(*   %conv1.i.119 = sext i16 %238 to i32 *)
cast v_conv1_i_119@sint32 v238@sint16;
(*   %mul.i.119 = mul nsw i32 %conv1.i.119, -758 *)
mul v_mul_i_119 v_conv1_i_119 (-758)@sint32;
(*   %call.i.119 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.119) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_119, v_call_i_119);
(*   %arrayidx11.119 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %239 = load i16, i16* %arrayidx11.119, align 2, !tbaa !3 *)
mov v239 mem0_238;
(*   %sub.119 = sub i16 %239, %call.i.119 *)
sub v_sub_119 v239 v_call_i_119;
(*   store i16 %sub.119, i16* %arrayidx9.119, align 2, !tbaa !3 *)
mov mem0_494 v_sub_119;
(*   %add21.119 = add i16 %239, %call.i.119 *)
add v_add21_119 v239 v_call_i_119;
(*   store i16 %add21.119, i16* %arrayidx11.119, align 2, !tbaa !3 *)
mov mem0_238 v_add21_119;
(*   %arrayidx9.120 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %240 = load i16, i16* %arrayidx9.120, align 2, !tbaa !3 *)
mov v240 mem0_496;
(*   %conv1.i.120 = sext i16 %240 to i32 *)
cast v_conv1_i_120@sint32 v240@sint16;
(*   %mul.i.120 = mul nsw i32 %conv1.i.120, -758 *)
mul v_mul_i_120 v_conv1_i_120 (-758)@sint32;
(*   %call.i.120 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.120) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_120, v_call_i_120);
(*   %arrayidx11.120 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %241 = load i16, i16* %arrayidx11.120, align 2, !tbaa !3 *)
mov v241 mem0_240;
(*   %sub.120 = sub i16 %241, %call.i.120 *)
sub v_sub_120 v241 v_call_i_120;
(*   store i16 %sub.120, i16* %arrayidx9.120, align 2, !tbaa !3 *)
mov mem0_496 v_sub_120;
(*   %add21.120 = add i16 %241, %call.i.120 *)
add v_add21_120 v241 v_call_i_120;
(*   store i16 %add21.120, i16* %arrayidx11.120, align 2, !tbaa !3 *)
mov mem0_240 v_add21_120;
(*   %arrayidx9.121 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %242 = load i16, i16* %arrayidx9.121, align 2, !tbaa !3 *)
mov v242 mem0_498;
(*   %conv1.i.121 = sext i16 %242 to i32 *)
cast v_conv1_i_121@sint32 v242@sint16;
(*   %mul.i.121 = mul nsw i32 %conv1.i.121, -758 *)
mul v_mul_i_121 v_conv1_i_121 (-758)@sint32;
(*   %call.i.121 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.121) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_121, v_call_i_121);
(*   %arrayidx11.121 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %243 = load i16, i16* %arrayidx11.121, align 2, !tbaa !3 *)
mov v243 mem0_242;
(*   %sub.121 = sub i16 %243, %call.i.121 *)
sub v_sub_121 v243 v_call_i_121;
(*   store i16 %sub.121, i16* %arrayidx9.121, align 2, !tbaa !3 *)
mov mem0_498 v_sub_121;
(*   %add21.121 = add i16 %243, %call.i.121 *)
add v_add21_121 v243 v_call_i_121;
(*   store i16 %add21.121, i16* %arrayidx11.121, align 2, !tbaa !3 *)
mov mem0_242 v_add21_121;
(*   %arrayidx9.122 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %244 = load i16, i16* %arrayidx9.122, align 2, !tbaa !3 *)
mov v244 mem0_500;
(*   %conv1.i.122 = sext i16 %244 to i32 *)
cast v_conv1_i_122@sint32 v244@sint16;
(*   %mul.i.122 = mul nsw i32 %conv1.i.122, -758 *)
mul v_mul_i_122 v_conv1_i_122 (-758)@sint32;
(*   %call.i.122 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.122) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_122, v_call_i_122);
(*   %arrayidx11.122 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %245 = load i16, i16* %arrayidx11.122, align 2, !tbaa !3 *)
mov v245 mem0_244;
(*   %sub.122 = sub i16 %245, %call.i.122 *)
sub v_sub_122 v245 v_call_i_122;
(*   store i16 %sub.122, i16* %arrayidx9.122, align 2, !tbaa !3 *)
mov mem0_500 v_sub_122;
(*   %add21.122 = add i16 %245, %call.i.122 *)
add v_add21_122 v245 v_call_i_122;
(*   store i16 %add21.122, i16* %arrayidx11.122, align 2, !tbaa !3 *)
mov mem0_244 v_add21_122;
(*   %arrayidx9.123 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %246 = load i16, i16* %arrayidx9.123, align 2, !tbaa !3 *)
mov v246 mem0_502;
(*   %conv1.i.123 = sext i16 %246 to i32 *)
cast v_conv1_i_123@sint32 v246@sint16;
(*   %mul.i.123 = mul nsw i32 %conv1.i.123, -758 *)
mul v_mul_i_123 v_conv1_i_123 (-758)@sint32;
(*   %call.i.123 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.123) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_123, v_call_i_123);
(*   %arrayidx11.123 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %247 = load i16, i16* %arrayidx11.123, align 2, !tbaa !3 *)
mov v247 mem0_246;
(*   %sub.123 = sub i16 %247, %call.i.123 *)
sub v_sub_123 v247 v_call_i_123;
(*   store i16 %sub.123, i16* %arrayidx9.123, align 2, !tbaa !3 *)
mov mem0_502 v_sub_123;
(*   %add21.123 = add i16 %247, %call.i.123 *)
add v_add21_123 v247 v_call_i_123;
(*   store i16 %add21.123, i16* %arrayidx11.123, align 2, !tbaa !3 *)
mov mem0_246 v_add21_123;
(*   %arrayidx9.124 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %248 = load i16, i16* %arrayidx9.124, align 2, !tbaa !3 *)
mov v248 mem0_504;
(*   %conv1.i.124 = sext i16 %248 to i32 *)
cast v_conv1_i_124@sint32 v248@sint16;
(*   %mul.i.124 = mul nsw i32 %conv1.i.124, -758 *)
mul v_mul_i_124 v_conv1_i_124 (-758)@sint32;
(*   %call.i.124 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.124) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_124, v_call_i_124);
(*   %arrayidx11.124 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %249 = load i16, i16* %arrayidx11.124, align 2, !tbaa !3 *)
mov v249 mem0_248;
(*   %sub.124 = sub i16 %249, %call.i.124 *)
sub v_sub_124 v249 v_call_i_124;
(*   store i16 %sub.124, i16* %arrayidx9.124, align 2, !tbaa !3 *)
mov mem0_504 v_sub_124;
(*   %add21.124 = add i16 %249, %call.i.124 *)
add v_add21_124 v249 v_call_i_124;
(*   store i16 %add21.124, i16* %arrayidx11.124, align 2, !tbaa !3 *)
mov mem0_248 v_add21_124;
(*   %arrayidx9.125 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %250 = load i16, i16* %arrayidx9.125, align 2, !tbaa !3 *)
mov v250 mem0_506;
(*   %conv1.i.125 = sext i16 %250 to i32 *)
cast v_conv1_i_125@sint32 v250@sint16;
(*   %mul.i.125 = mul nsw i32 %conv1.i.125, -758 *)
mul v_mul_i_125 v_conv1_i_125 (-758)@sint32;
(*   %call.i.125 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.125) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_125, v_call_i_125);
(*   %arrayidx11.125 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %251 = load i16, i16* %arrayidx11.125, align 2, !tbaa !3 *)
mov v251 mem0_250;
(*   %sub.125 = sub i16 %251, %call.i.125 *)
sub v_sub_125 v251 v_call_i_125;
(*   store i16 %sub.125, i16* %arrayidx9.125, align 2, !tbaa !3 *)
mov mem0_506 v_sub_125;
(*   %add21.125 = add i16 %251, %call.i.125 *)
add v_add21_125 v251 v_call_i_125;
(*   store i16 %add21.125, i16* %arrayidx11.125, align 2, !tbaa !3 *)
mov mem0_250 v_add21_125;
(*   %arrayidx9.126 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %252 = load i16, i16* %arrayidx9.126, align 2, !tbaa !3 *)
mov v252 mem0_508;
(*   %conv1.i.126 = sext i16 %252 to i32 *)
cast v_conv1_i_126@sint32 v252@sint16;
(*   %mul.i.126 = mul nsw i32 %conv1.i.126, -758 *)
mul v_mul_i_126 v_conv1_i_126 (-758)@sint32;
(*   %call.i.126 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.126) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_126, v_call_i_126);
(*   %arrayidx11.126 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %253 = load i16, i16* %arrayidx11.126, align 2, !tbaa !3 *)
mov v253 mem0_252;
(*   %sub.126 = sub i16 %253, %call.i.126 *)
sub v_sub_126 v253 v_call_i_126;
(*   store i16 %sub.126, i16* %arrayidx9.126, align 2, !tbaa !3 *)
mov mem0_508 v_sub_126;
(*   %add21.126 = add i16 %253, %call.i.126 *)
add v_add21_126 v253 v_call_i_126;
(*   store i16 %add21.126, i16* %arrayidx11.126, align 2, !tbaa !3 *)
mov mem0_252 v_add21_126;
(*   %arrayidx9.127 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %254 = load i16, i16* %arrayidx9.127, align 2, !tbaa !3 *)
mov v254 mem0_510;
(*   %conv1.i.127 = sext i16 %254 to i32 *)
cast v_conv1_i_127@sint32 v254@sint16;
(*   %mul.i.127 = mul nsw i32 %conv1.i.127, -758 *)
mul v_mul_i_127 v_conv1_i_127 (-758)@sint32;
(*   %call.i.127 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.127) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_127, v_call_i_127);
(*   %arrayidx11.127 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %255 = load i16, i16* %arrayidx11.127, align 2, !tbaa !3 *)
mov v255 mem0_254;
(*   %sub.127 = sub i16 %255, %call.i.127 *)
sub v_sub_127 v255 v_call_i_127;
(*   store i16 %sub.127, i16* %arrayidx9.127, align 2, !tbaa !3 *)
mov mem0_510 v_sub_127;
(*   %add21.127 = add i16 %255, %call.i.127 *)
add v_add21_127 v255 v_call_i_127;
(*   store i16 %add21.127, i16* %arrayidx11.127, align 2, !tbaa !3 *)
mov mem0_254 v_add21_127;

cut and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3) + 
mem0_8*(x**4) + mem0_10*(x**5) + mem0_12*(x**6) + mem0_14*(x**7) + 
mem0_16*(x**8) + mem0_18*(x**9) + mem0_20*(x**10) + mem0_22*(x**11) + 
mem0_24*(x**12) + mem0_26*(x**13) + mem0_28*(x**14) + mem0_30*(x**15) + 
mem0_32*(x**16) + mem0_34*(x**17) + mem0_36*(x**18) + mem0_38*(x**19) + 
mem0_40*(x**20) + mem0_42*(x**21) + mem0_44*(x**22) + mem0_46*(x**23) + 
mem0_48*(x**24) + mem0_50*(x**25) + mem0_52*(x**26) + mem0_54*(x**27) + 
mem0_56*(x**28) + mem0_58*(x**29) + mem0_60*(x**30) + mem0_62*(x**31) + 
mem0_64*(x**32) + mem0_66*(x**33) + mem0_68*(x**34) + mem0_70*(x**35) + 
mem0_72*(x**36) + mem0_74*(x**37) + mem0_76*(x**38) + mem0_78*(x**39) + 
mem0_80*(x**40) + mem0_82*(x**41) + mem0_84*(x**42) + mem0_86*(x**43) + 
mem0_88*(x**44) + mem0_90*(x**45) + mem0_92*(x**46) + mem0_94*(x**47) + 
mem0_96*(x**48) + mem0_98*(x**49) + mem0_100*(x**50) + mem0_102*(x**51) + 
mem0_104*(x**52) + mem0_106*(x**53) + mem0_108*(x**54) + mem0_110*(x**55) + 
mem0_112*(x**56) + mem0_114*(x**57) + mem0_116*(x**58) + mem0_118*(x**59) + 
mem0_120*(x**60) + mem0_122*(x**61) + mem0_124*(x**62) + mem0_126*(x**63) + 
mem0_128*(x**64) + mem0_130*(x**65) + mem0_132*(x**66) + mem0_134*(x**67) + 
mem0_136*(x**68) + mem0_138*(x**69) + mem0_140*(x**70) + mem0_142*(x**71) + 
mem0_144*(x**72) + mem0_146*(x**73) + mem0_148*(x**74) + mem0_150*(x**75) + 
mem0_152*(x**76) + mem0_154*(x**77) + mem0_156*(x**78) + mem0_158*(x**79) + 
mem0_160*(x**80) + mem0_162*(x**81) + mem0_164*(x**82) + mem0_166*(x**83) + 
mem0_168*(x**84) + mem0_170*(x**85) + mem0_172*(x**86) + mem0_174*(x**87) + 
mem0_176*(x**88) + mem0_178*(x**89) + mem0_180*(x**90) + mem0_182*(x**91) + 
mem0_184*(x**92) + mem0_186*(x**93) + mem0_188*(x**94) + mem0_190*(x**95) + 
mem0_192*(x**96) + mem0_194*(x**97) + mem0_196*(x**98) + mem0_198*(x**99) + 
mem0_200*(x**100) + mem0_202*(x**101) + mem0_204*(x**102) + mem0_206*(x**103) + 
mem0_208*(x**104) + mem0_210*(x**105) + mem0_212*(x**106) + mem0_214*(x**107) + 
mem0_216*(x**108) + mem0_218*(x**109) + mem0_220*(x**110) + mem0_222*(x**111) + 
mem0_224*(x**112) + mem0_226*(x**113) + mem0_228*(x**114) + mem0_230*(x**115) + 
mem0_232*(x**116) + mem0_234*(x**117) + mem0_236*(x**118) + mem0_238*(x**119) + 
mem0_240*(x**120) + mem0_242*(x**121) + mem0_244*(x**122) + mem0_246*(x**123) + 
mem0_248*(x**124) + mem0_250*(x**125) + mem0_252*(x**126) + mem0_254*(x**127)
)
[3329, x**128 - 1729],
eqmod 
input_polynomial * input_polynomial
(
mem0_256*(x**0) + mem0_258*(x**1) + mem0_260*(x**2) + mem0_262*(x**3) + 
mem0_264*(x**4) + mem0_266*(x**5) + mem0_268*(x**6) + mem0_270*(x**7) + 
mem0_272*(x**8) + mem0_274*(x**9) + mem0_276*(x**10) + mem0_278*(x**11) + 
mem0_280*(x**12) + mem0_282*(x**13) + mem0_284*(x**14) + mem0_286*(x**15) + 
mem0_288*(x**16) + mem0_290*(x**17) + mem0_292*(x**18) + mem0_294*(x**19) + 
mem0_296*(x**20) + mem0_298*(x**21) + mem0_300*(x**22) + mem0_302*(x**23) + 
mem0_304*(x**24) + mem0_306*(x**25) + mem0_308*(x**26) + mem0_310*(x**27) + 
mem0_312*(x**28) + mem0_314*(x**29) + mem0_316*(x**30) + mem0_318*(x**31) + 
mem0_320*(x**32) + mem0_322*(x**33) + mem0_324*(x**34) + mem0_326*(x**35) + 
mem0_328*(x**36) + mem0_330*(x**37) + mem0_332*(x**38) + mem0_334*(x**39) + 
mem0_336*(x**40) + mem0_338*(x**41) + mem0_340*(x**42) + mem0_342*(x**43) + 
mem0_344*(x**44) + mem0_346*(x**45) + mem0_348*(x**46) + mem0_350*(x**47) + 
mem0_352*(x**48) + mem0_354*(x**49) + mem0_356*(x**50) + mem0_358*(x**51) + 
mem0_360*(x**52) + mem0_362*(x**53) + mem0_364*(x**54) + mem0_366*(x**55) + 
mem0_368*(x**56) + mem0_370*(x**57) + mem0_372*(x**58) + mem0_374*(x**59) + 
mem0_376*(x**60) + mem0_378*(x**61) + mem0_380*(x**62) + mem0_382*(x**63) + 
mem0_384*(x**64) + mem0_386*(x**65) + mem0_388*(x**66) + mem0_390*(x**67) + 
mem0_392*(x**68) + mem0_394*(x**69) + mem0_396*(x**70) + mem0_398*(x**71) + 
mem0_400*(x**72) + mem0_402*(x**73) + mem0_404*(x**74) + mem0_406*(x**75) + 
mem0_408*(x**76) + mem0_410*(x**77) + mem0_412*(x**78) + mem0_414*(x**79) + 
mem0_416*(x**80) + mem0_418*(x**81) + mem0_420*(x**82) + mem0_422*(x**83) + 
mem0_424*(x**84) + mem0_426*(x**85) + mem0_428*(x**86) + mem0_430*(x**87) + 
mem0_432*(x**88) + mem0_434*(x**89) + mem0_436*(x**90) + mem0_438*(x**91) + 
mem0_440*(x**92) + mem0_442*(x**93) + mem0_444*(x**94) + mem0_446*(x**95) + 
mem0_448*(x**96) + mem0_450*(x**97) + mem0_452*(x**98) + mem0_454*(x**99) + 
mem0_456*(x**100) + mem0_458*(x**101) + mem0_460*(x**102) + mem0_462*(x**103) + 
mem0_464*(x**104) + mem0_466*(x**105) + mem0_468*(x**106) + mem0_470*(x**107) + 
mem0_472*(x**108) + mem0_474*(x**109) + mem0_476*(x**110) + mem0_478*(x**111) + 
mem0_480*(x**112) + mem0_482*(x**113) + mem0_484*(x**114) + mem0_486*(x**115) + 
mem0_488*(x**116) + mem0_490*(x**117) + mem0_492*(x**118) + mem0_494*(x**119) + 
mem0_496*(x**120) + mem0_498*(x**121) + mem0_500*(x**122) + mem0_502*(x**123) + 
mem0_504*(x**124) + mem0_506*(x**125) + mem0_508*(x**126) + mem0_510*(x**127)
)
[3329, x**128 - 1600]
] && and [
    (-3)@16 * 3329@16 <s mem0_0, mem0_0 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_2, mem0_2 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_4, mem0_4 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_6, mem0_6 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_8, mem0_8 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_10, mem0_10 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_12, mem0_12 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_14, mem0_14 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_16, mem0_16 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_18, mem0_18 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_20, mem0_20 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_22, mem0_22 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_24, mem0_24 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_26, mem0_26 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_28, mem0_28 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_30, mem0_30 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_32, mem0_32 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_34, mem0_34 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_36, mem0_36 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_38, mem0_38 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_40, mem0_40 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_42, mem0_42 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_44, mem0_44 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_46, mem0_46 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_48, mem0_48 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_50, mem0_50 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_52, mem0_52 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_54, mem0_54 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_56, mem0_56 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_58, mem0_58 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_60, mem0_60 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_62, mem0_62 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_64, mem0_64 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_66, mem0_66 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_68, mem0_68 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_70, mem0_70 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_72, mem0_72 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_74, mem0_74 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_76, mem0_76 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_78, mem0_78 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_80, mem0_80 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_82, mem0_82 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_84, mem0_84 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_86, mem0_86 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_88, mem0_88 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_90, mem0_90 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_92, mem0_92 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_94, mem0_94 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_96, mem0_96 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_98, mem0_98 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_100, mem0_100 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_102, mem0_102 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_104, mem0_104 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_106, mem0_106 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_108, mem0_108 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_110, mem0_110 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_112, mem0_112 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_114, mem0_114 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_116, mem0_116 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_118, mem0_118 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_120, mem0_120 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_122, mem0_122 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_124, mem0_124 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_126, mem0_126 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_128, mem0_128 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_130, mem0_130 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_132, mem0_132 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_134, mem0_134 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_136, mem0_136 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_138, mem0_138 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_140, mem0_140 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_142, mem0_142 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_144, mem0_144 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_146, mem0_146 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_148, mem0_148 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_150, mem0_150 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_152, mem0_152 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_154, mem0_154 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_156, mem0_156 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_158, mem0_158 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_160, mem0_160 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_162, mem0_162 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_164, mem0_164 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_166, mem0_166 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_168, mem0_168 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_170, mem0_170 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_172, mem0_172 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_174, mem0_174 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_176, mem0_176 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_178, mem0_178 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_180, mem0_180 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_182, mem0_182 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_184, mem0_184 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_186, mem0_186 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_188, mem0_188 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_190, mem0_190 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_192, mem0_192 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_194, mem0_194 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_196, mem0_196 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_198, mem0_198 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_200, mem0_200 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_202, mem0_202 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_204, mem0_204 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_206, mem0_206 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_208, mem0_208 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_210, mem0_210 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_212, mem0_212 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_214, mem0_214 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_216, mem0_216 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_218, mem0_218 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_220, mem0_220 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_222, mem0_222 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_224, mem0_224 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_226, mem0_226 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_228, mem0_228 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_230, mem0_230 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_232, mem0_232 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_234, mem0_234 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_236, mem0_236 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_238, mem0_238 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_240, mem0_240 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_242, mem0_242 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_244, mem0_244 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_246, mem0_246 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_248, mem0_248 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_250, mem0_250 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_252, mem0_252 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_254, mem0_254 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_256, mem0_256 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_258, mem0_258 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_260, mem0_260 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_262, mem0_262 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_264, mem0_264 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_266, mem0_266 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_268, mem0_268 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_270, mem0_270 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_272, mem0_272 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_274, mem0_274 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_276, mem0_276 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_278, mem0_278 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_280, mem0_280 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_282, mem0_282 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_284, mem0_284 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_286, mem0_286 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_288, mem0_288 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_290, mem0_290 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_292, mem0_292 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_294, mem0_294 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_296, mem0_296 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_298, mem0_298 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_300, mem0_300 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_302, mem0_302 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_304, mem0_304 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_306, mem0_306 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_308, mem0_308 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_310, mem0_310 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_312, mem0_312 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_314, mem0_314 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_316, mem0_316 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_318, mem0_318 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_320, mem0_320 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_322, mem0_322 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_324, mem0_324 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_326, mem0_326 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_328, mem0_328 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_330, mem0_330 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_332, mem0_332 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_334, mem0_334 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_336, mem0_336 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_338, mem0_338 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_340, mem0_340 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_342, mem0_342 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_344, mem0_344 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_346, mem0_346 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_348, mem0_348 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_350, mem0_350 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_352, mem0_352 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_354, mem0_354 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_356, mem0_356 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_358, mem0_358 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_360, mem0_360 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_362, mem0_362 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_364, mem0_364 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_366, mem0_366 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_368, mem0_368 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_370, mem0_370 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_372, mem0_372 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_374, mem0_374 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_376, mem0_376 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_378, mem0_378 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_380, mem0_380 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_382, mem0_382 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_384, mem0_384 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_386, mem0_386 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_388, mem0_388 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_390, mem0_390 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_392, mem0_392 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_394, mem0_394 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_396, mem0_396 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_398, mem0_398 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_400, mem0_400 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_402, mem0_402 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_404, mem0_404 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_406, mem0_406 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_408, mem0_408 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_410, mem0_410 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_412, mem0_412 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_414, mem0_414 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_416, mem0_416 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_418, mem0_418 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_420, mem0_420 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_422, mem0_422 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_424, mem0_424 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_426, mem0_426 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_428, mem0_428 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_430, mem0_430 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_432, mem0_432 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_434, mem0_434 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_436, mem0_436 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_438, mem0_438 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_440, mem0_440 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_442, mem0_442 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_444, mem0_444 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_446, mem0_446 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_448, mem0_448 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_450, mem0_450 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_452, mem0_452 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_454, mem0_454 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_456, mem0_456 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_458, mem0_458 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_460, mem0_460 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_462, mem0_462 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_464, mem0_464 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_466, mem0_466 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_468, mem0_468 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_470, mem0_470 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_472, mem0_472 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_474, mem0_474 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_476, mem0_476 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_478, mem0_478 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_480, mem0_480 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_482, mem0_482 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_484, mem0_484 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_486, mem0_486 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_488, mem0_488 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_490, mem0_490 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_492, mem0_492 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_494, mem0_494 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_496, mem0_496 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_498, mem0_498 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_500, mem0_500 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_502, mem0_502 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_504, mem0_504 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_506, mem0_506 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_508, mem0_508 <s 3@16 * 3329@16,
    (-3)@16 * 3329@16 <s mem0_510, mem0_510 <s 3@16 * 3329@16
];

(* NOTE: k = 2 *)

(*   %arrayidx9.1 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %256 = load i16, i16* %arrayidx9.1, align 2, !tbaa !3 *)
mov v256 mem0_128;
(*   %conv1.i.1 = sext i16 %256 to i32 *)
cast v_conv1_i_1@sint32 v256@sint16;
(*   %mul.i.1 = mul nsw i32 %conv1.i.1, -359 *)
mul v_mul_i_1 v_conv1_i_1 (-359)@sint32;
(*   %call.i.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1, v_call_i_1);
(*   %257 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v257 mem0_0;
(*   %sub.1 = sub i16 %257, %call.i.1 *)
sub v_sub_1 v257 v_call_i_1;
(*   store i16 %sub.1, i16* %arrayidx9.1, align 2, !tbaa !3 *)
mov mem0_128 v_sub_1;
(*   %add21.1 = add i16 %257, %call.i.1 *)
add v_add21_1 v257 v_call_i_1;
(*   store i16 %add21.1, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21_1;
(*   %arrayidx9.1.1 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %258 = load i16, i16* %arrayidx9.1.1, align 2, !tbaa !3 *)
mov v258 mem0_130;
(*   %conv1.i.1.1 = sext i16 %258 to i32 *)
cast v_conv1_i_1_1@sint32 v258@sint16;
(*   %mul.i.1.1 = mul nsw i32 %conv1.i.1.1, -359 *)
mul v_mul_i_1_1 v_conv1_i_1_1 (-359)@sint32;
(*   %call.i.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_1, v_call_i_1_1);
(*   %arrayidx11.1.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %259 = load i16, i16* %arrayidx11.1.1, align 2, !tbaa !3 *)
mov v259 mem0_2;
(*   %sub.1.1 = sub i16 %259, %call.i.1.1 *)
sub v_sub_1_1 v259 v_call_i_1_1;
(*   store i16 %sub.1.1, i16* %arrayidx9.1.1, align 2, !tbaa !3 *)
mov mem0_130 v_sub_1_1;
(*   %add21.1.1 = add i16 %259, %call.i.1.1 *)
add v_add21_1_1 v259 v_call_i_1_1;
(*   store i16 %add21.1.1, i16* %arrayidx11.1.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_1_1;
(*   %arrayidx9.1.2 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %260 = load i16, i16* %arrayidx9.1.2, align 2, !tbaa !3 *)
mov v260 mem0_132;
(*   %conv1.i.1.2 = sext i16 %260 to i32 *)
cast v_conv1_i_1_2@sint32 v260@sint16;
(*   %mul.i.1.2 = mul nsw i32 %conv1.i.1.2, -359 *)
mul v_mul_i_1_2 v_conv1_i_1_2 (-359)@sint32;
(*   %call.i.1.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_2, v_call_i_1_2);
(*   %arrayidx11.1.2 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %261 = load i16, i16* %arrayidx11.1.2, align 2, !tbaa !3 *)
mov v261 mem0_4;
(*   %sub.1.2 = sub i16 %261, %call.i.1.2 *)
sub v_sub_1_2 v261 v_call_i_1_2;
(*   store i16 %sub.1.2, i16* %arrayidx9.1.2, align 2, !tbaa !3 *)
mov mem0_132 v_sub_1_2;
(*   %add21.1.2 = add i16 %261, %call.i.1.2 *)
add v_add21_1_2 v261 v_call_i_1_2;
(*   store i16 %add21.1.2, i16* %arrayidx11.1.2, align 2, !tbaa !3 *)
mov mem0_4 v_add21_1_2;
(*   %arrayidx9.1.3 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %262 = load i16, i16* %arrayidx9.1.3, align 2, !tbaa !3 *)
mov v262 mem0_134;
(*   %conv1.i.1.3 = sext i16 %262 to i32 *)
cast v_conv1_i_1_3@sint32 v262@sint16;
(*   %mul.i.1.3 = mul nsw i32 %conv1.i.1.3, -359 *)
mul v_mul_i_1_3 v_conv1_i_1_3 (-359)@sint32;
(*   %call.i.1.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_3, v_call_i_1_3);
(*   %arrayidx11.1.3 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %263 = load i16, i16* %arrayidx11.1.3, align 2, !tbaa !3 *)
mov v263 mem0_6;
(*   %sub.1.3 = sub i16 %263, %call.i.1.3 *)
sub v_sub_1_3 v263 v_call_i_1_3;
(*   store i16 %sub.1.3, i16* %arrayidx9.1.3, align 2, !tbaa !3 *)
mov mem0_134 v_sub_1_3;
(*   %add21.1.3 = add i16 %263, %call.i.1.3 *)
add v_add21_1_3 v263 v_call_i_1_3;
(*   store i16 %add21.1.3, i16* %arrayidx11.1.3, align 2, !tbaa !3 *)
mov mem0_6 v_add21_1_3;
(*   %arrayidx9.1.4 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %264 = load i16, i16* %arrayidx9.1.4, align 2, !tbaa !3 *)
mov v264 mem0_136;
(*   %conv1.i.1.4 = sext i16 %264 to i32 *)
cast v_conv1_i_1_4@sint32 v264@sint16;
(*   %mul.i.1.4 = mul nsw i32 %conv1.i.1.4, -359 *)
mul v_mul_i_1_4 v_conv1_i_1_4 (-359)@sint32;
(*   %call.i.1.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_4, v_call_i_1_4);
(*   %arrayidx11.1.4 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %265 = load i16, i16* %arrayidx11.1.4, align 2, !tbaa !3 *)
mov v265 mem0_8;
(*   %sub.1.4 = sub i16 %265, %call.i.1.4 *)
sub v_sub_1_4 v265 v_call_i_1_4;
(*   store i16 %sub.1.4, i16* %arrayidx9.1.4, align 2, !tbaa !3 *)
mov mem0_136 v_sub_1_4;
(*   %add21.1.4 = add i16 %265, %call.i.1.4 *)
add v_add21_1_4 v265 v_call_i_1_4;
(*   store i16 %add21.1.4, i16* %arrayidx11.1.4, align 2, !tbaa !3 *)
mov mem0_8 v_add21_1_4;
(*   %arrayidx9.1.5 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %266 = load i16, i16* %arrayidx9.1.5, align 2, !tbaa !3 *)
mov v266 mem0_138;
(*   %conv1.i.1.5 = sext i16 %266 to i32 *)
cast v_conv1_i_1_5@sint32 v266@sint16;
(*   %mul.i.1.5 = mul nsw i32 %conv1.i.1.5, -359 *)
mul v_mul_i_1_5 v_conv1_i_1_5 (-359)@sint32;
(*   %call.i.1.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_5, v_call_i_1_5);
(*   %arrayidx11.1.5 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %267 = load i16, i16* %arrayidx11.1.5, align 2, !tbaa !3 *)
mov v267 mem0_10;
(*   %sub.1.5 = sub i16 %267, %call.i.1.5 *)
sub v_sub_1_5 v267 v_call_i_1_5;
(*   store i16 %sub.1.5, i16* %arrayidx9.1.5, align 2, !tbaa !3 *)
mov mem0_138 v_sub_1_5;
(*   %add21.1.5 = add i16 %267, %call.i.1.5 *)
add v_add21_1_5 v267 v_call_i_1_5;
(*   store i16 %add21.1.5, i16* %arrayidx11.1.5, align 2, !tbaa !3 *)
mov mem0_10 v_add21_1_5;
(*   %arrayidx9.1.6 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %268 = load i16, i16* %arrayidx9.1.6, align 2, !tbaa !3 *)
mov v268 mem0_140;
(*   %conv1.i.1.6 = sext i16 %268 to i32 *)
cast v_conv1_i_1_6@sint32 v268@sint16;
(*   %mul.i.1.6 = mul nsw i32 %conv1.i.1.6, -359 *)
mul v_mul_i_1_6 v_conv1_i_1_6 (-359)@sint32;
(*   %call.i.1.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_6, v_call_i_1_6);
(*   %arrayidx11.1.6 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %269 = load i16, i16* %arrayidx11.1.6, align 2, !tbaa !3 *)
mov v269 mem0_12;
(*   %sub.1.6 = sub i16 %269, %call.i.1.6 *)
sub v_sub_1_6 v269 v_call_i_1_6;
(*   store i16 %sub.1.6, i16* %arrayidx9.1.6, align 2, !tbaa !3 *)
mov mem0_140 v_sub_1_6;
(*   %add21.1.6 = add i16 %269, %call.i.1.6 *)
add v_add21_1_6 v269 v_call_i_1_6;
(*   store i16 %add21.1.6, i16* %arrayidx11.1.6, align 2, !tbaa !3 *)
mov mem0_12 v_add21_1_6;
(*   %arrayidx9.1.7 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %270 = load i16, i16* %arrayidx9.1.7, align 2, !tbaa !3 *)
mov v270 mem0_142;
(*   %conv1.i.1.7 = sext i16 %270 to i32 *)
cast v_conv1_i_1_7@sint32 v270@sint16;
(*   %mul.i.1.7 = mul nsw i32 %conv1.i.1.7, -359 *)
mul v_mul_i_1_7 v_conv1_i_1_7 (-359)@sint32;
(*   %call.i.1.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_7, v_call_i_1_7);
(*   %arrayidx11.1.7 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %271 = load i16, i16* %arrayidx11.1.7, align 2, !tbaa !3 *)
mov v271 mem0_14;
(*   %sub.1.7 = sub i16 %271, %call.i.1.7 *)
sub v_sub_1_7 v271 v_call_i_1_7;
(*   store i16 %sub.1.7, i16* %arrayidx9.1.7, align 2, !tbaa !3 *)
mov mem0_142 v_sub_1_7;
(*   %add21.1.7 = add i16 %271, %call.i.1.7 *)
add v_add21_1_7 v271 v_call_i_1_7;
(*   store i16 %add21.1.7, i16* %arrayidx11.1.7, align 2, !tbaa !3 *)
mov mem0_14 v_add21_1_7;
(*   %arrayidx9.1.8 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %272 = load i16, i16* %arrayidx9.1.8, align 2, !tbaa !3 *)
mov v272 mem0_144;
(*   %conv1.i.1.8 = sext i16 %272 to i32 *)
cast v_conv1_i_1_8@sint32 v272@sint16;
(*   %mul.i.1.8 = mul nsw i32 %conv1.i.1.8, -359 *)
mul v_mul_i_1_8 v_conv1_i_1_8 (-359)@sint32;
(*   %call.i.1.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_8, v_call_i_1_8);
(*   %arrayidx11.1.8 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %273 = load i16, i16* %arrayidx11.1.8, align 2, !tbaa !3 *)
mov v273 mem0_16;
(*   %sub.1.8 = sub i16 %273, %call.i.1.8 *)
sub v_sub_1_8 v273 v_call_i_1_8;
(*   store i16 %sub.1.8, i16* %arrayidx9.1.8, align 2, !tbaa !3 *)
mov mem0_144 v_sub_1_8;
(*   %add21.1.8 = add i16 %273, %call.i.1.8 *)
add v_add21_1_8 v273 v_call_i_1_8;
(*   store i16 %add21.1.8, i16* %arrayidx11.1.8, align 2, !tbaa !3 *)
mov mem0_16 v_add21_1_8;
(*   %arrayidx9.1.9 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %274 = load i16, i16* %arrayidx9.1.9, align 2, !tbaa !3 *)
mov v274 mem0_146;
(*   %conv1.i.1.9 = sext i16 %274 to i32 *)
cast v_conv1_i_1_9@sint32 v274@sint16;
(*   %mul.i.1.9 = mul nsw i32 %conv1.i.1.9, -359 *)
mul v_mul_i_1_9 v_conv1_i_1_9 (-359)@sint32;
(*   %call.i.1.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_9, v_call_i_1_9);
(*   %arrayidx11.1.9 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %275 = load i16, i16* %arrayidx11.1.9, align 2, !tbaa !3 *)
mov v275 mem0_18;
(*   %sub.1.9 = sub i16 %275, %call.i.1.9 *)
sub v_sub_1_9 v275 v_call_i_1_9;
(*   store i16 %sub.1.9, i16* %arrayidx9.1.9, align 2, !tbaa !3 *)
mov mem0_146 v_sub_1_9;
(*   %add21.1.9 = add i16 %275, %call.i.1.9 *)
add v_add21_1_9 v275 v_call_i_1_9;
(*   store i16 %add21.1.9, i16* %arrayidx11.1.9, align 2, !tbaa !3 *)
mov mem0_18 v_add21_1_9;
(*   %arrayidx9.1.10 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %276 = load i16, i16* %arrayidx9.1.10, align 2, !tbaa !3 *)
mov v276 mem0_148;
(*   %conv1.i.1.10 = sext i16 %276 to i32 *)
cast v_conv1_i_1_10@sint32 v276@sint16;
(*   %mul.i.1.10 = mul nsw i32 %conv1.i.1.10, -359 *)
mul v_mul_i_1_10 v_conv1_i_1_10 (-359)@sint32;
(*   %call.i.1.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_10, v_call_i_1_10);
(*   %arrayidx11.1.10 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %277 = load i16, i16* %arrayidx11.1.10, align 2, !tbaa !3 *)
mov v277 mem0_20;
(*   %sub.1.10 = sub i16 %277, %call.i.1.10 *)
sub v_sub_1_10 v277 v_call_i_1_10;
(*   store i16 %sub.1.10, i16* %arrayidx9.1.10, align 2, !tbaa !3 *)
mov mem0_148 v_sub_1_10;
(*   %add21.1.10 = add i16 %277, %call.i.1.10 *)
add v_add21_1_10 v277 v_call_i_1_10;
(*   store i16 %add21.1.10, i16* %arrayidx11.1.10, align 2, !tbaa !3 *)
mov mem0_20 v_add21_1_10;
(*   %arrayidx9.1.11 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %278 = load i16, i16* %arrayidx9.1.11, align 2, !tbaa !3 *)
mov v278 mem0_150;
(*   %conv1.i.1.11 = sext i16 %278 to i32 *)
cast v_conv1_i_1_11@sint32 v278@sint16;
(*   %mul.i.1.11 = mul nsw i32 %conv1.i.1.11, -359 *)
mul v_mul_i_1_11 v_conv1_i_1_11 (-359)@sint32;
(*   %call.i.1.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_11, v_call_i_1_11);
(*   %arrayidx11.1.11 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %279 = load i16, i16* %arrayidx11.1.11, align 2, !tbaa !3 *)
mov v279 mem0_22;
(*   %sub.1.11 = sub i16 %279, %call.i.1.11 *)
sub v_sub_1_11 v279 v_call_i_1_11;
(*   store i16 %sub.1.11, i16* %arrayidx9.1.11, align 2, !tbaa !3 *)
mov mem0_150 v_sub_1_11;
(*   %add21.1.11 = add i16 %279, %call.i.1.11 *)
add v_add21_1_11 v279 v_call_i_1_11;
(*   store i16 %add21.1.11, i16* %arrayidx11.1.11, align 2, !tbaa !3 *)
mov mem0_22 v_add21_1_11;
(*   %arrayidx9.1.12 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %280 = load i16, i16* %arrayidx9.1.12, align 2, !tbaa !3 *)
mov v280 mem0_152;
(*   %conv1.i.1.12 = sext i16 %280 to i32 *)
cast v_conv1_i_1_12@sint32 v280@sint16;
(*   %mul.i.1.12 = mul nsw i32 %conv1.i.1.12, -359 *)
mul v_mul_i_1_12 v_conv1_i_1_12 (-359)@sint32;
(*   %call.i.1.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_12, v_call_i_1_12);
(*   %arrayidx11.1.12 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %281 = load i16, i16* %arrayidx11.1.12, align 2, !tbaa !3 *)
mov v281 mem0_24;
(*   %sub.1.12 = sub i16 %281, %call.i.1.12 *)
sub v_sub_1_12 v281 v_call_i_1_12;
(*   store i16 %sub.1.12, i16* %arrayidx9.1.12, align 2, !tbaa !3 *)
mov mem0_152 v_sub_1_12;
(*   %add21.1.12 = add i16 %281, %call.i.1.12 *)
add v_add21_1_12 v281 v_call_i_1_12;
(*   store i16 %add21.1.12, i16* %arrayidx11.1.12, align 2, !tbaa !3 *)
mov mem0_24 v_add21_1_12;
(*   %arrayidx9.1.13 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %282 = load i16, i16* %arrayidx9.1.13, align 2, !tbaa !3 *)
mov v282 mem0_154;
(*   %conv1.i.1.13 = sext i16 %282 to i32 *)
cast v_conv1_i_1_13@sint32 v282@sint16;
(*   %mul.i.1.13 = mul nsw i32 %conv1.i.1.13, -359 *)
mul v_mul_i_1_13 v_conv1_i_1_13 (-359)@sint32;
(*   %call.i.1.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_13, v_call_i_1_13);
(*   %arrayidx11.1.13 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %283 = load i16, i16* %arrayidx11.1.13, align 2, !tbaa !3 *)
mov v283 mem0_26;
(*   %sub.1.13 = sub i16 %283, %call.i.1.13 *)
sub v_sub_1_13 v283 v_call_i_1_13;
(*   store i16 %sub.1.13, i16* %arrayidx9.1.13, align 2, !tbaa !3 *)
mov mem0_154 v_sub_1_13;
(*   %add21.1.13 = add i16 %283, %call.i.1.13 *)
add v_add21_1_13 v283 v_call_i_1_13;
(*   store i16 %add21.1.13, i16* %arrayidx11.1.13, align 2, !tbaa !3 *)
mov mem0_26 v_add21_1_13;
(*   %arrayidx9.1.14 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %284 = load i16, i16* %arrayidx9.1.14, align 2, !tbaa !3 *)
mov v284 mem0_156;
(*   %conv1.i.1.14 = sext i16 %284 to i32 *)
cast v_conv1_i_1_14@sint32 v284@sint16;
(*   %mul.i.1.14 = mul nsw i32 %conv1.i.1.14, -359 *)
mul v_mul_i_1_14 v_conv1_i_1_14 (-359)@sint32;
(*   %call.i.1.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_14, v_call_i_1_14);
(*   %arrayidx11.1.14 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %285 = load i16, i16* %arrayidx11.1.14, align 2, !tbaa !3 *)
mov v285 mem0_28;
(*   %sub.1.14 = sub i16 %285, %call.i.1.14 *)
sub v_sub_1_14 v285 v_call_i_1_14;
(*   store i16 %sub.1.14, i16* %arrayidx9.1.14, align 2, !tbaa !3 *)
mov mem0_156 v_sub_1_14;
(*   %add21.1.14 = add i16 %285, %call.i.1.14 *)
add v_add21_1_14 v285 v_call_i_1_14;
(*   store i16 %add21.1.14, i16* %arrayidx11.1.14, align 2, !tbaa !3 *)
mov mem0_28 v_add21_1_14;
(*   %arrayidx9.1.15 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %286 = load i16, i16* %arrayidx9.1.15, align 2, !tbaa !3 *)
mov v286 mem0_158;
(*   %conv1.i.1.15 = sext i16 %286 to i32 *)
cast v_conv1_i_1_15@sint32 v286@sint16;
(*   %mul.i.1.15 = mul nsw i32 %conv1.i.1.15, -359 *)
mul v_mul_i_1_15 v_conv1_i_1_15 (-359)@sint32;
(*   %call.i.1.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_15, v_call_i_1_15);
(*   %arrayidx11.1.15 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %287 = load i16, i16* %arrayidx11.1.15, align 2, !tbaa !3 *)
mov v287 mem0_30;
(*   %sub.1.15 = sub i16 %287, %call.i.1.15 *)
sub v_sub_1_15 v287 v_call_i_1_15;
(*   store i16 %sub.1.15, i16* %arrayidx9.1.15, align 2, !tbaa !3 *)
mov mem0_158 v_sub_1_15;
(*   %add21.1.15 = add i16 %287, %call.i.1.15 *)
add v_add21_1_15 v287 v_call_i_1_15;
(*   store i16 %add21.1.15, i16* %arrayidx11.1.15, align 2, !tbaa !3 *)
mov mem0_30 v_add21_1_15;
(*   %arrayidx9.1.16 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %288 = load i16, i16* %arrayidx9.1.16, align 2, !tbaa !3 *)
mov v288 mem0_160;
(*   %conv1.i.1.16 = sext i16 %288 to i32 *)
cast v_conv1_i_1_16@sint32 v288@sint16;
(*   %mul.i.1.16 = mul nsw i32 %conv1.i.1.16, -359 *)
mul v_mul_i_1_16 v_conv1_i_1_16 (-359)@sint32;
(*   %call.i.1.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_16, v_call_i_1_16);
(*   %arrayidx11.1.16 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %289 = load i16, i16* %arrayidx11.1.16, align 2, !tbaa !3 *)
mov v289 mem0_32;
(*   %sub.1.16 = sub i16 %289, %call.i.1.16 *)
sub v_sub_1_16 v289 v_call_i_1_16;
(*   store i16 %sub.1.16, i16* %arrayidx9.1.16, align 2, !tbaa !3 *)
mov mem0_160 v_sub_1_16;
(*   %add21.1.16 = add i16 %289, %call.i.1.16 *)
add v_add21_1_16 v289 v_call_i_1_16;
(*   store i16 %add21.1.16, i16* %arrayidx11.1.16, align 2, !tbaa !3 *)
mov mem0_32 v_add21_1_16;
(*   %arrayidx9.1.17 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %290 = load i16, i16* %arrayidx9.1.17, align 2, !tbaa !3 *)
mov v290 mem0_162;
(*   %conv1.i.1.17 = sext i16 %290 to i32 *)
cast v_conv1_i_1_17@sint32 v290@sint16;
(*   %mul.i.1.17 = mul nsw i32 %conv1.i.1.17, -359 *)
mul v_mul_i_1_17 v_conv1_i_1_17 (-359)@sint32;
(*   %call.i.1.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_17, v_call_i_1_17);
(*   %arrayidx11.1.17 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %291 = load i16, i16* %arrayidx11.1.17, align 2, !tbaa !3 *)
mov v291 mem0_34;
(*   %sub.1.17 = sub i16 %291, %call.i.1.17 *)
sub v_sub_1_17 v291 v_call_i_1_17;
(*   store i16 %sub.1.17, i16* %arrayidx9.1.17, align 2, !tbaa !3 *)
mov mem0_162 v_sub_1_17;
(*   %add21.1.17 = add i16 %291, %call.i.1.17 *)
add v_add21_1_17 v291 v_call_i_1_17;
(*   store i16 %add21.1.17, i16* %arrayidx11.1.17, align 2, !tbaa !3 *)
mov mem0_34 v_add21_1_17;
(*   %arrayidx9.1.18 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %292 = load i16, i16* %arrayidx9.1.18, align 2, !tbaa !3 *)
mov v292 mem0_164;
(*   %conv1.i.1.18 = sext i16 %292 to i32 *)
cast v_conv1_i_1_18@sint32 v292@sint16;
(*   %mul.i.1.18 = mul nsw i32 %conv1.i.1.18, -359 *)
mul v_mul_i_1_18 v_conv1_i_1_18 (-359)@sint32;
(*   %call.i.1.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_18, v_call_i_1_18);
(*   %arrayidx11.1.18 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %293 = load i16, i16* %arrayidx11.1.18, align 2, !tbaa !3 *)
mov v293 mem0_36;
(*   %sub.1.18 = sub i16 %293, %call.i.1.18 *)
sub v_sub_1_18 v293 v_call_i_1_18;
(*   store i16 %sub.1.18, i16* %arrayidx9.1.18, align 2, !tbaa !3 *)
mov mem0_164 v_sub_1_18;
(*   %add21.1.18 = add i16 %293, %call.i.1.18 *)
add v_add21_1_18 v293 v_call_i_1_18;
(*   store i16 %add21.1.18, i16* %arrayidx11.1.18, align 2, !tbaa !3 *)
mov mem0_36 v_add21_1_18;
(*   %arrayidx9.1.19 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %294 = load i16, i16* %arrayidx9.1.19, align 2, !tbaa !3 *)
mov v294 mem0_166;
(*   %conv1.i.1.19 = sext i16 %294 to i32 *)
cast v_conv1_i_1_19@sint32 v294@sint16;
(*   %mul.i.1.19 = mul nsw i32 %conv1.i.1.19, -359 *)
mul v_mul_i_1_19 v_conv1_i_1_19 (-359)@sint32;
(*   %call.i.1.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_19, v_call_i_1_19);
(*   %arrayidx11.1.19 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %295 = load i16, i16* %arrayidx11.1.19, align 2, !tbaa !3 *)
mov v295 mem0_38;
(*   %sub.1.19 = sub i16 %295, %call.i.1.19 *)
sub v_sub_1_19 v295 v_call_i_1_19;
(*   store i16 %sub.1.19, i16* %arrayidx9.1.19, align 2, !tbaa !3 *)
mov mem0_166 v_sub_1_19;
(*   %add21.1.19 = add i16 %295, %call.i.1.19 *)
add v_add21_1_19 v295 v_call_i_1_19;
(*   store i16 %add21.1.19, i16* %arrayidx11.1.19, align 2, !tbaa !3 *)
mov mem0_38 v_add21_1_19;
(*   %arrayidx9.1.20 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %296 = load i16, i16* %arrayidx9.1.20, align 2, !tbaa !3 *)
mov v296 mem0_168;
(*   %conv1.i.1.20 = sext i16 %296 to i32 *)
cast v_conv1_i_1_20@sint32 v296@sint16;
(*   %mul.i.1.20 = mul nsw i32 %conv1.i.1.20, -359 *)
mul v_mul_i_1_20 v_conv1_i_1_20 (-359)@sint32;
(*   %call.i.1.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_20, v_call_i_1_20);
(*   %arrayidx11.1.20 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %297 = load i16, i16* %arrayidx11.1.20, align 2, !tbaa !3 *)
mov v297 mem0_40;
(*   %sub.1.20 = sub i16 %297, %call.i.1.20 *)
sub v_sub_1_20 v297 v_call_i_1_20;
(*   store i16 %sub.1.20, i16* %arrayidx9.1.20, align 2, !tbaa !3 *)
mov mem0_168 v_sub_1_20;
(*   %add21.1.20 = add i16 %297, %call.i.1.20 *)
add v_add21_1_20 v297 v_call_i_1_20;
(*   store i16 %add21.1.20, i16* %arrayidx11.1.20, align 2, !tbaa !3 *)
mov mem0_40 v_add21_1_20;
(*   %arrayidx9.1.21 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %298 = load i16, i16* %arrayidx9.1.21, align 2, !tbaa !3 *)
mov v298 mem0_170;
(*   %conv1.i.1.21 = sext i16 %298 to i32 *)
cast v_conv1_i_1_21@sint32 v298@sint16;
(*   %mul.i.1.21 = mul nsw i32 %conv1.i.1.21, -359 *)
mul v_mul_i_1_21 v_conv1_i_1_21 (-359)@sint32;
(*   %call.i.1.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_21, v_call_i_1_21);
(*   %arrayidx11.1.21 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %299 = load i16, i16* %arrayidx11.1.21, align 2, !tbaa !3 *)
mov v299 mem0_42;
(*   %sub.1.21 = sub i16 %299, %call.i.1.21 *)
sub v_sub_1_21 v299 v_call_i_1_21;
(*   store i16 %sub.1.21, i16* %arrayidx9.1.21, align 2, !tbaa !3 *)
mov mem0_170 v_sub_1_21;
(*   %add21.1.21 = add i16 %299, %call.i.1.21 *)
add v_add21_1_21 v299 v_call_i_1_21;
(*   store i16 %add21.1.21, i16* %arrayidx11.1.21, align 2, !tbaa !3 *)
mov mem0_42 v_add21_1_21;
(*   %arrayidx9.1.22 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %300 = load i16, i16* %arrayidx9.1.22, align 2, !tbaa !3 *)
mov v300 mem0_172;
(*   %conv1.i.1.22 = sext i16 %300 to i32 *)
cast v_conv1_i_1_22@sint32 v300@sint16;
(*   %mul.i.1.22 = mul nsw i32 %conv1.i.1.22, -359 *)
mul v_mul_i_1_22 v_conv1_i_1_22 (-359)@sint32;
(*   %call.i.1.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_22, v_call_i_1_22);
(*   %arrayidx11.1.22 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %301 = load i16, i16* %arrayidx11.1.22, align 2, !tbaa !3 *)
mov v301 mem0_44;
(*   %sub.1.22 = sub i16 %301, %call.i.1.22 *)
sub v_sub_1_22 v301 v_call_i_1_22;
(*   store i16 %sub.1.22, i16* %arrayidx9.1.22, align 2, !tbaa !3 *)
mov mem0_172 v_sub_1_22;
(*   %add21.1.22 = add i16 %301, %call.i.1.22 *)
add v_add21_1_22 v301 v_call_i_1_22;
(*   store i16 %add21.1.22, i16* %arrayidx11.1.22, align 2, !tbaa !3 *)
mov mem0_44 v_add21_1_22;
(*   %arrayidx9.1.23 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %302 = load i16, i16* %arrayidx9.1.23, align 2, !tbaa !3 *)
mov v302 mem0_174;
(*   %conv1.i.1.23 = sext i16 %302 to i32 *)
cast v_conv1_i_1_23@sint32 v302@sint16;
(*   %mul.i.1.23 = mul nsw i32 %conv1.i.1.23, -359 *)
mul v_mul_i_1_23 v_conv1_i_1_23 (-359)@sint32;
(*   %call.i.1.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_23, v_call_i_1_23);
(*   %arrayidx11.1.23 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %303 = load i16, i16* %arrayidx11.1.23, align 2, !tbaa !3 *)
mov v303 mem0_46;
(*   %sub.1.23 = sub i16 %303, %call.i.1.23 *)
sub v_sub_1_23 v303 v_call_i_1_23;
(*   store i16 %sub.1.23, i16* %arrayidx9.1.23, align 2, !tbaa !3 *)
mov mem0_174 v_sub_1_23;
(*   %add21.1.23 = add i16 %303, %call.i.1.23 *)
add v_add21_1_23 v303 v_call_i_1_23;
(*   store i16 %add21.1.23, i16* %arrayidx11.1.23, align 2, !tbaa !3 *)
mov mem0_46 v_add21_1_23;
(*   %arrayidx9.1.24 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %304 = load i16, i16* %arrayidx9.1.24, align 2, !tbaa !3 *)
mov v304 mem0_176;
(*   %conv1.i.1.24 = sext i16 %304 to i32 *)
cast v_conv1_i_1_24@sint32 v304@sint16;
(*   %mul.i.1.24 = mul nsw i32 %conv1.i.1.24, -359 *)
mul v_mul_i_1_24 v_conv1_i_1_24 (-359)@sint32;
(*   %call.i.1.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_24, v_call_i_1_24);
(*   %arrayidx11.1.24 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %305 = load i16, i16* %arrayidx11.1.24, align 2, !tbaa !3 *)
mov v305 mem0_48;
(*   %sub.1.24 = sub i16 %305, %call.i.1.24 *)
sub v_sub_1_24 v305 v_call_i_1_24;
(*   store i16 %sub.1.24, i16* %arrayidx9.1.24, align 2, !tbaa !3 *)
mov mem0_176 v_sub_1_24;
(*   %add21.1.24 = add i16 %305, %call.i.1.24 *)
add v_add21_1_24 v305 v_call_i_1_24;
(*   store i16 %add21.1.24, i16* %arrayidx11.1.24, align 2, !tbaa !3 *)
mov mem0_48 v_add21_1_24;
(*   %arrayidx9.1.25 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %306 = load i16, i16* %arrayidx9.1.25, align 2, !tbaa !3 *)
mov v306 mem0_178;
(*   %conv1.i.1.25 = sext i16 %306 to i32 *)
cast v_conv1_i_1_25@sint32 v306@sint16;
(*   %mul.i.1.25 = mul nsw i32 %conv1.i.1.25, -359 *)
mul v_mul_i_1_25 v_conv1_i_1_25 (-359)@sint32;
(*   %call.i.1.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_25, v_call_i_1_25);
(*   %arrayidx11.1.25 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %307 = load i16, i16* %arrayidx11.1.25, align 2, !tbaa !3 *)
mov v307 mem0_50;
(*   %sub.1.25 = sub i16 %307, %call.i.1.25 *)
sub v_sub_1_25 v307 v_call_i_1_25;
(*   store i16 %sub.1.25, i16* %arrayidx9.1.25, align 2, !tbaa !3 *)
mov mem0_178 v_sub_1_25;
(*   %add21.1.25 = add i16 %307, %call.i.1.25 *)
add v_add21_1_25 v307 v_call_i_1_25;
(*   store i16 %add21.1.25, i16* %arrayidx11.1.25, align 2, !tbaa !3 *)
mov mem0_50 v_add21_1_25;
(*   %arrayidx9.1.26 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %308 = load i16, i16* %arrayidx9.1.26, align 2, !tbaa !3 *)
mov v308 mem0_180;
(*   %conv1.i.1.26 = sext i16 %308 to i32 *)
cast v_conv1_i_1_26@sint32 v308@sint16;
(*   %mul.i.1.26 = mul nsw i32 %conv1.i.1.26, -359 *)
mul v_mul_i_1_26 v_conv1_i_1_26 (-359)@sint32;
(*   %call.i.1.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_26, v_call_i_1_26);
(*   %arrayidx11.1.26 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %309 = load i16, i16* %arrayidx11.1.26, align 2, !tbaa !3 *)
mov v309 mem0_52;
(*   %sub.1.26 = sub i16 %309, %call.i.1.26 *)
sub v_sub_1_26 v309 v_call_i_1_26;
(*   store i16 %sub.1.26, i16* %arrayidx9.1.26, align 2, !tbaa !3 *)
mov mem0_180 v_sub_1_26;
(*   %add21.1.26 = add i16 %309, %call.i.1.26 *)
add v_add21_1_26 v309 v_call_i_1_26;
(*   store i16 %add21.1.26, i16* %arrayidx11.1.26, align 2, !tbaa !3 *)
mov mem0_52 v_add21_1_26;
(*   %arrayidx9.1.27 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %310 = load i16, i16* %arrayidx9.1.27, align 2, !tbaa !3 *)
mov v310 mem0_182;
(*   %conv1.i.1.27 = sext i16 %310 to i32 *)
cast v_conv1_i_1_27@sint32 v310@sint16;
(*   %mul.i.1.27 = mul nsw i32 %conv1.i.1.27, -359 *)
mul v_mul_i_1_27 v_conv1_i_1_27 (-359)@sint32;
(*   %call.i.1.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_27, v_call_i_1_27);
(*   %arrayidx11.1.27 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %311 = load i16, i16* %arrayidx11.1.27, align 2, !tbaa !3 *)
mov v311 mem0_54;
(*   %sub.1.27 = sub i16 %311, %call.i.1.27 *)
sub v_sub_1_27 v311 v_call_i_1_27;
(*   store i16 %sub.1.27, i16* %arrayidx9.1.27, align 2, !tbaa !3 *)
mov mem0_182 v_sub_1_27;
(*   %add21.1.27 = add i16 %311, %call.i.1.27 *)
add v_add21_1_27 v311 v_call_i_1_27;
(*   store i16 %add21.1.27, i16* %arrayidx11.1.27, align 2, !tbaa !3 *)
mov mem0_54 v_add21_1_27;
(*   %arrayidx9.1.28 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %312 = load i16, i16* %arrayidx9.1.28, align 2, !tbaa !3 *)
mov v312 mem0_184;
(*   %conv1.i.1.28 = sext i16 %312 to i32 *)
cast v_conv1_i_1_28@sint32 v312@sint16;
(*   %mul.i.1.28 = mul nsw i32 %conv1.i.1.28, -359 *)
mul v_mul_i_1_28 v_conv1_i_1_28 (-359)@sint32;
(*   %call.i.1.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_28, v_call_i_1_28);
(*   %arrayidx11.1.28 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %313 = load i16, i16* %arrayidx11.1.28, align 2, !tbaa !3 *)
mov v313 mem0_56;
(*   %sub.1.28 = sub i16 %313, %call.i.1.28 *)
sub v_sub_1_28 v313 v_call_i_1_28;
(*   store i16 %sub.1.28, i16* %arrayidx9.1.28, align 2, !tbaa !3 *)
mov mem0_184 v_sub_1_28;
(*   %add21.1.28 = add i16 %313, %call.i.1.28 *)
add v_add21_1_28 v313 v_call_i_1_28;
(*   store i16 %add21.1.28, i16* %arrayidx11.1.28, align 2, !tbaa !3 *)
mov mem0_56 v_add21_1_28;
(*   %arrayidx9.1.29 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %314 = load i16, i16* %arrayidx9.1.29, align 2, !tbaa !3 *)
mov v314 mem0_186;
(*   %conv1.i.1.29 = sext i16 %314 to i32 *)
cast v_conv1_i_1_29@sint32 v314@sint16;
(*   %mul.i.1.29 = mul nsw i32 %conv1.i.1.29, -359 *)
mul v_mul_i_1_29 v_conv1_i_1_29 (-359)@sint32;
(*   %call.i.1.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_29, v_call_i_1_29);
(*   %arrayidx11.1.29 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %315 = load i16, i16* %arrayidx11.1.29, align 2, !tbaa !3 *)
mov v315 mem0_58;
(*   %sub.1.29 = sub i16 %315, %call.i.1.29 *)
sub v_sub_1_29 v315 v_call_i_1_29;
(*   store i16 %sub.1.29, i16* %arrayidx9.1.29, align 2, !tbaa !3 *)
mov mem0_186 v_sub_1_29;
(*   %add21.1.29 = add i16 %315, %call.i.1.29 *)
add v_add21_1_29 v315 v_call_i_1_29;
(*   store i16 %add21.1.29, i16* %arrayidx11.1.29, align 2, !tbaa !3 *)
mov mem0_58 v_add21_1_29;
(*   %arrayidx9.1.30 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %316 = load i16, i16* %arrayidx9.1.30, align 2, !tbaa !3 *)
mov v316 mem0_188;
(*   %conv1.i.1.30 = sext i16 %316 to i32 *)
cast v_conv1_i_1_30@sint32 v316@sint16;
(*   %mul.i.1.30 = mul nsw i32 %conv1.i.1.30, -359 *)
mul v_mul_i_1_30 v_conv1_i_1_30 (-359)@sint32;
(*   %call.i.1.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_30, v_call_i_1_30);
(*   %arrayidx11.1.30 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %317 = load i16, i16* %arrayidx11.1.30, align 2, !tbaa !3 *)
mov v317 mem0_60;
(*   %sub.1.30 = sub i16 %317, %call.i.1.30 *)
sub v_sub_1_30 v317 v_call_i_1_30;
(*   store i16 %sub.1.30, i16* %arrayidx9.1.30, align 2, !tbaa !3 *)
mov mem0_188 v_sub_1_30;
(*   %add21.1.30 = add i16 %317, %call.i.1.30 *)
add v_add21_1_30 v317 v_call_i_1_30;
(*   store i16 %add21.1.30, i16* %arrayidx11.1.30, align 2, !tbaa !3 *)
mov mem0_60 v_add21_1_30;
(*   %arrayidx9.1.31 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %318 = load i16, i16* %arrayidx9.1.31, align 2, !tbaa !3 *)
mov v318 mem0_190;
(*   %conv1.i.1.31 = sext i16 %318 to i32 *)
cast v_conv1_i_1_31@sint32 v318@sint16;
(*   %mul.i.1.31 = mul nsw i32 %conv1.i.1.31, -359 *)
mul v_mul_i_1_31 v_conv1_i_1_31 (-359)@sint32;
(*   %call.i.1.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_31, v_call_i_1_31);
(*   %arrayidx11.1.31 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %319 = load i16, i16* %arrayidx11.1.31, align 2, !tbaa !3 *)
mov v319 mem0_62;
(*   %sub.1.31 = sub i16 %319, %call.i.1.31 *)
sub v_sub_1_31 v319 v_call_i_1_31;
(*   store i16 %sub.1.31, i16* %arrayidx9.1.31, align 2, !tbaa !3 *)
mov mem0_190 v_sub_1_31;
(*   %add21.1.31 = add i16 %319, %call.i.1.31 *)
add v_add21_1_31 v319 v_call_i_1_31;
(*   store i16 %add21.1.31, i16* %arrayidx11.1.31, align 2, !tbaa !3 *)
mov mem0_62 v_add21_1_31;
(*   %arrayidx9.1.32 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %320 = load i16, i16* %arrayidx9.1.32, align 2, !tbaa !3 *)
mov v320 mem0_192;
(*   %conv1.i.1.32 = sext i16 %320 to i32 *)
cast v_conv1_i_1_32@sint32 v320@sint16;
(*   %mul.i.1.32 = mul nsw i32 %conv1.i.1.32, -359 *)
mul v_mul_i_1_32 v_conv1_i_1_32 (-359)@sint32;
(*   %call.i.1.32 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.32) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_32, v_call_i_1_32);
(*   %arrayidx11.1.32 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %321 = load i16, i16* %arrayidx11.1.32, align 2, !tbaa !3 *)
mov v321 mem0_64;
(*   %sub.1.32 = sub i16 %321, %call.i.1.32 *)
sub v_sub_1_32 v321 v_call_i_1_32;
(*   store i16 %sub.1.32, i16* %arrayidx9.1.32, align 2, !tbaa !3 *)
mov mem0_192 v_sub_1_32;
(*   %add21.1.32 = add i16 %321, %call.i.1.32 *)
add v_add21_1_32 v321 v_call_i_1_32;
(*   store i16 %add21.1.32, i16* %arrayidx11.1.32, align 2, !tbaa !3 *)
mov mem0_64 v_add21_1_32;
(*   %arrayidx9.1.33 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %322 = load i16, i16* %arrayidx9.1.33, align 2, !tbaa !3 *)
mov v322 mem0_194;
(*   %conv1.i.1.33 = sext i16 %322 to i32 *)
cast v_conv1_i_1_33@sint32 v322@sint16;
(*   %mul.i.1.33 = mul nsw i32 %conv1.i.1.33, -359 *)
mul v_mul_i_1_33 v_conv1_i_1_33 (-359)@sint32;
(*   %call.i.1.33 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.33) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_33, v_call_i_1_33);
(*   %arrayidx11.1.33 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %323 = load i16, i16* %arrayidx11.1.33, align 2, !tbaa !3 *)
mov v323 mem0_66;
(*   %sub.1.33 = sub i16 %323, %call.i.1.33 *)
sub v_sub_1_33 v323 v_call_i_1_33;
(*   store i16 %sub.1.33, i16* %arrayidx9.1.33, align 2, !tbaa !3 *)
mov mem0_194 v_sub_1_33;
(*   %add21.1.33 = add i16 %323, %call.i.1.33 *)
add v_add21_1_33 v323 v_call_i_1_33;
(*   store i16 %add21.1.33, i16* %arrayidx11.1.33, align 2, !tbaa !3 *)
mov mem0_66 v_add21_1_33;
(*   %arrayidx9.1.34 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %324 = load i16, i16* %arrayidx9.1.34, align 2, !tbaa !3 *)
mov v324 mem0_196;
(*   %conv1.i.1.34 = sext i16 %324 to i32 *)
cast v_conv1_i_1_34@sint32 v324@sint16;
(*   %mul.i.1.34 = mul nsw i32 %conv1.i.1.34, -359 *)
mul v_mul_i_1_34 v_conv1_i_1_34 (-359)@sint32;
(*   %call.i.1.34 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.34) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_34, v_call_i_1_34);
(*   %arrayidx11.1.34 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %325 = load i16, i16* %arrayidx11.1.34, align 2, !tbaa !3 *)
mov v325 mem0_68;
(*   %sub.1.34 = sub i16 %325, %call.i.1.34 *)
sub v_sub_1_34 v325 v_call_i_1_34;
(*   store i16 %sub.1.34, i16* %arrayidx9.1.34, align 2, !tbaa !3 *)
mov mem0_196 v_sub_1_34;
(*   %add21.1.34 = add i16 %325, %call.i.1.34 *)
add v_add21_1_34 v325 v_call_i_1_34;
(*   store i16 %add21.1.34, i16* %arrayidx11.1.34, align 2, !tbaa !3 *)
mov mem0_68 v_add21_1_34;
(*   %arrayidx9.1.35 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %326 = load i16, i16* %arrayidx9.1.35, align 2, !tbaa !3 *)
mov v326 mem0_198;
(*   %conv1.i.1.35 = sext i16 %326 to i32 *)
cast v_conv1_i_1_35@sint32 v326@sint16;
(*   %mul.i.1.35 = mul nsw i32 %conv1.i.1.35, -359 *)
mul v_mul_i_1_35 v_conv1_i_1_35 (-359)@sint32;
(*   %call.i.1.35 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.35) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_35, v_call_i_1_35);
(*   %arrayidx11.1.35 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %327 = load i16, i16* %arrayidx11.1.35, align 2, !tbaa !3 *)
mov v327 mem0_70;
(*   %sub.1.35 = sub i16 %327, %call.i.1.35 *)
sub v_sub_1_35 v327 v_call_i_1_35;
(*   store i16 %sub.1.35, i16* %arrayidx9.1.35, align 2, !tbaa !3 *)
mov mem0_198 v_sub_1_35;
(*   %add21.1.35 = add i16 %327, %call.i.1.35 *)
add v_add21_1_35 v327 v_call_i_1_35;
(*   store i16 %add21.1.35, i16* %arrayidx11.1.35, align 2, !tbaa !3 *)
mov mem0_70 v_add21_1_35;
(*   %arrayidx9.1.36 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %328 = load i16, i16* %arrayidx9.1.36, align 2, !tbaa !3 *)
mov v328 mem0_200;
(*   %conv1.i.1.36 = sext i16 %328 to i32 *)
cast v_conv1_i_1_36@sint32 v328@sint16;
(*   %mul.i.1.36 = mul nsw i32 %conv1.i.1.36, -359 *)
mul v_mul_i_1_36 v_conv1_i_1_36 (-359)@sint32;
(*   %call.i.1.36 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.36) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_36, v_call_i_1_36);
(*   %arrayidx11.1.36 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %329 = load i16, i16* %arrayidx11.1.36, align 2, !tbaa !3 *)
mov v329 mem0_72;
(*   %sub.1.36 = sub i16 %329, %call.i.1.36 *)
sub v_sub_1_36 v329 v_call_i_1_36;
(*   store i16 %sub.1.36, i16* %arrayidx9.1.36, align 2, !tbaa !3 *)
mov mem0_200 v_sub_1_36;
(*   %add21.1.36 = add i16 %329, %call.i.1.36 *)
add v_add21_1_36 v329 v_call_i_1_36;
(*   store i16 %add21.1.36, i16* %arrayidx11.1.36, align 2, !tbaa !3 *)
mov mem0_72 v_add21_1_36;
(*   %arrayidx9.1.37 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %330 = load i16, i16* %arrayidx9.1.37, align 2, !tbaa !3 *)
mov v330 mem0_202;
(*   %conv1.i.1.37 = sext i16 %330 to i32 *)
cast v_conv1_i_1_37@sint32 v330@sint16;
(*   %mul.i.1.37 = mul nsw i32 %conv1.i.1.37, -359 *)
mul v_mul_i_1_37 v_conv1_i_1_37 (-359)@sint32;
(*   %call.i.1.37 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.37) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_37, v_call_i_1_37);
(*   %arrayidx11.1.37 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %331 = load i16, i16* %arrayidx11.1.37, align 2, !tbaa !3 *)
mov v331 mem0_74;
(*   %sub.1.37 = sub i16 %331, %call.i.1.37 *)
sub v_sub_1_37 v331 v_call_i_1_37;
(*   store i16 %sub.1.37, i16* %arrayidx9.1.37, align 2, !tbaa !3 *)
mov mem0_202 v_sub_1_37;
(*   %add21.1.37 = add i16 %331, %call.i.1.37 *)
add v_add21_1_37 v331 v_call_i_1_37;
(*   store i16 %add21.1.37, i16* %arrayidx11.1.37, align 2, !tbaa !3 *)
mov mem0_74 v_add21_1_37;
(*   %arrayidx9.1.38 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %332 = load i16, i16* %arrayidx9.1.38, align 2, !tbaa !3 *)
mov v332 mem0_204;
(*   %conv1.i.1.38 = sext i16 %332 to i32 *)
cast v_conv1_i_1_38@sint32 v332@sint16;
(*   %mul.i.1.38 = mul nsw i32 %conv1.i.1.38, -359 *)
mul v_mul_i_1_38 v_conv1_i_1_38 (-359)@sint32;
(*   %call.i.1.38 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.38) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_38, v_call_i_1_38);
(*   %arrayidx11.1.38 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %333 = load i16, i16* %arrayidx11.1.38, align 2, !tbaa !3 *)
mov v333 mem0_76;
(*   %sub.1.38 = sub i16 %333, %call.i.1.38 *)
sub v_sub_1_38 v333 v_call_i_1_38;
(*   store i16 %sub.1.38, i16* %arrayidx9.1.38, align 2, !tbaa !3 *)
mov mem0_204 v_sub_1_38;
(*   %add21.1.38 = add i16 %333, %call.i.1.38 *)
add v_add21_1_38 v333 v_call_i_1_38;
(*   store i16 %add21.1.38, i16* %arrayidx11.1.38, align 2, !tbaa !3 *)
mov mem0_76 v_add21_1_38;
(*   %arrayidx9.1.39 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %334 = load i16, i16* %arrayidx9.1.39, align 2, !tbaa !3 *)
mov v334 mem0_206;
(*   %conv1.i.1.39 = sext i16 %334 to i32 *)
cast v_conv1_i_1_39@sint32 v334@sint16;
(*   %mul.i.1.39 = mul nsw i32 %conv1.i.1.39, -359 *)
mul v_mul_i_1_39 v_conv1_i_1_39 (-359)@sint32;
(*   %call.i.1.39 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.39) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_39, v_call_i_1_39);
(*   %arrayidx11.1.39 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %335 = load i16, i16* %arrayidx11.1.39, align 2, !tbaa !3 *)
mov v335 mem0_78;
(*   %sub.1.39 = sub i16 %335, %call.i.1.39 *)
sub v_sub_1_39 v335 v_call_i_1_39;
(*   store i16 %sub.1.39, i16* %arrayidx9.1.39, align 2, !tbaa !3 *)
mov mem0_206 v_sub_1_39;
(*   %add21.1.39 = add i16 %335, %call.i.1.39 *)
add v_add21_1_39 v335 v_call_i_1_39;
(*   store i16 %add21.1.39, i16* %arrayidx11.1.39, align 2, !tbaa !3 *)
mov mem0_78 v_add21_1_39;
(*   %arrayidx9.1.40 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %336 = load i16, i16* %arrayidx9.1.40, align 2, !tbaa !3 *)
mov v336 mem0_208;
(*   %conv1.i.1.40 = sext i16 %336 to i32 *)
cast v_conv1_i_1_40@sint32 v336@sint16;
(*   %mul.i.1.40 = mul nsw i32 %conv1.i.1.40, -359 *)
mul v_mul_i_1_40 v_conv1_i_1_40 (-359)@sint32;
(*   %call.i.1.40 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.40) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_40, v_call_i_1_40);
(*   %arrayidx11.1.40 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %337 = load i16, i16* %arrayidx11.1.40, align 2, !tbaa !3 *)
mov v337 mem0_80;
(*   %sub.1.40 = sub i16 %337, %call.i.1.40 *)
sub v_sub_1_40 v337 v_call_i_1_40;
(*   store i16 %sub.1.40, i16* %arrayidx9.1.40, align 2, !tbaa !3 *)
mov mem0_208 v_sub_1_40;
(*   %add21.1.40 = add i16 %337, %call.i.1.40 *)
add v_add21_1_40 v337 v_call_i_1_40;
(*   store i16 %add21.1.40, i16* %arrayidx11.1.40, align 2, !tbaa !3 *)
mov mem0_80 v_add21_1_40;
(*   %arrayidx9.1.41 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %338 = load i16, i16* %arrayidx9.1.41, align 2, !tbaa !3 *)
mov v338 mem0_210;
(*   %conv1.i.1.41 = sext i16 %338 to i32 *)
cast v_conv1_i_1_41@sint32 v338@sint16;
(*   %mul.i.1.41 = mul nsw i32 %conv1.i.1.41, -359 *)
mul v_mul_i_1_41 v_conv1_i_1_41 (-359)@sint32;
(*   %call.i.1.41 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.41) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_41, v_call_i_1_41);
(*   %arrayidx11.1.41 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %339 = load i16, i16* %arrayidx11.1.41, align 2, !tbaa !3 *)
mov v339 mem0_82;
(*   %sub.1.41 = sub i16 %339, %call.i.1.41 *)
sub v_sub_1_41 v339 v_call_i_1_41;
(*   store i16 %sub.1.41, i16* %arrayidx9.1.41, align 2, !tbaa !3 *)
mov mem0_210 v_sub_1_41;
(*   %add21.1.41 = add i16 %339, %call.i.1.41 *)
add v_add21_1_41 v339 v_call_i_1_41;
(*   store i16 %add21.1.41, i16* %arrayidx11.1.41, align 2, !tbaa !3 *)
mov mem0_82 v_add21_1_41;
(*   %arrayidx9.1.42 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %340 = load i16, i16* %arrayidx9.1.42, align 2, !tbaa !3 *)
mov v340 mem0_212;
(*   %conv1.i.1.42 = sext i16 %340 to i32 *)
cast v_conv1_i_1_42@sint32 v340@sint16;
(*   %mul.i.1.42 = mul nsw i32 %conv1.i.1.42, -359 *)
mul v_mul_i_1_42 v_conv1_i_1_42 (-359)@sint32;
(*   %call.i.1.42 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.42) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_42, v_call_i_1_42);
(*   %arrayidx11.1.42 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %341 = load i16, i16* %arrayidx11.1.42, align 2, !tbaa !3 *)
mov v341 mem0_84;
(*   %sub.1.42 = sub i16 %341, %call.i.1.42 *)
sub v_sub_1_42 v341 v_call_i_1_42;
(*   store i16 %sub.1.42, i16* %arrayidx9.1.42, align 2, !tbaa !3 *)
mov mem0_212 v_sub_1_42;
(*   %add21.1.42 = add i16 %341, %call.i.1.42 *)
add v_add21_1_42 v341 v_call_i_1_42;
(*   store i16 %add21.1.42, i16* %arrayidx11.1.42, align 2, !tbaa !3 *)
mov mem0_84 v_add21_1_42;
(*   %arrayidx9.1.43 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %342 = load i16, i16* %arrayidx9.1.43, align 2, !tbaa !3 *)
mov v342 mem0_214;
(*   %conv1.i.1.43 = sext i16 %342 to i32 *)
cast v_conv1_i_1_43@sint32 v342@sint16;
(*   %mul.i.1.43 = mul nsw i32 %conv1.i.1.43, -359 *)
mul v_mul_i_1_43 v_conv1_i_1_43 (-359)@sint32;
(*   %call.i.1.43 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.43) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_43, v_call_i_1_43);
(*   %arrayidx11.1.43 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %343 = load i16, i16* %arrayidx11.1.43, align 2, !tbaa !3 *)
mov v343 mem0_86;
(*   %sub.1.43 = sub i16 %343, %call.i.1.43 *)
sub v_sub_1_43 v343 v_call_i_1_43;
(*   store i16 %sub.1.43, i16* %arrayidx9.1.43, align 2, !tbaa !3 *)
mov mem0_214 v_sub_1_43;
(*   %add21.1.43 = add i16 %343, %call.i.1.43 *)
add v_add21_1_43 v343 v_call_i_1_43;
(*   store i16 %add21.1.43, i16* %arrayidx11.1.43, align 2, !tbaa !3 *)
mov mem0_86 v_add21_1_43;
(*   %arrayidx9.1.44 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %344 = load i16, i16* %arrayidx9.1.44, align 2, !tbaa !3 *)
mov v344 mem0_216;
(*   %conv1.i.1.44 = sext i16 %344 to i32 *)
cast v_conv1_i_1_44@sint32 v344@sint16;
(*   %mul.i.1.44 = mul nsw i32 %conv1.i.1.44, -359 *)
mul v_mul_i_1_44 v_conv1_i_1_44 (-359)@sint32;
(*   %call.i.1.44 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.44) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_44, v_call_i_1_44);
(*   %arrayidx11.1.44 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %345 = load i16, i16* %arrayidx11.1.44, align 2, !tbaa !3 *)
mov v345 mem0_88;
(*   %sub.1.44 = sub i16 %345, %call.i.1.44 *)
sub v_sub_1_44 v345 v_call_i_1_44;
(*   store i16 %sub.1.44, i16* %arrayidx9.1.44, align 2, !tbaa !3 *)
mov mem0_216 v_sub_1_44;
(*   %add21.1.44 = add i16 %345, %call.i.1.44 *)
add v_add21_1_44 v345 v_call_i_1_44;
(*   store i16 %add21.1.44, i16* %arrayidx11.1.44, align 2, !tbaa !3 *)
mov mem0_88 v_add21_1_44;
(*   %arrayidx9.1.45 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %346 = load i16, i16* %arrayidx9.1.45, align 2, !tbaa !3 *)
mov v346 mem0_218;
(*   %conv1.i.1.45 = sext i16 %346 to i32 *)
cast v_conv1_i_1_45@sint32 v346@sint16;
(*   %mul.i.1.45 = mul nsw i32 %conv1.i.1.45, -359 *)
mul v_mul_i_1_45 v_conv1_i_1_45 (-359)@sint32;
(*   %call.i.1.45 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.45) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_45, v_call_i_1_45);
(*   %arrayidx11.1.45 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %347 = load i16, i16* %arrayidx11.1.45, align 2, !tbaa !3 *)
mov v347 mem0_90;
(*   %sub.1.45 = sub i16 %347, %call.i.1.45 *)
sub v_sub_1_45 v347 v_call_i_1_45;
(*   store i16 %sub.1.45, i16* %arrayidx9.1.45, align 2, !tbaa !3 *)
mov mem0_218 v_sub_1_45;
(*   %add21.1.45 = add i16 %347, %call.i.1.45 *)
add v_add21_1_45 v347 v_call_i_1_45;
(*   store i16 %add21.1.45, i16* %arrayidx11.1.45, align 2, !tbaa !3 *)
mov mem0_90 v_add21_1_45;
(*   %arrayidx9.1.46 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %348 = load i16, i16* %arrayidx9.1.46, align 2, !tbaa !3 *)
mov v348 mem0_220;
(*   %conv1.i.1.46 = sext i16 %348 to i32 *)
cast v_conv1_i_1_46@sint32 v348@sint16;
(*   %mul.i.1.46 = mul nsw i32 %conv1.i.1.46, -359 *)
mul v_mul_i_1_46 v_conv1_i_1_46 (-359)@sint32;
(*   %call.i.1.46 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.46) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_46, v_call_i_1_46);
(*   %arrayidx11.1.46 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %349 = load i16, i16* %arrayidx11.1.46, align 2, !tbaa !3 *)
mov v349 mem0_92;
(*   %sub.1.46 = sub i16 %349, %call.i.1.46 *)
sub v_sub_1_46 v349 v_call_i_1_46;
(*   store i16 %sub.1.46, i16* %arrayidx9.1.46, align 2, !tbaa !3 *)
mov mem0_220 v_sub_1_46;
(*   %add21.1.46 = add i16 %349, %call.i.1.46 *)
add v_add21_1_46 v349 v_call_i_1_46;
(*   store i16 %add21.1.46, i16* %arrayidx11.1.46, align 2, !tbaa !3 *)
mov mem0_92 v_add21_1_46;
(*   %arrayidx9.1.47 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %350 = load i16, i16* %arrayidx9.1.47, align 2, !tbaa !3 *)
mov v350 mem0_222;
(*   %conv1.i.1.47 = sext i16 %350 to i32 *)
cast v_conv1_i_1_47@sint32 v350@sint16;
(*   %mul.i.1.47 = mul nsw i32 %conv1.i.1.47, -359 *)
mul v_mul_i_1_47 v_conv1_i_1_47 (-359)@sint32;
(*   %call.i.1.47 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.47) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_47, v_call_i_1_47);
(*   %arrayidx11.1.47 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %351 = load i16, i16* %arrayidx11.1.47, align 2, !tbaa !3 *)
mov v351 mem0_94;
(*   %sub.1.47 = sub i16 %351, %call.i.1.47 *)
sub v_sub_1_47 v351 v_call_i_1_47;
(*   store i16 %sub.1.47, i16* %arrayidx9.1.47, align 2, !tbaa !3 *)
mov mem0_222 v_sub_1_47;
(*   %add21.1.47 = add i16 %351, %call.i.1.47 *)
add v_add21_1_47 v351 v_call_i_1_47;
(*   store i16 %add21.1.47, i16* %arrayidx11.1.47, align 2, !tbaa !3 *)
mov mem0_94 v_add21_1_47;
(*   %arrayidx9.1.48 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %352 = load i16, i16* %arrayidx9.1.48, align 2, !tbaa !3 *)
mov v352 mem0_224;
(*   %conv1.i.1.48 = sext i16 %352 to i32 *)
cast v_conv1_i_1_48@sint32 v352@sint16;
(*   %mul.i.1.48 = mul nsw i32 %conv1.i.1.48, -359 *)
mul v_mul_i_1_48 v_conv1_i_1_48 (-359)@sint32;
(*   %call.i.1.48 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.48) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_48, v_call_i_1_48);
(*   %arrayidx11.1.48 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %353 = load i16, i16* %arrayidx11.1.48, align 2, !tbaa !3 *)
mov v353 mem0_96;
(*   %sub.1.48 = sub i16 %353, %call.i.1.48 *)
sub v_sub_1_48 v353 v_call_i_1_48;
(*   store i16 %sub.1.48, i16* %arrayidx9.1.48, align 2, !tbaa !3 *)
mov mem0_224 v_sub_1_48;
(*   %add21.1.48 = add i16 %353, %call.i.1.48 *)
add v_add21_1_48 v353 v_call_i_1_48;
(*   store i16 %add21.1.48, i16* %arrayidx11.1.48, align 2, !tbaa !3 *)
mov mem0_96 v_add21_1_48;
(*   %arrayidx9.1.49 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %354 = load i16, i16* %arrayidx9.1.49, align 2, !tbaa !3 *)
mov v354 mem0_226;
(*   %conv1.i.1.49 = sext i16 %354 to i32 *)
cast v_conv1_i_1_49@sint32 v354@sint16;
(*   %mul.i.1.49 = mul nsw i32 %conv1.i.1.49, -359 *)
mul v_mul_i_1_49 v_conv1_i_1_49 (-359)@sint32;
(*   %call.i.1.49 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.49) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_49, v_call_i_1_49);
(*   %arrayidx11.1.49 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %355 = load i16, i16* %arrayidx11.1.49, align 2, !tbaa !3 *)
mov v355 mem0_98;
(*   %sub.1.49 = sub i16 %355, %call.i.1.49 *)
sub v_sub_1_49 v355 v_call_i_1_49;
(*   store i16 %sub.1.49, i16* %arrayidx9.1.49, align 2, !tbaa !3 *)
mov mem0_226 v_sub_1_49;
(*   %add21.1.49 = add i16 %355, %call.i.1.49 *)
add v_add21_1_49 v355 v_call_i_1_49;
(*   store i16 %add21.1.49, i16* %arrayidx11.1.49, align 2, !tbaa !3 *)
mov mem0_98 v_add21_1_49;
(*   %arrayidx9.1.50 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %356 = load i16, i16* %arrayidx9.1.50, align 2, !tbaa !3 *)
mov v356 mem0_228;
(*   %conv1.i.1.50 = sext i16 %356 to i32 *)
cast v_conv1_i_1_50@sint32 v356@sint16;
(*   %mul.i.1.50 = mul nsw i32 %conv1.i.1.50, -359 *)
mul v_mul_i_1_50 v_conv1_i_1_50 (-359)@sint32;
(*   %call.i.1.50 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.50) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_50, v_call_i_1_50);
(*   %arrayidx11.1.50 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %357 = load i16, i16* %arrayidx11.1.50, align 2, !tbaa !3 *)
mov v357 mem0_100;
(*   %sub.1.50 = sub i16 %357, %call.i.1.50 *)
sub v_sub_1_50 v357 v_call_i_1_50;
(*   store i16 %sub.1.50, i16* %arrayidx9.1.50, align 2, !tbaa !3 *)
mov mem0_228 v_sub_1_50;
(*   %add21.1.50 = add i16 %357, %call.i.1.50 *)
add v_add21_1_50 v357 v_call_i_1_50;
(*   store i16 %add21.1.50, i16* %arrayidx11.1.50, align 2, !tbaa !3 *)
mov mem0_100 v_add21_1_50;
(*   %arrayidx9.1.51 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %358 = load i16, i16* %arrayidx9.1.51, align 2, !tbaa !3 *)
mov v358 mem0_230;
(*   %conv1.i.1.51 = sext i16 %358 to i32 *)
cast v_conv1_i_1_51@sint32 v358@sint16;
(*   %mul.i.1.51 = mul nsw i32 %conv1.i.1.51, -359 *)
mul v_mul_i_1_51 v_conv1_i_1_51 (-359)@sint32;
(*   %call.i.1.51 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.51) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_51, v_call_i_1_51);
(*   %arrayidx11.1.51 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %359 = load i16, i16* %arrayidx11.1.51, align 2, !tbaa !3 *)
mov v359 mem0_102;
(*   %sub.1.51 = sub i16 %359, %call.i.1.51 *)
sub v_sub_1_51 v359 v_call_i_1_51;
(*   store i16 %sub.1.51, i16* %arrayidx9.1.51, align 2, !tbaa !3 *)
mov mem0_230 v_sub_1_51;
(*   %add21.1.51 = add i16 %359, %call.i.1.51 *)
add v_add21_1_51 v359 v_call_i_1_51;
(*   store i16 %add21.1.51, i16* %arrayidx11.1.51, align 2, !tbaa !3 *)
mov mem0_102 v_add21_1_51;
(*   %arrayidx9.1.52 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %360 = load i16, i16* %arrayidx9.1.52, align 2, !tbaa !3 *)
mov v360 mem0_232;
(*   %conv1.i.1.52 = sext i16 %360 to i32 *)
cast v_conv1_i_1_52@sint32 v360@sint16;
(*   %mul.i.1.52 = mul nsw i32 %conv1.i.1.52, -359 *)
mul v_mul_i_1_52 v_conv1_i_1_52 (-359)@sint32;
(*   %call.i.1.52 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.52) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_52, v_call_i_1_52);
(*   %arrayidx11.1.52 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %361 = load i16, i16* %arrayidx11.1.52, align 2, !tbaa !3 *)
mov v361 mem0_104;
(*   %sub.1.52 = sub i16 %361, %call.i.1.52 *)
sub v_sub_1_52 v361 v_call_i_1_52;
(*   store i16 %sub.1.52, i16* %arrayidx9.1.52, align 2, !tbaa !3 *)
mov mem0_232 v_sub_1_52;
(*   %add21.1.52 = add i16 %361, %call.i.1.52 *)
add v_add21_1_52 v361 v_call_i_1_52;
(*   store i16 %add21.1.52, i16* %arrayidx11.1.52, align 2, !tbaa !3 *)
mov mem0_104 v_add21_1_52;
(*   %arrayidx9.1.53 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %362 = load i16, i16* %arrayidx9.1.53, align 2, !tbaa !3 *)
mov v362 mem0_234;
(*   %conv1.i.1.53 = sext i16 %362 to i32 *)
cast v_conv1_i_1_53@sint32 v362@sint16;
(*   %mul.i.1.53 = mul nsw i32 %conv1.i.1.53, -359 *)
mul v_mul_i_1_53 v_conv1_i_1_53 (-359)@sint32;
(*   %call.i.1.53 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.53) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_53, v_call_i_1_53);
(*   %arrayidx11.1.53 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %363 = load i16, i16* %arrayidx11.1.53, align 2, !tbaa !3 *)
mov v363 mem0_106;
(*   %sub.1.53 = sub i16 %363, %call.i.1.53 *)
sub v_sub_1_53 v363 v_call_i_1_53;
(*   store i16 %sub.1.53, i16* %arrayidx9.1.53, align 2, !tbaa !3 *)
mov mem0_234 v_sub_1_53;
(*   %add21.1.53 = add i16 %363, %call.i.1.53 *)
add v_add21_1_53 v363 v_call_i_1_53;
(*   store i16 %add21.1.53, i16* %arrayidx11.1.53, align 2, !tbaa !3 *)
mov mem0_106 v_add21_1_53;
(*   %arrayidx9.1.54 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %364 = load i16, i16* %arrayidx9.1.54, align 2, !tbaa !3 *)
mov v364 mem0_236;
(*   %conv1.i.1.54 = sext i16 %364 to i32 *)
cast v_conv1_i_1_54@sint32 v364@sint16;
(*   %mul.i.1.54 = mul nsw i32 %conv1.i.1.54, -359 *)
mul v_mul_i_1_54 v_conv1_i_1_54 (-359)@sint32;
(*   %call.i.1.54 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.54) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_54, v_call_i_1_54);
(*   %arrayidx11.1.54 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %365 = load i16, i16* %arrayidx11.1.54, align 2, !tbaa !3 *)
mov v365 mem0_108;
(*   %sub.1.54 = sub i16 %365, %call.i.1.54 *)
sub v_sub_1_54 v365 v_call_i_1_54;
(*   store i16 %sub.1.54, i16* %arrayidx9.1.54, align 2, !tbaa !3 *)
mov mem0_236 v_sub_1_54;
(*   %add21.1.54 = add i16 %365, %call.i.1.54 *)
add v_add21_1_54 v365 v_call_i_1_54;
(*   store i16 %add21.1.54, i16* %arrayidx11.1.54, align 2, !tbaa !3 *)
mov mem0_108 v_add21_1_54;
(*   %arrayidx9.1.55 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %366 = load i16, i16* %arrayidx9.1.55, align 2, !tbaa !3 *)
mov v366 mem0_238;
(*   %conv1.i.1.55 = sext i16 %366 to i32 *)
cast v_conv1_i_1_55@sint32 v366@sint16;
(*   %mul.i.1.55 = mul nsw i32 %conv1.i.1.55, -359 *)
mul v_mul_i_1_55 v_conv1_i_1_55 (-359)@sint32;
(*   %call.i.1.55 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.55) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_55, v_call_i_1_55);
(*   %arrayidx11.1.55 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %367 = load i16, i16* %arrayidx11.1.55, align 2, !tbaa !3 *)
mov v367 mem0_110;
(*   %sub.1.55 = sub i16 %367, %call.i.1.55 *)
sub v_sub_1_55 v367 v_call_i_1_55;
(*   store i16 %sub.1.55, i16* %arrayidx9.1.55, align 2, !tbaa !3 *)
mov mem0_238 v_sub_1_55;
(*   %add21.1.55 = add i16 %367, %call.i.1.55 *)
add v_add21_1_55 v367 v_call_i_1_55;
(*   store i16 %add21.1.55, i16* %arrayidx11.1.55, align 2, !tbaa !3 *)
mov mem0_110 v_add21_1_55;
(*   %arrayidx9.1.56 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %368 = load i16, i16* %arrayidx9.1.56, align 2, !tbaa !3 *)
mov v368 mem0_240;
(*   %conv1.i.1.56 = sext i16 %368 to i32 *)
cast v_conv1_i_1_56@sint32 v368@sint16;
(*   %mul.i.1.56 = mul nsw i32 %conv1.i.1.56, -359 *)
mul v_mul_i_1_56 v_conv1_i_1_56 (-359)@sint32;
(*   %call.i.1.56 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.56) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_56, v_call_i_1_56);
(*   %arrayidx11.1.56 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %369 = load i16, i16* %arrayidx11.1.56, align 2, !tbaa !3 *)
mov v369 mem0_112;
(*   %sub.1.56 = sub i16 %369, %call.i.1.56 *)
sub v_sub_1_56 v369 v_call_i_1_56;
(*   store i16 %sub.1.56, i16* %arrayidx9.1.56, align 2, !tbaa !3 *)
mov mem0_240 v_sub_1_56;
(*   %add21.1.56 = add i16 %369, %call.i.1.56 *)
add v_add21_1_56 v369 v_call_i_1_56;
(*   store i16 %add21.1.56, i16* %arrayidx11.1.56, align 2, !tbaa !3 *)
mov mem0_112 v_add21_1_56;
(*   %arrayidx9.1.57 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %370 = load i16, i16* %arrayidx9.1.57, align 2, !tbaa !3 *)
mov v370 mem0_242;
(*   %conv1.i.1.57 = sext i16 %370 to i32 *)
cast v_conv1_i_1_57@sint32 v370@sint16;
(*   %mul.i.1.57 = mul nsw i32 %conv1.i.1.57, -359 *)
mul v_mul_i_1_57 v_conv1_i_1_57 (-359)@sint32;
(*   %call.i.1.57 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.57) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_57, v_call_i_1_57);
(*   %arrayidx11.1.57 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %371 = load i16, i16* %arrayidx11.1.57, align 2, !tbaa !3 *)
mov v371 mem0_114;
(*   %sub.1.57 = sub i16 %371, %call.i.1.57 *)
sub v_sub_1_57 v371 v_call_i_1_57;
(*   store i16 %sub.1.57, i16* %arrayidx9.1.57, align 2, !tbaa !3 *)
mov mem0_242 v_sub_1_57;
(*   %add21.1.57 = add i16 %371, %call.i.1.57 *)
add v_add21_1_57 v371 v_call_i_1_57;
(*   store i16 %add21.1.57, i16* %arrayidx11.1.57, align 2, !tbaa !3 *)
mov mem0_114 v_add21_1_57;
(*   %arrayidx9.1.58 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %372 = load i16, i16* %arrayidx9.1.58, align 2, !tbaa !3 *)
mov v372 mem0_244;
(*   %conv1.i.1.58 = sext i16 %372 to i32 *)
cast v_conv1_i_1_58@sint32 v372@sint16;
(*   %mul.i.1.58 = mul nsw i32 %conv1.i.1.58, -359 *)
mul v_mul_i_1_58 v_conv1_i_1_58 (-359)@sint32;
(*   %call.i.1.58 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.58) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_58, v_call_i_1_58);
(*   %arrayidx11.1.58 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %373 = load i16, i16* %arrayidx11.1.58, align 2, !tbaa !3 *)
mov v373 mem0_116;
(*   %sub.1.58 = sub i16 %373, %call.i.1.58 *)
sub v_sub_1_58 v373 v_call_i_1_58;
(*   store i16 %sub.1.58, i16* %arrayidx9.1.58, align 2, !tbaa !3 *)
mov mem0_244 v_sub_1_58;
(*   %add21.1.58 = add i16 %373, %call.i.1.58 *)
add v_add21_1_58 v373 v_call_i_1_58;
(*   store i16 %add21.1.58, i16* %arrayidx11.1.58, align 2, !tbaa !3 *)
mov mem0_116 v_add21_1_58;
(*   %arrayidx9.1.59 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %374 = load i16, i16* %arrayidx9.1.59, align 2, !tbaa !3 *)
mov v374 mem0_246;
(*   %conv1.i.1.59 = sext i16 %374 to i32 *)
cast v_conv1_i_1_59@sint32 v374@sint16;
(*   %mul.i.1.59 = mul nsw i32 %conv1.i.1.59, -359 *)
mul v_mul_i_1_59 v_conv1_i_1_59 (-359)@sint32;
(*   %call.i.1.59 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.59) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_59, v_call_i_1_59);
(*   %arrayidx11.1.59 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %375 = load i16, i16* %arrayidx11.1.59, align 2, !tbaa !3 *)
mov v375 mem0_118;
(*   %sub.1.59 = sub i16 %375, %call.i.1.59 *)
sub v_sub_1_59 v375 v_call_i_1_59;
(*   store i16 %sub.1.59, i16* %arrayidx9.1.59, align 2, !tbaa !3 *)
mov mem0_246 v_sub_1_59;
(*   %add21.1.59 = add i16 %375, %call.i.1.59 *)
add v_add21_1_59 v375 v_call_i_1_59;
(*   store i16 %add21.1.59, i16* %arrayidx11.1.59, align 2, !tbaa !3 *)
mov mem0_118 v_add21_1_59;
(*   %arrayidx9.1.60 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %376 = load i16, i16* %arrayidx9.1.60, align 2, !tbaa !3 *)
mov v376 mem0_248;
(*   %conv1.i.1.60 = sext i16 %376 to i32 *)
cast v_conv1_i_1_60@sint32 v376@sint16;
(*   %mul.i.1.60 = mul nsw i32 %conv1.i.1.60, -359 *)
mul v_mul_i_1_60 v_conv1_i_1_60 (-359)@sint32;
(*   %call.i.1.60 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.60) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_60, v_call_i_1_60);
(*   %arrayidx11.1.60 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %377 = load i16, i16* %arrayidx11.1.60, align 2, !tbaa !3 *)
mov v377 mem0_120;
(*   %sub.1.60 = sub i16 %377, %call.i.1.60 *)
sub v_sub_1_60 v377 v_call_i_1_60;
(*   store i16 %sub.1.60, i16* %arrayidx9.1.60, align 2, !tbaa !3 *)
mov mem0_248 v_sub_1_60;
(*   %add21.1.60 = add i16 %377, %call.i.1.60 *)
add v_add21_1_60 v377 v_call_i_1_60;
(*   store i16 %add21.1.60, i16* %arrayidx11.1.60, align 2, !tbaa !3 *)
mov mem0_120 v_add21_1_60;
(*   %arrayidx9.1.61 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %378 = load i16, i16* %arrayidx9.1.61, align 2, !tbaa !3 *)
mov v378 mem0_250;
(*   %conv1.i.1.61 = sext i16 %378 to i32 *)
cast v_conv1_i_1_61@sint32 v378@sint16;
(*   %mul.i.1.61 = mul nsw i32 %conv1.i.1.61, -359 *)
mul v_mul_i_1_61 v_conv1_i_1_61 (-359)@sint32;
(*   %call.i.1.61 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.61) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_61, v_call_i_1_61);
(*   %arrayidx11.1.61 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %379 = load i16, i16* %arrayidx11.1.61, align 2, !tbaa !3 *)
mov v379 mem0_122;
(*   %sub.1.61 = sub i16 %379, %call.i.1.61 *)
sub v_sub_1_61 v379 v_call_i_1_61;
(*   store i16 %sub.1.61, i16* %arrayidx9.1.61, align 2, !tbaa !3 *)
mov mem0_250 v_sub_1_61;
(*   %add21.1.61 = add i16 %379, %call.i.1.61 *)
add v_add21_1_61 v379 v_call_i_1_61;
(*   store i16 %add21.1.61, i16* %arrayidx11.1.61, align 2, !tbaa !3 *)
mov mem0_122 v_add21_1_61;
(*   %arrayidx9.1.62 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %380 = load i16, i16* %arrayidx9.1.62, align 2, !tbaa !3 *)
mov v380 mem0_252;
(*   %conv1.i.1.62 = sext i16 %380 to i32 *)
cast v_conv1_i_1_62@sint32 v380@sint16;
(*   %mul.i.1.62 = mul nsw i32 %conv1.i.1.62, -359 *)
mul v_mul_i_1_62 v_conv1_i_1_62 (-359)@sint32;
(*   %call.i.1.62 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.62) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_62, v_call_i_1_62);
(*   %arrayidx11.1.62 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %381 = load i16, i16* %arrayidx11.1.62, align 2, !tbaa !3 *)
mov v381 mem0_124;
(*   %sub.1.62 = sub i16 %381, %call.i.1.62 *)
sub v_sub_1_62 v381 v_call_i_1_62;
(*   store i16 %sub.1.62, i16* %arrayidx9.1.62, align 2, !tbaa !3 *)
mov mem0_252 v_sub_1_62;
(*   %add21.1.62 = add i16 %381, %call.i.1.62 *)
add v_add21_1_62 v381 v_call_i_1_62;
(*   store i16 %add21.1.62, i16* %arrayidx11.1.62, align 2, !tbaa !3 *)
mov mem0_124 v_add21_1_62;
(*   %arrayidx9.1.63 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %382 = load i16, i16* %arrayidx9.1.63, align 2, !tbaa !3 *)
mov v382 mem0_254;
(*   %conv1.i.1.63 = sext i16 %382 to i32 *)
cast v_conv1_i_1_63@sint32 v382@sint16;
(*   %mul.i.1.63 = mul nsw i32 %conv1.i.1.63, -359 *)
mul v_mul_i_1_63 v_conv1_i_1_63 (-359)@sint32;
(*   %call.i.1.63 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.63) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_63, v_call_i_1_63);
(*   %arrayidx11.1.63 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %383 = load i16, i16* %arrayidx11.1.63, align 2, !tbaa !3 *)
mov v383 mem0_126;
(*   %sub.1.63 = sub i16 %383, %call.i.1.63 *)
sub v_sub_1_63 v383 v_call_i_1_63;
(*   store i16 %sub.1.63, i16* %arrayidx9.1.63, align 2, !tbaa !3 *)
mov mem0_254 v_sub_1_63;
(*   %add21.1.63 = add i16 %383, %call.i.1.63 *)
add v_add21_1_63 v383 v_call_i_1_63;
(*   store i16 %add21.1.63, i16* %arrayidx11.1.63, align 2, !tbaa !3 *)
mov mem0_126 v_add21_1_63;

(* NOTE: k = 3 *)

(*   %arrayidx9.1.1278 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %384 = load i16, i16* %arrayidx9.1.1278, align 2, !tbaa !3 *)
mov v384 mem0_384;
(*   %conv1.i.1.1279 = sext i16 %384 to i32 *)
cast v_conv1_i_1_1279@sint32 v384@sint16;
(*   %mul.i.1.1280 = mul nsw i32 %conv1.i.1.1279, -1517 *)
mul v_mul_i_1_1280 v_conv1_i_1_1279 (-1517)@sint32;
(*   %call.i.1.1281 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.1280) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_1280, v_call_i_1_1281);
(*   %arrayidx11.1.1282 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %385 = load i16, i16* %arrayidx11.1.1282, align 2, !tbaa !3 *)
mov v385 mem0_256;
(*   %sub.1.1283 = sub i16 %385, %call.i.1.1281 *)
sub v_sub_1_1283 v385 v_call_i_1_1281;
(*   store i16 %sub.1.1283, i16* %arrayidx9.1.1278, align 2, !tbaa !3 *)
mov mem0_384 v_sub_1_1283;
(*   %add21.1.1284 = add i16 %385, %call.i.1.1281 *)
add v_add21_1_1284 v385 v_call_i_1_1281;
(*   store i16 %add21.1.1284, i16* %arrayidx11.1.1282, align 2, !tbaa !3 *)
mov mem0_256 v_add21_1_1284;
(*   %arrayidx9.1.1.1 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %386 = load i16, i16* %arrayidx9.1.1.1, align 2, !tbaa !3 *)
mov v386 mem0_386;
(*   %conv1.i.1.1.1 = sext i16 %386 to i32 *)
cast v_conv1_i_1_1_1@sint32 v386@sint16;
(*   %mul.i.1.1.1 = mul nsw i32 %conv1.i.1.1.1, -1517 *)
mul v_mul_i_1_1_1 v_conv1_i_1_1_1 (-1517)@sint32;
(*   %call.i.1.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_1_1, v_call_i_1_1_1);
(*   %arrayidx11.1.1.1 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %387 = load i16, i16* %arrayidx11.1.1.1, align 2, !tbaa !3 *)
mov v387 mem0_258;
(*   %sub.1.1.1 = sub i16 %387, %call.i.1.1.1 *)
sub v_sub_1_1_1 v387 v_call_i_1_1_1;
(*   store i16 %sub.1.1.1, i16* %arrayidx9.1.1.1, align 2, !tbaa !3 *)
mov mem0_386 v_sub_1_1_1;
(*   %add21.1.1.1 = add i16 %387, %call.i.1.1.1 *)
add v_add21_1_1_1 v387 v_call_i_1_1_1;
(*   store i16 %add21.1.1.1, i16* %arrayidx11.1.1.1, align 2, !tbaa !3 *)
mov mem0_258 v_add21_1_1_1;
(*   %arrayidx9.1.2.1 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %388 = load i16, i16* %arrayidx9.1.2.1, align 2, !tbaa !3 *)
mov v388 mem0_388;
(*   %conv1.i.1.2.1 = sext i16 %388 to i32 *)
cast v_conv1_i_1_2_1@sint32 v388@sint16;
(*   %mul.i.1.2.1 = mul nsw i32 %conv1.i.1.2.1, -1517 *)
mul v_mul_i_1_2_1 v_conv1_i_1_2_1 (-1517)@sint32;
(*   %call.i.1.2.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.2.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_2_1, v_call_i_1_2_1);
(*   %arrayidx11.1.2.1 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %389 = load i16, i16* %arrayidx11.1.2.1, align 2, !tbaa !3 *)
mov v389 mem0_260;
(*   %sub.1.2.1 = sub i16 %389, %call.i.1.2.1 *)
sub v_sub_1_2_1 v389 v_call_i_1_2_1;
(*   store i16 %sub.1.2.1, i16* %arrayidx9.1.2.1, align 2, !tbaa !3 *)
mov mem0_388 v_sub_1_2_1;
(*   %add21.1.2.1 = add i16 %389, %call.i.1.2.1 *)
add v_add21_1_2_1 v389 v_call_i_1_2_1;
(*   store i16 %add21.1.2.1, i16* %arrayidx11.1.2.1, align 2, !tbaa !3 *)
mov mem0_260 v_add21_1_2_1;
(*   %arrayidx9.1.3.1 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %390 = load i16, i16* %arrayidx9.1.3.1, align 2, !tbaa !3 *)
mov v390 mem0_390;
(*   %conv1.i.1.3.1 = sext i16 %390 to i32 *)
cast v_conv1_i_1_3_1@sint32 v390@sint16;
(*   %mul.i.1.3.1 = mul nsw i32 %conv1.i.1.3.1, -1517 *)
mul v_mul_i_1_3_1 v_conv1_i_1_3_1 (-1517)@sint32;
(*   %call.i.1.3.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.3.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_3_1, v_call_i_1_3_1);
(*   %arrayidx11.1.3.1 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %391 = load i16, i16* %arrayidx11.1.3.1, align 2, !tbaa !3 *)
mov v391 mem0_262;
(*   %sub.1.3.1 = sub i16 %391, %call.i.1.3.1 *)
sub v_sub_1_3_1 v391 v_call_i_1_3_1;
(*   store i16 %sub.1.3.1, i16* %arrayidx9.1.3.1, align 2, !tbaa !3 *)
mov mem0_390 v_sub_1_3_1;
(*   %add21.1.3.1 = add i16 %391, %call.i.1.3.1 *)
add v_add21_1_3_1 v391 v_call_i_1_3_1;
(*   store i16 %add21.1.3.1, i16* %arrayidx11.1.3.1, align 2, !tbaa !3 *)
mov mem0_262 v_add21_1_3_1;
(*   %arrayidx9.1.4.1 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %392 = load i16, i16* %arrayidx9.1.4.1, align 2, !tbaa !3 *)
mov v392 mem0_392;
(*   %conv1.i.1.4.1 = sext i16 %392 to i32 *)
cast v_conv1_i_1_4_1@sint32 v392@sint16;
(*   %mul.i.1.4.1 = mul nsw i32 %conv1.i.1.4.1, -1517 *)
mul v_mul_i_1_4_1 v_conv1_i_1_4_1 (-1517)@sint32;
(*   %call.i.1.4.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.4.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_4_1, v_call_i_1_4_1);
(*   %arrayidx11.1.4.1 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %393 = load i16, i16* %arrayidx11.1.4.1, align 2, !tbaa !3 *)
mov v393 mem0_264;
(*   %sub.1.4.1 = sub i16 %393, %call.i.1.4.1 *)
sub v_sub_1_4_1 v393 v_call_i_1_4_1;
(*   store i16 %sub.1.4.1, i16* %arrayidx9.1.4.1, align 2, !tbaa !3 *)
mov mem0_392 v_sub_1_4_1;
(*   %add21.1.4.1 = add i16 %393, %call.i.1.4.1 *)
add v_add21_1_4_1 v393 v_call_i_1_4_1;
(*   store i16 %add21.1.4.1, i16* %arrayidx11.1.4.1, align 2, !tbaa !3 *)
mov mem0_264 v_add21_1_4_1;
(*   %arrayidx9.1.5.1 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %394 = load i16, i16* %arrayidx9.1.5.1, align 2, !tbaa !3 *)
mov v394 mem0_394;
(*   %conv1.i.1.5.1 = sext i16 %394 to i32 *)
cast v_conv1_i_1_5_1@sint32 v394@sint16;
(*   %mul.i.1.5.1 = mul nsw i32 %conv1.i.1.5.1, -1517 *)
mul v_mul_i_1_5_1 v_conv1_i_1_5_1 (-1517)@sint32;
(*   %call.i.1.5.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.5.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_5_1, v_call_i_1_5_1);
(*   %arrayidx11.1.5.1 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %395 = load i16, i16* %arrayidx11.1.5.1, align 2, !tbaa !3 *)
mov v395 mem0_266;
(*   %sub.1.5.1 = sub i16 %395, %call.i.1.5.1 *)
sub v_sub_1_5_1 v395 v_call_i_1_5_1;
(*   store i16 %sub.1.5.1, i16* %arrayidx9.1.5.1, align 2, !tbaa !3 *)
mov mem0_394 v_sub_1_5_1;
(*   %add21.1.5.1 = add i16 %395, %call.i.1.5.1 *)
add v_add21_1_5_1 v395 v_call_i_1_5_1;
(*   store i16 %add21.1.5.1, i16* %arrayidx11.1.5.1, align 2, !tbaa !3 *)
mov mem0_266 v_add21_1_5_1;
(*   %arrayidx9.1.6.1 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %396 = load i16, i16* %arrayidx9.1.6.1, align 2, !tbaa !3 *)
mov v396 mem0_396;
(*   %conv1.i.1.6.1 = sext i16 %396 to i32 *)
cast v_conv1_i_1_6_1@sint32 v396@sint16;
(*   %mul.i.1.6.1 = mul nsw i32 %conv1.i.1.6.1, -1517 *)
mul v_mul_i_1_6_1 v_conv1_i_1_6_1 (-1517)@sint32;
(*   %call.i.1.6.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.6.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_6_1, v_call_i_1_6_1);
(*   %arrayidx11.1.6.1 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %397 = load i16, i16* %arrayidx11.1.6.1, align 2, !tbaa !3 *)
mov v397 mem0_268;
(*   %sub.1.6.1 = sub i16 %397, %call.i.1.6.1 *)
sub v_sub_1_6_1 v397 v_call_i_1_6_1;
(*   store i16 %sub.1.6.1, i16* %arrayidx9.1.6.1, align 2, !tbaa !3 *)
mov mem0_396 v_sub_1_6_1;
(*   %add21.1.6.1 = add i16 %397, %call.i.1.6.1 *)
add v_add21_1_6_1 v397 v_call_i_1_6_1;
(*   store i16 %add21.1.6.1, i16* %arrayidx11.1.6.1, align 2, !tbaa !3 *)
mov mem0_268 v_add21_1_6_1;
(*   %arrayidx9.1.7.1 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %398 = load i16, i16* %arrayidx9.1.7.1, align 2, !tbaa !3 *)
mov v398 mem0_398;
(*   %conv1.i.1.7.1 = sext i16 %398 to i32 *)
cast v_conv1_i_1_7_1@sint32 v398@sint16;
(*   %mul.i.1.7.1 = mul nsw i32 %conv1.i.1.7.1, -1517 *)
mul v_mul_i_1_7_1 v_conv1_i_1_7_1 (-1517)@sint32;
(*   %call.i.1.7.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.7.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_7_1, v_call_i_1_7_1);
(*   %arrayidx11.1.7.1 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %399 = load i16, i16* %arrayidx11.1.7.1, align 2, !tbaa !3 *)
mov v399 mem0_270;
(*   %sub.1.7.1 = sub i16 %399, %call.i.1.7.1 *)
sub v_sub_1_7_1 v399 v_call_i_1_7_1;
(*   store i16 %sub.1.7.1, i16* %arrayidx9.1.7.1, align 2, !tbaa !3 *)
mov mem0_398 v_sub_1_7_1;
(*   %add21.1.7.1 = add i16 %399, %call.i.1.7.1 *)
add v_add21_1_7_1 v399 v_call_i_1_7_1;
(*   store i16 %add21.1.7.1, i16* %arrayidx11.1.7.1, align 2, !tbaa !3 *)
mov mem0_270 v_add21_1_7_1;
(*   %arrayidx9.1.8.1 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %400 = load i16, i16* %arrayidx9.1.8.1, align 2, !tbaa !3 *)
mov v400 mem0_400;
(*   %conv1.i.1.8.1 = sext i16 %400 to i32 *)
cast v_conv1_i_1_8_1@sint32 v400@sint16;
(*   %mul.i.1.8.1 = mul nsw i32 %conv1.i.1.8.1, -1517 *)
mul v_mul_i_1_8_1 v_conv1_i_1_8_1 (-1517)@sint32;
(*   %call.i.1.8.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.8.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_8_1, v_call_i_1_8_1);
(*   %arrayidx11.1.8.1 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %401 = load i16, i16* %arrayidx11.1.8.1, align 2, !tbaa !3 *)
mov v401 mem0_272;
(*   %sub.1.8.1 = sub i16 %401, %call.i.1.8.1 *)
sub v_sub_1_8_1 v401 v_call_i_1_8_1;
(*   store i16 %sub.1.8.1, i16* %arrayidx9.1.8.1, align 2, !tbaa !3 *)
mov mem0_400 v_sub_1_8_1;
(*   %add21.1.8.1 = add i16 %401, %call.i.1.8.1 *)
add v_add21_1_8_1 v401 v_call_i_1_8_1;
(*   store i16 %add21.1.8.1, i16* %arrayidx11.1.8.1, align 2, !tbaa !3 *)
mov mem0_272 v_add21_1_8_1;
(*   %arrayidx9.1.9.1 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %402 = load i16, i16* %arrayidx9.1.9.1, align 2, !tbaa !3 *)
mov v402 mem0_402;
(*   %conv1.i.1.9.1 = sext i16 %402 to i32 *)
cast v_conv1_i_1_9_1@sint32 v402@sint16;
(*   %mul.i.1.9.1 = mul nsw i32 %conv1.i.1.9.1, -1517 *)
mul v_mul_i_1_9_1 v_conv1_i_1_9_1 (-1517)@sint32;
(*   %call.i.1.9.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.9.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_9_1, v_call_i_1_9_1);
(*   %arrayidx11.1.9.1 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %403 = load i16, i16* %arrayidx11.1.9.1, align 2, !tbaa !3 *)
mov v403 mem0_274;
(*   %sub.1.9.1 = sub i16 %403, %call.i.1.9.1 *)
sub v_sub_1_9_1 v403 v_call_i_1_9_1;
(*   store i16 %sub.1.9.1, i16* %arrayidx9.1.9.1, align 2, !tbaa !3 *)
mov mem0_402 v_sub_1_9_1;
(*   %add21.1.9.1 = add i16 %403, %call.i.1.9.1 *)
add v_add21_1_9_1 v403 v_call_i_1_9_1;
(*   store i16 %add21.1.9.1, i16* %arrayidx11.1.9.1, align 2, !tbaa !3 *)
mov mem0_274 v_add21_1_9_1;
(*   %arrayidx9.1.10.1 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %404 = load i16, i16* %arrayidx9.1.10.1, align 2, !tbaa !3 *)
mov v404 mem0_404;
(*   %conv1.i.1.10.1 = sext i16 %404 to i32 *)
cast v_conv1_i_1_10_1@sint32 v404@sint16;
(*   %mul.i.1.10.1 = mul nsw i32 %conv1.i.1.10.1, -1517 *)
mul v_mul_i_1_10_1 v_conv1_i_1_10_1 (-1517)@sint32;
(*   %call.i.1.10.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.10.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_10_1, v_call_i_1_10_1);
(*   %arrayidx11.1.10.1 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %405 = load i16, i16* %arrayidx11.1.10.1, align 2, !tbaa !3 *)
mov v405 mem0_276;
(*   %sub.1.10.1 = sub i16 %405, %call.i.1.10.1 *)
sub v_sub_1_10_1 v405 v_call_i_1_10_1;
(*   store i16 %sub.1.10.1, i16* %arrayidx9.1.10.1, align 2, !tbaa !3 *)
mov mem0_404 v_sub_1_10_1;
(*   %add21.1.10.1 = add i16 %405, %call.i.1.10.1 *)
add v_add21_1_10_1 v405 v_call_i_1_10_1;
(*   store i16 %add21.1.10.1, i16* %arrayidx11.1.10.1, align 2, !tbaa !3 *)
mov mem0_276 v_add21_1_10_1;
(*   %arrayidx9.1.11.1 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %406 = load i16, i16* %arrayidx9.1.11.1, align 2, !tbaa !3 *)
mov v406 mem0_406;
(*   %conv1.i.1.11.1 = sext i16 %406 to i32 *)
cast v_conv1_i_1_11_1@sint32 v406@sint16;
(*   %mul.i.1.11.1 = mul nsw i32 %conv1.i.1.11.1, -1517 *)
mul v_mul_i_1_11_1 v_conv1_i_1_11_1 (-1517)@sint32;
(*   %call.i.1.11.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.11.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_11_1, v_call_i_1_11_1);
(*   %arrayidx11.1.11.1 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %407 = load i16, i16* %arrayidx11.1.11.1, align 2, !tbaa !3 *)
mov v407 mem0_278;
(*   %sub.1.11.1 = sub i16 %407, %call.i.1.11.1 *)
sub v_sub_1_11_1 v407 v_call_i_1_11_1;
(*   store i16 %sub.1.11.1, i16* %arrayidx9.1.11.1, align 2, !tbaa !3 *)
mov mem0_406 v_sub_1_11_1;
(*   %add21.1.11.1 = add i16 %407, %call.i.1.11.1 *)
add v_add21_1_11_1 v407 v_call_i_1_11_1;
(*   store i16 %add21.1.11.1, i16* %arrayidx11.1.11.1, align 2, !tbaa !3 *)
mov mem0_278 v_add21_1_11_1;
(*   %arrayidx9.1.12.1 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %408 = load i16, i16* %arrayidx9.1.12.1, align 2, !tbaa !3 *)
mov v408 mem0_408;
(*   %conv1.i.1.12.1 = sext i16 %408 to i32 *)
cast v_conv1_i_1_12_1@sint32 v408@sint16;
(*   %mul.i.1.12.1 = mul nsw i32 %conv1.i.1.12.1, -1517 *)
mul v_mul_i_1_12_1 v_conv1_i_1_12_1 (-1517)@sint32;
(*   %call.i.1.12.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.12.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_12_1, v_call_i_1_12_1);
(*   %arrayidx11.1.12.1 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %409 = load i16, i16* %arrayidx11.1.12.1, align 2, !tbaa !3 *)
mov v409 mem0_280;
(*   %sub.1.12.1 = sub i16 %409, %call.i.1.12.1 *)
sub v_sub_1_12_1 v409 v_call_i_1_12_1;
(*   store i16 %sub.1.12.1, i16* %arrayidx9.1.12.1, align 2, !tbaa !3 *)
mov mem0_408 v_sub_1_12_1;
(*   %add21.1.12.1 = add i16 %409, %call.i.1.12.1 *)
add v_add21_1_12_1 v409 v_call_i_1_12_1;
(*   store i16 %add21.1.12.1, i16* %arrayidx11.1.12.1, align 2, !tbaa !3 *)
mov mem0_280 v_add21_1_12_1;
(*   %arrayidx9.1.13.1 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %410 = load i16, i16* %arrayidx9.1.13.1, align 2, !tbaa !3 *)
mov v410 mem0_410;
(*   %conv1.i.1.13.1 = sext i16 %410 to i32 *)
cast v_conv1_i_1_13_1@sint32 v410@sint16;
(*   %mul.i.1.13.1 = mul nsw i32 %conv1.i.1.13.1, -1517 *)
mul v_mul_i_1_13_1 v_conv1_i_1_13_1 (-1517)@sint32;
(*   %call.i.1.13.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.13.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_13_1, v_call_i_1_13_1);
(*   %arrayidx11.1.13.1 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %411 = load i16, i16* %arrayidx11.1.13.1, align 2, !tbaa !3 *)
mov v411 mem0_282;
(*   %sub.1.13.1 = sub i16 %411, %call.i.1.13.1 *)
sub v_sub_1_13_1 v411 v_call_i_1_13_1;
(*   store i16 %sub.1.13.1, i16* %arrayidx9.1.13.1, align 2, !tbaa !3 *)
mov mem0_410 v_sub_1_13_1;
(*   %add21.1.13.1 = add i16 %411, %call.i.1.13.1 *)
add v_add21_1_13_1 v411 v_call_i_1_13_1;
(*   store i16 %add21.1.13.1, i16* %arrayidx11.1.13.1, align 2, !tbaa !3 *)
mov mem0_282 v_add21_1_13_1;
(*   %arrayidx9.1.14.1 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %412 = load i16, i16* %arrayidx9.1.14.1, align 2, !tbaa !3 *)
mov v412 mem0_412;
(*   %conv1.i.1.14.1 = sext i16 %412 to i32 *)
cast v_conv1_i_1_14_1@sint32 v412@sint16;
(*   %mul.i.1.14.1 = mul nsw i32 %conv1.i.1.14.1, -1517 *)
mul v_mul_i_1_14_1 v_conv1_i_1_14_1 (-1517)@sint32;
(*   %call.i.1.14.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.14.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_14_1, v_call_i_1_14_1);
(*   %arrayidx11.1.14.1 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %413 = load i16, i16* %arrayidx11.1.14.1, align 2, !tbaa !3 *)
mov v413 mem0_284;
(*   %sub.1.14.1 = sub i16 %413, %call.i.1.14.1 *)
sub v_sub_1_14_1 v413 v_call_i_1_14_1;
(*   store i16 %sub.1.14.1, i16* %arrayidx9.1.14.1, align 2, !tbaa !3 *)
mov mem0_412 v_sub_1_14_1;
(*   %add21.1.14.1 = add i16 %413, %call.i.1.14.1 *)
add v_add21_1_14_1 v413 v_call_i_1_14_1;
(*   store i16 %add21.1.14.1, i16* %arrayidx11.1.14.1, align 2, !tbaa !3 *)
mov mem0_284 v_add21_1_14_1;
(*   %arrayidx9.1.15.1 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %414 = load i16, i16* %arrayidx9.1.15.1, align 2, !tbaa !3 *)
mov v414 mem0_414;
(*   %conv1.i.1.15.1 = sext i16 %414 to i32 *)
cast v_conv1_i_1_15_1@sint32 v414@sint16;
(*   %mul.i.1.15.1 = mul nsw i32 %conv1.i.1.15.1, -1517 *)
mul v_mul_i_1_15_1 v_conv1_i_1_15_1 (-1517)@sint32;
(*   %call.i.1.15.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.15.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_15_1, v_call_i_1_15_1);
(*   %arrayidx11.1.15.1 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %415 = load i16, i16* %arrayidx11.1.15.1, align 2, !tbaa !3 *)
mov v415 mem0_286;
(*   %sub.1.15.1 = sub i16 %415, %call.i.1.15.1 *)
sub v_sub_1_15_1 v415 v_call_i_1_15_1;
(*   store i16 %sub.1.15.1, i16* %arrayidx9.1.15.1, align 2, !tbaa !3 *)
mov mem0_414 v_sub_1_15_1;
(*   %add21.1.15.1 = add i16 %415, %call.i.1.15.1 *)
add v_add21_1_15_1 v415 v_call_i_1_15_1;
(*   store i16 %add21.1.15.1, i16* %arrayidx11.1.15.1, align 2, !tbaa !3 *)
mov mem0_286 v_add21_1_15_1;
(*   %arrayidx9.1.16.1 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %416 = load i16, i16* %arrayidx9.1.16.1, align 2, !tbaa !3 *)
mov v416 mem0_416;
(*   %conv1.i.1.16.1 = sext i16 %416 to i32 *)
cast v_conv1_i_1_16_1@sint32 v416@sint16;
(*   %mul.i.1.16.1 = mul nsw i32 %conv1.i.1.16.1, -1517 *)
mul v_mul_i_1_16_1 v_conv1_i_1_16_1 (-1517)@sint32;
(*   %call.i.1.16.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.16.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_16_1, v_call_i_1_16_1);
(*   %arrayidx11.1.16.1 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %417 = load i16, i16* %arrayidx11.1.16.1, align 2, !tbaa !3 *)
mov v417 mem0_288;
(*   %sub.1.16.1 = sub i16 %417, %call.i.1.16.1 *)
sub v_sub_1_16_1 v417 v_call_i_1_16_1;
(*   store i16 %sub.1.16.1, i16* %arrayidx9.1.16.1, align 2, !tbaa !3 *)
mov mem0_416 v_sub_1_16_1;
(*   %add21.1.16.1 = add i16 %417, %call.i.1.16.1 *)
add v_add21_1_16_1 v417 v_call_i_1_16_1;
(*   store i16 %add21.1.16.1, i16* %arrayidx11.1.16.1, align 2, !tbaa !3 *)
mov mem0_288 v_add21_1_16_1;
(*   %arrayidx9.1.17.1 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %418 = load i16, i16* %arrayidx9.1.17.1, align 2, !tbaa !3 *)
mov v418 mem0_418;
(*   %conv1.i.1.17.1 = sext i16 %418 to i32 *)
cast v_conv1_i_1_17_1@sint32 v418@sint16;
(*   %mul.i.1.17.1 = mul nsw i32 %conv1.i.1.17.1, -1517 *)
mul v_mul_i_1_17_1 v_conv1_i_1_17_1 (-1517)@sint32;
(*   %call.i.1.17.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.17.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_17_1, v_call_i_1_17_1);
(*   %arrayidx11.1.17.1 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %419 = load i16, i16* %arrayidx11.1.17.1, align 2, !tbaa !3 *)
mov v419 mem0_290;
(*   %sub.1.17.1 = sub i16 %419, %call.i.1.17.1 *)
sub v_sub_1_17_1 v419 v_call_i_1_17_1;
(*   store i16 %sub.1.17.1, i16* %arrayidx9.1.17.1, align 2, !tbaa !3 *)
mov mem0_418 v_sub_1_17_1;
(*   %add21.1.17.1 = add i16 %419, %call.i.1.17.1 *)
add v_add21_1_17_1 v419 v_call_i_1_17_1;
(*   store i16 %add21.1.17.1, i16* %arrayidx11.1.17.1, align 2, !tbaa !3 *)
mov mem0_290 v_add21_1_17_1;
(*   %arrayidx9.1.18.1 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %420 = load i16, i16* %arrayidx9.1.18.1, align 2, !tbaa !3 *)
mov v420 mem0_420;
(*   %conv1.i.1.18.1 = sext i16 %420 to i32 *)
cast v_conv1_i_1_18_1@sint32 v420@sint16;
(*   %mul.i.1.18.1 = mul nsw i32 %conv1.i.1.18.1, -1517 *)
mul v_mul_i_1_18_1 v_conv1_i_1_18_1 (-1517)@sint32;
(*   %call.i.1.18.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.18.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_18_1, v_call_i_1_18_1);
(*   %arrayidx11.1.18.1 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %421 = load i16, i16* %arrayidx11.1.18.1, align 2, !tbaa !3 *)
mov v421 mem0_292;
(*   %sub.1.18.1 = sub i16 %421, %call.i.1.18.1 *)
sub v_sub_1_18_1 v421 v_call_i_1_18_1;
(*   store i16 %sub.1.18.1, i16* %arrayidx9.1.18.1, align 2, !tbaa !3 *)
mov mem0_420 v_sub_1_18_1;
(*   %add21.1.18.1 = add i16 %421, %call.i.1.18.1 *)
add v_add21_1_18_1 v421 v_call_i_1_18_1;
(*   store i16 %add21.1.18.1, i16* %arrayidx11.1.18.1, align 2, !tbaa !3 *)
mov mem0_292 v_add21_1_18_1;
(*   %arrayidx9.1.19.1 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %422 = load i16, i16* %arrayidx9.1.19.1, align 2, !tbaa !3 *)
mov v422 mem0_422;
(*   %conv1.i.1.19.1 = sext i16 %422 to i32 *)
cast v_conv1_i_1_19_1@sint32 v422@sint16;
(*   %mul.i.1.19.1 = mul nsw i32 %conv1.i.1.19.1, -1517 *)
mul v_mul_i_1_19_1 v_conv1_i_1_19_1 (-1517)@sint32;
(*   %call.i.1.19.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.19.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_19_1, v_call_i_1_19_1);
(*   %arrayidx11.1.19.1 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %423 = load i16, i16* %arrayidx11.1.19.1, align 2, !tbaa !3 *)
mov v423 mem0_294;
(*   %sub.1.19.1 = sub i16 %423, %call.i.1.19.1 *)
sub v_sub_1_19_1 v423 v_call_i_1_19_1;
(*   store i16 %sub.1.19.1, i16* %arrayidx9.1.19.1, align 2, !tbaa !3 *)
mov mem0_422 v_sub_1_19_1;
(*   %add21.1.19.1 = add i16 %423, %call.i.1.19.1 *)
add v_add21_1_19_1 v423 v_call_i_1_19_1;
(*   store i16 %add21.1.19.1, i16* %arrayidx11.1.19.1, align 2, !tbaa !3 *)
mov mem0_294 v_add21_1_19_1;
(*   %arrayidx9.1.20.1 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %424 = load i16, i16* %arrayidx9.1.20.1, align 2, !tbaa !3 *)
mov v424 mem0_424;
(*   %conv1.i.1.20.1 = sext i16 %424 to i32 *)
cast v_conv1_i_1_20_1@sint32 v424@sint16;
(*   %mul.i.1.20.1 = mul nsw i32 %conv1.i.1.20.1, -1517 *)
mul v_mul_i_1_20_1 v_conv1_i_1_20_1 (-1517)@sint32;
(*   %call.i.1.20.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.20.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_20_1, v_call_i_1_20_1);
(*   %arrayidx11.1.20.1 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %425 = load i16, i16* %arrayidx11.1.20.1, align 2, !tbaa !3 *)
mov v425 mem0_296;
(*   %sub.1.20.1 = sub i16 %425, %call.i.1.20.1 *)
sub v_sub_1_20_1 v425 v_call_i_1_20_1;
(*   store i16 %sub.1.20.1, i16* %arrayidx9.1.20.1, align 2, !tbaa !3 *)
mov mem0_424 v_sub_1_20_1;
(*   %add21.1.20.1 = add i16 %425, %call.i.1.20.1 *)
add v_add21_1_20_1 v425 v_call_i_1_20_1;
(*   store i16 %add21.1.20.1, i16* %arrayidx11.1.20.1, align 2, !tbaa !3 *)
mov mem0_296 v_add21_1_20_1;
(*   %arrayidx9.1.21.1 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %426 = load i16, i16* %arrayidx9.1.21.1, align 2, !tbaa !3 *)
mov v426 mem0_426;
(*   %conv1.i.1.21.1 = sext i16 %426 to i32 *)
cast v_conv1_i_1_21_1@sint32 v426@sint16;
(*   %mul.i.1.21.1 = mul nsw i32 %conv1.i.1.21.1, -1517 *)
mul v_mul_i_1_21_1 v_conv1_i_1_21_1 (-1517)@sint32;
(*   %call.i.1.21.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.21.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_21_1, v_call_i_1_21_1);
(*   %arrayidx11.1.21.1 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %427 = load i16, i16* %arrayidx11.1.21.1, align 2, !tbaa !3 *)
mov v427 mem0_298;
(*   %sub.1.21.1 = sub i16 %427, %call.i.1.21.1 *)
sub v_sub_1_21_1 v427 v_call_i_1_21_1;
(*   store i16 %sub.1.21.1, i16* %arrayidx9.1.21.1, align 2, !tbaa !3 *)
mov mem0_426 v_sub_1_21_1;
(*   %add21.1.21.1 = add i16 %427, %call.i.1.21.1 *)
add v_add21_1_21_1 v427 v_call_i_1_21_1;
(*   store i16 %add21.1.21.1, i16* %arrayidx11.1.21.1, align 2, !tbaa !3 *)
mov mem0_298 v_add21_1_21_1;
(*   %arrayidx9.1.22.1 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %428 = load i16, i16* %arrayidx9.1.22.1, align 2, !tbaa !3 *)
mov v428 mem0_428;
(*   %conv1.i.1.22.1 = sext i16 %428 to i32 *)
cast v_conv1_i_1_22_1@sint32 v428@sint16;
(*   %mul.i.1.22.1 = mul nsw i32 %conv1.i.1.22.1, -1517 *)
mul v_mul_i_1_22_1 v_conv1_i_1_22_1 (-1517)@sint32;
(*   %call.i.1.22.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.22.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_22_1, v_call_i_1_22_1);
(*   %arrayidx11.1.22.1 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %429 = load i16, i16* %arrayidx11.1.22.1, align 2, !tbaa !3 *)
mov v429 mem0_300;
(*   %sub.1.22.1 = sub i16 %429, %call.i.1.22.1 *)
sub v_sub_1_22_1 v429 v_call_i_1_22_1;
(*   store i16 %sub.1.22.1, i16* %arrayidx9.1.22.1, align 2, !tbaa !3 *)
mov mem0_428 v_sub_1_22_1;
(*   %add21.1.22.1 = add i16 %429, %call.i.1.22.1 *)
add v_add21_1_22_1 v429 v_call_i_1_22_1;
(*   store i16 %add21.1.22.1, i16* %arrayidx11.1.22.1, align 2, !tbaa !3 *)
mov mem0_300 v_add21_1_22_1;
(*   %arrayidx9.1.23.1 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %430 = load i16, i16* %arrayidx9.1.23.1, align 2, !tbaa !3 *)
mov v430 mem0_430;
(*   %conv1.i.1.23.1 = sext i16 %430 to i32 *)
cast v_conv1_i_1_23_1@sint32 v430@sint16;
(*   %mul.i.1.23.1 = mul nsw i32 %conv1.i.1.23.1, -1517 *)
mul v_mul_i_1_23_1 v_conv1_i_1_23_1 (-1517)@sint32;
(*   %call.i.1.23.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.23.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_23_1, v_call_i_1_23_1);
(*   %arrayidx11.1.23.1 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %431 = load i16, i16* %arrayidx11.1.23.1, align 2, !tbaa !3 *)
mov v431 mem0_302;
(*   %sub.1.23.1 = sub i16 %431, %call.i.1.23.1 *)
sub v_sub_1_23_1 v431 v_call_i_1_23_1;
(*   store i16 %sub.1.23.1, i16* %arrayidx9.1.23.1, align 2, !tbaa !3 *)
mov mem0_430 v_sub_1_23_1;
(*   %add21.1.23.1 = add i16 %431, %call.i.1.23.1 *)
add v_add21_1_23_1 v431 v_call_i_1_23_1;
(*   store i16 %add21.1.23.1, i16* %arrayidx11.1.23.1, align 2, !tbaa !3 *)
mov mem0_302 v_add21_1_23_1;
(*   %arrayidx9.1.24.1 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %432 = load i16, i16* %arrayidx9.1.24.1, align 2, !tbaa !3 *)
mov v432 mem0_432;
(*   %conv1.i.1.24.1 = sext i16 %432 to i32 *)
cast v_conv1_i_1_24_1@sint32 v432@sint16;
(*   %mul.i.1.24.1 = mul nsw i32 %conv1.i.1.24.1, -1517 *)
mul v_mul_i_1_24_1 v_conv1_i_1_24_1 (-1517)@sint32;
(*   %call.i.1.24.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.24.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_24_1, v_call_i_1_24_1);
(*   %arrayidx11.1.24.1 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %433 = load i16, i16* %arrayidx11.1.24.1, align 2, !tbaa !3 *)
mov v433 mem0_304;
(*   %sub.1.24.1 = sub i16 %433, %call.i.1.24.1 *)
sub v_sub_1_24_1 v433 v_call_i_1_24_1;
(*   store i16 %sub.1.24.1, i16* %arrayidx9.1.24.1, align 2, !tbaa !3 *)
mov mem0_432 v_sub_1_24_1;
(*   %add21.1.24.1 = add i16 %433, %call.i.1.24.1 *)
add v_add21_1_24_1 v433 v_call_i_1_24_1;
(*   store i16 %add21.1.24.1, i16* %arrayidx11.1.24.1, align 2, !tbaa !3 *)
mov mem0_304 v_add21_1_24_1;
(*   %arrayidx9.1.25.1 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %434 = load i16, i16* %arrayidx9.1.25.1, align 2, !tbaa !3 *)
mov v434 mem0_434;
(*   %conv1.i.1.25.1 = sext i16 %434 to i32 *)
cast v_conv1_i_1_25_1@sint32 v434@sint16;
(*   %mul.i.1.25.1 = mul nsw i32 %conv1.i.1.25.1, -1517 *)
mul v_mul_i_1_25_1 v_conv1_i_1_25_1 (-1517)@sint32;
(*   %call.i.1.25.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.25.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_25_1, v_call_i_1_25_1);
(*   %arrayidx11.1.25.1 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %435 = load i16, i16* %arrayidx11.1.25.1, align 2, !tbaa !3 *)
mov v435 mem0_306;
(*   %sub.1.25.1 = sub i16 %435, %call.i.1.25.1 *)
sub v_sub_1_25_1 v435 v_call_i_1_25_1;
(*   store i16 %sub.1.25.1, i16* %arrayidx9.1.25.1, align 2, !tbaa !3 *)
mov mem0_434 v_sub_1_25_1;
(*   %add21.1.25.1 = add i16 %435, %call.i.1.25.1 *)
add v_add21_1_25_1 v435 v_call_i_1_25_1;
(*   store i16 %add21.1.25.1, i16* %arrayidx11.1.25.1, align 2, !tbaa !3 *)
mov mem0_306 v_add21_1_25_1;
(*   %arrayidx9.1.26.1 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %436 = load i16, i16* %arrayidx9.1.26.1, align 2, !tbaa !3 *)
mov v436 mem0_436;
(*   %conv1.i.1.26.1 = sext i16 %436 to i32 *)
cast v_conv1_i_1_26_1@sint32 v436@sint16;
(*   %mul.i.1.26.1 = mul nsw i32 %conv1.i.1.26.1, -1517 *)
mul v_mul_i_1_26_1 v_conv1_i_1_26_1 (-1517)@sint32;
(*   %call.i.1.26.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.26.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_26_1, v_call_i_1_26_1);
(*   %arrayidx11.1.26.1 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %437 = load i16, i16* %arrayidx11.1.26.1, align 2, !tbaa !3 *)
mov v437 mem0_308;
(*   %sub.1.26.1 = sub i16 %437, %call.i.1.26.1 *)
sub v_sub_1_26_1 v437 v_call_i_1_26_1;
(*   store i16 %sub.1.26.1, i16* %arrayidx9.1.26.1, align 2, !tbaa !3 *)
mov mem0_436 v_sub_1_26_1;
(*   %add21.1.26.1 = add i16 %437, %call.i.1.26.1 *)
add v_add21_1_26_1 v437 v_call_i_1_26_1;
(*   store i16 %add21.1.26.1, i16* %arrayidx11.1.26.1, align 2, !tbaa !3 *)
mov mem0_308 v_add21_1_26_1;
(*   %arrayidx9.1.27.1 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %438 = load i16, i16* %arrayidx9.1.27.1, align 2, !tbaa !3 *)
mov v438 mem0_438;
(*   %conv1.i.1.27.1 = sext i16 %438 to i32 *)
cast v_conv1_i_1_27_1@sint32 v438@sint16;
(*   %mul.i.1.27.1 = mul nsw i32 %conv1.i.1.27.1, -1517 *)
mul v_mul_i_1_27_1 v_conv1_i_1_27_1 (-1517)@sint32;
(*   %call.i.1.27.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.27.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_27_1, v_call_i_1_27_1);
(*   %arrayidx11.1.27.1 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %439 = load i16, i16* %arrayidx11.1.27.1, align 2, !tbaa !3 *)
mov v439 mem0_310;
(*   %sub.1.27.1 = sub i16 %439, %call.i.1.27.1 *)
sub v_sub_1_27_1 v439 v_call_i_1_27_1;
(*   store i16 %sub.1.27.1, i16* %arrayidx9.1.27.1, align 2, !tbaa !3 *)
mov mem0_438 v_sub_1_27_1;
(*   %add21.1.27.1 = add i16 %439, %call.i.1.27.1 *)
add v_add21_1_27_1 v439 v_call_i_1_27_1;
(*   store i16 %add21.1.27.1, i16* %arrayidx11.1.27.1, align 2, !tbaa !3 *)
mov mem0_310 v_add21_1_27_1;
(*   %arrayidx9.1.28.1 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %440 = load i16, i16* %arrayidx9.1.28.1, align 2, !tbaa !3 *)
mov v440 mem0_440;
(*   %conv1.i.1.28.1 = sext i16 %440 to i32 *)
cast v_conv1_i_1_28_1@sint32 v440@sint16;
(*   %mul.i.1.28.1 = mul nsw i32 %conv1.i.1.28.1, -1517 *)
mul v_mul_i_1_28_1 v_conv1_i_1_28_1 (-1517)@sint32;
(*   %call.i.1.28.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.28.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_28_1, v_call_i_1_28_1);
(*   %arrayidx11.1.28.1 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %441 = load i16, i16* %arrayidx11.1.28.1, align 2, !tbaa !3 *)
mov v441 mem0_312;
(*   %sub.1.28.1 = sub i16 %441, %call.i.1.28.1 *)
sub v_sub_1_28_1 v441 v_call_i_1_28_1;
(*   store i16 %sub.1.28.1, i16* %arrayidx9.1.28.1, align 2, !tbaa !3 *)
mov mem0_440 v_sub_1_28_1;
(*   %add21.1.28.1 = add i16 %441, %call.i.1.28.1 *)
add v_add21_1_28_1 v441 v_call_i_1_28_1;
(*   store i16 %add21.1.28.1, i16* %arrayidx11.1.28.1, align 2, !tbaa !3 *)
mov mem0_312 v_add21_1_28_1;
(*   %arrayidx9.1.29.1 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %442 = load i16, i16* %arrayidx9.1.29.1, align 2, !tbaa !3 *)
mov v442 mem0_442;
(*   %conv1.i.1.29.1 = sext i16 %442 to i32 *)
cast v_conv1_i_1_29_1@sint32 v442@sint16;
(*   %mul.i.1.29.1 = mul nsw i32 %conv1.i.1.29.1, -1517 *)
mul v_mul_i_1_29_1 v_conv1_i_1_29_1 (-1517)@sint32;
(*   %call.i.1.29.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.29.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_29_1, v_call_i_1_29_1);
(*   %arrayidx11.1.29.1 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %443 = load i16, i16* %arrayidx11.1.29.1, align 2, !tbaa !3 *)
mov v443 mem0_314;
(*   %sub.1.29.1 = sub i16 %443, %call.i.1.29.1 *)
sub v_sub_1_29_1 v443 v_call_i_1_29_1;
(*   store i16 %sub.1.29.1, i16* %arrayidx9.1.29.1, align 2, !tbaa !3 *)
mov mem0_442 v_sub_1_29_1;
(*   %add21.1.29.1 = add i16 %443, %call.i.1.29.1 *)
add v_add21_1_29_1 v443 v_call_i_1_29_1;
(*   store i16 %add21.1.29.1, i16* %arrayidx11.1.29.1, align 2, !tbaa !3 *)
mov mem0_314 v_add21_1_29_1;
(*   %arrayidx9.1.30.1 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %444 = load i16, i16* %arrayidx9.1.30.1, align 2, !tbaa !3 *)
mov v444 mem0_444;
(*   %conv1.i.1.30.1 = sext i16 %444 to i32 *)
cast v_conv1_i_1_30_1@sint32 v444@sint16;
(*   %mul.i.1.30.1 = mul nsw i32 %conv1.i.1.30.1, -1517 *)
mul v_mul_i_1_30_1 v_conv1_i_1_30_1 (-1517)@sint32;
(*   %call.i.1.30.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.30.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_30_1, v_call_i_1_30_1);
(*   %arrayidx11.1.30.1 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %445 = load i16, i16* %arrayidx11.1.30.1, align 2, !tbaa !3 *)
mov v445 mem0_316;
(*   %sub.1.30.1 = sub i16 %445, %call.i.1.30.1 *)
sub v_sub_1_30_1 v445 v_call_i_1_30_1;
(*   store i16 %sub.1.30.1, i16* %arrayidx9.1.30.1, align 2, !tbaa !3 *)
mov mem0_444 v_sub_1_30_1;
(*   %add21.1.30.1 = add i16 %445, %call.i.1.30.1 *)
add v_add21_1_30_1 v445 v_call_i_1_30_1;
(*   store i16 %add21.1.30.1, i16* %arrayidx11.1.30.1, align 2, !tbaa !3 *)
mov mem0_316 v_add21_1_30_1;
(*   %arrayidx9.1.31.1 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %446 = load i16, i16* %arrayidx9.1.31.1, align 2, !tbaa !3 *)
mov v446 mem0_446;
(*   %conv1.i.1.31.1 = sext i16 %446 to i32 *)
cast v_conv1_i_1_31_1@sint32 v446@sint16;
(*   %mul.i.1.31.1 = mul nsw i32 %conv1.i.1.31.1, -1517 *)
mul v_mul_i_1_31_1 v_conv1_i_1_31_1 (-1517)@sint32;
(*   %call.i.1.31.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.31.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_31_1, v_call_i_1_31_1);
(*   %arrayidx11.1.31.1 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %447 = load i16, i16* %arrayidx11.1.31.1, align 2, !tbaa !3 *)
mov v447 mem0_318;
(*   %sub.1.31.1 = sub i16 %447, %call.i.1.31.1 *)
sub v_sub_1_31_1 v447 v_call_i_1_31_1;
(*   store i16 %sub.1.31.1, i16* %arrayidx9.1.31.1, align 2, !tbaa !3 *)
mov mem0_446 v_sub_1_31_1;
(*   %add21.1.31.1 = add i16 %447, %call.i.1.31.1 *)
add v_add21_1_31_1 v447 v_call_i_1_31_1;
(*   store i16 %add21.1.31.1, i16* %arrayidx11.1.31.1, align 2, !tbaa !3 *)
mov mem0_318 v_add21_1_31_1;
(*   %arrayidx9.1.32.1 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %448 = load i16, i16* %arrayidx9.1.32.1, align 2, !tbaa !3 *)
mov v448 mem0_448;
(*   %conv1.i.1.32.1 = sext i16 %448 to i32 *)
cast v_conv1_i_1_32_1@sint32 v448@sint16;
(*   %mul.i.1.32.1 = mul nsw i32 %conv1.i.1.32.1, -1517 *)
mul v_mul_i_1_32_1 v_conv1_i_1_32_1 (-1517)@sint32;
(*   %call.i.1.32.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.32.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_32_1, v_call_i_1_32_1);
(*   %arrayidx11.1.32.1 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %449 = load i16, i16* %arrayidx11.1.32.1, align 2, !tbaa !3 *)
mov v449 mem0_320;
(*   %sub.1.32.1 = sub i16 %449, %call.i.1.32.1 *)
sub v_sub_1_32_1 v449 v_call_i_1_32_1;
(*   store i16 %sub.1.32.1, i16* %arrayidx9.1.32.1, align 2, !tbaa !3 *)
mov mem0_448 v_sub_1_32_1;
(*   %add21.1.32.1 = add i16 %449, %call.i.1.32.1 *)
add v_add21_1_32_1 v449 v_call_i_1_32_1;
(*   store i16 %add21.1.32.1, i16* %arrayidx11.1.32.1, align 2, !tbaa !3 *)
mov mem0_320 v_add21_1_32_1;
(*   %arrayidx9.1.33.1 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %450 = load i16, i16* %arrayidx9.1.33.1, align 2, !tbaa !3 *)
mov v450 mem0_450;
(*   %conv1.i.1.33.1 = sext i16 %450 to i32 *)
cast v_conv1_i_1_33_1@sint32 v450@sint16;
(*   %mul.i.1.33.1 = mul nsw i32 %conv1.i.1.33.1, -1517 *)
mul v_mul_i_1_33_1 v_conv1_i_1_33_1 (-1517)@sint32;
(*   %call.i.1.33.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.33.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_33_1, v_call_i_1_33_1);
(*   %arrayidx11.1.33.1 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %451 = load i16, i16* %arrayidx11.1.33.1, align 2, !tbaa !3 *)
mov v451 mem0_322;
(*   %sub.1.33.1 = sub i16 %451, %call.i.1.33.1 *)
sub v_sub_1_33_1 v451 v_call_i_1_33_1;
(*   store i16 %sub.1.33.1, i16* %arrayidx9.1.33.1, align 2, !tbaa !3 *)
mov mem0_450 v_sub_1_33_1;
(*   %add21.1.33.1 = add i16 %451, %call.i.1.33.1 *)
add v_add21_1_33_1 v451 v_call_i_1_33_1;
(*   store i16 %add21.1.33.1, i16* %arrayidx11.1.33.1, align 2, !tbaa !3 *)
mov mem0_322 v_add21_1_33_1;
(*   %arrayidx9.1.34.1 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %452 = load i16, i16* %arrayidx9.1.34.1, align 2, !tbaa !3 *)
mov v452 mem0_452;
(*   %conv1.i.1.34.1 = sext i16 %452 to i32 *)
cast v_conv1_i_1_34_1@sint32 v452@sint16;
(*   %mul.i.1.34.1 = mul nsw i32 %conv1.i.1.34.1, -1517 *)
mul v_mul_i_1_34_1 v_conv1_i_1_34_1 (-1517)@sint32;
(*   %call.i.1.34.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.34.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_34_1, v_call_i_1_34_1);
(*   %arrayidx11.1.34.1 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %453 = load i16, i16* %arrayidx11.1.34.1, align 2, !tbaa !3 *)
mov v453 mem0_324;
(*   %sub.1.34.1 = sub i16 %453, %call.i.1.34.1 *)
sub v_sub_1_34_1 v453 v_call_i_1_34_1;
(*   store i16 %sub.1.34.1, i16* %arrayidx9.1.34.1, align 2, !tbaa !3 *)
mov mem0_452 v_sub_1_34_1;
(*   %add21.1.34.1 = add i16 %453, %call.i.1.34.1 *)
add v_add21_1_34_1 v453 v_call_i_1_34_1;
(*   store i16 %add21.1.34.1, i16* %arrayidx11.1.34.1, align 2, !tbaa !3 *)
mov mem0_324 v_add21_1_34_1;
(*   %arrayidx9.1.35.1 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %454 = load i16, i16* %arrayidx9.1.35.1, align 2, !tbaa !3 *)
mov v454 mem0_454;
(*   %conv1.i.1.35.1 = sext i16 %454 to i32 *)
cast v_conv1_i_1_35_1@sint32 v454@sint16;
(*   %mul.i.1.35.1 = mul nsw i32 %conv1.i.1.35.1, -1517 *)
mul v_mul_i_1_35_1 v_conv1_i_1_35_1 (-1517)@sint32;
(*   %call.i.1.35.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.35.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_35_1, v_call_i_1_35_1);
(*   %arrayidx11.1.35.1 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %455 = load i16, i16* %arrayidx11.1.35.1, align 2, !tbaa !3 *)
mov v455 mem0_326;
(*   %sub.1.35.1 = sub i16 %455, %call.i.1.35.1 *)
sub v_sub_1_35_1 v455 v_call_i_1_35_1;
(*   store i16 %sub.1.35.1, i16* %arrayidx9.1.35.1, align 2, !tbaa !3 *)
mov mem0_454 v_sub_1_35_1;
(*   %add21.1.35.1 = add i16 %455, %call.i.1.35.1 *)
add v_add21_1_35_1 v455 v_call_i_1_35_1;
(*   store i16 %add21.1.35.1, i16* %arrayidx11.1.35.1, align 2, !tbaa !3 *)
mov mem0_326 v_add21_1_35_1;
(*   %arrayidx9.1.36.1 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %456 = load i16, i16* %arrayidx9.1.36.1, align 2, !tbaa !3 *)
mov v456 mem0_456;
(*   %conv1.i.1.36.1 = sext i16 %456 to i32 *)
cast v_conv1_i_1_36_1@sint32 v456@sint16;
(*   %mul.i.1.36.1 = mul nsw i32 %conv1.i.1.36.1, -1517 *)
mul v_mul_i_1_36_1 v_conv1_i_1_36_1 (-1517)@sint32;
(*   %call.i.1.36.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.36.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_36_1, v_call_i_1_36_1);
(*   %arrayidx11.1.36.1 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %457 = load i16, i16* %arrayidx11.1.36.1, align 2, !tbaa !3 *)
mov v457 mem0_328;
(*   %sub.1.36.1 = sub i16 %457, %call.i.1.36.1 *)
sub v_sub_1_36_1 v457 v_call_i_1_36_1;
(*   store i16 %sub.1.36.1, i16* %arrayidx9.1.36.1, align 2, !tbaa !3 *)
mov mem0_456 v_sub_1_36_1;
(*   %add21.1.36.1 = add i16 %457, %call.i.1.36.1 *)
add v_add21_1_36_1 v457 v_call_i_1_36_1;
(*   store i16 %add21.1.36.1, i16* %arrayidx11.1.36.1, align 2, !tbaa !3 *)
mov mem0_328 v_add21_1_36_1;
(*   %arrayidx9.1.37.1 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %458 = load i16, i16* %arrayidx9.1.37.1, align 2, !tbaa !3 *)
mov v458 mem0_458;
(*   %conv1.i.1.37.1 = sext i16 %458 to i32 *)
cast v_conv1_i_1_37_1@sint32 v458@sint16;
(*   %mul.i.1.37.1 = mul nsw i32 %conv1.i.1.37.1, -1517 *)
mul v_mul_i_1_37_1 v_conv1_i_1_37_1 (-1517)@sint32;
(*   %call.i.1.37.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.37.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_37_1, v_call_i_1_37_1);
(*   %arrayidx11.1.37.1 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %459 = load i16, i16* %arrayidx11.1.37.1, align 2, !tbaa !3 *)
mov v459 mem0_330;
(*   %sub.1.37.1 = sub i16 %459, %call.i.1.37.1 *)
sub v_sub_1_37_1 v459 v_call_i_1_37_1;
(*   store i16 %sub.1.37.1, i16* %arrayidx9.1.37.1, align 2, !tbaa !3 *)
mov mem0_458 v_sub_1_37_1;
(*   %add21.1.37.1 = add i16 %459, %call.i.1.37.1 *)
add v_add21_1_37_1 v459 v_call_i_1_37_1;
(*   store i16 %add21.1.37.1, i16* %arrayidx11.1.37.1, align 2, !tbaa !3 *)
mov mem0_330 v_add21_1_37_1;
(*   %arrayidx9.1.38.1 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %460 = load i16, i16* %arrayidx9.1.38.1, align 2, !tbaa !3 *)
mov v460 mem0_460;
(*   %conv1.i.1.38.1 = sext i16 %460 to i32 *)
cast v_conv1_i_1_38_1@sint32 v460@sint16;
(*   %mul.i.1.38.1 = mul nsw i32 %conv1.i.1.38.1, -1517 *)
mul v_mul_i_1_38_1 v_conv1_i_1_38_1 (-1517)@sint32;
(*   %call.i.1.38.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.38.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_38_1, v_call_i_1_38_1);
(*   %arrayidx11.1.38.1 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %461 = load i16, i16* %arrayidx11.1.38.1, align 2, !tbaa !3 *)
mov v461 mem0_332;
(*   %sub.1.38.1 = sub i16 %461, %call.i.1.38.1 *)
sub v_sub_1_38_1 v461 v_call_i_1_38_1;
(*   store i16 %sub.1.38.1, i16* %arrayidx9.1.38.1, align 2, !tbaa !3 *)
mov mem0_460 v_sub_1_38_1;
(*   %add21.1.38.1 = add i16 %461, %call.i.1.38.1 *)
add v_add21_1_38_1 v461 v_call_i_1_38_1;
(*   store i16 %add21.1.38.1, i16* %arrayidx11.1.38.1, align 2, !tbaa !3 *)
mov mem0_332 v_add21_1_38_1;
(*   %arrayidx9.1.39.1 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %462 = load i16, i16* %arrayidx9.1.39.1, align 2, !tbaa !3 *)
mov v462 mem0_462;
(*   %conv1.i.1.39.1 = sext i16 %462 to i32 *)
cast v_conv1_i_1_39_1@sint32 v462@sint16;
(*   %mul.i.1.39.1 = mul nsw i32 %conv1.i.1.39.1, -1517 *)
mul v_mul_i_1_39_1 v_conv1_i_1_39_1 (-1517)@sint32;
(*   %call.i.1.39.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.39.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_39_1, v_call_i_1_39_1);
(*   %arrayidx11.1.39.1 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %463 = load i16, i16* %arrayidx11.1.39.1, align 2, !tbaa !3 *)
mov v463 mem0_334;
(*   %sub.1.39.1 = sub i16 %463, %call.i.1.39.1 *)
sub v_sub_1_39_1 v463 v_call_i_1_39_1;
(*   store i16 %sub.1.39.1, i16* %arrayidx9.1.39.1, align 2, !tbaa !3 *)
mov mem0_462 v_sub_1_39_1;
(*   %add21.1.39.1 = add i16 %463, %call.i.1.39.1 *)
add v_add21_1_39_1 v463 v_call_i_1_39_1;
(*   store i16 %add21.1.39.1, i16* %arrayidx11.1.39.1, align 2, !tbaa !3 *)
mov mem0_334 v_add21_1_39_1;
(*   %arrayidx9.1.40.1 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %464 = load i16, i16* %arrayidx9.1.40.1, align 2, !tbaa !3 *)
mov v464 mem0_464;
(*   %conv1.i.1.40.1 = sext i16 %464 to i32 *)
cast v_conv1_i_1_40_1@sint32 v464@sint16;
(*   %mul.i.1.40.1 = mul nsw i32 %conv1.i.1.40.1, -1517 *)
mul v_mul_i_1_40_1 v_conv1_i_1_40_1 (-1517)@sint32;
(*   %call.i.1.40.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.40.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_40_1, v_call_i_1_40_1);
(*   %arrayidx11.1.40.1 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %465 = load i16, i16* %arrayidx11.1.40.1, align 2, !tbaa !3 *)
mov v465 mem0_336;
(*   %sub.1.40.1 = sub i16 %465, %call.i.1.40.1 *)
sub v_sub_1_40_1 v465 v_call_i_1_40_1;
(*   store i16 %sub.1.40.1, i16* %arrayidx9.1.40.1, align 2, !tbaa !3 *)
mov mem0_464 v_sub_1_40_1;
(*   %add21.1.40.1 = add i16 %465, %call.i.1.40.1 *)
add v_add21_1_40_1 v465 v_call_i_1_40_1;
(*   store i16 %add21.1.40.1, i16* %arrayidx11.1.40.1, align 2, !tbaa !3 *)
mov mem0_336 v_add21_1_40_1;
(*   %arrayidx9.1.41.1 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %466 = load i16, i16* %arrayidx9.1.41.1, align 2, !tbaa !3 *)
mov v466 mem0_466;
(*   %conv1.i.1.41.1 = sext i16 %466 to i32 *)
cast v_conv1_i_1_41_1@sint32 v466@sint16;
(*   %mul.i.1.41.1 = mul nsw i32 %conv1.i.1.41.1, -1517 *)
mul v_mul_i_1_41_1 v_conv1_i_1_41_1 (-1517)@sint32;
(*   %call.i.1.41.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.41.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_41_1, v_call_i_1_41_1);
(*   %arrayidx11.1.41.1 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %467 = load i16, i16* %arrayidx11.1.41.1, align 2, !tbaa !3 *)
mov v467 mem0_338;
(*   %sub.1.41.1 = sub i16 %467, %call.i.1.41.1 *)
sub v_sub_1_41_1 v467 v_call_i_1_41_1;
(*   store i16 %sub.1.41.1, i16* %arrayidx9.1.41.1, align 2, !tbaa !3 *)
mov mem0_466 v_sub_1_41_1;
(*   %add21.1.41.1 = add i16 %467, %call.i.1.41.1 *)
add v_add21_1_41_1 v467 v_call_i_1_41_1;
(*   store i16 %add21.1.41.1, i16* %arrayidx11.1.41.1, align 2, !tbaa !3 *)
mov mem0_338 v_add21_1_41_1;
(*   %arrayidx9.1.42.1 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %468 = load i16, i16* %arrayidx9.1.42.1, align 2, !tbaa !3 *)
mov v468 mem0_468;
(*   %conv1.i.1.42.1 = sext i16 %468 to i32 *)
cast v_conv1_i_1_42_1@sint32 v468@sint16;
(*   %mul.i.1.42.1 = mul nsw i32 %conv1.i.1.42.1, -1517 *)
mul v_mul_i_1_42_1 v_conv1_i_1_42_1 (-1517)@sint32;
(*   %call.i.1.42.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.42.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_42_1, v_call_i_1_42_1);
(*   %arrayidx11.1.42.1 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %469 = load i16, i16* %arrayidx11.1.42.1, align 2, !tbaa !3 *)
mov v469 mem0_340;
(*   %sub.1.42.1 = sub i16 %469, %call.i.1.42.1 *)
sub v_sub_1_42_1 v469 v_call_i_1_42_1;
(*   store i16 %sub.1.42.1, i16* %arrayidx9.1.42.1, align 2, !tbaa !3 *)
mov mem0_468 v_sub_1_42_1;
(*   %add21.1.42.1 = add i16 %469, %call.i.1.42.1 *)
add v_add21_1_42_1 v469 v_call_i_1_42_1;
(*   store i16 %add21.1.42.1, i16* %arrayidx11.1.42.1, align 2, !tbaa !3 *)
mov mem0_340 v_add21_1_42_1;
(*   %arrayidx9.1.43.1 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %470 = load i16, i16* %arrayidx9.1.43.1, align 2, !tbaa !3 *)
mov v470 mem0_470;
(*   %conv1.i.1.43.1 = sext i16 %470 to i32 *)
cast v_conv1_i_1_43_1@sint32 v470@sint16;
(*   %mul.i.1.43.1 = mul nsw i32 %conv1.i.1.43.1, -1517 *)
mul v_mul_i_1_43_1 v_conv1_i_1_43_1 (-1517)@sint32;
(*   %call.i.1.43.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.43.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_43_1, v_call_i_1_43_1);
(*   %arrayidx11.1.43.1 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %471 = load i16, i16* %arrayidx11.1.43.1, align 2, !tbaa !3 *)
mov v471 mem0_342;
(*   %sub.1.43.1 = sub i16 %471, %call.i.1.43.1 *)
sub v_sub_1_43_1 v471 v_call_i_1_43_1;
(*   store i16 %sub.1.43.1, i16* %arrayidx9.1.43.1, align 2, !tbaa !3 *)
mov mem0_470 v_sub_1_43_1;
(*   %add21.1.43.1 = add i16 %471, %call.i.1.43.1 *)
add v_add21_1_43_1 v471 v_call_i_1_43_1;
(*   store i16 %add21.1.43.1, i16* %arrayidx11.1.43.1, align 2, !tbaa !3 *)
mov mem0_342 v_add21_1_43_1;
(*   %arrayidx9.1.44.1 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %472 = load i16, i16* %arrayidx9.1.44.1, align 2, !tbaa !3 *)
mov v472 mem0_472;
(*   %conv1.i.1.44.1 = sext i16 %472 to i32 *)
cast v_conv1_i_1_44_1@sint32 v472@sint16;
(*   %mul.i.1.44.1 = mul nsw i32 %conv1.i.1.44.1, -1517 *)
mul v_mul_i_1_44_1 v_conv1_i_1_44_1 (-1517)@sint32;
(*   %call.i.1.44.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.44.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_44_1, v_call_i_1_44_1);
(*   %arrayidx11.1.44.1 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %473 = load i16, i16* %arrayidx11.1.44.1, align 2, !tbaa !3 *)
mov v473 mem0_344;
(*   %sub.1.44.1 = sub i16 %473, %call.i.1.44.1 *)
sub v_sub_1_44_1 v473 v_call_i_1_44_1;
(*   store i16 %sub.1.44.1, i16* %arrayidx9.1.44.1, align 2, !tbaa !3 *)
mov mem0_472 v_sub_1_44_1;
(*   %add21.1.44.1 = add i16 %473, %call.i.1.44.1 *)
add v_add21_1_44_1 v473 v_call_i_1_44_1;
(*   store i16 %add21.1.44.1, i16* %arrayidx11.1.44.1, align 2, !tbaa !3 *)
mov mem0_344 v_add21_1_44_1;
(*   %arrayidx9.1.45.1 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %474 = load i16, i16* %arrayidx9.1.45.1, align 2, !tbaa !3 *)
mov v474 mem0_474;
(*   %conv1.i.1.45.1 = sext i16 %474 to i32 *)
cast v_conv1_i_1_45_1@sint32 v474@sint16;
(*   %mul.i.1.45.1 = mul nsw i32 %conv1.i.1.45.1, -1517 *)
mul v_mul_i_1_45_1 v_conv1_i_1_45_1 (-1517)@sint32;
(*   %call.i.1.45.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.45.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_45_1, v_call_i_1_45_1);
(*   %arrayidx11.1.45.1 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %475 = load i16, i16* %arrayidx11.1.45.1, align 2, !tbaa !3 *)
mov v475 mem0_346;
(*   %sub.1.45.1 = sub i16 %475, %call.i.1.45.1 *)
sub v_sub_1_45_1 v475 v_call_i_1_45_1;
(*   store i16 %sub.1.45.1, i16* %arrayidx9.1.45.1, align 2, !tbaa !3 *)
mov mem0_474 v_sub_1_45_1;
(*   %add21.1.45.1 = add i16 %475, %call.i.1.45.1 *)
add v_add21_1_45_1 v475 v_call_i_1_45_1;
(*   store i16 %add21.1.45.1, i16* %arrayidx11.1.45.1, align 2, !tbaa !3 *)
mov mem0_346 v_add21_1_45_1;
(*   %arrayidx9.1.46.1 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %476 = load i16, i16* %arrayidx9.1.46.1, align 2, !tbaa !3 *)
mov v476 mem0_476;
(*   %conv1.i.1.46.1 = sext i16 %476 to i32 *)
cast v_conv1_i_1_46_1@sint32 v476@sint16;
(*   %mul.i.1.46.1 = mul nsw i32 %conv1.i.1.46.1, -1517 *)
mul v_mul_i_1_46_1 v_conv1_i_1_46_1 (-1517)@sint32;
(*   %call.i.1.46.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.46.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_46_1, v_call_i_1_46_1);
(*   %arrayidx11.1.46.1 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %477 = load i16, i16* %arrayidx11.1.46.1, align 2, !tbaa !3 *)
mov v477 mem0_348;
(*   %sub.1.46.1 = sub i16 %477, %call.i.1.46.1 *)
sub v_sub_1_46_1 v477 v_call_i_1_46_1;
(*   store i16 %sub.1.46.1, i16* %arrayidx9.1.46.1, align 2, !tbaa !3 *)
mov mem0_476 v_sub_1_46_1;
(*   %add21.1.46.1 = add i16 %477, %call.i.1.46.1 *)
add v_add21_1_46_1 v477 v_call_i_1_46_1;
(*   store i16 %add21.1.46.1, i16* %arrayidx11.1.46.1, align 2, !tbaa !3 *)
mov mem0_348 v_add21_1_46_1;
(*   %arrayidx9.1.47.1 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %478 = load i16, i16* %arrayidx9.1.47.1, align 2, !tbaa !3 *)
mov v478 mem0_478;
(*   %conv1.i.1.47.1 = sext i16 %478 to i32 *)
cast v_conv1_i_1_47_1@sint32 v478@sint16;
(*   %mul.i.1.47.1 = mul nsw i32 %conv1.i.1.47.1, -1517 *)
mul v_mul_i_1_47_1 v_conv1_i_1_47_1 (-1517)@sint32;
(*   %call.i.1.47.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.47.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_47_1, v_call_i_1_47_1);
(*   %arrayidx11.1.47.1 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %479 = load i16, i16* %arrayidx11.1.47.1, align 2, !tbaa !3 *)
mov v479 mem0_350;
(*   %sub.1.47.1 = sub i16 %479, %call.i.1.47.1 *)
sub v_sub_1_47_1 v479 v_call_i_1_47_1;
(*   store i16 %sub.1.47.1, i16* %arrayidx9.1.47.1, align 2, !tbaa !3 *)
mov mem0_478 v_sub_1_47_1;
(*   %add21.1.47.1 = add i16 %479, %call.i.1.47.1 *)
add v_add21_1_47_1 v479 v_call_i_1_47_1;
(*   store i16 %add21.1.47.1, i16* %arrayidx11.1.47.1, align 2, !tbaa !3 *)
mov mem0_350 v_add21_1_47_1;
(*   %arrayidx9.1.48.1 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %480 = load i16, i16* %arrayidx9.1.48.1, align 2, !tbaa !3 *)
mov v480 mem0_480;
(*   %conv1.i.1.48.1 = sext i16 %480 to i32 *)
cast v_conv1_i_1_48_1@sint32 v480@sint16;
(*   %mul.i.1.48.1 = mul nsw i32 %conv1.i.1.48.1, -1517 *)
mul v_mul_i_1_48_1 v_conv1_i_1_48_1 (-1517)@sint32;
(*   %call.i.1.48.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.48.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_48_1, v_call_i_1_48_1);
(*   %arrayidx11.1.48.1 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %481 = load i16, i16* %arrayidx11.1.48.1, align 2, !tbaa !3 *)
mov v481 mem0_352;
(*   %sub.1.48.1 = sub i16 %481, %call.i.1.48.1 *)
sub v_sub_1_48_1 v481 v_call_i_1_48_1;
(*   store i16 %sub.1.48.1, i16* %arrayidx9.1.48.1, align 2, !tbaa !3 *)
mov mem0_480 v_sub_1_48_1;
(*   %add21.1.48.1 = add i16 %481, %call.i.1.48.1 *)
add v_add21_1_48_1 v481 v_call_i_1_48_1;
(*   store i16 %add21.1.48.1, i16* %arrayidx11.1.48.1, align 2, !tbaa !3 *)
mov mem0_352 v_add21_1_48_1;
(*   %arrayidx9.1.49.1 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %482 = load i16, i16* %arrayidx9.1.49.1, align 2, !tbaa !3 *)
mov v482 mem0_482;
(*   %conv1.i.1.49.1 = sext i16 %482 to i32 *)
cast v_conv1_i_1_49_1@sint32 v482@sint16;
(*   %mul.i.1.49.1 = mul nsw i32 %conv1.i.1.49.1, -1517 *)
mul v_mul_i_1_49_1 v_conv1_i_1_49_1 (-1517)@sint32;
(*   %call.i.1.49.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.49.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_49_1, v_call_i_1_49_1);
(*   %arrayidx11.1.49.1 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %483 = load i16, i16* %arrayidx11.1.49.1, align 2, !tbaa !3 *)
mov v483 mem0_354;
(*   %sub.1.49.1 = sub i16 %483, %call.i.1.49.1 *)
sub v_sub_1_49_1 v483 v_call_i_1_49_1;
(*   store i16 %sub.1.49.1, i16* %arrayidx9.1.49.1, align 2, !tbaa !3 *)
mov mem0_482 v_sub_1_49_1;
(*   %add21.1.49.1 = add i16 %483, %call.i.1.49.1 *)
add v_add21_1_49_1 v483 v_call_i_1_49_1;
(*   store i16 %add21.1.49.1, i16* %arrayidx11.1.49.1, align 2, !tbaa !3 *)
mov mem0_354 v_add21_1_49_1;
(*   %arrayidx9.1.50.1 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %484 = load i16, i16* %arrayidx9.1.50.1, align 2, !tbaa !3 *)
mov v484 mem0_484;
(*   %conv1.i.1.50.1 = sext i16 %484 to i32 *)
cast v_conv1_i_1_50_1@sint32 v484@sint16;
(*   %mul.i.1.50.1 = mul nsw i32 %conv1.i.1.50.1, -1517 *)
mul v_mul_i_1_50_1 v_conv1_i_1_50_1 (-1517)@sint32;
(*   %call.i.1.50.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.50.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_50_1, v_call_i_1_50_1);
(*   %arrayidx11.1.50.1 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %485 = load i16, i16* %arrayidx11.1.50.1, align 2, !tbaa !3 *)
mov v485 mem0_356;
(*   %sub.1.50.1 = sub i16 %485, %call.i.1.50.1 *)
sub v_sub_1_50_1 v485 v_call_i_1_50_1;
(*   store i16 %sub.1.50.1, i16* %arrayidx9.1.50.1, align 2, !tbaa !3 *)
mov mem0_484 v_sub_1_50_1;
(*   %add21.1.50.1 = add i16 %485, %call.i.1.50.1 *)
add v_add21_1_50_1 v485 v_call_i_1_50_1;
(*   store i16 %add21.1.50.1, i16* %arrayidx11.1.50.1, align 2, !tbaa !3 *)
mov mem0_356 v_add21_1_50_1;
(*   %arrayidx9.1.51.1 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %486 = load i16, i16* %arrayidx9.1.51.1, align 2, !tbaa !3 *)
mov v486 mem0_486;
(*   %conv1.i.1.51.1 = sext i16 %486 to i32 *)
cast v_conv1_i_1_51_1@sint32 v486@sint16;
(*   %mul.i.1.51.1 = mul nsw i32 %conv1.i.1.51.1, -1517 *)
mul v_mul_i_1_51_1 v_conv1_i_1_51_1 (-1517)@sint32;
(*   %call.i.1.51.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.51.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_51_1, v_call_i_1_51_1);
(*   %arrayidx11.1.51.1 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %487 = load i16, i16* %arrayidx11.1.51.1, align 2, !tbaa !3 *)
mov v487 mem0_358;
(*   %sub.1.51.1 = sub i16 %487, %call.i.1.51.1 *)
sub v_sub_1_51_1 v487 v_call_i_1_51_1;
(*   store i16 %sub.1.51.1, i16* %arrayidx9.1.51.1, align 2, !tbaa !3 *)
mov mem0_486 v_sub_1_51_1;
(*   %add21.1.51.1 = add i16 %487, %call.i.1.51.1 *)
add v_add21_1_51_1 v487 v_call_i_1_51_1;
(*   store i16 %add21.1.51.1, i16* %arrayidx11.1.51.1, align 2, !tbaa !3 *)
mov mem0_358 v_add21_1_51_1;
(*   %arrayidx9.1.52.1 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %488 = load i16, i16* %arrayidx9.1.52.1, align 2, !tbaa !3 *)
mov v488 mem0_488;
(*   %conv1.i.1.52.1 = sext i16 %488 to i32 *)
cast v_conv1_i_1_52_1@sint32 v488@sint16;
(*   %mul.i.1.52.1 = mul nsw i32 %conv1.i.1.52.1, -1517 *)
mul v_mul_i_1_52_1 v_conv1_i_1_52_1 (-1517)@sint32;
(*   %call.i.1.52.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.52.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_52_1, v_call_i_1_52_1);
(*   %arrayidx11.1.52.1 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %489 = load i16, i16* %arrayidx11.1.52.1, align 2, !tbaa !3 *)
mov v489 mem0_360;
(*   %sub.1.52.1 = sub i16 %489, %call.i.1.52.1 *)
sub v_sub_1_52_1 v489 v_call_i_1_52_1;
(*   store i16 %sub.1.52.1, i16* %arrayidx9.1.52.1, align 2, !tbaa !3 *)
mov mem0_488 v_sub_1_52_1;
(*   %add21.1.52.1 = add i16 %489, %call.i.1.52.1 *)
add v_add21_1_52_1 v489 v_call_i_1_52_1;
(*   store i16 %add21.1.52.1, i16* %arrayidx11.1.52.1, align 2, !tbaa !3 *)
mov mem0_360 v_add21_1_52_1;
(*   %arrayidx9.1.53.1 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %490 = load i16, i16* %arrayidx9.1.53.1, align 2, !tbaa !3 *)
mov v490 mem0_490;
(*   %conv1.i.1.53.1 = sext i16 %490 to i32 *)
cast v_conv1_i_1_53_1@sint32 v490@sint16;
(*   %mul.i.1.53.1 = mul nsw i32 %conv1.i.1.53.1, -1517 *)
mul v_mul_i_1_53_1 v_conv1_i_1_53_1 (-1517)@sint32;
(*   %call.i.1.53.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.53.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_53_1, v_call_i_1_53_1);
(*   %arrayidx11.1.53.1 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %491 = load i16, i16* %arrayidx11.1.53.1, align 2, !tbaa !3 *)
mov v491 mem0_362;
(*   %sub.1.53.1 = sub i16 %491, %call.i.1.53.1 *)
sub v_sub_1_53_1 v491 v_call_i_1_53_1;
(*   store i16 %sub.1.53.1, i16* %arrayidx9.1.53.1, align 2, !tbaa !3 *)
mov mem0_490 v_sub_1_53_1;
(*   %add21.1.53.1 = add i16 %491, %call.i.1.53.1 *)
add v_add21_1_53_1 v491 v_call_i_1_53_1;
(*   store i16 %add21.1.53.1, i16* %arrayidx11.1.53.1, align 2, !tbaa !3 *)
mov mem0_362 v_add21_1_53_1;
(*   %arrayidx9.1.54.1 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %492 = load i16, i16* %arrayidx9.1.54.1, align 2, !tbaa !3 *)
mov v492 mem0_492;
(*   %conv1.i.1.54.1 = sext i16 %492 to i32 *)
cast v_conv1_i_1_54_1@sint32 v492@sint16;
(*   %mul.i.1.54.1 = mul nsw i32 %conv1.i.1.54.1, -1517 *)
mul v_mul_i_1_54_1 v_conv1_i_1_54_1 (-1517)@sint32;
(*   %call.i.1.54.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.54.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_54_1, v_call_i_1_54_1);
(*   %arrayidx11.1.54.1 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %493 = load i16, i16* %arrayidx11.1.54.1, align 2, !tbaa !3 *)
mov v493 mem0_364;
(*   %sub.1.54.1 = sub i16 %493, %call.i.1.54.1 *)
sub v_sub_1_54_1 v493 v_call_i_1_54_1;
(*   store i16 %sub.1.54.1, i16* %arrayidx9.1.54.1, align 2, !tbaa !3 *)
mov mem0_492 v_sub_1_54_1;
(*   %add21.1.54.1 = add i16 %493, %call.i.1.54.1 *)
add v_add21_1_54_1 v493 v_call_i_1_54_1;
(*   store i16 %add21.1.54.1, i16* %arrayidx11.1.54.1, align 2, !tbaa !3 *)
mov mem0_364 v_add21_1_54_1;
(*   %arrayidx9.1.55.1 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %494 = load i16, i16* %arrayidx9.1.55.1, align 2, !tbaa !3 *)
mov v494 mem0_494;
(*   %conv1.i.1.55.1 = sext i16 %494 to i32 *)
cast v_conv1_i_1_55_1@sint32 v494@sint16;
(*   %mul.i.1.55.1 = mul nsw i32 %conv1.i.1.55.1, -1517 *)
mul v_mul_i_1_55_1 v_conv1_i_1_55_1 (-1517)@sint32;
(*   %call.i.1.55.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.55.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_55_1, v_call_i_1_55_1);
(*   %arrayidx11.1.55.1 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %495 = load i16, i16* %arrayidx11.1.55.1, align 2, !tbaa !3 *)
mov v495 mem0_366;
(*   %sub.1.55.1 = sub i16 %495, %call.i.1.55.1 *)
sub v_sub_1_55_1 v495 v_call_i_1_55_1;
(*   store i16 %sub.1.55.1, i16* %arrayidx9.1.55.1, align 2, !tbaa !3 *)
mov mem0_494 v_sub_1_55_1;
(*   %add21.1.55.1 = add i16 %495, %call.i.1.55.1 *)
add v_add21_1_55_1 v495 v_call_i_1_55_1;
(*   store i16 %add21.1.55.1, i16* %arrayidx11.1.55.1, align 2, !tbaa !3 *)
mov mem0_366 v_add21_1_55_1;
(*   %arrayidx9.1.56.1 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %496 = load i16, i16* %arrayidx9.1.56.1, align 2, !tbaa !3 *)
mov v496 mem0_496;
(*   %conv1.i.1.56.1 = sext i16 %496 to i32 *)
cast v_conv1_i_1_56_1@sint32 v496@sint16;
(*   %mul.i.1.56.1 = mul nsw i32 %conv1.i.1.56.1, -1517 *)
mul v_mul_i_1_56_1 v_conv1_i_1_56_1 (-1517)@sint32;
(*   %call.i.1.56.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.56.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_56_1, v_call_i_1_56_1);
(*   %arrayidx11.1.56.1 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %497 = load i16, i16* %arrayidx11.1.56.1, align 2, !tbaa !3 *)
mov v497 mem0_368;
(*   %sub.1.56.1 = sub i16 %497, %call.i.1.56.1 *)
sub v_sub_1_56_1 v497 v_call_i_1_56_1;
(*   store i16 %sub.1.56.1, i16* %arrayidx9.1.56.1, align 2, !tbaa !3 *)
mov mem0_496 v_sub_1_56_1;
(*   %add21.1.56.1 = add i16 %497, %call.i.1.56.1 *)
add v_add21_1_56_1 v497 v_call_i_1_56_1;
(*   store i16 %add21.1.56.1, i16* %arrayidx11.1.56.1, align 2, !tbaa !3 *)
mov mem0_368 v_add21_1_56_1;
(*   %arrayidx9.1.57.1 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %498 = load i16, i16* %arrayidx9.1.57.1, align 2, !tbaa !3 *)
mov v498 mem0_498;
(*   %conv1.i.1.57.1 = sext i16 %498 to i32 *)
cast v_conv1_i_1_57_1@sint32 v498@sint16;
(*   %mul.i.1.57.1 = mul nsw i32 %conv1.i.1.57.1, -1517 *)
mul v_mul_i_1_57_1 v_conv1_i_1_57_1 (-1517)@sint32;
(*   %call.i.1.57.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.57.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_57_1, v_call_i_1_57_1);
(*   %arrayidx11.1.57.1 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %499 = load i16, i16* %arrayidx11.1.57.1, align 2, !tbaa !3 *)
mov v499 mem0_370;
(*   %sub.1.57.1 = sub i16 %499, %call.i.1.57.1 *)
sub v_sub_1_57_1 v499 v_call_i_1_57_1;
(*   store i16 %sub.1.57.1, i16* %arrayidx9.1.57.1, align 2, !tbaa !3 *)
mov mem0_498 v_sub_1_57_1;
(*   %add21.1.57.1 = add i16 %499, %call.i.1.57.1 *)
add v_add21_1_57_1 v499 v_call_i_1_57_1;
(*   store i16 %add21.1.57.1, i16* %arrayidx11.1.57.1, align 2, !tbaa !3 *)
mov mem0_370 v_add21_1_57_1;
(*   %arrayidx9.1.58.1 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %500 = load i16, i16* %arrayidx9.1.58.1, align 2, !tbaa !3 *)
mov v500 mem0_500;
(*   %conv1.i.1.58.1 = sext i16 %500 to i32 *)
cast v_conv1_i_1_58_1@sint32 v500@sint16;
(*   %mul.i.1.58.1 = mul nsw i32 %conv1.i.1.58.1, -1517 *)
mul v_mul_i_1_58_1 v_conv1_i_1_58_1 (-1517)@sint32;
(*   %call.i.1.58.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.58.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_58_1, v_call_i_1_58_1);
(*   %arrayidx11.1.58.1 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %501 = load i16, i16* %arrayidx11.1.58.1, align 2, !tbaa !3 *)
mov v501 mem0_372;
(*   %sub.1.58.1 = sub i16 %501, %call.i.1.58.1 *)
sub v_sub_1_58_1 v501 v_call_i_1_58_1;
(*   store i16 %sub.1.58.1, i16* %arrayidx9.1.58.1, align 2, !tbaa !3 *)
mov mem0_500 v_sub_1_58_1;
(*   %add21.1.58.1 = add i16 %501, %call.i.1.58.1 *)
add v_add21_1_58_1 v501 v_call_i_1_58_1;
(*   store i16 %add21.1.58.1, i16* %arrayidx11.1.58.1, align 2, !tbaa !3 *)
mov mem0_372 v_add21_1_58_1;
(*   %arrayidx9.1.59.1 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %502 = load i16, i16* %arrayidx9.1.59.1, align 2, !tbaa !3 *)
mov v502 mem0_502;
(*   %conv1.i.1.59.1 = sext i16 %502 to i32 *)
cast v_conv1_i_1_59_1@sint32 v502@sint16;
(*   %mul.i.1.59.1 = mul nsw i32 %conv1.i.1.59.1, -1517 *)
mul v_mul_i_1_59_1 v_conv1_i_1_59_1 (-1517)@sint32;
(*   %call.i.1.59.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.59.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_59_1, v_call_i_1_59_1);
(*   %arrayidx11.1.59.1 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %503 = load i16, i16* %arrayidx11.1.59.1, align 2, !tbaa !3 *)
mov v503 mem0_374;
(*   %sub.1.59.1 = sub i16 %503, %call.i.1.59.1 *)
sub v_sub_1_59_1 v503 v_call_i_1_59_1;
(*   store i16 %sub.1.59.1, i16* %arrayidx9.1.59.1, align 2, !tbaa !3 *)
mov mem0_502 v_sub_1_59_1;
(*   %add21.1.59.1 = add i16 %503, %call.i.1.59.1 *)
add v_add21_1_59_1 v503 v_call_i_1_59_1;
(*   store i16 %add21.1.59.1, i16* %arrayidx11.1.59.1, align 2, !tbaa !3 *)
mov mem0_374 v_add21_1_59_1;
(*   %arrayidx9.1.60.1 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %504 = load i16, i16* %arrayidx9.1.60.1, align 2, !tbaa !3 *)
mov v504 mem0_504;
(*   %conv1.i.1.60.1 = sext i16 %504 to i32 *)
cast v_conv1_i_1_60_1@sint32 v504@sint16;
(*   %mul.i.1.60.1 = mul nsw i32 %conv1.i.1.60.1, -1517 *)
mul v_mul_i_1_60_1 v_conv1_i_1_60_1 (-1517)@sint32;
(*   %call.i.1.60.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.60.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_60_1, v_call_i_1_60_1);
(*   %arrayidx11.1.60.1 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %505 = load i16, i16* %arrayidx11.1.60.1, align 2, !tbaa !3 *)
mov v505 mem0_376;
(*   %sub.1.60.1 = sub i16 %505, %call.i.1.60.1 *)
sub v_sub_1_60_1 v505 v_call_i_1_60_1;
(*   store i16 %sub.1.60.1, i16* %arrayidx9.1.60.1, align 2, !tbaa !3 *)
mov mem0_504 v_sub_1_60_1;
(*   %add21.1.60.1 = add i16 %505, %call.i.1.60.1 *)
add v_add21_1_60_1 v505 v_call_i_1_60_1;
(*   store i16 %add21.1.60.1, i16* %arrayidx11.1.60.1, align 2, !tbaa !3 *)
mov mem0_376 v_add21_1_60_1;
(*   %arrayidx9.1.61.1 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %506 = load i16, i16* %arrayidx9.1.61.1, align 2, !tbaa !3 *)
mov v506 mem0_506;
(*   %conv1.i.1.61.1 = sext i16 %506 to i32 *)
cast v_conv1_i_1_61_1@sint32 v506@sint16;
(*   %mul.i.1.61.1 = mul nsw i32 %conv1.i.1.61.1, -1517 *)
mul v_mul_i_1_61_1 v_conv1_i_1_61_1 (-1517)@sint32;
(*   %call.i.1.61.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.61.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_61_1, v_call_i_1_61_1);
(*   %arrayidx11.1.61.1 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %507 = load i16, i16* %arrayidx11.1.61.1, align 2, !tbaa !3 *)
mov v507 mem0_378;
(*   %sub.1.61.1 = sub i16 %507, %call.i.1.61.1 *)
sub v_sub_1_61_1 v507 v_call_i_1_61_1;
(*   store i16 %sub.1.61.1, i16* %arrayidx9.1.61.1, align 2, !tbaa !3 *)
mov mem0_506 v_sub_1_61_1;
(*   %add21.1.61.1 = add i16 %507, %call.i.1.61.1 *)
add v_add21_1_61_1 v507 v_call_i_1_61_1;
(*   store i16 %add21.1.61.1, i16* %arrayidx11.1.61.1, align 2, !tbaa !3 *)
mov mem0_378 v_add21_1_61_1;
(*   %arrayidx9.1.62.1 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %508 = load i16, i16* %arrayidx9.1.62.1, align 2, !tbaa !3 *)
mov v508 mem0_508;
(*   %conv1.i.1.62.1 = sext i16 %508 to i32 *)
cast v_conv1_i_1_62_1@sint32 v508@sint16;
(*   %mul.i.1.62.1 = mul nsw i32 %conv1.i.1.62.1, -1517 *)
mul v_mul_i_1_62_1 v_conv1_i_1_62_1 (-1517)@sint32;
(*   %call.i.1.62.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.62.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_62_1, v_call_i_1_62_1);
(*   %arrayidx11.1.62.1 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %509 = load i16, i16* %arrayidx11.1.62.1, align 2, !tbaa !3 *)
mov v509 mem0_380;
(*   %sub.1.62.1 = sub i16 %509, %call.i.1.62.1 *)
sub v_sub_1_62_1 v509 v_call_i_1_62_1;
(*   store i16 %sub.1.62.1, i16* %arrayidx9.1.62.1, align 2, !tbaa !3 *)
mov mem0_508 v_sub_1_62_1;
(*   %add21.1.62.1 = add i16 %509, %call.i.1.62.1 *)
add v_add21_1_62_1 v509 v_call_i_1_62_1;
(*   store i16 %add21.1.62.1, i16* %arrayidx11.1.62.1, align 2, !tbaa !3 *)
mov mem0_380 v_add21_1_62_1;
(*   %arrayidx9.1.63.1 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %510 = load i16, i16* %arrayidx9.1.63.1, align 2, !tbaa !3 *)
mov v510 mem0_510;
(*   %conv1.i.1.63.1 = sext i16 %510 to i32 *)
cast v_conv1_i_1_63_1@sint32 v510@sint16;
(*   %mul.i.1.63.1 = mul nsw i32 %conv1.i.1.63.1, -1517 *)
mul v_mul_i_1_63_1 v_conv1_i_1_63_1 (-1517)@sint32;
(*   %call.i.1.63.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.1.63.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_1_63_1, v_call_i_1_63_1);
(*   %arrayidx11.1.63.1 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %511 = load i16, i16* %arrayidx11.1.63.1, align 2, !tbaa !3 *)
mov v511 mem0_382;
(*   %sub.1.63.1 = sub i16 %511, %call.i.1.63.1 *)
sub v_sub_1_63_1 v511 v_call_i_1_63_1;
(*   store i16 %sub.1.63.1, i16* %arrayidx9.1.63.1, align 2, !tbaa !3 *)
mov mem0_510 v_sub_1_63_1;
(*   %add21.1.63.1 = add i16 %511, %call.i.1.63.1 *)
add v_add21_1_63_1 v511 v_call_i_1_63_1;
(*   store i16 %add21.1.63.1, i16* %arrayidx11.1.63.1, align 2, !tbaa !3 *)
mov mem0_382 v_add21_1_63_1;


cut and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3) + 
mem0_8*(x**4) + mem0_10*(x**5) + mem0_12*(x**6) + mem0_14*(x**7) + 
mem0_16*(x**8) + mem0_18*(x**9) + mem0_20*(x**10) + mem0_22*(x**11) + 
mem0_24*(x**12) + mem0_26*(x**13) + mem0_28*(x**14) + mem0_30*(x**15) + 
mem0_32*(x**16) + mem0_34*(x**17) + mem0_36*(x**18) + mem0_38*(x**19) + 
mem0_40*(x**20) + mem0_42*(x**21) + mem0_44*(x**22) + mem0_46*(x**23) + 
mem0_48*(x**24) + mem0_50*(x**25) + mem0_52*(x**26) + mem0_54*(x**27) + 
mem0_56*(x**28) + mem0_58*(x**29) + mem0_60*(x**30) + mem0_62*(x**31) + 
mem0_64*(x**32) + mem0_66*(x**33) + mem0_68*(x**34) + mem0_70*(x**35) + 
mem0_72*(x**36) + mem0_74*(x**37) + mem0_76*(x**38) + mem0_78*(x**39) + 
mem0_80*(x**40) + mem0_82*(x**41) + mem0_84*(x**42) + mem0_86*(x**43) + 
mem0_88*(x**44) + mem0_90*(x**45) + mem0_92*(x**46) + mem0_94*(x**47) + 
mem0_96*(x**48) + mem0_98*(x**49) + mem0_100*(x**50) + mem0_102*(x**51) + 
mem0_104*(x**52) + mem0_106*(x**53) + mem0_108*(x**54) + mem0_110*(x**55) + 
mem0_112*(x**56) + mem0_114*(x**57) + mem0_116*(x**58) + mem0_118*(x**59) + 
mem0_120*(x**60) + mem0_122*(x**61) + mem0_124*(x**62) + mem0_126*(x**63)
)
[3329, x**64 - 2580],
eqmod 
input_polynomial * input_polynomial
(
mem0_128*(x**0) + mem0_130*(x**1) + mem0_132*(x**2) + mem0_134*(x**3) + 
mem0_136*(x**4) + mem0_138*(x**5) + mem0_140*(x**6) + mem0_142*(x**7) + 
mem0_144*(x**8) + mem0_146*(x**9) + mem0_148*(x**10) + mem0_150*(x**11) + 
mem0_152*(x**12) + mem0_154*(x**13) + mem0_156*(x**14) + mem0_158*(x**15) + 
mem0_160*(x**16) + mem0_162*(x**17) + mem0_164*(x**18) + mem0_166*(x**19) + 
mem0_168*(x**20) + mem0_170*(x**21) + mem0_172*(x**22) + mem0_174*(x**23) + 
mem0_176*(x**24) + mem0_178*(x**25) + mem0_180*(x**26) + mem0_182*(x**27) + 
mem0_184*(x**28) + mem0_186*(x**29) + mem0_188*(x**30) + mem0_190*(x**31) + 
mem0_192*(x**32) + mem0_194*(x**33) + mem0_196*(x**34) + mem0_198*(x**35) + 
mem0_200*(x**36) + mem0_202*(x**37) + mem0_204*(x**38) + mem0_206*(x**39) + 
mem0_208*(x**40) + mem0_210*(x**41) + mem0_212*(x**42) + mem0_214*(x**43) + 
mem0_216*(x**44) + mem0_218*(x**45) + mem0_220*(x**46) + mem0_222*(x**47) + 
mem0_224*(x**48) + mem0_226*(x**49) + mem0_228*(x**50) + mem0_230*(x**51) + 
mem0_232*(x**52) + mem0_234*(x**53) + mem0_236*(x**54) + mem0_238*(x**55) + 
mem0_240*(x**56) + mem0_242*(x**57) + mem0_244*(x**58) + mem0_246*(x**59) + 
mem0_248*(x**60) + mem0_250*(x**61) + mem0_252*(x**62) + mem0_254*(x**63)
)
[3329, x**64 - 749],
eqmod 
input_polynomial * input_polynomial
(
mem0_256*(x**0) + mem0_258*(x**1) + mem0_260*(x**2) + mem0_262*(x**3) + 
mem0_264*(x**4) + mem0_266*(x**5) + mem0_268*(x**6) + mem0_270*(x**7) + 
mem0_272*(x**8) + mem0_274*(x**9) + mem0_276*(x**10) + mem0_278*(x**11) + 
mem0_280*(x**12) + mem0_282*(x**13) + mem0_284*(x**14) + mem0_286*(x**15) + 
mem0_288*(x**16) + mem0_290*(x**17) + mem0_292*(x**18) + mem0_294*(x**19) + 
mem0_296*(x**20) + mem0_298*(x**21) + mem0_300*(x**22) + mem0_302*(x**23) + 
mem0_304*(x**24) + mem0_306*(x**25) + mem0_308*(x**26) + mem0_310*(x**27) + 
mem0_312*(x**28) + mem0_314*(x**29) + mem0_316*(x**30) + mem0_318*(x**31) + 
mem0_320*(x**32) + mem0_322*(x**33) + mem0_324*(x**34) + mem0_326*(x**35) + 
mem0_328*(x**36) + mem0_330*(x**37) + mem0_332*(x**38) + mem0_334*(x**39) + 
mem0_336*(x**40) + mem0_338*(x**41) + mem0_340*(x**42) + mem0_342*(x**43) + 
mem0_344*(x**44) + mem0_346*(x**45) + mem0_348*(x**46) + mem0_350*(x**47) + 
mem0_352*(x**48) + mem0_354*(x**49) + mem0_356*(x**50) + mem0_358*(x**51) + 
mem0_360*(x**52) + mem0_362*(x**53) + mem0_364*(x**54) + mem0_366*(x**55) + 
mem0_368*(x**56) + mem0_370*(x**57) + mem0_372*(x**58) + mem0_374*(x**59) + 
mem0_376*(x**60) + mem0_378*(x**61) + mem0_380*(x**62) + mem0_382*(x**63)
)
[3329, x**64 - 3289],
eqmod 
input_polynomial * input_polynomial
(
mem0_384*(x**0) + mem0_386*(x**1) + mem0_388*(x**2) + mem0_390*(x**3) + 
mem0_392*(x**4) + mem0_394*(x**5) + mem0_396*(x**6) + mem0_398*(x**7) + 
mem0_400*(x**8) + mem0_402*(x**9) + mem0_404*(x**10) + mem0_406*(x**11) + 
mem0_408*(x**12) + mem0_410*(x**13) + mem0_412*(x**14) + mem0_414*(x**15) + 
mem0_416*(x**16) + mem0_418*(x**17) + mem0_420*(x**18) + mem0_422*(x**19) + 
mem0_424*(x**20) + mem0_426*(x**21) + mem0_428*(x**22) + mem0_430*(x**23) + 
mem0_432*(x**24) + mem0_434*(x**25) + mem0_436*(x**26) + mem0_438*(x**27) + 
mem0_440*(x**28) + mem0_442*(x**29) + mem0_444*(x**30) + mem0_446*(x**31) + 
mem0_448*(x**32) + mem0_450*(x**33) + mem0_452*(x**34) + mem0_454*(x**35) + 
mem0_456*(x**36) + mem0_458*(x**37) + mem0_460*(x**38) + mem0_462*(x**39) + 
mem0_464*(x**40) + mem0_466*(x**41) + mem0_468*(x**42) + mem0_470*(x**43) + 
mem0_472*(x**44) + mem0_474*(x**45) + mem0_476*(x**46) + mem0_478*(x**47) + 
mem0_480*(x**48) + mem0_482*(x**49) + mem0_484*(x**50) + mem0_486*(x**51) + 
mem0_488*(x**52) + mem0_490*(x**53) + mem0_492*(x**54) + mem0_494*(x**55) + 
mem0_496*(x**56) + mem0_498*(x**57) + mem0_500*(x**58) + mem0_502*(x**59) + 
mem0_504*(x**60) + mem0_506*(x**61) + mem0_508*(x**62) + mem0_510*(x**63)
)
[3329, x**64 - 40]
] && and [
   (-4)@16 * 3329@16 <s mem0_0, mem0_0 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_2, mem0_2 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_4, mem0_4 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_6, mem0_6 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_8, mem0_8 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_10, mem0_10 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_12, mem0_12 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_14, mem0_14 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_16, mem0_16 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_18, mem0_18 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_20, mem0_20 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_22, mem0_22 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_24, mem0_24 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_26, mem0_26 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_28, mem0_28 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_30, mem0_30 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_32, mem0_32 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_34, mem0_34 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_36, mem0_36 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_38, mem0_38 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_40, mem0_40 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_42, mem0_42 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_44, mem0_44 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_46, mem0_46 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_48, mem0_48 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_50, mem0_50 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_52, mem0_52 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_54, mem0_54 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_56, mem0_56 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_58, mem0_58 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_60, mem0_60 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_62, mem0_62 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_64, mem0_64 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_66, mem0_66 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_68, mem0_68 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_70, mem0_70 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_72, mem0_72 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_74, mem0_74 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_76, mem0_76 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_78, mem0_78 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_80, mem0_80 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_82, mem0_82 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_84, mem0_84 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_86, mem0_86 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_88, mem0_88 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_90, mem0_90 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_92, mem0_92 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_94, mem0_94 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_96, mem0_96 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_98, mem0_98 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_100, mem0_100 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_102, mem0_102 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_104, mem0_104 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_106, mem0_106 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_108, mem0_108 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_110, mem0_110 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_112, mem0_112 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_114, mem0_114 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_116, mem0_116 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_118, mem0_118 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_120, mem0_120 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_122, mem0_122 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_124, mem0_124 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_126, mem0_126 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_128, mem0_128 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_130, mem0_130 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_132, mem0_132 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_134, mem0_134 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_136, mem0_136 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_138, mem0_138 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_140, mem0_140 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_142, mem0_142 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_144, mem0_144 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_146, mem0_146 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_148, mem0_148 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_150, mem0_150 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_152, mem0_152 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_154, mem0_154 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_156, mem0_156 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_158, mem0_158 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_160, mem0_160 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_162, mem0_162 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_164, mem0_164 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_166, mem0_166 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_168, mem0_168 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_170, mem0_170 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_172, mem0_172 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_174, mem0_174 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_176, mem0_176 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_178, mem0_178 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_180, mem0_180 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_182, mem0_182 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_184, mem0_184 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_186, mem0_186 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_188, mem0_188 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_190, mem0_190 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_192, mem0_192 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_194, mem0_194 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_196, mem0_196 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_198, mem0_198 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_200, mem0_200 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_202, mem0_202 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_204, mem0_204 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_206, mem0_206 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_208, mem0_208 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_210, mem0_210 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_212, mem0_212 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_214, mem0_214 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_216, mem0_216 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_218, mem0_218 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_220, mem0_220 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_222, mem0_222 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_224, mem0_224 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_226, mem0_226 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_228, mem0_228 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_230, mem0_230 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_232, mem0_232 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_234, mem0_234 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_236, mem0_236 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_238, mem0_238 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_240, mem0_240 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_242, mem0_242 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_244, mem0_244 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_246, mem0_246 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_248, mem0_248 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_250, mem0_250 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_252, mem0_252 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_254, mem0_254 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_256, mem0_256 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_258, mem0_258 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_260, mem0_260 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_262, mem0_262 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_264, mem0_264 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_266, mem0_266 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_268, mem0_268 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_270, mem0_270 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_272, mem0_272 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_274, mem0_274 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_276, mem0_276 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_278, mem0_278 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_280, mem0_280 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_282, mem0_282 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_284, mem0_284 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_286, mem0_286 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_288, mem0_288 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_290, mem0_290 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_292, mem0_292 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_294, mem0_294 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_296, mem0_296 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_298, mem0_298 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_300, mem0_300 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_302, mem0_302 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_304, mem0_304 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_306, mem0_306 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_308, mem0_308 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_310, mem0_310 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_312, mem0_312 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_314, mem0_314 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_316, mem0_316 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_318, mem0_318 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_320, mem0_320 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_322, mem0_322 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_324, mem0_324 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_326, mem0_326 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_328, mem0_328 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_330, mem0_330 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_332, mem0_332 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_334, mem0_334 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_336, mem0_336 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_338, mem0_338 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_340, mem0_340 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_342, mem0_342 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_344, mem0_344 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_346, mem0_346 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_348, mem0_348 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_350, mem0_350 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_352, mem0_352 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_354, mem0_354 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_356, mem0_356 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_358, mem0_358 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_360, mem0_360 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_362, mem0_362 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_364, mem0_364 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_366, mem0_366 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_368, mem0_368 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_370, mem0_370 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_372, mem0_372 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_374, mem0_374 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_376, mem0_376 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_378, mem0_378 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_380, mem0_380 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_382, mem0_382 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_384, mem0_384 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_386, mem0_386 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_388, mem0_388 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_390, mem0_390 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_392, mem0_392 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_394, mem0_394 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_396, mem0_396 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_398, mem0_398 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_400, mem0_400 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_402, mem0_402 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_404, mem0_404 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_406, mem0_406 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_408, mem0_408 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_410, mem0_410 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_412, mem0_412 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_414, mem0_414 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_416, mem0_416 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_418, mem0_418 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_420, mem0_420 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_422, mem0_422 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_424, mem0_424 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_426, mem0_426 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_428, mem0_428 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_430, mem0_430 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_432, mem0_432 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_434, mem0_434 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_436, mem0_436 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_438, mem0_438 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_440, mem0_440 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_442, mem0_442 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_444, mem0_444 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_446, mem0_446 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_448, mem0_448 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_450, mem0_450 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_452, mem0_452 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_454, mem0_454 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_456, mem0_456 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_458, mem0_458 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_460, mem0_460 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_462, mem0_462 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_464, mem0_464 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_466, mem0_466 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_468, mem0_468 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_470, mem0_470 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_472, mem0_472 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_474, mem0_474 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_476, mem0_476 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_478, mem0_478 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_480, mem0_480 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_482, mem0_482 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_484, mem0_484 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_486, mem0_486 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_488, mem0_488 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_490, mem0_490 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_492, mem0_492 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_494, mem0_494 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_496, mem0_496 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_498, mem0_498 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_500, mem0_500 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_502, mem0_502 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_504, mem0_504 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_506, mem0_506 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_508, mem0_508 <s 4@16 * 3329@16,
   (-4)@16 * 3329@16 <s mem0_510, mem0_510 <s 4@16 * 3329@16
];


(* NOTE: k = 4 *)

(*   %arrayidx9.2 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %512 = load i16, i16* %arrayidx9.2, align 2, !tbaa !3 *)
mov v512 mem0_64;
(*   %conv1.i.2 = sext i16 %512 to i32 *)
cast v_conv1_i_2@sint32 v512@sint16;
(*   %mul.i.2 = mul nsw i32 %conv1.i.2, 1493 *)
mul v_mul_i_2 v_conv1_i_2 (1493)@sint32;
(*   %call.i.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2, v_call_i_2);
(*   %513 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v513 mem0_0;
(*   %sub.2 = sub i16 %513, %call.i.2 *)
sub v_sub_2 v513 v_call_i_2;
(*   store i16 %sub.2, i16* %arrayidx9.2, align 2, !tbaa !3 *)
mov mem0_64 v_sub_2;
(*   %add21.2 = add i16 %513, %call.i.2 *)
add v_add21_2 v513 v_call_i_2;
(*   store i16 %add21.2, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21_2;
(*   %arrayidx9.2.1 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %514 = load i16, i16* %arrayidx9.2.1, align 2, !tbaa !3 *)
mov v514 mem0_66;
(*   %conv1.i.2.1 = sext i16 %514 to i32 *)
cast v_conv1_i_2_1@sint32 v514@sint16;
(*   %mul.i.2.1 = mul nsw i32 %conv1.i.2.1, 1493 *)
mul v_mul_i_2_1 v_conv1_i_2_1 (1493)@sint32;
(*   %call.i.2.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_1, v_call_i_2_1);
(*   %arrayidx11.2.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %515 = load i16, i16* %arrayidx11.2.1, align 2, !tbaa !3 *)
mov v515 mem0_2;
(*   %sub.2.1 = sub i16 %515, %call.i.2.1 *)
sub v_sub_2_1 v515 v_call_i_2_1;
(*   store i16 %sub.2.1, i16* %arrayidx9.2.1, align 2, !tbaa !3 *)
mov mem0_66 v_sub_2_1;
(*   %add21.2.1 = add i16 %515, %call.i.2.1 *)
add v_add21_2_1 v515 v_call_i_2_1;
(*   store i16 %add21.2.1, i16* %arrayidx11.2.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_2_1;
(*   %arrayidx9.2.2 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %516 = load i16, i16* %arrayidx9.2.2, align 2, !tbaa !3 *)
mov v516 mem0_68;
(*   %conv1.i.2.2 = sext i16 %516 to i32 *)
cast v_conv1_i_2_2@sint32 v516@sint16;
(*   %mul.i.2.2 = mul nsw i32 %conv1.i.2.2, 1493 *)
mul v_mul_i_2_2 v_conv1_i_2_2 (1493)@sint32;
(*   %call.i.2.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_2, v_call_i_2_2);
(*   %arrayidx11.2.2 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %517 = load i16, i16* %arrayidx11.2.2, align 2, !tbaa !3 *)
mov v517 mem0_4;
(*   %sub.2.2 = sub i16 %517, %call.i.2.2 *)
sub v_sub_2_2 v517 v_call_i_2_2;
(*   store i16 %sub.2.2, i16* %arrayidx9.2.2, align 2, !tbaa !3 *)
mov mem0_68 v_sub_2_2;
(*   %add21.2.2 = add i16 %517, %call.i.2.2 *)
add v_add21_2_2 v517 v_call_i_2_2;
(*   store i16 %add21.2.2, i16* %arrayidx11.2.2, align 2, !tbaa !3 *)
mov mem0_4 v_add21_2_2;
(*   %arrayidx9.2.3 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %518 = load i16, i16* %arrayidx9.2.3, align 2, !tbaa !3 *)
mov v518 mem0_70;
(*   %conv1.i.2.3 = sext i16 %518 to i32 *)
cast v_conv1_i_2_3@sint32 v518@sint16;
(*   %mul.i.2.3 = mul nsw i32 %conv1.i.2.3, 1493 *)
mul v_mul_i_2_3 v_conv1_i_2_3 (1493)@sint32;
(*   %call.i.2.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_3, v_call_i_2_3);
(*   %arrayidx11.2.3 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %519 = load i16, i16* %arrayidx11.2.3, align 2, !tbaa !3 *)
mov v519 mem0_6;
(*   %sub.2.3 = sub i16 %519, %call.i.2.3 *)
sub v_sub_2_3 v519 v_call_i_2_3;
(*   store i16 %sub.2.3, i16* %arrayidx9.2.3, align 2, !tbaa !3 *)
mov mem0_70 v_sub_2_3;
(*   %add21.2.3 = add i16 %519, %call.i.2.3 *)
add v_add21_2_3 v519 v_call_i_2_3;
(*   store i16 %add21.2.3, i16* %arrayidx11.2.3, align 2, !tbaa !3 *)
mov mem0_6 v_add21_2_3;
(*   %arrayidx9.2.4 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %520 = load i16, i16* %arrayidx9.2.4, align 2, !tbaa !3 *)
mov v520 mem0_72;
(*   %conv1.i.2.4 = sext i16 %520 to i32 *)
cast v_conv1_i_2_4@sint32 v520@sint16;
(*   %mul.i.2.4 = mul nsw i32 %conv1.i.2.4, 1493 *)
mul v_mul_i_2_4 v_conv1_i_2_4 (1493)@sint32;
(*   %call.i.2.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_4, v_call_i_2_4);
(*   %arrayidx11.2.4 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %521 = load i16, i16* %arrayidx11.2.4, align 2, !tbaa !3 *)
mov v521 mem0_8;
(*   %sub.2.4 = sub i16 %521, %call.i.2.4 *)
sub v_sub_2_4 v521 v_call_i_2_4;
(*   store i16 %sub.2.4, i16* %arrayidx9.2.4, align 2, !tbaa !3 *)
mov mem0_72 v_sub_2_4;
(*   %add21.2.4 = add i16 %521, %call.i.2.4 *)
add v_add21_2_4 v521 v_call_i_2_4;
(*   store i16 %add21.2.4, i16* %arrayidx11.2.4, align 2, !tbaa !3 *)
mov mem0_8 v_add21_2_4;
(*   %arrayidx9.2.5 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %522 = load i16, i16* %arrayidx9.2.5, align 2, !tbaa !3 *)
mov v522 mem0_74;
(*   %conv1.i.2.5 = sext i16 %522 to i32 *)
cast v_conv1_i_2_5@sint32 v522@sint16;
(*   %mul.i.2.5 = mul nsw i32 %conv1.i.2.5, 1493 *)
mul v_mul_i_2_5 v_conv1_i_2_5 (1493)@sint32;
(*   %call.i.2.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_5, v_call_i_2_5);
(*   %arrayidx11.2.5 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %523 = load i16, i16* %arrayidx11.2.5, align 2, !tbaa !3 *)
mov v523 mem0_10;
(*   %sub.2.5 = sub i16 %523, %call.i.2.5 *)
sub v_sub_2_5 v523 v_call_i_2_5;
(*   store i16 %sub.2.5, i16* %arrayidx9.2.5, align 2, !tbaa !3 *)
mov mem0_74 v_sub_2_5;
(*   %add21.2.5 = add i16 %523, %call.i.2.5 *)
add v_add21_2_5 v523 v_call_i_2_5;
(*   store i16 %add21.2.5, i16* %arrayidx11.2.5, align 2, !tbaa !3 *)
mov mem0_10 v_add21_2_5;
(*   %arrayidx9.2.6 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %524 = load i16, i16* %arrayidx9.2.6, align 2, !tbaa !3 *)
mov v524 mem0_76;
(*   %conv1.i.2.6 = sext i16 %524 to i32 *)
cast v_conv1_i_2_6@sint32 v524@sint16;
(*   %mul.i.2.6 = mul nsw i32 %conv1.i.2.6, 1493 *)
mul v_mul_i_2_6 v_conv1_i_2_6 (1493)@sint32;
(*   %call.i.2.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_6, v_call_i_2_6);
(*   %arrayidx11.2.6 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %525 = load i16, i16* %arrayidx11.2.6, align 2, !tbaa !3 *)
mov v525 mem0_12;
(*   %sub.2.6 = sub i16 %525, %call.i.2.6 *)
sub v_sub_2_6 v525 v_call_i_2_6;
(*   store i16 %sub.2.6, i16* %arrayidx9.2.6, align 2, !tbaa !3 *)
mov mem0_76 v_sub_2_6;
(*   %add21.2.6 = add i16 %525, %call.i.2.6 *)
add v_add21_2_6 v525 v_call_i_2_6;
(*   store i16 %add21.2.6, i16* %arrayidx11.2.6, align 2, !tbaa !3 *)
mov mem0_12 v_add21_2_6;
(*   %arrayidx9.2.7 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %526 = load i16, i16* %arrayidx9.2.7, align 2, !tbaa !3 *)
mov v526 mem0_78;
(*   %conv1.i.2.7 = sext i16 %526 to i32 *)
cast v_conv1_i_2_7@sint32 v526@sint16;
(*   %mul.i.2.7 = mul nsw i32 %conv1.i.2.7, 1493 *)
mul v_mul_i_2_7 v_conv1_i_2_7 (1493)@sint32;
(*   %call.i.2.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_7, v_call_i_2_7);
(*   %arrayidx11.2.7 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %527 = load i16, i16* %arrayidx11.2.7, align 2, !tbaa !3 *)
mov v527 mem0_14;
(*   %sub.2.7 = sub i16 %527, %call.i.2.7 *)
sub v_sub_2_7 v527 v_call_i_2_7;
(*   store i16 %sub.2.7, i16* %arrayidx9.2.7, align 2, !tbaa !3 *)
mov mem0_78 v_sub_2_7;
(*   %add21.2.7 = add i16 %527, %call.i.2.7 *)
add v_add21_2_7 v527 v_call_i_2_7;
(*   store i16 %add21.2.7, i16* %arrayidx11.2.7, align 2, !tbaa !3 *)
mov mem0_14 v_add21_2_7;
(*   %arrayidx9.2.8 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %528 = load i16, i16* %arrayidx9.2.8, align 2, !tbaa !3 *)
mov v528 mem0_80;
(*   %conv1.i.2.8 = sext i16 %528 to i32 *)
cast v_conv1_i_2_8@sint32 v528@sint16;
(*   %mul.i.2.8 = mul nsw i32 %conv1.i.2.8, 1493 *)
mul v_mul_i_2_8 v_conv1_i_2_8 (1493)@sint32;
(*   %call.i.2.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_8, v_call_i_2_8);
(*   %arrayidx11.2.8 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %529 = load i16, i16* %arrayidx11.2.8, align 2, !tbaa !3 *)
mov v529 mem0_16;
(*   %sub.2.8 = sub i16 %529, %call.i.2.8 *)
sub v_sub_2_8 v529 v_call_i_2_8;
(*   store i16 %sub.2.8, i16* %arrayidx9.2.8, align 2, !tbaa !3 *)
mov mem0_80 v_sub_2_8;
(*   %add21.2.8 = add i16 %529, %call.i.2.8 *)
add v_add21_2_8 v529 v_call_i_2_8;
(*   store i16 %add21.2.8, i16* %arrayidx11.2.8, align 2, !tbaa !3 *)
mov mem0_16 v_add21_2_8;
(*   %arrayidx9.2.9 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %530 = load i16, i16* %arrayidx9.2.9, align 2, !tbaa !3 *)
mov v530 mem0_82;
(*   %conv1.i.2.9 = sext i16 %530 to i32 *)
cast v_conv1_i_2_9@sint32 v530@sint16;
(*   %mul.i.2.9 = mul nsw i32 %conv1.i.2.9, 1493 *)
mul v_mul_i_2_9 v_conv1_i_2_9 (1493)@sint32;
(*   %call.i.2.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_9, v_call_i_2_9);
(*   %arrayidx11.2.9 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %531 = load i16, i16* %arrayidx11.2.9, align 2, !tbaa !3 *)
mov v531 mem0_18;
(*   %sub.2.9 = sub i16 %531, %call.i.2.9 *)
sub v_sub_2_9 v531 v_call_i_2_9;
(*   store i16 %sub.2.9, i16* %arrayidx9.2.9, align 2, !tbaa !3 *)
mov mem0_82 v_sub_2_9;
(*   %add21.2.9 = add i16 %531, %call.i.2.9 *)
add v_add21_2_9 v531 v_call_i_2_9;
(*   store i16 %add21.2.9, i16* %arrayidx11.2.9, align 2, !tbaa !3 *)
mov mem0_18 v_add21_2_9;
(*   %arrayidx9.2.10 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %532 = load i16, i16* %arrayidx9.2.10, align 2, !tbaa !3 *)
mov v532 mem0_84;
(*   %conv1.i.2.10 = sext i16 %532 to i32 *)
cast v_conv1_i_2_10@sint32 v532@sint16;
(*   %mul.i.2.10 = mul nsw i32 %conv1.i.2.10, 1493 *)
mul v_mul_i_2_10 v_conv1_i_2_10 (1493)@sint32;
(*   %call.i.2.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_10, v_call_i_2_10);
(*   %arrayidx11.2.10 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %533 = load i16, i16* %arrayidx11.2.10, align 2, !tbaa !3 *)
mov v533 mem0_20;
(*   %sub.2.10 = sub i16 %533, %call.i.2.10 *)
sub v_sub_2_10 v533 v_call_i_2_10;
(*   store i16 %sub.2.10, i16* %arrayidx9.2.10, align 2, !tbaa !3 *)
mov mem0_84 v_sub_2_10;
(*   %add21.2.10 = add i16 %533, %call.i.2.10 *)
add v_add21_2_10 v533 v_call_i_2_10;
(*   store i16 %add21.2.10, i16* %arrayidx11.2.10, align 2, !tbaa !3 *)
mov mem0_20 v_add21_2_10;
(*   %arrayidx9.2.11 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %534 = load i16, i16* %arrayidx9.2.11, align 2, !tbaa !3 *)
mov v534 mem0_86;
(*   %conv1.i.2.11 = sext i16 %534 to i32 *)
cast v_conv1_i_2_11@sint32 v534@sint16;
(*   %mul.i.2.11 = mul nsw i32 %conv1.i.2.11, 1493 *)
mul v_mul_i_2_11 v_conv1_i_2_11 (1493)@sint32;
(*   %call.i.2.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_11, v_call_i_2_11);
(*   %arrayidx11.2.11 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %535 = load i16, i16* %arrayidx11.2.11, align 2, !tbaa !3 *)
mov v535 mem0_22;
(*   %sub.2.11 = sub i16 %535, %call.i.2.11 *)
sub v_sub_2_11 v535 v_call_i_2_11;
(*   store i16 %sub.2.11, i16* %arrayidx9.2.11, align 2, !tbaa !3 *)
mov mem0_86 v_sub_2_11;
(*   %add21.2.11 = add i16 %535, %call.i.2.11 *)
add v_add21_2_11 v535 v_call_i_2_11;
(*   store i16 %add21.2.11, i16* %arrayidx11.2.11, align 2, !tbaa !3 *)
mov mem0_22 v_add21_2_11;
(*   %arrayidx9.2.12 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %536 = load i16, i16* %arrayidx9.2.12, align 2, !tbaa !3 *)
mov v536 mem0_88;
(*   %conv1.i.2.12 = sext i16 %536 to i32 *)
cast v_conv1_i_2_12@sint32 v536@sint16;
(*   %mul.i.2.12 = mul nsw i32 %conv1.i.2.12, 1493 *)
mul v_mul_i_2_12 v_conv1_i_2_12 (1493)@sint32;
(*   %call.i.2.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_12, v_call_i_2_12);
(*   %arrayidx11.2.12 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %537 = load i16, i16* %arrayidx11.2.12, align 2, !tbaa !3 *)
mov v537 mem0_24;
(*   %sub.2.12 = sub i16 %537, %call.i.2.12 *)
sub v_sub_2_12 v537 v_call_i_2_12;
(*   store i16 %sub.2.12, i16* %arrayidx9.2.12, align 2, !tbaa !3 *)
mov mem0_88 v_sub_2_12;
(*   %add21.2.12 = add i16 %537, %call.i.2.12 *)
add v_add21_2_12 v537 v_call_i_2_12;
(*   store i16 %add21.2.12, i16* %arrayidx11.2.12, align 2, !tbaa !3 *)
mov mem0_24 v_add21_2_12;
(*   %arrayidx9.2.13 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %538 = load i16, i16* %arrayidx9.2.13, align 2, !tbaa !3 *)
mov v538 mem0_90;
(*   %conv1.i.2.13 = sext i16 %538 to i32 *)
cast v_conv1_i_2_13@sint32 v538@sint16;
(*   %mul.i.2.13 = mul nsw i32 %conv1.i.2.13, 1493 *)
mul v_mul_i_2_13 v_conv1_i_2_13 (1493)@sint32;
(*   %call.i.2.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_13, v_call_i_2_13);
(*   %arrayidx11.2.13 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %539 = load i16, i16* %arrayidx11.2.13, align 2, !tbaa !3 *)
mov v539 mem0_26;
(*   %sub.2.13 = sub i16 %539, %call.i.2.13 *)
sub v_sub_2_13 v539 v_call_i_2_13;
(*   store i16 %sub.2.13, i16* %arrayidx9.2.13, align 2, !tbaa !3 *)
mov mem0_90 v_sub_2_13;
(*   %add21.2.13 = add i16 %539, %call.i.2.13 *)
add v_add21_2_13 v539 v_call_i_2_13;
(*   store i16 %add21.2.13, i16* %arrayidx11.2.13, align 2, !tbaa !3 *)
mov mem0_26 v_add21_2_13;
(*   %arrayidx9.2.14 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %540 = load i16, i16* %arrayidx9.2.14, align 2, !tbaa !3 *)
mov v540 mem0_92;
(*   %conv1.i.2.14 = sext i16 %540 to i32 *)
cast v_conv1_i_2_14@sint32 v540@sint16;
(*   %mul.i.2.14 = mul nsw i32 %conv1.i.2.14, 1493 *)
mul v_mul_i_2_14 v_conv1_i_2_14 (1493)@sint32;
(*   %call.i.2.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_14, v_call_i_2_14);
(*   %arrayidx11.2.14 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %541 = load i16, i16* %arrayidx11.2.14, align 2, !tbaa !3 *)
mov v541 mem0_28;
(*   %sub.2.14 = sub i16 %541, %call.i.2.14 *)
sub v_sub_2_14 v541 v_call_i_2_14;
(*   store i16 %sub.2.14, i16* %arrayidx9.2.14, align 2, !tbaa !3 *)
mov mem0_92 v_sub_2_14;
(*   %add21.2.14 = add i16 %541, %call.i.2.14 *)
add v_add21_2_14 v541 v_call_i_2_14;
(*   store i16 %add21.2.14, i16* %arrayidx11.2.14, align 2, !tbaa !3 *)
mov mem0_28 v_add21_2_14;
(*   %arrayidx9.2.15 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %542 = load i16, i16* %arrayidx9.2.15, align 2, !tbaa !3 *)
mov v542 mem0_94;
(*   %conv1.i.2.15 = sext i16 %542 to i32 *)
cast v_conv1_i_2_15@sint32 v542@sint16;
(*   %mul.i.2.15 = mul nsw i32 %conv1.i.2.15, 1493 *)
mul v_mul_i_2_15 v_conv1_i_2_15 (1493)@sint32;
(*   %call.i.2.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_15, v_call_i_2_15);
(*   %arrayidx11.2.15 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %543 = load i16, i16* %arrayidx11.2.15, align 2, !tbaa !3 *)
mov v543 mem0_30;
(*   %sub.2.15 = sub i16 %543, %call.i.2.15 *)
sub v_sub_2_15 v543 v_call_i_2_15;
(*   store i16 %sub.2.15, i16* %arrayidx9.2.15, align 2, !tbaa !3 *)
mov mem0_94 v_sub_2_15;
(*   %add21.2.15 = add i16 %543, %call.i.2.15 *)
add v_add21_2_15 v543 v_call_i_2_15;
(*   store i16 %add21.2.15, i16* %arrayidx11.2.15, align 2, !tbaa !3 *)
mov mem0_30 v_add21_2_15;
(*   %arrayidx9.2.16 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %544 = load i16, i16* %arrayidx9.2.16, align 2, !tbaa !3 *)
mov v544 mem0_96;
(*   %conv1.i.2.16 = sext i16 %544 to i32 *)
cast v_conv1_i_2_16@sint32 v544@sint16;
(*   %mul.i.2.16 = mul nsw i32 %conv1.i.2.16, 1493 *)
mul v_mul_i_2_16 v_conv1_i_2_16 (1493)@sint32;
(*   %call.i.2.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_16, v_call_i_2_16);
(*   %arrayidx11.2.16 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %545 = load i16, i16* %arrayidx11.2.16, align 2, !tbaa !3 *)
mov v545 mem0_32;
(*   %sub.2.16 = sub i16 %545, %call.i.2.16 *)
sub v_sub_2_16 v545 v_call_i_2_16;
(*   store i16 %sub.2.16, i16* %arrayidx9.2.16, align 2, !tbaa !3 *)
mov mem0_96 v_sub_2_16;
(*   %add21.2.16 = add i16 %545, %call.i.2.16 *)
add v_add21_2_16 v545 v_call_i_2_16;
(*   store i16 %add21.2.16, i16* %arrayidx11.2.16, align 2, !tbaa !3 *)
mov mem0_32 v_add21_2_16;
(*   %arrayidx9.2.17 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %546 = load i16, i16* %arrayidx9.2.17, align 2, !tbaa !3 *)
mov v546 mem0_98;
(*   %conv1.i.2.17 = sext i16 %546 to i32 *)
cast v_conv1_i_2_17@sint32 v546@sint16;
(*   %mul.i.2.17 = mul nsw i32 %conv1.i.2.17, 1493 *)
mul v_mul_i_2_17 v_conv1_i_2_17 (1493)@sint32;
(*   %call.i.2.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_17, v_call_i_2_17);
(*   %arrayidx11.2.17 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %547 = load i16, i16* %arrayidx11.2.17, align 2, !tbaa !3 *)
mov v547 mem0_34;
(*   %sub.2.17 = sub i16 %547, %call.i.2.17 *)
sub v_sub_2_17 v547 v_call_i_2_17;
(*   store i16 %sub.2.17, i16* %arrayidx9.2.17, align 2, !tbaa !3 *)
mov mem0_98 v_sub_2_17;
(*   %add21.2.17 = add i16 %547, %call.i.2.17 *)
add v_add21_2_17 v547 v_call_i_2_17;
(*   store i16 %add21.2.17, i16* %arrayidx11.2.17, align 2, !tbaa !3 *)
mov mem0_34 v_add21_2_17;
(*   %arrayidx9.2.18 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %548 = load i16, i16* %arrayidx9.2.18, align 2, !tbaa !3 *)
mov v548 mem0_100;
(*   %conv1.i.2.18 = sext i16 %548 to i32 *)
cast v_conv1_i_2_18@sint32 v548@sint16;
(*   %mul.i.2.18 = mul nsw i32 %conv1.i.2.18, 1493 *)
mul v_mul_i_2_18 v_conv1_i_2_18 (1493)@sint32;
(*   %call.i.2.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_18, v_call_i_2_18);
(*   %arrayidx11.2.18 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %549 = load i16, i16* %arrayidx11.2.18, align 2, !tbaa !3 *)
mov v549 mem0_36;
(*   %sub.2.18 = sub i16 %549, %call.i.2.18 *)
sub v_sub_2_18 v549 v_call_i_2_18;
(*   store i16 %sub.2.18, i16* %arrayidx9.2.18, align 2, !tbaa !3 *)
mov mem0_100 v_sub_2_18;
(*   %add21.2.18 = add i16 %549, %call.i.2.18 *)
add v_add21_2_18 v549 v_call_i_2_18;
(*   store i16 %add21.2.18, i16* %arrayidx11.2.18, align 2, !tbaa !3 *)
mov mem0_36 v_add21_2_18;
(*   %arrayidx9.2.19 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %550 = load i16, i16* %arrayidx9.2.19, align 2, !tbaa !3 *)
mov v550 mem0_102;
(*   %conv1.i.2.19 = sext i16 %550 to i32 *)
cast v_conv1_i_2_19@sint32 v550@sint16;
(*   %mul.i.2.19 = mul nsw i32 %conv1.i.2.19, 1493 *)
mul v_mul_i_2_19 v_conv1_i_2_19 (1493)@sint32;
(*   %call.i.2.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_19, v_call_i_2_19);
(*   %arrayidx11.2.19 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %551 = load i16, i16* %arrayidx11.2.19, align 2, !tbaa !3 *)
mov v551 mem0_38;
(*   %sub.2.19 = sub i16 %551, %call.i.2.19 *)
sub v_sub_2_19 v551 v_call_i_2_19;
(*   store i16 %sub.2.19, i16* %arrayidx9.2.19, align 2, !tbaa !3 *)
mov mem0_102 v_sub_2_19;
(*   %add21.2.19 = add i16 %551, %call.i.2.19 *)
add v_add21_2_19 v551 v_call_i_2_19;
(*   store i16 %add21.2.19, i16* %arrayidx11.2.19, align 2, !tbaa !3 *)
mov mem0_38 v_add21_2_19;
(*   %arrayidx9.2.20 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %552 = load i16, i16* %arrayidx9.2.20, align 2, !tbaa !3 *)
mov v552 mem0_104;
(*   %conv1.i.2.20 = sext i16 %552 to i32 *)
cast v_conv1_i_2_20@sint32 v552@sint16;
(*   %mul.i.2.20 = mul nsw i32 %conv1.i.2.20, 1493 *)
mul v_mul_i_2_20 v_conv1_i_2_20 (1493)@sint32;
(*   %call.i.2.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_20, v_call_i_2_20);
(*   %arrayidx11.2.20 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %553 = load i16, i16* %arrayidx11.2.20, align 2, !tbaa !3 *)
mov v553 mem0_40;
(*   %sub.2.20 = sub i16 %553, %call.i.2.20 *)
sub v_sub_2_20 v553 v_call_i_2_20;
(*   store i16 %sub.2.20, i16* %arrayidx9.2.20, align 2, !tbaa !3 *)
mov mem0_104 v_sub_2_20;
(*   %add21.2.20 = add i16 %553, %call.i.2.20 *)
add v_add21_2_20 v553 v_call_i_2_20;
(*   store i16 %add21.2.20, i16* %arrayidx11.2.20, align 2, !tbaa !3 *)
mov mem0_40 v_add21_2_20;
(*   %arrayidx9.2.21 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %554 = load i16, i16* %arrayidx9.2.21, align 2, !tbaa !3 *)
mov v554 mem0_106;
(*   %conv1.i.2.21 = sext i16 %554 to i32 *)
cast v_conv1_i_2_21@sint32 v554@sint16;
(*   %mul.i.2.21 = mul nsw i32 %conv1.i.2.21, 1493 *)
mul v_mul_i_2_21 v_conv1_i_2_21 (1493)@sint32;
(*   %call.i.2.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_21, v_call_i_2_21);
(*   %arrayidx11.2.21 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %555 = load i16, i16* %arrayidx11.2.21, align 2, !tbaa !3 *)
mov v555 mem0_42;
(*   %sub.2.21 = sub i16 %555, %call.i.2.21 *)
sub v_sub_2_21 v555 v_call_i_2_21;
(*   store i16 %sub.2.21, i16* %arrayidx9.2.21, align 2, !tbaa !3 *)
mov mem0_106 v_sub_2_21;
(*   %add21.2.21 = add i16 %555, %call.i.2.21 *)
add v_add21_2_21 v555 v_call_i_2_21;
(*   store i16 %add21.2.21, i16* %arrayidx11.2.21, align 2, !tbaa !3 *)
mov mem0_42 v_add21_2_21;
(*   %arrayidx9.2.22 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %556 = load i16, i16* %arrayidx9.2.22, align 2, !tbaa !3 *)
mov v556 mem0_108;
(*   %conv1.i.2.22 = sext i16 %556 to i32 *)
cast v_conv1_i_2_22@sint32 v556@sint16;
(*   %mul.i.2.22 = mul nsw i32 %conv1.i.2.22, 1493 *)
mul v_mul_i_2_22 v_conv1_i_2_22 (1493)@sint32;
(*   %call.i.2.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_22, v_call_i_2_22);
(*   %arrayidx11.2.22 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %557 = load i16, i16* %arrayidx11.2.22, align 2, !tbaa !3 *)
mov v557 mem0_44;
(*   %sub.2.22 = sub i16 %557, %call.i.2.22 *)
sub v_sub_2_22 v557 v_call_i_2_22;
(*   store i16 %sub.2.22, i16* %arrayidx9.2.22, align 2, !tbaa !3 *)
mov mem0_108 v_sub_2_22;
(*   %add21.2.22 = add i16 %557, %call.i.2.22 *)
add v_add21_2_22 v557 v_call_i_2_22;
(*   store i16 %add21.2.22, i16* %arrayidx11.2.22, align 2, !tbaa !3 *)
mov mem0_44 v_add21_2_22;
(*   %arrayidx9.2.23 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %558 = load i16, i16* %arrayidx9.2.23, align 2, !tbaa !3 *)
mov v558 mem0_110;
(*   %conv1.i.2.23 = sext i16 %558 to i32 *)
cast v_conv1_i_2_23@sint32 v558@sint16;
(*   %mul.i.2.23 = mul nsw i32 %conv1.i.2.23, 1493 *)
mul v_mul_i_2_23 v_conv1_i_2_23 (1493)@sint32;
(*   %call.i.2.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_23, v_call_i_2_23);
(*   %arrayidx11.2.23 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %559 = load i16, i16* %arrayidx11.2.23, align 2, !tbaa !3 *)
mov v559 mem0_46;
(*   %sub.2.23 = sub i16 %559, %call.i.2.23 *)
sub v_sub_2_23 v559 v_call_i_2_23;
(*   store i16 %sub.2.23, i16* %arrayidx9.2.23, align 2, !tbaa !3 *)
mov mem0_110 v_sub_2_23;
(*   %add21.2.23 = add i16 %559, %call.i.2.23 *)
add v_add21_2_23 v559 v_call_i_2_23;
(*   store i16 %add21.2.23, i16* %arrayidx11.2.23, align 2, !tbaa !3 *)
mov mem0_46 v_add21_2_23;
(*   %arrayidx9.2.24 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %560 = load i16, i16* %arrayidx9.2.24, align 2, !tbaa !3 *)
mov v560 mem0_112;
(*   %conv1.i.2.24 = sext i16 %560 to i32 *)
cast v_conv1_i_2_24@sint32 v560@sint16;
(*   %mul.i.2.24 = mul nsw i32 %conv1.i.2.24, 1493 *)
mul v_mul_i_2_24 v_conv1_i_2_24 (1493)@sint32;
(*   %call.i.2.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_24, v_call_i_2_24);
(*   %arrayidx11.2.24 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %561 = load i16, i16* %arrayidx11.2.24, align 2, !tbaa !3 *)
mov v561 mem0_48;
(*   %sub.2.24 = sub i16 %561, %call.i.2.24 *)
sub v_sub_2_24 v561 v_call_i_2_24;
(*   store i16 %sub.2.24, i16* %arrayidx9.2.24, align 2, !tbaa !3 *)
mov mem0_112 v_sub_2_24;
(*   %add21.2.24 = add i16 %561, %call.i.2.24 *)
add v_add21_2_24 v561 v_call_i_2_24;
(*   store i16 %add21.2.24, i16* %arrayidx11.2.24, align 2, !tbaa !3 *)
mov mem0_48 v_add21_2_24;
(*   %arrayidx9.2.25 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %562 = load i16, i16* %arrayidx9.2.25, align 2, !tbaa !3 *)
mov v562 mem0_114;
(*   %conv1.i.2.25 = sext i16 %562 to i32 *)
cast v_conv1_i_2_25@sint32 v562@sint16;
(*   %mul.i.2.25 = mul nsw i32 %conv1.i.2.25, 1493 *)
mul v_mul_i_2_25 v_conv1_i_2_25 (1493)@sint32;
(*   %call.i.2.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_25, v_call_i_2_25);
(*   %arrayidx11.2.25 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %563 = load i16, i16* %arrayidx11.2.25, align 2, !tbaa !3 *)
mov v563 mem0_50;
(*   %sub.2.25 = sub i16 %563, %call.i.2.25 *)
sub v_sub_2_25 v563 v_call_i_2_25;
(*   store i16 %sub.2.25, i16* %arrayidx9.2.25, align 2, !tbaa !3 *)
mov mem0_114 v_sub_2_25;
(*   %add21.2.25 = add i16 %563, %call.i.2.25 *)
add v_add21_2_25 v563 v_call_i_2_25;
(*   store i16 %add21.2.25, i16* %arrayidx11.2.25, align 2, !tbaa !3 *)
mov mem0_50 v_add21_2_25;
(*   %arrayidx9.2.26 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %564 = load i16, i16* %arrayidx9.2.26, align 2, !tbaa !3 *)
mov v564 mem0_116;
(*   %conv1.i.2.26 = sext i16 %564 to i32 *)
cast v_conv1_i_2_26@sint32 v564@sint16;
(*   %mul.i.2.26 = mul nsw i32 %conv1.i.2.26, 1493 *)
mul v_mul_i_2_26 v_conv1_i_2_26 (1493)@sint32;
(*   %call.i.2.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_26, v_call_i_2_26);
(*   %arrayidx11.2.26 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %565 = load i16, i16* %arrayidx11.2.26, align 2, !tbaa !3 *)
mov v565 mem0_52;
(*   %sub.2.26 = sub i16 %565, %call.i.2.26 *)
sub v_sub_2_26 v565 v_call_i_2_26;
(*   store i16 %sub.2.26, i16* %arrayidx9.2.26, align 2, !tbaa !3 *)
mov mem0_116 v_sub_2_26;
(*   %add21.2.26 = add i16 %565, %call.i.2.26 *)
add v_add21_2_26 v565 v_call_i_2_26;
(*   store i16 %add21.2.26, i16* %arrayidx11.2.26, align 2, !tbaa !3 *)
mov mem0_52 v_add21_2_26;
(*   %arrayidx9.2.27 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %566 = load i16, i16* %arrayidx9.2.27, align 2, !tbaa !3 *)
mov v566 mem0_118;
(*   %conv1.i.2.27 = sext i16 %566 to i32 *)
cast v_conv1_i_2_27@sint32 v566@sint16;
(*   %mul.i.2.27 = mul nsw i32 %conv1.i.2.27, 1493 *)
mul v_mul_i_2_27 v_conv1_i_2_27 (1493)@sint32;
(*   %call.i.2.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_27, v_call_i_2_27);
(*   %arrayidx11.2.27 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %567 = load i16, i16* %arrayidx11.2.27, align 2, !tbaa !3 *)
mov v567 mem0_54;
(*   %sub.2.27 = sub i16 %567, %call.i.2.27 *)
sub v_sub_2_27 v567 v_call_i_2_27;
(*   store i16 %sub.2.27, i16* %arrayidx9.2.27, align 2, !tbaa !3 *)
mov mem0_118 v_sub_2_27;
(*   %add21.2.27 = add i16 %567, %call.i.2.27 *)
add v_add21_2_27 v567 v_call_i_2_27;
(*   store i16 %add21.2.27, i16* %arrayidx11.2.27, align 2, !tbaa !3 *)
mov mem0_54 v_add21_2_27;
(*   %arrayidx9.2.28 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %568 = load i16, i16* %arrayidx9.2.28, align 2, !tbaa !3 *)
mov v568 mem0_120;
(*   %conv1.i.2.28 = sext i16 %568 to i32 *)
cast v_conv1_i_2_28@sint32 v568@sint16;
(*   %mul.i.2.28 = mul nsw i32 %conv1.i.2.28, 1493 *)
mul v_mul_i_2_28 v_conv1_i_2_28 (1493)@sint32;
(*   %call.i.2.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_28, v_call_i_2_28);
(*   %arrayidx11.2.28 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %569 = load i16, i16* %arrayidx11.2.28, align 2, !tbaa !3 *)
mov v569 mem0_56;
(*   %sub.2.28 = sub i16 %569, %call.i.2.28 *)
sub v_sub_2_28 v569 v_call_i_2_28;
(*   store i16 %sub.2.28, i16* %arrayidx9.2.28, align 2, !tbaa !3 *)
mov mem0_120 v_sub_2_28;
(*   %add21.2.28 = add i16 %569, %call.i.2.28 *)
add v_add21_2_28 v569 v_call_i_2_28;
(*   store i16 %add21.2.28, i16* %arrayidx11.2.28, align 2, !tbaa !3 *)
mov mem0_56 v_add21_2_28;
(*   %arrayidx9.2.29 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %570 = load i16, i16* %arrayidx9.2.29, align 2, !tbaa !3 *)
mov v570 mem0_122;
(*   %conv1.i.2.29 = sext i16 %570 to i32 *)
cast v_conv1_i_2_29@sint32 v570@sint16;
(*   %mul.i.2.29 = mul nsw i32 %conv1.i.2.29, 1493 *)
mul v_mul_i_2_29 v_conv1_i_2_29 (1493)@sint32;
(*   %call.i.2.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_29, v_call_i_2_29);
(*   %arrayidx11.2.29 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %571 = load i16, i16* %arrayidx11.2.29, align 2, !tbaa !3 *)
mov v571 mem0_58;
(*   %sub.2.29 = sub i16 %571, %call.i.2.29 *)
sub v_sub_2_29 v571 v_call_i_2_29;
(*   store i16 %sub.2.29, i16* %arrayidx9.2.29, align 2, !tbaa !3 *)
mov mem0_122 v_sub_2_29;
(*   %add21.2.29 = add i16 %571, %call.i.2.29 *)
add v_add21_2_29 v571 v_call_i_2_29;
(*   store i16 %add21.2.29, i16* %arrayidx11.2.29, align 2, !tbaa !3 *)
mov mem0_58 v_add21_2_29;
(*   %arrayidx9.2.30 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %572 = load i16, i16* %arrayidx9.2.30, align 2, !tbaa !3 *)
mov v572 mem0_124;
(*   %conv1.i.2.30 = sext i16 %572 to i32 *)
cast v_conv1_i_2_30@sint32 v572@sint16;
(*   %mul.i.2.30 = mul nsw i32 %conv1.i.2.30, 1493 *)
mul v_mul_i_2_30 v_conv1_i_2_30 (1493)@sint32;
(*   %call.i.2.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_30, v_call_i_2_30);
(*   %arrayidx11.2.30 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %573 = load i16, i16* %arrayidx11.2.30, align 2, !tbaa !3 *)
mov v573 mem0_60;
(*   %sub.2.30 = sub i16 %573, %call.i.2.30 *)
sub v_sub_2_30 v573 v_call_i_2_30;
(*   store i16 %sub.2.30, i16* %arrayidx9.2.30, align 2, !tbaa !3 *)
mov mem0_124 v_sub_2_30;
(*   %add21.2.30 = add i16 %573, %call.i.2.30 *)
add v_add21_2_30 v573 v_call_i_2_30;
(*   store i16 %add21.2.30, i16* %arrayidx11.2.30, align 2, !tbaa !3 *)
mov mem0_60 v_add21_2_30;
(*   %arrayidx9.2.31 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %574 = load i16, i16* %arrayidx9.2.31, align 2, !tbaa !3 *)
mov v574 mem0_126;
(*   %conv1.i.2.31 = sext i16 %574 to i32 *)
cast v_conv1_i_2_31@sint32 v574@sint16;
(*   %mul.i.2.31 = mul nsw i32 %conv1.i.2.31, 1493 *)
mul v_mul_i_2_31 v_conv1_i_2_31 (1493)@sint32;
(*   %call.i.2.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_31, v_call_i_2_31);
(*   %arrayidx11.2.31 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %575 = load i16, i16* %arrayidx11.2.31, align 2, !tbaa !3 *)
mov v575 mem0_62;
(*   %sub.2.31 = sub i16 %575, %call.i.2.31 *)
sub v_sub_2_31 v575 v_call_i_2_31;
(*   store i16 %sub.2.31, i16* %arrayidx9.2.31, align 2, !tbaa !3 *)
mov mem0_126 v_sub_2_31;
(*   %add21.2.31 = add i16 %575, %call.i.2.31 *)
add v_add21_2_31 v575 v_call_i_2_31;
(*   store i16 %add21.2.31, i16* %arrayidx11.2.31, align 2, !tbaa !3 *)
mov mem0_62 v_add21_2_31;

(* NOTE: k = 5 *)

(*   %arrayidx9.2.1248 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %576 = load i16, i16* %arrayidx9.2.1248, align 2, !tbaa !3 *)
mov v576 mem0_192;
(*   %conv1.i.2.1249 = sext i16 %576 to i32 *)
cast v_conv1_i_2_1249@sint32 v576@sint16;
(*   %mul.i.2.1250 = mul nsw i32 %conv1.i.2.1249, 1422 *)
mul v_mul_i_2_1250 v_conv1_i_2_1249 (1422)@sint32;
(*   %call.i.2.1251 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.1250) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_1250, v_call_i_2_1251);
(*   %arrayidx11.2.1252 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %577 = load i16, i16* %arrayidx11.2.1252, align 2, !tbaa !3 *)
mov v577 mem0_128;
(*   %sub.2.1253 = sub i16 %577, %call.i.2.1251 *)
sub v_sub_2_1253 v577 v_call_i_2_1251;
(*   store i16 %sub.2.1253, i16* %arrayidx9.2.1248, align 2, !tbaa !3 *)
mov mem0_192 v_sub_2_1253;
(*   %add21.2.1254 = add i16 %577, %call.i.2.1251 *)
add v_add21_2_1254 v577 v_call_i_2_1251;
(*   store i16 %add21.2.1254, i16* %arrayidx11.2.1252, align 2, !tbaa !3 *)
mov mem0_128 v_add21_2_1254;
(*   %arrayidx9.2.1.1 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %578 = load i16, i16* %arrayidx9.2.1.1, align 2, !tbaa !3 *)
mov v578 mem0_194;
(*   %conv1.i.2.1.1 = sext i16 %578 to i32 *)
cast v_conv1_i_2_1_1@sint32 v578@sint16;
(*   %mul.i.2.1.1 = mul nsw i32 %conv1.i.2.1.1, 1422 *)
mul v_mul_i_2_1_1 v_conv1_i_2_1_1 (1422)@sint32;
(*   %call.i.2.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_1_1, v_call_i_2_1_1);
(*   %arrayidx11.2.1.1 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %579 = load i16, i16* %arrayidx11.2.1.1, align 2, !tbaa !3 *)
mov v579 mem0_130;
(*   %sub.2.1.1 = sub i16 %579, %call.i.2.1.1 *)
sub v_sub_2_1_1 v579 v_call_i_2_1_1;
(*   store i16 %sub.2.1.1, i16* %arrayidx9.2.1.1, align 2, !tbaa !3 *)
mov mem0_194 v_sub_2_1_1;
(*   %add21.2.1.1 = add i16 %579, %call.i.2.1.1 *)
add v_add21_2_1_1 v579 v_call_i_2_1_1;
(*   store i16 %add21.2.1.1, i16* %arrayidx11.2.1.1, align 2, !tbaa !3 *)
mov mem0_130 v_add21_2_1_1;
(*   %arrayidx9.2.2.1 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %580 = load i16, i16* %arrayidx9.2.2.1, align 2, !tbaa !3 *)
mov v580 mem0_196;
(*   %conv1.i.2.2.1 = sext i16 %580 to i32 *)
cast v_conv1_i_2_2_1@sint32 v580@sint16;
(*   %mul.i.2.2.1 = mul nsw i32 %conv1.i.2.2.1, 1422 *)
mul v_mul_i_2_2_1 v_conv1_i_2_2_1 (1422)@sint32;
(*   %call.i.2.2.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.2.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_2_1, v_call_i_2_2_1);
(*   %arrayidx11.2.2.1 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %581 = load i16, i16* %arrayidx11.2.2.1, align 2, !tbaa !3 *)
mov v581 mem0_132;
(*   %sub.2.2.1 = sub i16 %581, %call.i.2.2.1 *)
sub v_sub_2_2_1 v581 v_call_i_2_2_1;
(*   store i16 %sub.2.2.1, i16* %arrayidx9.2.2.1, align 2, !tbaa !3 *)
mov mem0_196 v_sub_2_2_1;
(*   %add21.2.2.1 = add i16 %581, %call.i.2.2.1 *)
add v_add21_2_2_1 v581 v_call_i_2_2_1;
(*   store i16 %add21.2.2.1, i16* %arrayidx11.2.2.1, align 2, !tbaa !3 *)
mov mem0_132 v_add21_2_2_1;
(*   %arrayidx9.2.3.1 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %582 = load i16, i16* %arrayidx9.2.3.1, align 2, !tbaa !3 *)
mov v582 mem0_198;
(*   %conv1.i.2.3.1 = sext i16 %582 to i32 *)
cast v_conv1_i_2_3_1@sint32 v582@sint16;
(*   %mul.i.2.3.1 = mul nsw i32 %conv1.i.2.3.1, 1422 *)
mul v_mul_i_2_3_1 v_conv1_i_2_3_1 (1422)@sint32;
(*   %call.i.2.3.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.3.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_3_1, v_call_i_2_3_1);
(*   %arrayidx11.2.3.1 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %583 = load i16, i16* %arrayidx11.2.3.1, align 2, !tbaa !3 *)
mov v583 mem0_134;
(*   %sub.2.3.1 = sub i16 %583, %call.i.2.3.1 *)
sub v_sub_2_3_1 v583 v_call_i_2_3_1;
(*   store i16 %sub.2.3.1, i16* %arrayidx9.2.3.1, align 2, !tbaa !3 *)
mov mem0_198 v_sub_2_3_1;
(*   %add21.2.3.1 = add i16 %583, %call.i.2.3.1 *)
add v_add21_2_3_1 v583 v_call_i_2_3_1;
(*   store i16 %add21.2.3.1, i16* %arrayidx11.2.3.1, align 2, !tbaa !3 *)
mov mem0_134 v_add21_2_3_1;
(*   %arrayidx9.2.4.1 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %584 = load i16, i16* %arrayidx9.2.4.1, align 2, !tbaa !3 *)
mov v584 mem0_200;
(*   %conv1.i.2.4.1 = sext i16 %584 to i32 *)
cast v_conv1_i_2_4_1@sint32 v584@sint16;
(*   %mul.i.2.4.1 = mul nsw i32 %conv1.i.2.4.1, 1422 *)
mul v_mul_i_2_4_1 v_conv1_i_2_4_1 (1422)@sint32;
(*   %call.i.2.4.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.4.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_4_1, v_call_i_2_4_1);
(*   %arrayidx11.2.4.1 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %585 = load i16, i16* %arrayidx11.2.4.1, align 2, !tbaa !3 *)
mov v585 mem0_136;
(*   %sub.2.4.1 = sub i16 %585, %call.i.2.4.1 *)
sub v_sub_2_4_1 v585 v_call_i_2_4_1;
(*   store i16 %sub.2.4.1, i16* %arrayidx9.2.4.1, align 2, !tbaa !3 *)
mov mem0_200 v_sub_2_4_1;
(*   %add21.2.4.1 = add i16 %585, %call.i.2.4.1 *)
add v_add21_2_4_1 v585 v_call_i_2_4_1;
(*   store i16 %add21.2.4.1, i16* %arrayidx11.2.4.1, align 2, !tbaa !3 *)
mov mem0_136 v_add21_2_4_1;
(*   %arrayidx9.2.5.1 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %586 = load i16, i16* %arrayidx9.2.5.1, align 2, !tbaa !3 *)
mov v586 mem0_202;
(*   %conv1.i.2.5.1 = sext i16 %586 to i32 *)
cast v_conv1_i_2_5_1@sint32 v586@sint16;
(*   %mul.i.2.5.1 = mul nsw i32 %conv1.i.2.5.1, 1422 *)
mul v_mul_i_2_5_1 v_conv1_i_2_5_1 (1422)@sint32;
(*   %call.i.2.5.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.5.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_5_1, v_call_i_2_5_1);
(*   %arrayidx11.2.5.1 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %587 = load i16, i16* %arrayidx11.2.5.1, align 2, !tbaa !3 *)
mov v587 mem0_138;
(*   %sub.2.5.1 = sub i16 %587, %call.i.2.5.1 *)
sub v_sub_2_5_1 v587 v_call_i_2_5_1;
(*   store i16 %sub.2.5.1, i16* %arrayidx9.2.5.1, align 2, !tbaa !3 *)
mov mem0_202 v_sub_2_5_1;
(*   %add21.2.5.1 = add i16 %587, %call.i.2.5.1 *)
add v_add21_2_5_1 v587 v_call_i_2_5_1;
(*   store i16 %add21.2.5.1, i16* %arrayidx11.2.5.1, align 2, !tbaa !3 *)
mov mem0_138 v_add21_2_5_1;
(*   %arrayidx9.2.6.1 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %588 = load i16, i16* %arrayidx9.2.6.1, align 2, !tbaa !3 *)
mov v588 mem0_204;
(*   %conv1.i.2.6.1 = sext i16 %588 to i32 *)
cast v_conv1_i_2_6_1@sint32 v588@sint16;
(*   %mul.i.2.6.1 = mul nsw i32 %conv1.i.2.6.1, 1422 *)
mul v_mul_i_2_6_1 v_conv1_i_2_6_1 (1422)@sint32;
(*   %call.i.2.6.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.6.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_6_1, v_call_i_2_6_1);
(*   %arrayidx11.2.6.1 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %589 = load i16, i16* %arrayidx11.2.6.1, align 2, !tbaa !3 *)
mov v589 mem0_140;
(*   %sub.2.6.1 = sub i16 %589, %call.i.2.6.1 *)
sub v_sub_2_6_1 v589 v_call_i_2_6_1;
(*   store i16 %sub.2.6.1, i16* %arrayidx9.2.6.1, align 2, !tbaa !3 *)
mov mem0_204 v_sub_2_6_1;
(*   %add21.2.6.1 = add i16 %589, %call.i.2.6.1 *)
add v_add21_2_6_1 v589 v_call_i_2_6_1;
(*   store i16 %add21.2.6.1, i16* %arrayidx11.2.6.1, align 2, !tbaa !3 *)
mov mem0_140 v_add21_2_6_1;
(*   %arrayidx9.2.7.1 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %590 = load i16, i16* %arrayidx9.2.7.1, align 2, !tbaa !3 *)
mov v590 mem0_206;
(*   %conv1.i.2.7.1 = sext i16 %590 to i32 *)
cast v_conv1_i_2_7_1@sint32 v590@sint16;
(*   %mul.i.2.7.1 = mul nsw i32 %conv1.i.2.7.1, 1422 *)
mul v_mul_i_2_7_1 v_conv1_i_2_7_1 (1422)@sint32;
(*   %call.i.2.7.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.7.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_7_1, v_call_i_2_7_1);
(*   %arrayidx11.2.7.1 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %591 = load i16, i16* %arrayidx11.2.7.1, align 2, !tbaa !3 *)
mov v591 mem0_142;
(*   %sub.2.7.1 = sub i16 %591, %call.i.2.7.1 *)
sub v_sub_2_7_1 v591 v_call_i_2_7_1;
(*   store i16 %sub.2.7.1, i16* %arrayidx9.2.7.1, align 2, !tbaa !3 *)
mov mem0_206 v_sub_2_7_1;
(*   %add21.2.7.1 = add i16 %591, %call.i.2.7.1 *)
add v_add21_2_7_1 v591 v_call_i_2_7_1;
(*   store i16 %add21.2.7.1, i16* %arrayidx11.2.7.1, align 2, !tbaa !3 *)
mov mem0_142 v_add21_2_7_1;
(*   %arrayidx9.2.8.1 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %592 = load i16, i16* %arrayidx9.2.8.1, align 2, !tbaa !3 *)
mov v592 mem0_208;
(*   %conv1.i.2.8.1 = sext i16 %592 to i32 *)
cast v_conv1_i_2_8_1@sint32 v592@sint16;
(*   %mul.i.2.8.1 = mul nsw i32 %conv1.i.2.8.1, 1422 *)
mul v_mul_i_2_8_1 v_conv1_i_2_8_1 (1422)@sint32;
(*   %call.i.2.8.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.8.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_8_1, v_call_i_2_8_1);
(*   %arrayidx11.2.8.1 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %593 = load i16, i16* %arrayidx11.2.8.1, align 2, !tbaa !3 *)
mov v593 mem0_144;
(*   %sub.2.8.1 = sub i16 %593, %call.i.2.8.1 *)
sub v_sub_2_8_1 v593 v_call_i_2_8_1;
(*   store i16 %sub.2.8.1, i16* %arrayidx9.2.8.1, align 2, !tbaa !3 *)
mov mem0_208 v_sub_2_8_1;
(*   %add21.2.8.1 = add i16 %593, %call.i.2.8.1 *)
add v_add21_2_8_1 v593 v_call_i_2_8_1;
(*   store i16 %add21.2.8.1, i16* %arrayidx11.2.8.1, align 2, !tbaa !3 *)
mov mem0_144 v_add21_2_8_1;
(*   %arrayidx9.2.9.1 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %594 = load i16, i16* %arrayidx9.2.9.1, align 2, !tbaa !3 *)
mov v594 mem0_210;
(*   %conv1.i.2.9.1 = sext i16 %594 to i32 *)
cast v_conv1_i_2_9_1@sint32 v594@sint16;
(*   %mul.i.2.9.1 = mul nsw i32 %conv1.i.2.9.1, 1422 *)
mul v_mul_i_2_9_1 v_conv1_i_2_9_1 (1422)@sint32;
(*   %call.i.2.9.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.9.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_9_1, v_call_i_2_9_1);
(*   %arrayidx11.2.9.1 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %595 = load i16, i16* %arrayidx11.2.9.1, align 2, !tbaa !3 *)
mov v595 mem0_146;
(*   %sub.2.9.1 = sub i16 %595, %call.i.2.9.1 *)
sub v_sub_2_9_1 v595 v_call_i_2_9_1;
(*   store i16 %sub.2.9.1, i16* %arrayidx9.2.9.1, align 2, !tbaa !3 *)
mov mem0_210 v_sub_2_9_1;
(*   %add21.2.9.1 = add i16 %595, %call.i.2.9.1 *)
add v_add21_2_9_1 v595 v_call_i_2_9_1;
(*   store i16 %add21.2.9.1, i16* %arrayidx11.2.9.1, align 2, !tbaa !3 *)
mov mem0_146 v_add21_2_9_1;
(*   %arrayidx9.2.10.1 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %596 = load i16, i16* %arrayidx9.2.10.1, align 2, !tbaa !3 *)
mov v596 mem0_212;
(*   %conv1.i.2.10.1 = sext i16 %596 to i32 *)
cast v_conv1_i_2_10_1@sint32 v596@sint16;
(*   %mul.i.2.10.1 = mul nsw i32 %conv1.i.2.10.1, 1422 *)
mul v_mul_i_2_10_1 v_conv1_i_2_10_1 (1422)@sint32;
(*   %call.i.2.10.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.10.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_10_1, v_call_i_2_10_1);
(*   %arrayidx11.2.10.1 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %597 = load i16, i16* %arrayidx11.2.10.1, align 2, !tbaa !3 *)
mov v597 mem0_148;
(*   %sub.2.10.1 = sub i16 %597, %call.i.2.10.1 *)
sub v_sub_2_10_1 v597 v_call_i_2_10_1;
(*   store i16 %sub.2.10.1, i16* %arrayidx9.2.10.1, align 2, !tbaa !3 *)
mov mem0_212 v_sub_2_10_1;
(*   %add21.2.10.1 = add i16 %597, %call.i.2.10.1 *)
add v_add21_2_10_1 v597 v_call_i_2_10_1;
(*   store i16 %add21.2.10.1, i16* %arrayidx11.2.10.1, align 2, !tbaa !3 *)
mov mem0_148 v_add21_2_10_1;
(*   %arrayidx9.2.11.1 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %598 = load i16, i16* %arrayidx9.2.11.1, align 2, !tbaa !3 *)
mov v598 mem0_214;
(*   %conv1.i.2.11.1 = sext i16 %598 to i32 *)
cast v_conv1_i_2_11_1@sint32 v598@sint16;
(*   %mul.i.2.11.1 = mul nsw i32 %conv1.i.2.11.1, 1422 *)
mul v_mul_i_2_11_1 v_conv1_i_2_11_1 (1422)@sint32;
(*   %call.i.2.11.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.11.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_11_1, v_call_i_2_11_1);
(*   %arrayidx11.2.11.1 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %599 = load i16, i16* %arrayidx11.2.11.1, align 2, !tbaa !3 *)
mov v599 mem0_150;
(*   %sub.2.11.1 = sub i16 %599, %call.i.2.11.1 *)
sub v_sub_2_11_1 v599 v_call_i_2_11_1;
(*   store i16 %sub.2.11.1, i16* %arrayidx9.2.11.1, align 2, !tbaa !3 *)
mov mem0_214 v_sub_2_11_1;
(*   %add21.2.11.1 = add i16 %599, %call.i.2.11.1 *)
add v_add21_2_11_1 v599 v_call_i_2_11_1;
(*   store i16 %add21.2.11.1, i16* %arrayidx11.2.11.1, align 2, !tbaa !3 *)
mov mem0_150 v_add21_2_11_1;
(*   %arrayidx9.2.12.1 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %600 = load i16, i16* %arrayidx9.2.12.1, align 2, !tbaa !3 *)
mov v600 mem0_216;
(*   %conv1.i.2.12.1 = sext i16 %600 to i32 *)
cast v_conv1_i_2_12_1@sint32 v600@sint16;
(*   %mul.i.2.12.1 = mul nsw i32 %conv1.i.2.12.1, 1422 *)
mul v_mul_i_2_12_1 v_conv1_i_2_12_1 (1422)@sint32;
(*   %call.i.2.12.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.12.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_12_1, v_call_i_2_12_1);
(*   %arrayidx11.2.12.1 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %601 = load i16, i16* %arrayidx11.2.12.1, align 2, !tbaa !3 *)
mov v601 mem0_152;
(*   %sub.2.12.1 = sub i16 %601, %call.i.2.12.1 *)
sub v_sub_2_12_1 v601 v_call_i_2_12_1;
(*   store i16 %sub.2.12.1, i16* %arrayidx9.2.12.1, align 2, !tbaa !3 *)
mov mem0_216 v_sub_2_12_1;
(*   %add21.2.12.1 = add i16 %601, %call.i.2.12.1 *)
add v_add21_2_12_1 v601 v_call_i_2_12_1;
(*   store i16 %add21.2.12.1, i16* %arrayidx11.2.12.1, align 2, !tbaa !3 *)
mov mem0_152 v_add21_2_12_1;
(*   %arrayidx9.2.13.1 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %602 = load i16, i16* %arrayidx9.2.13.1, align 2, !tbaa !3 *)
mov v602 mem0_218;
(*   %conv1.i.2.13.1 = sext i16 %602 to i32 *)
cast v_conv1_i_2_13_1@sint32 v602@sint16;
(*   %mul.i.2.13.1 = mul nsw i32 %conv1.i.2.13.1, 1422 *)
mul v_mul_i_2_13_1 v_conv1_i_2_13_1 (1422)@sint32;
(*   %call.i.2.13.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.13.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_13_1, v_call_i_2_13_1);
(*   %arrayidx11.2.13.1 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %603 = load i16, i16* %arrayidx11.2.13.1, align 2, !tbaa !3 *)
mov v603 mem0_154;
(*   %sub.2.13.1 = sub i16 %603, %call.i.2.13.1 *)
sub v_sub_2_13_1 v603 v_call_i_2_13_1;
(*   store i16 %sub.2.13.1, i16* %arrayidx9.2.13.1, align 2, !tbaa !3 *)
mov mem0_218 v_sub_2_13_1;
(*   %add21.2.13.1 = add i16 %603, %call.i.2.13.1 *)
add v_add21_2_13_1 v603 v_call_i_2_13_1;
(*   store i16 %add21.2.13.1, i16* %arrayidx11.2.13.1, align 2, !tbaa !3 *)
mov mem0_154 v_add21_2_13_1;
(*   %arrayidx9.2.14.1 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %604 = load i16, i16* %arrayidx9.2.14.1, align 2, !tbaa !3 *)
mov v604 mem0_220;
(*   %conv1.i.2.14.1 = sext i16 %604 to i32 *)
cast v_conv1_i_2_14_1@sint32 v604@sint16;
(*   %mul.i.2.14.1 = mul nsw i32 %conv1.i.2.14.1, 1422 *)
mul v_mul_i_2_14_1 v_conv1_i_2_14_1 (1422)@sint32;
(*   %call.i.2.14.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.14.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_14_1, v_call_i_2_14_1);
(*   %arrayidx11.2.14.1 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %605 = load i16, i16* %arrayidx11.2.14.1, align 2, !tbaa !3 *)
mov v605 mem0_156;
(*   %sub.2.14.1 = sub i16 %605, %call.i.2.14.1 *)
sub v_sub_2_14_1 v605 v_call_i_2_14_1;
(*   store i16 %sub.2.14.1, i16* %arrayidx9.2.14.1, align 2, !tbaa !3 *)
mov mem0_220 v_sub_2_14_1;
(*   %add21.2.14.1 = add i16 %605, %call.i.2.14.1 *)
add v_add21_2_14_1 v605 v_call_i_2_14_1;
(*   store i16 %add21.2.14.1, i16* %arrayidx11.2.14.1, align 2, !tbaa !3 *)
mov mem0_156 v_add21_2_14_1;
(*   %arrayidx9.2.15.1 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %606 = load i16, i16* %arrayidx9.2.15.1, align 2, !tbaa !3 *)
mov v606 mem0_222;
(*   %conv1.i.2.15.1 = sext i16 %606 to i32 *)
cast v_conv1_i_2_15_1@sint32 v606@sint16;
(*   %mul.i.2.15.1 = mul nsw i32 %conv1.i.2.15.1, 1422 *)
mul v_mul_i_2_15_1 v_conv1_i_2_15_1 (1422)@sint32;
(*   %call.i.2.15.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.15.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_15_1, v_call_i_2_15_1);
(*   %arrayidx11.2.15.1 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %607 = load i16, i16* %arrayidx11.2.15.1, align 2, !tbaa !3 *)
mov v607 mem0_158;
(*   %sub.2.15.1 = sub i16 %607, %call.i.2.15.1 *)
sub v_sub_2_15_1 v607 v_call_i_2_15_1;
(*   store i16 %sub.2.15.1, i16* %arrayidx9.2.15.1, align 2, !tbaa !3 *)
mov mem0_222 v_sub_2_15_1;
(*   %add21.2.15.1 = add i16 %607, %call.i.2.15.1 *)
add v_add21_2_15_1 v607 v_call_i_2_15_1;
(*   store i16 %add21.2.15.1, i16* %arrayidx11.2.15.1, align 2, !tbaa !3 *)
mov mem0_158 v_add21_2_15_1;
(*   %arrayidx9.2.16.1 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %608 = load i16, i16* %arrayidx9.2.16.1, align 2, !tbaa !3 *)
mov v608 mem0_224;
(*   %conv1.i.2.16.1 = sext i16 %608 to i32 *)
cast v_conv1_i_2_16_1@sint32 v608@sint16;
(*   %mul.i.2.16.1 = mul nsw i32 %conv1.i.2.16.1, 1422 *)
mul v_mul_i_2_16_1 v_conv1_i_2_16_1 (1422)@sint32;
(*   %call.i.2.16.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.16.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_16_1, v_call_i_2_16_1);
(*   %arrayidx11.2.16.1 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %609 = load i16, i16* %arrayidx11.2.16.1, align 2, !tbaa !3 *)
mov v609 mem0_160;
(*   %sub.2.16.1 = sub i16 %609, %call.i.2.16.1 *)
sub v_sub_2_16_1 v609 v_call_i_2_16_1;
(*   store i16 %sub.2.16.1, i16* %arrayidx9.2.16.1, align 2, !tbaa !3 *)
mov mem0_224 v_sub_2_16_1;
(*   %add21.2.16.1 = add i16 %609, %call.i.2.16.1 *)
add v_add21_2_16_1 v609 v_call_i_2_16_1;
(*   store i16 %add21.2.16.1, i16* %arrayidx11.2.16.1, align 2, !tbaa !3 *)
mov mem0_160 v_add21_2_16_1;
(*   %arrayidx9.2.17.1 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %610 = load i16, i16* %arrayidx9.2.17.1, align 2, !tbaa !3 *)
mov v610 mem0_226;
(*   %conv1.i.2.17.1 = sext i16 %610 to i32 *)
cast v_conv1_i_2_17_1@sint32 v610@sint16;
(*   %mul.i.2.17.1 = mul nsw i32 %conv1.i.2.17.1, 1422 *)
mul v_mul_i_2_17_1 v_conv1_i_2_17_1 (1422)@sint32;
(*   %call.i.2.17.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.17.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_17_1, v_call_i_2_17_1);
(*   %arrayidx11.2.17.1 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %611 = load i16, i16* %arrayidx11.2.17.1, align 2, !tbaa !3 *)
mov v611 mem0_162;
(*   %sub.2.17.1 = sub i16 %611, %call.i.2.17.1 *)
sub v_sub_2_17_1 v611 v_call_i_2_17_1;
(*   store i16 %sub.2.17.1, i16* %arrayidx9.2.17.1, align 2, !tbaa !3 *)
mov mem0_226 v_sub_2_17_1;
(*   %add21.2.17.1 = add i16 %611, %call.i.2.17.1 *)
add v_add21_2_17_1 v611 v_call_i_2_17_1;
(*   store i16 %add21.2.17.1, i16* %arrayidx11.2.17.1, align 2, !tbaa !3 *)
mov mem0_162 v_add21_2_17_1;
(*   %arrayidx9.2.18.1 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %612 = load i16, i16* %arrayidx9.2.18.1, align 2, !tbaa !3 *)
mov v612 mem0_228;
(*   %conv1.i.2.18.1 = sext i16 %612 to i32 *)
cast v_conv1_i_2_18_1@sint32 v612@sint16;
(*   %mul.i.2.18.1 = mul nsw i32 %conv1.i.2.18.1, 1422 *)
mul v_mul_i_2_18_1 v_conv1_i_2_18_1 (1422)@sint32;
(*   %call.i.2.18.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.18.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_18_1, v_call_i_2_18_1);
(*   %arrayidx11.2.18.1 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %613 = load i16, i16* %arrayidx11.2.18.1, align 2, !tbaa !3 *)
mov v613 mem0_164;
(*   %sub.2.18.1 = sub i16 %613, %call.i.2.18.1 *)
sub v_sub_2_18_1 v613 v_call_i_2_18_1;
(*   store i16 %sub.2.18.1, i16* %arrayidx9.2.18.1, align 2, !tbaa !3 *)
mov mem0_228 v_sub_2_18_1;
(*   %add21.2.18.1 = add i16 %613, %call.i.2.18.1 *)
add v_add21_2_18_1 v613 v_call_i_2_18_1;
(*   store i16 %add21.2.18.1, i16* %arrayidx11.2.18.1, align 2, !tbaa !3 *)
mov mem0_164 v_add21_2_18_1;
(*   %arrayidx9.2.19.1 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %614 = load i16, i16* %arrayidx9.2.19.1, align 2, !tbaa !3 *)
mov v614 mem0_230;
(*   %conv1.i.2.19.1 = sext i16 %614 to i32 *)
cast v_conv1_i_2_19_1@sint32 v614@sint16;
(*   %mul.i.2.19.1 = mul nsw i32 %conv1.i.2.19.1, 1422 *)
mul v_mul_i_2_19_1 v_conv1_i_2_19_1 (1422)@sint32;
(*   %call.i.2.19.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.19.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_19_1, v_call_i_2_19_1);
(*   %arrayidx11.2.19.1 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %615 = load i16, i16* %arrayidx11.2.19.1, align 2, !tbaa !3 *)
mov v615 mem0_166;
(*   %sub.2.19.1 = sub i16 %615, %call.i.2.19.1 *)
sub v_sub_2_19_1 v615 v_call_i_2_19_1;
(*   store i16 %sub.2.19.1, i16* %arrayidx9.2.19.1, align 2, !tbaa !3 *)
mov mem0_230 v_sub_2_19_1;
(*   %add21.2.19.1 = add i16 %615, %call.i.2.19.1 *)
add v_add21_2_19_1 v615 v_call_i_2_19_1;
(*   store i16 %add21.2.19.1, i16* %arrayidx11.2.19.1, align 2, !tbaa !3 *)
mov mem0_166 v_add21_2_19_1;
(*   %arrayidx9.2.20.1 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %616 = load i16, i16* %arrayidx9.2.20.1, align 2, !tbaa !3 *)
mov v616 mem0_232;
(*   %conv1.i.2.20.1 = sext i16 %616 to i32 *)
cast v_conv1_i_2_20_1@sint32 v616@sint16;
(*   %mul.i.2.20.1 = mul nsw i32 %conv1.i.2.20.1, 1422 *)
mul v_mul_i_2_20_1 v_conv1_i_2_20_1 (1422)@sint32;
(*   %call.i.2.20.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.20.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_20_1, v_call_i_2_20_1);
(*   %arrayidx11.2.20.1 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %617 = load i16, i16* %arrayidx11.2.20.1, align 2, !tbaa !3 *)
mov v617 mem0_168;
(*   %sub.2.20.1 = sub i16 %617, %call.i.2.20.1 *)
sub v_sub_2_20_1 v617 v_call_i_2_20_1;
(*   store i16 %sub.2.20.1, i16* %arrayidx9.2.20.1, align 2, !tbaa !3 *)
mov mem0_232 v_sub_2_20_1;
(*   %add21.2.20.1 = add i16 %617, %call.i.2.20.1 *)
add v_add21_2_20_1 v617 v_call_i_2_20_1;
(*   store i16 %add21.2.20.1, i16* %arrayidx11.2.20.1, align 2, !tbaa !3 *)
mov mem0_168 v_add21_2_20_1;
(*   %arrayidx9.2.21.1 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %618 = load i16, i16* %arrayidx9.2.21.1, align 2, !tbaa !3 *)
mov v618 mem0_234;
(*   %conv1.i.2.21.1 = sext i16 %618 to i32 *)
cast v_conv1_i_2_21_1@sint32 v618@sint16;
(*   %mul.i.2.21.1 = mul nsw i32 %conv1.i.2.21.1, 1422 *)
mul v_mul_i_2_21_1 v_conv1_i_2_21_1 (1422)@sint32;
(*   %call.i.2.21.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.21.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_21_1, v_call_i_2_21_1);
(*   %arrayidx11.2.21.1 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %619 = load i16, i16* %arrayidx11.2.21.1, align 2, !tbaa !3 *)
mov v619 mem0_170;
(*   %sub.2.21.1 = sub i16 %619, %call.i.2.21.1 *)
sub v_sub_2_21_1 v619 v_call_i_2_21_1;
(*   store i16 %sub.2.21.1, i16* %arrayidx9.2.21.1, align 2, !tbaa !3 *)
mov mem0_234 v_sub_2_21_1;
(*   %add21.2.21.1 = add i16 %619, %call.i.2.21.1 *)
add v_add21_2_21_1 v619 v_call_i_2_21_1;
(*   store i16 %add21.2.21.1, i16* %arrayidx11.2.21.1, align 2, !tbaa !3 *)
mov mem0_170 v_add21_2_21_1;
(*   %arrayidx9.2.22.1 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %620 = load i16, i16* %arrayidx9.2.22.1, align 2, !tbaa !3 *)
mov v620 mem0_236;
(*   %conv1.i.2.22.1 = sext i16 %620 to i32 *)
cast v_conv1_i_2_22_1@sint32 v620@sint16;
(*   %mul.i.2.22.1 = mul nsw i32 %conv1.i.2.22.1, 1422 *)
mul v_mul_i_2_22_1 v_conv1_i_2_22_1 (1422)@sint32;
(*   %call.i.2.22.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.22.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_22_1, v_call_i_2_22_1);
(*   %arrayidx11.2.22.1 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %621 = load i16, i16* %arrayidx11.2.22.1, align 2, !tbaa !3 *)
mov v621 mem0_172;
(*   %sub.2.22.1 = sub i16 %621, %call.i.2.22.1 *)
sub v_sub_2_22_1 v621 v_call_i_2_22_1;
(*   store i16 %sub.2.22.1, i16* %arrayidx9.2.22.1, align 2, !tbaa !3 *)
mov mem0_236 v_sub_2_22_1;
(*   %add21.2.22.1 = add i16 %621, %call.i.2.22.1 *)
add v_add21_2_22_1 v621 v_call_i_2_22_1;
(*   store i16 %add21.2.22.1, i16* %arrayidx11.2.22.1, align 2, !tbaa !3 *)
mov mem0_172 v_add21_2_22_1;
(*   %arrayidx9.2.23.1 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %622 = load i16, i16* %arrayidx9.2.23.1, align 2, !tbaa !3 *)
mov v622 mem0_238;
(*   %conv1.i.2.23.1 = sext i16 %622 to i32 *)
cast v_conv1_i_2_23_1@sint32 v622@sint16;
(*   %mul.i.2.23.1 = mul nsw i32 %conv1.i.2.23.1, 1422 *)
mul v_mul_i_2_23_1 v_conv1_i_2_23_1 (1422)@sint32;
(*   %call.i.2.23.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.23.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_23_1, v_call_i_2_23_1);
(*   %arrayidx11.2.23.1 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %623 = load i16, i16* %arrayidx11.2.23.1, align 2, !tbaa !3 *)
mov v623 mem0_174;
(*   %sub.2.23.1 = sub i16 %623, %call.i.2.23.1 *)
sub v_sub_2_23_1 v623 v_call_i_2_23_1;
(*   store i16 %sub.2.23.1, i16* %arrayidx9.2.23.1, align 2, !tbaa !3 *)
mov mem0_238 v_sub_2_23_1;
(*   %add21.2.23.1 = add i16 %623, %call.i.2.23.1 *)
add v_add21_2_23_1 v623 v_call_i_2_23_1;
(*   store i16 %add21.2.23.1, i16* %arrayidx11.2.23.1, align 2, !tbaa !3 *)
mov mem0_174 v_add21_2_23_1;
(*   %arrayidx9.2.24.1 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %624 = load i16, i16* %arrayidx9.2.24.1, align 2, !tbaa !3 *)
mov v624 mem0_240;
(*   %conv1.i.2.24.1 = sext i16 %624 to i32 *)
cast v_conv1_i_2_24_1@sint32 v624@sint16;
(*   %mul.i.2.24.1 = mul nsw i32 %conv1.i.2.24.1, 1422 *)
mul v_mul_i_2_24_1 v_conv1_i_2_24_1 (1422)@sint32;
(*   %call.i.2.24.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.24.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_24_1, v_call_i_2_24_1);
(*   %arrayidx11.2.24.1 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %625 = load i16, i16* %arrayidx11.2.24.1, align 2, !tbaa !3 *)
mov v625 mem0_176;
(*   %sub.2.24.1 = sub i16 %625, %call.i.2.24.1 *)
sub v_sub_2_24_1 v625 v_call_i_2_24_1;
(*   store i16 %sub.2.24.1, i16* %arrayidx9.2.24.1, align 2, !tbaa !3 *)
mov mem0_240 v_sub_2_24_1;
(*   %add21.2.24.1 = add i16 %625, %call.i.2.24.1 *)
add v_add21_2_24_1 v625 v_call_i_2_24_1;
(*   store i16 %add21.2.24.1, i16* %arrayidx11.2.24.1, align 2, !tbaa !3 *)
mov mem0_176 v_add21_2_24_1;
(*   %arrayidx9.2.25.1 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %626 = load i16, i16* %arrayidx9.2.25.1, align 2, !tbaa !3 *)
mov v626 mem0_242;
(*   %conv1.i.2.25.1 = sext i16 %626 to i32 *)
cast v_conv1_i_2_25_1@sint32 v626@sint16;
(*   %mul.i.2.25.1 = mul nsw i32 %conv1.i.2.25.1, 1422 *)
mul v_mul_i_2_25_1 v_conv1_i_2_25_1 (1422)@sint32;
(*   %call.i.2.25.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.25.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_25_1, v_call_i_2_25_1);
(*   %arrayidx11.2.25.1 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %627 = load i16, i16* %arrayidx11.2.25.1, align 2, !tbaa !3 *)
mov v627 mem0_178;
(*   %sub.2.25.1 = sub i16 %627, %call.i.2.25.1 *)
sub v_sub_2_25_1 v627 v_call_i_2_25_1;
(*   store i16 %sub.2.25.1, i16* %arrayidx9.2.25.1, align 2, !tbaa !3 *)
mov mem0_242 v_sub_2_25_1;
(*   %add21.2.25.1 = add i16 %627, %call.i.2.25.1 *)
add v_add21_2_25_1 v627 v_call_i_2_25_1;
(*   store i16 %add21.2.25.1, i16* %arrayidx11.2.25.1, align 2, !tbaa !3 *)
mov mem0_178 v_add21_2_25_1;
(*   %arrayidx9.2.26.1 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %628 = load i16, i16* %arrayidx9.2.26.1, align 2, !tbaa !3 *)
mov v628 mem0_244;
(*   %conv1.i.2.26.1 = sext i16 %628 to i32 *)
cast v_conv1_i_2_26_1@sint32 v628@sint16;
(*   %mul.i.2.26.1 = mul nsw i32 %conv1.i.2.26.1, 1422 *)
mul v_mul_i_2_26_1 v_conv1_i_2_26_1 (1422)@sint32;
(*   %call.i.2.26.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.26.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_26_1, v_call_i_2_26_1);
(*   %arrayidx11.2.26.1 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %629 = load i16, i16* %arrayidx11.2.26.1, align 2, !tbaa !3 *)
mov v629 mem0_180;
(*   %sub.2.26.1 = sub i16 %629, %call.i.2.26.1 *)
sub v_sub_2_26_1 v629 v_call_i_2_26_1;
(*   store i16 %sub.2.26.1, i16* %arrayidx9.2.26.1, align 2, !tbaa !3 *)
mov mem0_244 v_sub_2_26_1;
(*   %add21.2.26.1 = add i16 %629, %call.i.2.26.1 *)
add v_add21_2_26_1 v629 v_call_i_2_26_1;
(*   store i16 %add21.2.26.1, i16* %arrayidx11.2.26.1, align 2, !tbaa !3 *)
mov mem0_180 v_add21_2_26_1;
(*   %arrayidx9.2.27.1 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %630 = load i16, i16* %arrayidx9.2.27.1, align 2, !tbaa !3 *)
mov v630 mem0_246;
(*   %conv1.i.2.27.1 = sext i16 %630 to i32 *)
cast v_conv1_i_2_27_1@sint32 v630@sint16;
(*   %mul.i.2.27.1 = mul nsw i32 %conv1.i.2.27.1, 1422 *)
mul v_mul_i_2_27_1 v_conv1_i_2_27_1 (1422)@sint32;
(*   %call.i.2.27.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.27.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_27_1, v_call_i_2_27_1);
(*   %arrayidx11.2.27.1 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %631 = load i16, i16* %arrayidx11.2.27.1, align 2, !tbaa !3 *)
mov v631 mem0_182;
(*   %sub.2.27.1 = sub i16 %631, %call.i.2.27.1 *)
sub v_sub_2_27_1 v631 v_call_i_2_27_1;
(*   store i16 %sub.2.27.1, i16* %arrayidx9.2.27.1, align 2, !tbaa !3 *)
mov mem0_246 v_sub_2_27_1;
(*   %add21.2.27.1 = add i16 %631, %call.i.2.27.1 *)
add v_add21_2_27_1 v631 v_call_i_2_27_1;
(*   store i16 %add21.2.27.1, i16* %arrayidx11.2.27.1, align 2, !tbaa !3 *)
mov mem0_182 v_add21_2_27_1;
(*   %arrayidx9.2.28.1 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %632 = load i16, i16* %arrayidx9.2.28.1, align 2, !tbaa !3 *)
mov v632 mem0_248;
(*   %conv1.i.2.28.1 = sext i16 %632 to i32 *)
cast v_conv1_i_2_28_1@sint32 v632@sint16;
(*   %mul.i.2.28.1 = mul nsw i32 %conv1.i.2.28.1, 1422 *)
mul v_mul_i_2_28_1 v_conv1_i_2_28_1 (1422)@sint32;
(*   %call.i.2.28.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.28.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_28_1, v_call_i_2_28_1);
(*   %arrayidx11.2.28.1 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %633 = load i16, i16* %arrayidx11.2.28.1, align 2, !tbaa !3 *)
mov v633 mem0_184;
(*   %sub.2.28.1 = sub i16 %633, %call.i.2.28.1 *)
sub v_sub_2_28_1 v633 v_call_i_2_28_1;
(*   store i16 %sub.2.28.1, i16* %arrayidx9.2.28.1, align 2, !tbaa !3 *)
mov mem0_248 v_sub_2_28_1;
(*   %add21.2.28.1 = add i16 %633, %call.i.2.28.1 *)
add v_add21_2_28_1 v633 v_call_i_2_28_1;
(*   store i16 %add21.2.28.1, i16* %arrayidx11.2.28.1, align 2, !tbaa !3 *)
mov mem0_184 v_add21_2_28_1;
(*   %arrayidx9.2.29.1 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %634 = load i16, i16* %arrayidx9.2.29.1, align 2, !tbaa !3 *)
mov v634 mem0_250;
(*   %conv1.i.2.29.1 = sext i16 %634 to i32 *)
cast v_conv1_i_2_29_1@sint32 v634@sint16;
(*   %mul.i.2.29.1 = mul nsw i32 %conv1.i.2.29.1, 1422 *)
mul v_mul_i_2_29_1 v_conv1_i_2_29_1 (1422)@sint32;
(*   %call.i.2.29.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.29.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_29_1, v_call_i_2_29_1);
(*   %arrayidx11.2.29.1 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %635 = load i16, i16* %arrayidx11.2.29.1, align 2, !tbaa !3 *)
mov v635 mem0_186;
(*   %sub.2.29.1 = sub i16 %635, %call.i.2.29.1 *)
sub v_sub_2_29_1 v635 v_call_i_2_29_1;
(*   store i16 %sub.2.29.1, i16* %arrayidx9.2.29.1, align 2, !tbaa !3 *)
mov mem0_250 v_sub_2_29_1;
(*   %add21.2.29.1 = add i16 %635, %call.i.2.29.1 *)
add v_add21_2_29_1 v635 v_call_i_2_29_1;
(*   store i16 %add21.2.29.1, i16* %arrayidx11.2.29.1, align 2, !tbaa !3 *)
mov mem0_186 v_add21_2_29_1;
(*   %arrayidx9.2.30.1 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %636 = load i16, i16* %arrayidx9.2.30.1, align 2, !tbaa !3 *)
mov v636 mem0_252;
(*   %conv1.i.2.30.1 = sext i16 %636 to i32 *)
cast v_conv1_i_2_30_1@sint32 v636@sint16;
(*   %mul.i.2.30.1 = mul nsw i32 %conv1.i.2.30.1, 1422 *)
mul v_mul_i_2_30_1 v_conv1_i_2_30_1 (1422)@sint32;
(*   %call.i.2.30.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.30.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_30_1, v_call_i_2_30_1);
(*   %arrayidx11.2.30.1 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %637 = load i16, i16* %arrayidx11.2.30.1, align 2, !tbaa !3 *)
mov v637 mem0_188;
(*   %sub.2.30.1 = sub i16 %637, %call.i.2.30.1 *)
sub v_sub_2_30_1 v637 v_call_i_2_30_1;
(*   store i16 %sub.2.30.1, i16* %arrayidx9.2.30.1, align 2, !tbaa !3 *)
mov mem0_252 v_sub_2_30_1;
(*   %add21.2.30.1 = add i16 %637, %call.i.2.30.1 *)
add v_add21_2_30_1 v637 v_call_i_2_30_1;
(*   store i16 %add21.2.30.1, i16* %arrayidx11.2.30.1, align 2, !tbaa !3 *)
mov mem0_188 v_add21_2_30_1;
(*   %arrayidx9.2.31.1 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %638 = load i16, i16* %arrayidx9.2.31.1, align 2, !tbaa !3 *)
mov v638 mem0_254;
(*   %conv1.i.2.31.1 = sext i16 %638 to i32 *)
cast v_conv1_i_2_31_1@sint32 v638@sint16;
(*   %mul.i.2.31.1 = mul nsw i32 %conv1.i.2.31.1, 1422 *)
mul v_mul_i_2_31_1 v_conv1_i_2_31_1 (1422)@sint32;
(*   %call.i.2.31.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.31.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_31_1, v_call_i_2_31_1);
(*   %arrayidx11.2.31.1 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %639 = load i16, i16* %arrayidx11.2.31.1, align 2, !tbaa !3 *)
mov v639 mem0_190;
(*   %sub.2.31.1 = sub i16 %639, %call.i.2.31.1 *)
sub v_sub_2_31_1 v639 v_call_i_2_31_1;
(*   store i16 %sub.2.31.1, i16* %arrayidx9.2.31.1, align 2, !tbaa !3 *)
mov mem0_254 v_sub_2_31_1;
(*   %add21.2.31.1 = add i16 %639, %call.i.2.31.1 *)
add v_add21_2_31_1 v639 v_call_i_2_31_1;
(*   store i16 %add21.2.31.1, i16* %arrayidx11.2.31.1, align 2, !tbaa !3 *)
mov mem0_190 v_add21_2_31_1;

(* NOTE: k = 6 *)

(*   %arrayidx9.2.2258 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %640 = load i16, i16* %arrayidx9.2.2258, align 2, !tbaa !3 *)
mov v640 mem0_320;
(*   %conv1.i.2.2259 = sext i16 %640 to i32 *)
cast v_conv1_i_2_2259@sint32 v640@sint16;
(*   %mul.i.2.2260 = mul nsw i32 %conv1.i.2.2259, 287 *)
mul v_mul_i_2_2260 v_conv1_i_2_2259 (287)@sint32;
(*   %call.i.2.2261 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.2260) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_2260, v_call_i_2_2261);
(*   %arrayidx11.2.2262 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %641 = load i16, i16* %arrayidx11.2.2262, align 2, !tbaa !3 *)
mov v641 mem0_256;
(*   %sub.2.2263 = sub i16 %641, %call.i.2.2261 *)
sub v_sub_2_2263 v641 v_call_i_2_2261;
(*   store i16 %sub.2.2263, i16* %arrayidx9.2.2258, align 2, !tbaa !3 *)
mov mem0_320 v_sub_2_2263;
(*   %add21.2.2264 = add i16 %641, %call.i.2.2261 *)
add v_add21_2_2264 v641 v_call_i_2_2261;
(*   store i16 %add21.2.2264, i16* %arrayidx11.2.2262, align 2, !tbaa !3 *)
mov mem0_256 v_add21_2_2264;
(*   %arrayidx9.2.1.2 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %642 = load i16, i16* %arrayidx9.2.1.2, align 2, !tbaa !3 *)
mov v642 mem0_322;
(*   %conv1.i.2.1.2 = sext i16 %642 to i32 *)
cast v_conv1_i_2_1_2@sint32 v642@sint16;
(*   %mul.i.2.1.2 = mul nsw i32 %conv1.i.2.1.2, 287 *)
mul v_mul_i_2_1_2 v_conv1_i_2_1_2 (287)@sint32;
(*   %call.i.2.1.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.1.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_1_2, v_call_i_2_1_2);
(*   %arrayidx11.2.1.2 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %643 = load i16, i16* %arrayidx11.2.1.2, align 2, !tbaa !3 *)
mov v643 mem0_258;
(*   %sub.2.1.2 = sub i16 %643, %call.i.2.1.2 *)
sub v_sub_2_1_2 v643 v_call_i_2_1_2;
(*   store i16 %sub.2.1.2, i16* %arrayidx9.2.1.2, align 2, !tbaa !3 *)
mov mem0_322 v_sub_2_1_2;
(*   %add21.2.1.2 = add i16 %643, %call.i.2.1.2 *)
add v_add21_2_1_2 v643 v_call_i_2_1_2;
(*   store i16 %add21.2.1.2, i16* %arrayidx11.2.1.2, align 2, !tbaa !3 *)
mov mem0_258 v_add21_2_1_2;
(*   %arrayidx9.2.2.2 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %644 = load i16, i16* %arrayidx9.2.2.2, align 2, !tbaa !3 *)
mov v644 mem0_324;
(*   %conv1.i.2.2.2 = sext i16 %644 to i32 *)
cast v_conv1_i_2_2_2@sint32 v644@sint16;
(*   %mul.i.2.2.2 = mul nsw i32 %conv1.i.2.2.2, 287 *)
mul v_mul_i_2_2_2 v_conv1_i_2_2_2 (287)@sint32;
(*   %call.i.2.2.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.2.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_2_2, v_call_i_2_2_2);
(*   %arrayidx11.2.2.2 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %645 = load i16, i16* %arrayidx11.2.2.2, align 2, !tbaa !3 *)
mov v645 mem0_260;
(*   %sub.2.2.2 = sub i16 %645, %call.i.2.2.2 *)
sub v_sub_2_2_2 v645 v_call_i_2_2_2;
(*   store i16 %sub.2.2.2, i16* %arrayidx9.2.2.2, align 2, !tbaa !3 *)
mov mem0_324 v_sub_2_2_2;
(*   %add21.2.2.2 = add i16 %645, %call.i.2.2.2 *)
add v_add21_2_2_2 v645 v_call_i_2_2_2;
(*   store i16 %add21.2.2.2, i16* %arrayidx11.2.2.2, align 2, !tbaa !3 *)
mov mem0_260 v_add21_2_2_2;
(*   %arrayidx9.2.3.2 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %646 = load i16, i16* %arrayidx9.2.3.2, align 2, !tbaa !3 *)
mov v646 mem0_326;
(*   %conv1.i.2.3.2 = sext i16 %646 to i32 *)
cast v_conv1_i_2_3_2@sint32 v646@sint16;
(*   %mul.i.2.3.2 = mul nsw i32 %conv1.i.2.3.2, 287 *)
mul v_mul_i_2_3_2 v_conv1_i_2_3_2 (287)@sint32;
(*   %call.i.2.3.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.3.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_3_2, v_call_i_2_3_2);
(*   %arrayidx11.2.3.2 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %647 = load i16, i16* %arrayidx11.2.3.2, align 2, !tbaa !3 *)
mov v647 mem0_262;
(*   %sub.2.3.2 = sub i16 %647, %call.i.2.3.2 *)
sub v_sub_2_3_2 v647 v_call_i_2_3_2;
(*   store i16 %sub.2.3.2, i16* %arrayidx9.2.3.2, align 2, !tbaa !3 *)
mov mem0_326 v_sub_2_3_2;
(*   %add21.2.3.2 = add i16 %647, %call.i.2.3.2 *)
add v_add21_2_3_2 v647 v_call_i_2_3_2;
(*   store i16 %add21.2.3.2, i16* %arrayidx11.2.3.2, align 2, !tbaa !3 *)
mov mem0_262 v_add21_2_3_2;
(*   %arrayidx9.2.4.2 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %648 = load i16, i16* %arrayidx9.2.4.2, align 2, !tbaa !3 *)
mov v648 mem0_328;
(*   %conv1.i.2.4.2 = sext i16 %648 to i32 *)
cast v_conv1_i_2_4_2@sint32 v648@sint16;
(*   %mul.i.2.4.2 = mul nsw i32 %conv1.i.2.4.2, 287 *)
mul v_mul_i_2_4_2 v_conv1_i_2_4_2 (287)@sint32;
(*   %call.i.2.4.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.4.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_4_2, v_call_i_2_4_2);
(*   %arrayidx11.2.4.2 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %649 = load i16, i16* %arrayidx11.2.4.2, align 2, !tbaa !3 *)
mov v649 mem0_264;
(*   %sub.2.4.2 = sub i16 %649, %call.i.2.4.2 *)
sub v_sub_2_4_2 v649 v_call_i_2_4_2;
(*   store i16 %sub.2.4.2, i16* %arrayidx9.2.4.2, align 2, !tbaa !3 *)
mov mem0_328 v_sub_2_4_2;
(*   %add21.2.4.2 = add i16 %649, %call.i.2.4.2 *)
add v_add21_2_4_2 v649 v_call_i_2_4_2;
(*   store i16 %add21.2.4.2, i16* %arrayidx11.2.4.2, align 2, !tbaa !3 *)
mov mem0_264 v_add21_2_4_2;
(*   %arrayidx9.2.5.2 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %650 = load i16, i16* %arrayidx9.2.5.2, align 2, !tbaa !3 *)
mov v650 mem0_330;
(*   %conv1.i.2.5.2 = sext i16 %650 to i32 *)
cast v_conv1_i_2_5_2@sint32 v650@sint16;
(*   %mul.i.2.5.2 = mul nsw i32 %conv1.i.2.5.2, 287 *)
mul v_mul_i_2_5_2 v_conv1_i_2_5_2 (287)@sint32;
(*   %call.i.2.5.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.5.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_5_2, v_call_i_2_5_2);
(*   %arrayidx11.2.5.2 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %651 = load i16, i16* %arrayidx11.2.5.2, align 2, !tbaa !3 *)
mov v651 mem0_266;
(*   %sub.2.5.2 = sub i16 %651, %call.i.2.5.2 *)
sub v_sub_2_5_2 v651 v_call_i_2_5_2;
(*   store i16 %sub.2.5.2, i16* %arrayidx9.2.5.2, align 2, !tbaa !3 *)
mov mem0_330 v_sub_2_5_2;
(*   %add21.2.5.2 = add i16 %651, %call.i.2.5.2 *)
add v_add21_2_5_2 v651 v_call_i_2_5_2;
(*   store i16 %add21.2.5.2, i16* %arrayidx11.2.5.2, align 2, !tbaa !3 *)
mov mem0_266 v_add21_2_5_2;
(*   %arrayidx9.2.6.2 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %652 = load i16, i16* %arrayidx9.2.6.2, align 2, !tbaa !3 *)
mov v652 mem0_332;
(*   %conv1.i.2.6.2 = sext i16 %652 to i32 *)
cast v_conv1_i_2_6_2@sint32 v652@sint16;
(*   %mul.i.2.6.2 = mul nsw i32 %conv1.i.2.6.2, 287 *)
mul v_mul_i_2_6_2 v_conv1_i_2_6_2 (287)@sint32;
(*   %call.i.2.6.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.6.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_6_2, v_call_i_2_6_2);
(*   %arrayidx11.2.6.2 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %653 = load i16, i16* %arrayidx11.2.6.2, align 2, !tbaa !3 *)
mov v653 mem0_268;
(*   %sub.2.6.2 = sub i16 %653, %call.i.2.6.2 *)
sub v_sub_2_6_2 v653 v_call_i_2_6_2;
(*   store i16 %sub.2.6.2, i16* %arrayidx9.2.6.2, align 2, !tbaa !3 *)
mov mem0_332 v_sub_2_6_2;
(*   %add21.2.6.2 = add i16 %653, %call.i.2.6.2 *)
add v_add21_2_6_2 v653 v_call_i_2_6_2;
(*   store i16 %add21.2.6.2, i16* %arrayidx11.2.6.2, align 2, !tbaa !3 *)
mov mem0_268 v_add21_2_6_2;
(*   %arrayidx9.2.7.2 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %654 = load i16, i16* %arrayidx9.2.7.2, align 2, !tbaa !3 *)
mov v654 mem0_334;
(*   %conv1.i.2.7.2 = sext i16 %654 to i32 *)
cast v_conv1_i_2_7_2@sint32 v654@sint16;
(*   %mul.i.2.7.2 = mul nsw i32 %conv1.i.2.7.2, 287 *)
mul v_mul_i_2_7_2 v_conv1_i_2_7_2 (287)@sint32;
(*   %call.i.2.7.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.7.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_7_2, v_call_i_2_7_2);
(*   %arrayidx11.2.7.2 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %655 = load i16, i16* %arrayidx11.2.7.2, align 2, !tbaa !3 *)
mov v655 mem0_270;
(*   %sub.2.7.2 = sub i16 %655, %call.i.2.7.2 *)
sub v_sub_2_7_2 v655 v_call_i_2_7_2;
(*   store i16 %sub.2.7.2, i16* %arrayidx9.2.7.2, align 2, !tbaa !3 *)
mov mem0_334 v_sub_2_7_2;
(*   %add21.2.7.2 = add i16 %655, %call.i.2.7.2 *)
add v_add21_2_7_2 v655 v_call_i_2_7_2;
(*   store i16 %add21.2.7.2, i16* %arrayidx11.2.7.2, align 2, !tbaa !3 *)
mov mem0_270 v_add21_2_7_2;
(*   %arrayidx9.2.8.2 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %656 = load i16, i16* %arrayidx9.2.8.2, align 2, !tbaa !3 *)
mov v656 mem0_336;
(*   %conv1.i.2.8.2 = sext i16 %656 to i32 *)
cast v_conv1_i_2_8_2@sint32 v656@sint16;
(*   %mul.i.2.8.2 = mul nsw i32 %conv1.i.2.8.2, 287 *)
mul v_mul_i_2_8_2 v_conv1_i_2_8_2 (287)@sint32;
(*   %call.i.2.8.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.8.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_8_2, v_call_i_2_8_2);
(*   %arrayidx11.2.8.2 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %657 = load i16, i16* %arrayidx11.2.8.2, align 2, !tbaa !3 *)
mov v657 mem0_272;
(*   %sub.2.8.2 = sub i16 %657, %call.i.2.8.2 *)
sub v_sub_2_8_2 v657 v_call_i_2_8_2;
(*   store i16 %sub.2.8.2, i16* %arrayidx9.2.8.2, align 2, !tbaa !3 *)
mov mem0_336 v_sub_2_8_2;
(*   %add21.2.8.2 = add i16 %657, %call.i.2.8.2 *)
add v_add21_2_8_2 v657 v_call_i_2_8_2;
(*   store i16 %add21.2.8.2, i16* %arrayidx11.2.8.2, align 2, !tbaa !3 *)
mov mem0_272 v_add21_2_8_2;
(*   %arrayidx9.2.9.2 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %658 = load i16, i16* %arrayidx9.2.9.2, align 2, !tbaa !3 *)
mov v658 mem0_338;
(*   %conv1.i.2.9.2 = sext i16 %658 to i32 *)
cast v_conv1_i_2_9_2@sint32 v658@sint16;
(*   %mul.i.2.9.2 = mul nsw i32 %conv1.i.2.9.2, 287 *)
mul v_mul_i_2_9_2 v_conv1_i_2_9_2 (287)@sint32;
(*   %call.i.2.9.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.9.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_9_2, v_call_i_2_9_2);
(*   %arrayidx11.2.9.2 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %659 = load i16, i16* %arrayidx11.2.9.2, align 2, !tbaa !3 *)
mov v659 mem0_274;
(*   %sub.2.9.2 = sub i16 %659, %call.i.2.9.2 *)
sub v_sub_2_9_2 v659 v_call_i_2_9_2;
(*   store i16 %sub.2.9.2, i16* %arrayidx9.2.9.2, align 2, !tbaa !3 *)
mov mem0_338 v_sub_2_9_2;
(*   %add21.2.9.2 = add i16 %659, %call.i.2.9.2 *)
add v_add21_2_9_2 v659 v_call_i_2_9_2;
(*   store i16 %add21.2.9.2, i16* %arrayidx11.2.9.2, align 2, !tbaa !3 *)
mov mem0_274 v_add21_2_9_2;
(*   %arrayidx9.2.10.2 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %660 = load i16, i16* %arrayidx9.2.10.2, align 2, !tbaa !3 *)
mov v660 mem0_340;
(*   %conv1.i.2.10.2 = sext i16 %660 to i32 *)
cast v_conv1_i_2_10_2@sint32 v660@sint16;
(*   %mul.i.2.10.2 = mul nsw i32 %conv1.i.2.10.2, 287 *)
mul v_mul_i_2_10_2 v_conv1_i_2_10_2 (287)@sint32;
(*   %call.i.2.10.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.10.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_10_2, v_call_i_2_10_2);
(*   %arrayidx11.2.10.2 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %661 = load i16, i16* %arrayidx11.2.10.2, align 2, !tbaa !3 *)
mov v661 mem0_276;
(*   %sub.2.10.2 = sub i16 %661, %call.i.2.10.2 *)
sub v_sub_2_10_2 v661 v_call_i_2_10_2;
(*   store i16 %sub.2.10.2, i16* %arrayidx9.2.10.2, align 2, !tbaa !3 *)
mov mem0_340 v_sub_2_10_2;
(*   %add21.2.10.2 = add i16 %661, %call.i.2.10.2 *)
add v_add21_2_10_2 v661 v_call_i_2_10_2;
(*   store i16 %add21.2.10.2, i16* %arrayidx11.2.10.2, align 2, !tbaa !3 *)
mov mem0_276 v_add21_2_10_2;
(*   %arrayidx9.2.11.2 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %662 = load i16, i16* %arrayidx9.2.11.2, align 2, !tbaa !3 *)
mov v662 mem0_342;
(*   %conv1.i.2.11.2 = sext i16 %662 to i32 *)
cast v_conv1_i_2_11_2@sint32 v662@sint16;
(*   %mul.i.2.11.2 = mul nsw i32 %conv1.i.2.11.2, 287 *)
mul v_mul_i_2_11_2 v_conv1_i_2_11_2 (287)@sint32;
(*   %call.i.2.11.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.11.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_11_2, v_call_i_2_11_2);
(*   %arrayidx11.2.11.2 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %663 = load i16, i16* %arrayidx11.2.11.2, align 2, !tbaa !3 *)
mov v663 mem0_278;
(*   %sub.2.11.2 = sub i16 %663, %call.i.2.11.2 *)
sub v_sub_2_11_2 v663 v_call_i_2_11_2;
(*   store i16 %sub.2.11.2, i16* %arrayidx9.2.11.2, align 2, !tbaa !3 *)
mov mem0_342 v_sub_2_11_2;
(*   %add21.2.11.2 = add i16 %663, %call.i.2.11.2 *)
add v_add21_2_11_2 v663 v_call_i_2_11_2;
(*   store i16 %add21.2.11.2, i16* %arrayidx11.2.11.2, align 2, !tbaa !3 *)
mov mem0_278 v_add21_2_11_2;
(*   %arrayidx9.2.12.2 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %664 = load i16, i16* %arrayidx9.2.12.2, align 2, !tbaa !3 *)
mov v664 mem0_344;
(*   %conv1.i.2.12.2 = sext i16 %664 to i32 *)
cast v_conv1_i_2_12_2@sint32 v664@sint16;
(*   %mul.i.2.12.2 = mul nsw i32 %conv1.i.2.12.2, 287 *)
mul v_mul_i_2_12_2 v_conv1_i_2_12_2 (287)@sint32;
(*   %call.i.2.12.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.12.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_12_2, v_call_i_2_12_2);
(*   %arrayidx11.2.12.2 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %665 = load i16, i16* %arrayidx11.2.12.2, align 2, !tbaa !3 *)
mov v665 mem0_280;
(*   %sub.2.12.2 = sub i16 %665, %call.i.2.12.2 *)
sub v_sub_2_12_2 v665 v_call_i_2_12_2;
(*   store i16 %sub.2.12.2, i16* %arrayidx9.2.12.2, align 2, !tbaa !3 *)
mov mem0_344 v_sub_2_12_2;
(*   %add21.2.12.2 = add i16 %665, %call.i.2.12.2 *)
add v_add21_2_12_2 v665 v_call_i_2_12_2;
(*   store i16 %add21.2.12.2, i16* %arrayidx11.2.12.2, align 2, !tbaa !3 *)
mov mem0_280 v_add21_2_12_2;
(*   %arrayidx9.2.13.2 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %666 = load i16, i16* %arrayidx9.2.13.2, align 2, !tbaa !3 *)
mov v666 mem0_346;
(*   %conv1.i.2.13.2 = sext i16 %666 to i32 *)
cast v_conv1_i_2_13_2@sint32 v666@sint16;
(*   %mul.i.2.13.2 = mul nsw i32 %conv1.i.2.13.2, 287 *)
mul v_mul_i_2_13_2 v_conv1_i_2_13_2 (287)@sint32;
(*   %call.i.2.13.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.13.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_13_2, v_call_i_2_13_2);
(*   %arrayidx11.2.13.2 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %667 = load i16, i16* %arrayidx11.2.13.2, align 2, !tbaa !3 *)
mov v667 mem0_282;
(*   %sub.2.13.2 = sub i16 %667, %call.i.2.13.2 *)
sub v_sub_2_13_2 v667 v_call_i_2_13_2;
(*   store i16 %sub.2.13.2, i16* %arrayidx9.2.13.2, align 2, !tbaa !3 *)
mov mem0_346 v_sub_2_13_2;
(*   %add21.2.13.2 = add i16 %667, %call.i.2.13.2 *)
add v_add21_2_13_2 v667 v_call_i_2_13_2;
(*   store i16 %add21.2.13.2, i16* %arrayidx11.2.13.2, align 2, !tbaa !3 *)
mov mem0_282 v_add21_2_13_2;
(*   %arrayidx9.2.14.2 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %668 = load i16, i16* %arrayidx9.2.14.2, align 2, !tbaa !3 *)
mov v668 mem0_348;
(*   %conv1.i.2.14.2 = sext i16 %668 to i32 *)
cast v_conv1_i_2_14_2@sint32 v668@sint16;
(*   %mul.i.2.14.2 = mul nsw i32 %conv1.i.2.14.2, 287 *)
mul v_mul_i_2_14_2 v_conv1_i_2_14_2 (287)@sint32;
(*   %call.i.2.14.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.14.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_14_2, v_call_i_2_14_2);
(*   %arrayidx11.2.14.2 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %669 = load i16, i16* %arrayidx11.2.14.2, align 2, !tbaa !3 *)
mov v669 mem0_284;
(*   %sub.2.14.2 = sub i16 %669, %call.i.2.14.2 *)
sub v_sub_2_14_2 v669 v_call_i_2_14_2;
(*   store i16 %sub.2.14.2, i16* %arrayidx9.2.14.2, align 2, !tbaa !3 *)
mov mem0_348 v_sub_2_14_2;
(*   %add21.2.14.2 = add i16 %669, %call.i.2.14.2 *)
add v_add21_2_14_2 v669 v_call_i_2_14_2;
(*   store i16 %add21.2.14.2, i16* %arrayidx11.2.14.2, align 2, !tbaa !3 *)
mov mem0_284 v_add21_2_14_2;
(*   %arrayidx9.2.15.2 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %670 = load i16, i16* %arrayidx9.2.15.2, align 2, !tbaa !3 *)
mov v670 mem0_350;
(*   %conv1.i.2.15.2 = sext i16 %670 to i32 *)
cast v_conv1_i_2_15_2@sint32 v670@sint16;
(*   %mul.i.2.15.2 = mul nsw i32 %conv1.i.2.15.2, 287 *)
mul v_mul_i_2_15_2 v_conv1_i_2_15_2 (287)@sint32;
(*   %call.i.2.15.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.15.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_15_2, v_call_i_2_15_2);
(*   %arrayidx11.2.15.2 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %671 = load i16, i16* %arrayidx11.2.15.2, align 2, !tbaa !3 *)
mov v671 mem0_286;
(*   %sub.2.15.2 = sub i16 %671, %call.i.2.15.2 *)
sub v_sub_2_15_2 v671 v_call_i_2_15_2;
(*   store i16 %sub.2.15.2, i16* %arrayidx9.2.15.2, align 2, !tbaa !3 *)
mov mem0_350 v_sub_2_15_2;
(*   %add21.2.15.2 = add i16 %671, %call.i.2.15.2 *)
add v_add21_2_15_2 v671 v_call_i_2_15_2;
(*   store i16 %add21.2.15.2, i16* %arrayidx11.2.15.2, align 2, !tbaa !3 *)
mov mem0_286 v_add21_2_15_2;
(*   %arrayidx9.2.16.2 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %672 = load i16, i16* %arrayidx9.2.16.2, align 2, !tbaa !3 *)
mov v672 mem0_352;
(*   %conv1.i.2.16.2 = sext i16 %672 to i32 *)
cast v_conv1_i_2_16_2@sint32 v672@sint16;
(*   %mul.i.2.16.2 = mul nsw i32 %conv1.i.2.16.2, 287 *)
mul v_mul_i_2_16_2 v_conv1_i_2_16_2 (287)@sint32;
(*   %call.i.2.16.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.16.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_16_2, v_call_i_2_16_2);
(*   %arrayidx11.2.16.2 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %673 = load i16, i16* %arrayidx11.2.16.2, align 2, !tbaa !3 *)
mov v673 mem0_288;
(*   %sub.2.16.2 = sub i16 %673, %call.i.2.16.2 *)
sub v_sub_2_16_2 v673 v_call_i_2_16_2;
(*   store i16 %sub.2.16.2, i16* %arrayidx9.2.16.2, align 2, !tbaa !3 *)
mov mem0_352 v_sub_2_16_2;
(*   %add21.2.16.2 = add i16 %673, %call.i.2.16.2 *)
add v_add21_2_16_2 v673 v_call_i_2_16_2;
(*   store i16 %add21.2.16.2, i16* %arrayidx11.2.16.2, align 2, !tbaa !3 *)
mov mem0_288 v_add21_2_16_2;
(*   %arrayidx9.2.17.2 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %674 = load i16, i16* %arrayidx9.2.17.2, align 2, !tbaa !3 *)
mov v674 mem0_354;
(*   %conv1.i.2.17.2 = sext i16 %674 to i32 *)
cast v_conv1_i_2_17_2@sint32 v674@sint16;
(*   %mul.i.2.17.2 = mul nsw i32 %conv1.i.2.17.2, 287 *)
mul v_mul_i_2_17_2 v_conv1_i_2_17_2 (287)@sint32;
(*   %call.i.2.17.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.17.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_17_2, v_call_i_2_17_2);
(*   %arrayidx11.2.17.2 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %675 = load i16, i16* %arrayidx11.2.17.2, align 2, !tbaa !3 *)
mov v675 mem0_290;
(*   %sub.2.17.2 = sub i16 %675, %call.i.2.17.2 *)
sub v_sub_2_17_2 v675 v_call_i_2_17_2;
(*   store i16 %sub.2.17.2, i16* %arrayidx9.2.17.2, align 2, !tbaa !3 *)
mov mem0_354 v_sub_2_17_2;
(*   %add21.2.17.2 = add i16 %675, %call.i.2.17.2 *)
add v_add21_2_17_2 v675 v_call_i_2_17_2;
(*   store i16 %add21.2.17.2, i16* %arrayidx11.2.17.2, align 2, !tbaa !3 *)
mov mem0_290 v_add21_2_17_2;
(*   %arrayidx9.2.18.2 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %676 = load i16, i16* %arrayidx9.2.18.2, align 2, !tbaa !3 *)
mov v676 mem0_356;
(*   %conv1.i.2.18.2 = sext i16 %676 to i32 *)
cast v_conv1_i_2_18_2@sint32 v676@sint16;
(*   %mul.i.2.18.2 = mul nsw i32 %conv1.i.2.18.2, 287 *)
mul v_mul_i_2_18_2 v_conv1_i_2_18_2 (287)@sint32;
(*   %call.i.2.18.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.18.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_18_2, v_call_i_2_18_2);
(*   %arrayidx11.2.18.2 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %677 = load i16, i16* %arrayidx11.2.18.2, align 2, !tbaa !3 *)
mov v677 mem0_292;
(*   %sub.2.18.2 = sub i16 %677, %call.i.2.18.2 *)
sub v_sub_2_18_2 v677 v_call_i_2_18_2;
(*   store i16 %sub.2.18.2, i16* %arrayidx9.2.18.2, align 2, !tbaa !3 *)
mov mem0_356 v_sub_2_18_2;
(*   %add21.2.18.2 = add i16 %677, %call.i.2.18.2 *)
add v_add21_2_18_2 v677 v_call_i_2_18_2;
(*   store i16 %add21.2.18.2, i16* %arrayidx11.2.18.2, align 2, !tbaa !3 *)
mov mem0_292 v_add21_2_18_2;
(*   %arrayidx9.2.19.2 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %678 = load i16, i16* %arrayidx9.2.19.2, align 2, !tbaa !3 *)
mov v678 mem0_358;
(*   %conv1.i.2.19.2 = sext i16 %678 to i32 *)
cast v_conv1_i_2_19_2@sint32 v678@sint16;
(*   %mul.i.2.19.2 = mul nsw i32 %conv1.i.2.19.2, 287 *)
mul v_mul_i_2_19_2 v_conv1_i_2_19_2 (287)@sint32;
(*   %call.i.2.19.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.19.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_19_2, v_call_i_2_19_2);
(*   %arrayidx11.2.19.2 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %679 = load i16, i16* %arrayidx11.2.19.2, align 2, !tbaa !3 *)
mov v679 mem0_294;
(*   %sub.2.19.2 = sub i16 %679, %call.i.2.19.2 *)
sub v_sub_2_19_2 v679 v_call_i_2_19_2;
(*   store i16 %sub.2.19.2, i16* %arrayidx9.2.19.2, align 2, !tbaa !3 *)
mov mem0_358 v_sub_2_19_2;
(*   %add21.2.19.2 = add i16 %679, %call.i.2.19.2 *)
add v_add21_2_19_2 v679 v_call_i_2_19_2;
(*   store i16 %add21.2.19.2, i16* %arrayidx11.2.19.2, align 2, !tbaa !3 *)
mov mem0_294 v_add21_2_19_2;
(*   %arrayidx9.2.20.2 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %680 = load i16, i16* %arrayidx9.2.20.2, align 2, !tbaa !3 *)
mov v680 mem0_360;
(*   %conv1.i.2.20.2 = sext i16 %680 to i32 *)
cast v_conv1_i_2_20_2@sint32 v680@sint16;
(*   %mul.i.2.20.2 = mul nsw i32 %conv1.i.2.20.2, 287 *)
mul v_mul_i_2_20_2 v_conv1_i_2_20_2 (287)@sint32;
(*   %call.i.2.20.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.20.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_20_2, v_call_i_2_20_2);
(*   %arrayidx11.2.20.2 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %681 = load i16, i16* %arrayidx11.2.20.2, align 2, !tbaa !3 *)
mov v681 mem0_296;
(*   %sub.2.20.2 = sub i16 %681, %call.i.2.20.2 *)
sub v_sub_2_20_2 v681 v_call_i_2_20_2;
(*   store i16 %sub.2.20.2, i16* %arrayidx9.2.20.2, align 2, !tbaa !3 *)
mov mem0_360 v_sub_2_20_2;
(*   %add21.2.20.2 = add i16 %681, %call.i.2.20.2 *)
add v_add21_2_20_2 v681 v_call_i_2_20_2;
(*   store i16 %add21.2.20.2, i16* %arrayidx11.2.20.2, align 2, !tbaa !3 *)
mov mem0_296 v_add21_2_20_2;
(*   %arrayidx9.2.21.2 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %682 = load i16, i16* %arrayidx9.2.21.2, align 2, !tbaa !3 *)
mov v682 mem0_362;
(*   %conv1.i.2.21.2 = sext i16 %682 to i32 *)
cast v_conv1_i_2_21_2@sint32 v682@sint16;
(*   %mul.i.2.21.2 = mul nsw i32 %conv1.i.2.21.2, 287 *)
mul v_mul_i_2_21_2 v_conv1_i_2_21_2 (287)@sint32;
(*   %call.i.2.21.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.21.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_21_2, v_call_i_2_21_2);
(*   %arrayidx11.2.21.2 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %683 = load i16, i16* %arrayidx11.2.21.2, align 2, !tbaa !3 *)
mov v683 mem0_298;
(*   %sub.2.21.2 = sub i16 %683, %call.i.2.21.2 *)
sub v_sub_2_21_2 v683 v_call_i_2_21_2;
(*   store i16 %sub.2.21.2, i16* %arrayidx9.2.21.2, align 2, !tbaa !3 *)
mov mem0_362 v_sub_2_21_2;
(*   %add21.2.21.2 = add i16 %683, %call.i.2.21.2 *)
add v_add21_2_21_2 v683 v_call_i_2_21_2;
(*   store i16 %add21.2.21.2, i16* %arrayidx11.2.21.2, align 2, !tbaa !3 *)
mov mem0_298 v_add21_2_21_2;
(*   %arrayidx9.2.22.2 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %684 = load i16, i16* %arrayidx9.2.22.2, align 2, !tbaa !3 *)
mov v684 mem0_364;
(*   %conv1.i.2.22.2 = sext i16 %684 to i32 *)
cast v_conv1_i_2_22_2@sint32 v684@sint16;
(*   %mul.i.2.22.2 = mul nsw i32 %conv1.i.2.22.2, 287 *)
mul v_mul_i_2_22_2 v_conv1_i_2_22_2 (287)@sint32;
(*   %call.i.2.22.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.22.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_22_2, v_call_i_2_22_2);
(*   %arrayidx11.2.22.2 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %685 = load i16, i16* %arrayidx11.2.22.2, align 2, !tbaa !3 *)
mov v685 mem0_300;
(*   %sub.2.22.2 = sub i16 %685, %call.i.2.22.2 *)
sub v_sub_2_22_2 v685 v_call_i_2_22_2;
(*   store i16 %sub.2.22.2, i16* %arrayidx9.2.22.2, align 2, !tbaa !3 *)
mov mem0_364 v_sub_2_22_2;
(*   %add21.2.22.2 = add i16 %685, %call.i.2.22.2 *)
add v_add21_2_22_2 v685 v_call_i_2_22_2;
(*   store i16 %add21.2.22.2, i16* %arrayidx11.2.22.2, align 2, !tbaa !3 *)
mov mem0_300 v_add21_2_22_2;
(*   %arrayidx9.2.23.2 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %686 = load i16, i16* %arrayidx9.2.23.2, align 2, !tbaa !3 *)
mov v686 mem0_366;
(*   %conv1.i.2.23.2 = sext i16 %686 to i32 *)
cast v_conv1_i_2_23_2@sint32 v686@sint16;
(*   %mul.i.2.23.2 = mul nsw i32 %conv1.i.2.23.2, 287 *)
mul v_mul_i_2_23_2 v_conv1_i_2_23_2 (287)@sint32;
(*   %call.i.2.23.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.23.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_23_2, v_call_i_2_23_2);
(*   %arrayidx11.2.23.2 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %687 = load i16, i16* %arrayidx11.2.23.2, align 2, !tbaa !3 *)
mov v687 mem0_302;
(*   %sub.2.23.2 = sub i16 %687, %call.i.2.23.2 *)
sub v_sub_2_23_2 v687 v_call_i_2_23_2;
(*   store i16 %sub.2.23.2, i16* %arrayidx9.2.23.2, align 2, !tbaa !3 *)
mov mem0_366 v_sub_2_23_2;
(*   %add21.2.23.2 = add i16 %687, %call.i.2.23.2 *)
add v_add21_2_23_2 v687 v_call_i_2_23_2;
(*   store i16 %add21.2.23.2, i16* %arrayidx11.2.23.2, align 2, !tbaa !3 *)
mov mem0_302 v_add21_2_23_2;
(*   %arrayidx9.2.24.2 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %688 = load i16, i16* %arrayidx9.2.24.2, align 2, !tbaa !3 *)
mov v688 mem0_368;
(*   %conv1.i.2.24.2 = sext i16 %688 to i32 *)
cast v_conv1_i_2_24_2@sint32 v688@sint16;
(*   %mul.i.2.24.2 = mul nsw i32 %conv1.i.2.24.2, 287 *)
mul v_mul_i_2_24_2 v_conv1_i_2_24_2 (287)@sint32;
(*   %call.i.2.24.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.24.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_24_2, v_call_i_2_24_2);
(*   %arrayidx11.2.24.2 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %689 = load i16, i16* %arrayidx11.2.24.2, align 2, !tbaa !3 *)
mov v689 mem0_304;
(*   %sub.2.24.2 = sub i16 %689, %call.i.2.24.2 *)
sub v_sub_2_24_2 v689 v_call_i_2_24_2;
(*   store i16 %sub.2.24.2, i16* %arrayidx9.2.24.2, align 2, !tbaa !3 *)
mov mem0_368 v_sub_2_24_2;
(*   %add21.2.24.2 = add i16 %689, %call.i.2.24.2 *)
add v_add21_2_24_2 v689 v_call_i_2_24_2;
(*   store i16 %add21.2.24.2, i16* %arrayidx11.2.24.2, align 2, !tbaa !3 *)
mov mem0_304 v_add21_2_24_2;
(*   %arrayidx9.2.25.2 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %690 = load i16, i16* %arrayidx9.2.25.2, align 2, !tbaa !3 *)
mov v690 mem0_370;
(*   %conv1.i.2.25.2 = sext i16 %690 to i32 *)
cast v_conv1_i_2_25_2@sint32 v690@sint16;
(*   %mul.i.2.25.2 = mul nsw i32 %conv1.i.2.25.2, 287 *)
mul v_mul_i_2_25_2 v_conv1_i_2_25_2 (287)@sint32;
(*   %call.i.2.25.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.25.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_25_2, v_call_i_2_25_2);
(*   %arrayidx11.2.25.2 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %691 = load i16, i16* %arrayidx11.2.25.2, align 2, !tbaa !3 *)
mov v691 mem0_306;
(*   %sub.2.25.2 = sub i16 %691, %call.i.2.25.2 *)
sub v_sub_2_25_2 v691 v_call_i_2_25_2;
(*   store i16 %sub.2.25.2, i16* %arrayidx9.2.25.2, align 2, !tbaa !3 *)
mov mem0_370 v_sub_2_25_2;
(*   %add21.2.25.2 = add i16 %691, %call.i.2.25.2 *)
add v_add21_2_25_2 v691 v_call_i_2_25_2;
(*   store i16 %add21.2.25.2, i16* %arrayidx11.2.25.2, align 2, !tbaa !3 *)
mov mem0_306 v_add21_2_25_2;
(*   %arrayidx9.2.26.2 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %692 = load i16, i16* %arrayidx9.2.26.2, align 2, !tbaa !3 *)
mov v692 mem0_372;
(*   %conv1.i.2.26.2 = sext i16 %692 to i32 *)
cast v_conv1_i_2_26_2@sint32 v692@sint16;
(*   %mul.i.2.26.2 = mul nsw i32 %conv1.i.2.26.2, 287 *)
mul v_mul_i_2_26_2 v_conv1_i_2_26_2 (287)@sint32;
(*   %call.i.2.26.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.26.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_26_2, v_call_i_2_26_2);
(*   %arrayidx11.2.26.2 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %693 = load i16, i16* %arrayidx11.2.26.2, align 2, !tbaa !3 *)
mov v693 mem0_308;
(*   %sub.2.26.2 = sub i16 %693, %call.i.2.26.2 *)
sub v_sub_2_26_2 v693 v_call_i_2_26_2;
(*   store i16 %sub.2.26.2, i16* %arrayidx9.2.26.2, align 2, !tbaa !3 *)
mov mem0_372 v_sub_2_26_2;
(*   %add21.2.26.2 = add i16 %693, %call.i.2.26.2 *)
add v_add21_2_26_2 v693 v_call_i_2_26_2;
(*   store i16 %add21.2.26.2, i16* %arrayidx11.2.26.2, align 2, !tbaa !3 *)
mov mem0_308 v_add21_2_26_2;
(*   %arrayidx9.2.27.2 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %694 = load i16, i16* %arrayidx9.2.27.2, align 2, !tbaa !3 *)
mov v694 mem0_374;
(*   %conv1.i.2.27.2 = sext i16 %694 to i32 *)
cast v_conv1_i_2_27_2@sint32 v694@sint16;
(*   %mul.i.2.27.2 = mul nsw i32 %conv1.i.2.27.2, 287 *)
mul v_mul_i_2_27_2 v_conv1_i_2_27_2 (287)@sint32;
(*   %call.i.2.27.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.27.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_27_2, v_call_i_2_27_2);
(*   %arrayidx11.2.27.2 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %695 = load i16, i16* %arrayidx11.2.27.2, align 2, !tbaa !3 *)
mov v695 mem0_310;
(*   %sub.2.27.2 = sub i16 %695, %call.i.2.27.2 *)
sub v_sub_2_27_2 v695 v_call_i_2_27_2;
(*   store i16 %sub.2.27.2, i16* %arrayidx9.2.27.2, align 2, !tbaa !3 *)
mov mem0_374 v_sub_2_27_2;
(*   %add21.2.27.2 = add i16 %695, %call.i.2.27.2 *)
add v_add21_2_27_2 v695 v_call_i_2_27_2;
(*   store i16 %add21.2.27.2, i16* %arrayidx11.2.27.2, align 2, !tbaa !3 *)
mov mem0_310 v_add21_2_27_2;
(*   %arrayidx9.2.28.2 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %696 = load i16, i16* %arrayidx9.2.28.2, align 2, !tbaa !3 *)
mov v696 mem0_376;
(*   %conv1.i.2.28.2 = sext i16 %696 to i32 *)
cast v_conv1_i_2_28_2@sint32 v696@sint16;
(*   %mul.i.2.28.2 = mul nsw i32 %conv1.i.2.28.2, 287 *)
mul v_mul_i_2_28_2 v_conv1_i_2_28_2 (287)@sint32;
(*   %call.i.2.28.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.28.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_28_2, v_call_i_2_28_2);
(*   %arrayidx11.2.28.2 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %697 = load i16, i16* %arrayidx11.2.28.2, align 2, !tbaa !3 *)
mov v697 mem0_312;
(*   %sub.2.28.2 = sub i16 %697, %call.i.2.28.2 *)
sub v_sub_2_28_2 v697 v_call_i_2_28_2;
(*   store i16 %sub.2.28.2, i16* %arrayidx9.2.28.2, align 2, !tbaa !3 *)
mov mem0_376 v_sub_2_28_2;
(*   %add21.2.28.2 = add i16 %697, %call.i.2.28.2 *)
add v_add21_2_28_2 v697 v_call_i_2_28_2;
(*   store i16 %add21.2.28.2, i16* %arrayidx11.2.28.2, align 2, !tbaa !3 *)
mov mem0_312 v_add21_2_28_2;
(*   %arrayidx9.2.29.2 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %698 = load i16, i16* %arrayidx9.2.29.2, align 2, !tbaa !3 *)
mov v698 mem0_378;
(*   %conv1.i.2.29.2 = sext i16 %698 to i32 *)
cast v_conv1_i_2_29_2@sint32 v698@sint16;
(*   %mul.i.2.29.2 = mul nsw i32 %conv1.i.2.29.2, 287 *)
mul v_mul_i_2_29_2 v_conv1_i_2_29_2 (287)@sint32;
(*   %call.i.2.29.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.29.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_29_2, v_call_i_2_29_2);
(*   %arrayidx11.2.29.2 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %699 = load i16, i16* %arrayidx11.2.29.2, align 2, !tbaa !3 *)
mov v699 mem0_314;
(*   %sub.2.29.2 = sub i16 %699, %call.i.2.29.2 *)
sub v_sub_2_29_2 v699 v_call_i_2_29_2;
(*   store i16 %sub.2.29.2, i16* %arrayidx9.2.29.2, align 2, !tbaa !3 *)
mov mem0_378 v_sub_2_29_2;
(*   %add21.2.29.2 = add i16 %699, %call.i.2.29.2 *)
add v_add21_2_29_2 v699 v_call_i_2_29_2;
(*   store i16 %add21.2.29.2, i16* %arrayidx11.2.29.2, align 2, !tbaa !3 *)
mov mem0_314 v_add21_2_29_2;
(*   %arrayidx9.2.30.2 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %700 = load i16, i16* %arrayidx9.2.30.2, align 2, !tbaa !3 *)
mov v700 mem0_380;
(*   %conv1.i.2.30.2 = sext i16 %700 to i32 *)
cast v_conv1_i_2_30_2@sint32 v700@sint16;
(*   %mul.i.2.30.2 = mul nsw i32 %conv1.i.2.30.2, 287 *)
mul v_mul_i_2_30_2 v_conv1_i_2_30_2 (287)@sint32;
(*   %call.i.2.30.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.30.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_30_2, v_call_i_2_30_2);
(*   %arrayidx11.2.30.2 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %701 = load i16, i16* %arrayidx11.2.30.2, align 2, !tbaa !3 *)
mov v701 mem0_316;
(*   %sub.2.30.2 = sub i16 %701, %call.i.2.30.2 *)
sub v_sub_2_30_2 v701 v_call_i_2_30_2;
(*   store i16 %sub.2.30.2, i16* %arrayidx9.2.30.2, align 2, !tbaa !3 *)
mov mem0_380 v_sub_2_30_2;
(*   %add21.2.30.2 = add i16 %701, %call.i.2.30.2 *)
add v_add21_2_30_2 v701 v_call_i_2_30_2;
(*   store i16 %add21.2.30.2, i16* %arrayidx11.2.30.2, align 2, !tbaa !3 *)
mov mem0_316 v_add21_2_30_2;
(*   %arrayidx9.2.31.2 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %702 = load i16, i16* %arrayidx9.2.31.2, align 2, !tbaa !3 *)
mov v702 mem0_382;
(*   %conv1.i.2.31.2 = sext i16 %702 to i32 *)
cast v_conv1_i_2_31_2@sint32 v702@sint16;
(*   %mul.i.2.31.2 = mul nsw i32 %conv1.i.2.31.2, 287 *)
mul v_mul_i_2_31_2 v_conv1_i_2_31_2 (287)@sint32;
(*   %call.i.2.31.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.31.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_31_2, v_call_i_2_31_2);
(*   %arrayidx11.2.31.2 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %703 = load i16, i16* %arrayidx11.2.31.2, align 2, !tbaa !3 *)
mov v703 mem0_318;
(*   %sub.2.31.2 = sub i16 %703, %call.i.2.31.2 *)
sub v_sub_2_31_2 v703 v_call_i_2_31_2;
(*   store i16 %sub.2.31.2, i16* %arrayidx9.2.31.2, align 2, !tbaa !3 *)
mov mem0_382 v_sub_2_31_2;
(*   %add21.2.31.2 = add i16 %703, %call.i.2.31.2 *)
add v_add21_2_31_2 v703 v_call_i_2_31_2;
(*   store i16 %add21.2.31.2, i16* %arrayidx11.2.31.2, align 2, !tbaa !3 *)
mov mem0_318 v_add21_2_31_2;

(* NOTE: k = 7 *)

(*   %arrayidx9.2.3268 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %704 = load i16, i16* %arrayidx9.2.3268, align 2, !tbaa !3 *)
mov v704 mem0_448;
(*   %conv1.i.2.3269 = sext i16 %704 to i32 *)
cast v_conv1_i_2_3269@sint32 v704@sint16;
(*   %mul.i.2.3270 = mul nsw i32 %conv1.i.2.3269, 202 *)
mul v_mul_i_2_3270 v_conv1_i_2_3269 (202)@sint32;
(*   %call.i.2.3271 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.3270) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_3270, v_call_i_2_3271);
(*   %arrayidx11.2.3272 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %705 = load i16, i16* %arrayidx11.2.3272, align 2, !tbaa !3 *)
mov v705 mem0_384;
(*   %sub.2.3273 = sub i16 %705, %call.i.2.3271 *)
sub v_sub_2_3273 v705 v_call_i_2_3271;
(*   store i16 %sub.2.3273, i16* %arrayidx9.2.3268, align 2, !tbaa !3 *)
mov mem0_448 v_sub_2_3273;
(*   %add21.2.3274 = add i16 %705, %call.i.2.3271 *)
add v_add21_2_3274 v705 v_call_i_2_3271;
(*   store i16 %add21.2.3274, i16* %arrayidx11.2.3272, align 2, !tbaa !3 *)
mov mem0_384 v_add21_2_3274;
(*   %arrayidx9.2.1.3 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %706 = load i16, i16* %arrayidx9.2.1.3, align 2, !tbaa !3 *)
mov v706 mem0_450;
(*   %conv1.i.2.1.3 = sext i16 %706 to i32 *)
cast v_conv1_i_2_1_3@sint32 v706@sint16;
(*   %mul.i.2.1.3 = mul nsw i32 %conv1.i.2.1.3, 202 *)
mul v_mul_i_2_1_3 v_conv1_i_2_1_3 (202)@sint32;
(*   %call.i.2.1.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.1.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_1_3, v_call_i_2_1_3);
(*   %arrayidx11.2.1.3 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %707 = load i16, i16* %arrayidx11.2.1.3, align 2, !tbaa !3 *)
mov v707 mem0_386;
(*   %sub.2.1.3 = sub i16 %707, %call.i.2.1.3 *)
sub v_sub_2_1_3 v707 v_call_i_2_1_3;
(*   store i16 %sub.2.1.3, i16* %arrayidx9.2.1.3, align 2, !tbaa !3 *)
mov mem0_450 v_sub_2_1_3;
(*   %add21.2.1.3 = add i16 %707, %call.i.2.1.3 *)
add v_add21_2_1_3 v707 v_call_i_2_1_3;
(*   store i16 %add21.2.1.3, i16* %arrayidx11.2.1.3, align 2, !tbaa !3 *)
mov mem0_386 v_add21_2_1_3;
(*   %arrayidx9.2.2.3 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %708 = load i16, i16* %arrayidx9.2.2.3, align 2, !tbaa !3 *)
mov v708 mem0_452;
(*   %conv1.i.2.2.3 = sext i16 %708 to i32 *)
cast v_conv1_i_2_2_3@sint32 v708@sint16;
(*   %mul.i.2.2.3 = mul nsw i32 %conv1.i.2.2.3, 202 *)
mul v_mul_i_2_2_3 v_conv1_i_2_2_3 (202)@sint32;
(*   %call.i.2.2.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.2.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_2_3, v_call_i_2_2_3);
(*   %arrayidx11.2.2.3 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %709 = load i16, i16* %arrayidx11.2.2.3, align 2, !tbaa !3 *)
mov v709 mem0_388;
(*   %sub.2.2.3 = sub i16 %709, %call.i.2.2.3 *)
sub v_sub_2_2_3 v709 v_call_i_2_2_3;
(*   store i16 %sub.2.2.3, i16* %arrayidx9.2.2.3, align 2, !tbaa !3 *)
mov mem0_452 v_sub_2_2_3;
(*   %add21.2.2.3 = add i16 %709, %call.i.2.2.3 *)
add v_add21_2_2_3 v709 v_call_i_2_2_3;
(*   store i16 %add21.2.2.3, i16* %arrayidx11.2.2.3, align 2, !tbaa !3 *)
mov mem0_388 v_add21_2_2_3;
(*   %arrayidx9.2.3.3 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %710 = load i16, i16* %arrayidx9.2.3.3, align 2, !tbaa !3 *)
mov v710 mem0_454;
(*   %conv1.i.2.3.3 = sext i16 %710 to i32 *)
cast v_conv1_i_2_3_3@sint32 v710@sint16;
(*   %mul.i.2.3.3 = mul nsw i32 %conv1.i.2.3.3, 202 *)
mul v_mul_i_2_3_3 v_conv1_i_2_3_3 (202)@sint32;
(*   %call.i.2.3.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.3.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_3_3, v_call_i_2_3_3);
(*   %arrayidx11.2.3.3 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %711 = load i16, i16* %arrayidx11.2.3.3, align 2, !tbaa !3 *)
mov v711 mem0_390;
(*   %sub.2.3.3 = sub i16 %711, %call.i.2.3.3 *)
sub v_sub_2_3_3 v711 v_call_i_2_3_3;
(*   store i16 %sub.2.3.3, i16* %arrayidx9.2.3.3, align 2, !tbaa !3 *)
mov mem0_454 v_sub_2_3_3;
(*   %add21.2.3.3 = add i16 %711, %call.i.2.3.3 *)
add v_add21_2_3_3 v711 v_call_i_2_3_3;
(*   store i16 %add21.2.3.3, i16* %arrayidx11.2.3.3, align 2, !tbaa !3 *)
mov mem0_390 v_add21_2_3_3;
(*   %arrayidx9.2.4.3 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %712 = load i16, i16* %arrayidx9.2.4.3, align 2, !tbaa !3 *)
mov v712 mem0_456;
(*   %conv1.i.2.4.3 = sext i16 %712 to i32 *)
cast v_conv1_i_2_4_3@sint32 v712@sint16;
(*   %mul.i.2.4.3 = mul nsw i32 %conv1.i.2.4.3, 202 *)
mul v_mul_i_2_4_3 v_conv1_i_2_4_3 (202)@sint32;
(*   %call.i.2.4.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.4.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_4_3, v_call_i_2_4_3);
(*   %arrayidx11.2.4.3 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %713 = load i16, i16* %arrayidx11.2.4.3, align 2, !tbaa !3 *)
mov v713 mem0_392;
(*   %sub.2.4.3 = sub i16 %713, %call.i.2.4.3 *)
sub v_sub_2_4_3 v713 v_call_i_2_4_3;
(*   store i16 %sub.2.4.3, i16* %arrayidx9.2.4.3, align 2, !tbaa !3 *)
mov mem0_456 v_sub_2_4_3;
(*   %add21.2.4.3 = add i16 %713, %call.i.2.4.3 *)
add v_add21_2_4_3 v713 v_call_i_2_4_3;
(*   store i16 %add21.2.4.3, i16* %arrayidx11.2.4.3, align 2, !tbaa !3 *)
mov mem0_392 v_add21_2_4_3;
(*   %arrayidx9.2.5.3 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %714 = load i16, i16* %arrayidx9.2.5.3, align 2, !tbaa !3 *)
mov v714 mem0_458;
(*   %conv1.i.2.5.3 = sext i16 %714 to i32 *)
cast v_conv1_i_2_5_3@sint32 v714@sint16;
(*   %mul.i.2.5.3 = mul nsw i32 %conv1.i.2.5.3, 202 *)
mul v_mul_i_2_5_3 v_conv1_i_2_5_3 (202)@sint32;
(*   %call.i.2.5.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.5.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_5_3, v_call_i_2_5_3);
(*   %arrayidx11.2.5.3 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %715 = load i16, i16* %arrayidx11.2.5.3, align 2, !tbaa !3 *)
mov v715 mem0_394;
(*   %sub.2.5.3 = sub i16 %715, %call.i.2.5.3 *)
sub v_sub_2_5_3 v715 v_call_i_2_5_3;
(*   store i16 %sub.2.5.3, i16* %arrayidx9.2.5.3, align 2, !tbaa !3 *)
mov mem0_458 v_sub_2_5_3;
(*   %add21.2.5.3 = add i16 %715, %call.i.2.5.3 *)
add v_add21_2_5_3 v715 v_call_i_2_5_3;
(*   store i16 %add21.2.5.3, i16* %arrayidx11.2.5.3, align 2, !tbaa !3 *)
mov mem0_394 v_add21_2_5_3;
(*   %arrayidx9.2.6.3 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %716 = load i16, i16* %arrayidx9.2.6.3, align 2, !tbaa !3 *)
mov v716 mem0_460;
(*   %conv1.i.2.6.3 = sext i16 %716 to i32 *)
cast v_conv1_i_2_6_3@sint32 v716@sint16;
(*   %mul.i.2.6.3 = mul nsw i32 %conv1.i.2.6.3, 202 *)
mul v_mul_i_2_6_3 v_conv1_i_2_6_3 (202)@sint32;
(*   %call.i.2.6.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.6.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_6_3, v_call_i_2_6_3);
(*   %arrayidx11.2.6.3 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %717 = load i16, i16* %arrayidx11.2.6.3, align 2, !tbaa !3 *)
mov v717 mem0_396;
(*   %sub.2.6.3 = sub i16 %717, %call.i.2.6.3 *)
sub v_sub_2_6_3 v717 v_call_i_2_6_3;
(*   store i16 %sub.2.6.3, i16* %arrayidx9.2.6.3, align 2, !tbaa !3 *)
mov mem0_460 v_sub_2_6_3;
(*   %add21.2.6.3 = add i16 %717, %call.i.2.6.3 *)
add v_add21_2_6_3 v717 v_call_i_2_6_3;
(*   store i16 %add21.2.6.3, i16* %arrayidx11.2.6.3, align 2, !tbaa !3 *)
mov mem0_396 v_add21_2_6_3;
(*   %arrayidx9.2.7.3 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %718 = load i16, i16* %arrayidx9.2.7.3, align 2, !tbaa !3 *)
mov v718 mem0_462;
(*   %conv1.i.2.7.3 = sext i16 %718 to i32 *)
cast v_conv1_i_2_7_3@sint32 v718@sint16;
(*   %mul.i.2.7.3 = mul nsw i32 %conv1.i.2.7.3, 202 *)
mul v_mul_i_2_7_3 v_conv1_i_2_7_3 (202)@sint32;
(*   %call.i.2.7.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.7.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_7_3, v_call_i_2_7_3);
(*   %arrayidx11.2.7.3 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %719 = load i16, i16* %arrayidx11.2.7.3, align 2, !tbaa !3 *)
mov v719 mem0_398;
(*   %sub.2.7.3 = sub i16 %719, %call.i.2.7.3 *)
sub v_sub_2_7_3 v719 v_call_i_2_7_3;
(*   store i16 %sub.2.7.3, i16* %arrayidx9.2.7.3, align 2, !tbaa !3 *)
mov mem0_462 v_sub_2_7_3;
(*   %add21.2.7.3 = add i16 %719, %call.i.2.7.3 *)
add v_add21_2_7_3 v719 v_call_i_2_7_3;
(*   store i16 %add21.2.7.3, i16* %arrayidx11.2.7.3, align 2, !tbaa !3 *)
mov mem0_398 v_add21_2_7_3;
(*   %arrayidx9.2.8.3 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %720 = load i16, i16* %arrayidx9.2.8.3, align 2, !tbaa !3 *)
mov v720 mem0_464;
(*   %conv1.i.2.8.3 = sext i16 %720 to i32 *)
cast v_conv1_i_2_8_3@sint32 v720@sint16;
(*   %mul.i.2.8.3 = mul nsw i32 %conv1.i.2.8.3, 202 *)
mul v_mul_i_2_8_3 v_conv1_i_2_8_3 (202)@sint32;
(*   %call.i.2.8.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.8.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_8_3, v_call_i_2_8_3);
(*   %arrayidx11.2.8.3 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %721 = load i16, i16* %arrayidx11.2.8.3, align 2, !tbaa !3 *)
mov v721 mem0_400;
(*   %sub.2.8.3 = sub i16 %721, %call.i.2.8.3 *)
sub v_sub_2_8_3 v721 v_call_i_2_8_3;
(*   store i16 %sub.2.8.3, i16* %arrayidx9.2.8.3, align 2, !tbaa !3 *)
mov mem0_464 v_sub_2_8_3;
(*   %add21.2.8.3 = add i16 %721, %call.i.2.8.3 *)
add v_add21_2_8_3 v721 v_call_i_2_8_3;
(*   store i16 %add21.2.8.3, i16* %arrayidx11.2.8.3, align 2, !tbaa !3 *)
mov mem0_400 v_add21_2_8_3;
(*   %arrayidx9.2.9.3 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %722 = load i16, i16* %arrayidx9.2.9.3, align 2, !tbaa !3 *)
mov v722 mem0_466;
(*   %conv1.i.2.9.3 = sext i16 %722 to i32 *)
cast v_conv1_i_2_9_3@sint32 v722@sint16;
(*   %mul.i.2.9.3 = mul nsw i32 %conv1.i.2.9.3, 202 *)
mul v_mul_i_2_9_3 v_conv1_i_2_9_3 (202)@sint32;
(*   %call.i.2.9.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.9.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_9_3, v_call_i_2_9_3);
(*   %arrayidx11.2.9.3 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %723 = load i16, i16* %arrayidx11.2.9.3, align 2, !tbaa !3 *)
mov v723 mem0_402;
(*   %sub.2.9.3 = sub i16 %723, %call.i.2.9.3 *)
sub v_sub_2_9_3 v723 v_call_i_2_9_3;
(*   store i16 %sub.2.9.3, i16* %arrayidx9.2.9.3, align 2, !tbaa !3 *)
mov mem0_466 v_sub_2_9_3;
(*   %add21.2.9.3 = add i16 %723, %call.i.2.9.3 *)
add v_add21_2_9_3 v723 v_call_i_2_9_3;
(*   store i16 %add21.2.9.3, i16* %arrayidx11.2.9.3, align 2, !tbaa !3 *)
mov mem0_402 v_add21_2_9_3;
(*   %arrayidx9.2.10.3 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %724 = load i16, i16* %arrayidx9.2.10.3, align 2, !tbaa !3 *)
mov v724 mem0_468;
(*   %conv1.i.2.10.3 = sext i16 %724 to i32 *)
cast v_conv1_i_2_10_3@sint32 v724@sint16;
(*   %mul.i.2.10.3 = mul nsw i32 %conv1.i.2.10.3, 202 *)
mul v_mul_i_2_10_3 v_conv1_i_2_10_3 (202)@sint32;
(*   %call.i.2.10.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.10.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_10_3, v_call_i_2_10_3);
(*   %arrayidx11.2.10.3 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %725 = load i16, i16* %arrayidx11.2.10.3, align 2, !tbaa !3 *)
mov v725 mem0_404;
(*   %sub.2.10.3 = sub i16 %725, %call.i.2.10.3 *)
sub v_sub_2_10_3 v725 v_call_i_2_10_3;
(*   store i16 %sub.2.10.3, i16* %arrayidx9.2.10.3, align 2, !tbaa !3 *)
mov mem0_468 v_sub_2_10_3;
(*   %add21.2.10.3 = add i16 %725, %call.i.2.10.3 *)
add v_add21_2_10_3 v725 v_call_i_2_10_3;
(*   store i16 %add21.2.10.3, i16* %arrayidx11.2.10.3, align 2, !tbaa !3 *)
mov mem0_404 v_add21_2_10_3;
(*   %arrayidx9.2.11.3 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %726 = load i16, i16* %arrayidx9.2.11.3, align 2, !tbaa !3 *)
mov v726 mem0_470;
(*   %conv1.i.2.11.3 = sext i16 %726 to i32 *)
cast v_conv1_i_2_11_3@sint32 v726@sint16;
(*   %mul.i.2.11.3 = mul nsw i32 %conv1.i.2.11.3, 202 *)
mul v_mul_i_2_11_3 v_conv1_i_2_11_3 (202)@sint32;
(*   %call.i.2.11.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.11.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_11_3, v_call_i_2_11_3);
(*   %arrayidx11.2.11.3 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %727 = load i16, i16* %arrayidx11.2.11.3, align 2, !tbaa !3 *)
mov v727 mem0_406;
(*   %sub.2.11.3 = sub i16 %727, %call.i.2.11.3 *)
sub v_sub_2_11_3 v727 v_call_i_2_11_3;
(*   store i16 %sub.2.11.3, i16* %arrayidx9.2.11.3, align 2, !tbaa !3 *)
mov mem0_470 v_sub_2_11_3;
(*   %add21.2.11.3 = add i16 %727, %call.i.2.11.3 *)
add v_add21_2_11_3 v727 v_call_i_2_11_3;
(*   store i16 %add21.2.11.3, i16* %arrayidx11.2.11.3, align 2, !tbaa !3 *)
mov mem0_406 v_add21_2_11_3;
(*   %arrayidx9.2.12.3 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %728 = load i16, i16* %arrayidx9.2.12.3, align 2, !tbaa !3 *)
mov v728 mem0_472;
(*   %conv1.i.2.12.3 = sext i16 %728 to i32 *)
cast v_conv1_i_2_12_3@sint32 v728@sint16;
(*   %mul.i.2.12.3 = mul nsw i32 %conv1.i.2.12.3, 202 *)
mul v_mul_i_2_12_3 v_conv1_i_2_12_3 (202)@sint32;
(*   %call.i.2.12.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.12.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_12_3, v_call_i_2_12_3);
(*   %arrayidx11.2.12.3 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %729 = load i16, i16* %arrayidx11.2.12.3, align 2, !tbaa !3 *)
mov v729 mem0_408;
(*   %sub.2.12.3 = sub i16 %729, %call.i.2.12.3 *)
sub v_sub_2_12_3 v729 v_call_i_2_12_3;
(*   store i16 %sub.2.12.3, i16* %arrayidx9.2.12.3, align 2, !tbaa !3 *)
mov mem0_472 v_sub_2_12_3;
(*   %add21.2.12.3 = add i16 %729, %call.i.2.12.3 *)
add v_add21_2_12_3 v729 v_call_i_2_12_3;
(*   store i16 %add21.2.12.3, i16* %arrayidx11.2.12.3, align 2, !tbaa !3 *)
mov mem0_408 v_add21_2_12_3;
(*   %arrayidx9.2.13.3 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %730 = load i16, i16* %arrayidx9.2.13.3, align 2, !tbaa !3 *)
mov v730 mem0_474;
(*   %conv1.i.2.13.3 = sext i16 %730 to i32 *)
cast v_conv1_i_2_13_3@sint32 v730@sint16;
(*   %mul.i.2.13.3 = mul nsw i32 %conv1.i.2.13.3, 202 *)
mul v_mul_i_2_13_3 v_conv1_i_2_13_3 (202)@sint32;
(*   %call.i.2.13.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.13.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_13_3, v_call_i_2_13_3);
(*   %arrayidx11.2.13.3 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %731 = load i16, i16* %arrayidx11.2.13.3, align 2, !tbaa !3 *)
mov v731 mem0_410;
(*   %sub.2.13.3 = sub i16 %731, %call.i.2.13.3 *)
sub v_sub_2_13_3 v731 v_call_i_2_13_3;
(*   store i16 %sub.2.13.3, i16* %arrayidx9.2.13.3, align 2, !tbaa !3 *)
mov mem0_474 v_sub_2_13_3;
(*   %add21.2.13.3 = add i16 %731, %call.i.2.13.3 *)
add v_add21_2_13_3 v731 v_call_i_2_13_3;
(*   store i16 %add21.2.13.3, i16* %arrayidx11.2.13.3, align 2, !tbaa !3 *)
mov mem0_410 v_add21_2_13_3;
(*   %arrayidx9.2.14.3 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %732 = load i16, i16* %arrayidx9.2.14.3, align 2, !tbaa !3 *)
mov v732 mem0_476;
(*   %conv1.i.2.14.3 = sext i16 %732 to i32 *)
cast v_conv1_i_2_14_3@sint32 v732@sint16;
(*   %mul.i.2.14.3 = mul nsw i32 %conv1.i.2.14.3, 202 *)
mul v_mul_i_2_14_3 v_conv1_i_2_14_3 (202)@sint32;
(*   %call.i.2.14.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.14.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_14_3, v_call_i_2_14_3);
(*   %arrayidx11.2.14.3 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %733 = load i16, i16* %arrayidx11.2.14.3, align 2, !tbaa !3 *)
mov v733 mem0_412;
(*   %sub.2.14.3 = sub i16 %733, %call.i.2.14.3 *)
sub v_sub_2_14_3 v733 v_call_i_2_14_3;
(*   store i16 %sub.2.14.3, i16* %arrayidx9.2.14.3, align 2, !tbaa !3 *)
mov mem0_476 v_sub_2_14_3;
(*   %add21.2.14.3 = add i16 %733, %call.i.2.14.3 *)
add v_add21_2_14_3 v733 v_call_i_2_14_3;
(*   store i16 %add21.2.14.3, i16* %arrayidx11.2.14.3, align 2, !tbaa !3 *)
mov mem0_412 v_add21_2_14_3;
(*   %arrayidx9.2.15.3 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %734 = load i16, i16* %arrayidx9.2.15.3, align 2, !tbaa !3 *)
mov v734 mem0_478;
(*   %conv1.i.2.15.3 = sext i16 %734 to i32 *)
cast v_conv1_i_2_15_3@sint32 v734@sint16;
(*   %mul.i.2.15.3 = mul nsw i32 %conv1.i.2.15.3, 202 *)
mul v_mul_i_2_15_3 v_conv1_i_2_15_3 (202)@sint32;
(*   %call.i.2.15.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.15.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_15_3, v_call_i_2_15_3);
(*   %arrayidx11.2.15.3 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %735 = load i16, i16* %arrayidx11.2.15.3, align 2, !tbaa !3 *)
mov v735 mem0_414;
(*   %sub.2.15.3 = sub i16 %735, %call.i.2.15.3 *)
sub v_sub_2_15_3 v735 v_call_i_2_15_3;
(*   store i16 %sub.2.15.3, i16* %arrayidx9.2.15.3, align 2, !tbaa !3 *)
mov mem0_478 v_sub_2_15_3;
(*   %add21.2.15.3 = add i16 %735, %call.i.2.15.3 *)
add v_add21_2_15_3 v735 v_call_i_2_15_3;
(*   store i16 %add21.2.15.3, i16* %arrayidx11.2.15.3, align 2, !tbaa !3 *)
mov mem0_414 v_add21_2_15_3;
(*   %arrayidx9.2.16.3 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %736 = load i16, i16* %arrayidx9.2.16.3, align 2, !tbaa !3 *)
mov v736 mem0_480;
(*   %conv1.i.2.16.3 = sext i16 %736 to i32 *)
cast v_conv1_i_2_16_3@sint32 v736@sint16;
(*   %mul.i.2.16.3 = mul nsw i32 %conv1.i.2.16.3, 202 *)
mul v_mul_i_2_16_3 v_conv1_i_2_16_3 (202)@sint32;
(*   %call.i.2.16.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.16.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_16_3, v_call_i_2_16_3);
(*   %arrayidx11.2.16.3 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %737 = load i16, i16* %arrayidx11.2.16.3, align 2, !tbaa !3 *)
mov v737 mem0_416;
(*   %sub.2.16.3 = sub i16 %737, %call.i.2.16.3 *)
sub v_sub_2_16_3 v737 v_call_i_2_16_3;
(*   store i16 %sub.2.16.3, i16* %arrayidx9.2.16.3, align 2, !tbaa !3 *)
mov mem0_480 v_sub_2_16_3;
(*   %add21.2.16.3 = add i16 %737, %call.i.2.16.3 *)
add v_add21_2_16_3 v737 v_call_i_2_16_3;
(*   store i16 %add21.2.16.3, i16* %arrayidx11.2.16.3, align 2, !tbaa !3 *)
mov mem0_416 v_add21_2_16_3;
(*   %arrayidx9.2.17.3 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %738 = load i16, i16* %arrayidx9.2.17.3, align 2, !tbaa !3 *)
mov v738 mem0_482;
(*   %conv1.i.2.17.3 = sext i16 %738 to i32 *)
cast v_conv1_i_2_17_3@sint32 v738@sint16;
(*   %mul.i.2.17.3 = mul nsw i32 %conv1.i.2.17.3, 202 *)
mul v_mul_i_2_17_3 v_conv1_i_2_17_3 (202)@sint32;
(*   %call.i.2.17.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.17.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_17_3, v_call_i_2_17_3);
(*   %arrayidx11.2.17.3 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %739 = load i16, i16* %arrayidx11.2.17.3, align 2, !tbaa !3 *)
mov v739 mem0_418;
(*   %sub.2.17.3 = sub i16 %739, %call.i.2.17.3 *)
sub v_sub_2_17_3 v739 v_call_i_2_17_3;
(*   store i16 %sub.2.17.3, i16* %arrayidx9.2.17.3, align 2, !tbaa !3 *)
mov mem0_482 v_sub_2_17_3;
(*   %add21.2.17.3 = add i16 %739, %call.i.2.17.3 *)
add v_add21_2_17_3 v739 v_call_i_2_17_3;
(*   store i16 %add21.2.17.3, i16* %arrayidx11.2.17.3, align 2, !tbaa !3 *)
mov mem0_418 v_add21_2_17_3;
(*   %arrayidx9.2.18.3 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %740 = load i16, i16* %arrayidx9.2.18.3, align 2, !tbaa !3 *)
mov v740 mem0_484;
(*   %conv1.i.2.18.3 = sext i16 %740 to i32 *)
cast v_conv1_i_2_18_3@sint32 v740@sint16;
(*   %mul.i.2.18.3 = mul nsw i32 %conv1.i.2.18.3, 202 *)
mul v_mul_i_2_18_3 v_conv1_i_2_18_3 (202)@sint32;
(*   %call.i.2.18.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.18.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_18_3, v_call_i_2_18_3);
(*   %arrayidx11.2.18.3 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %741 = load i16, i16* %arrayidx11.2.18.3, align 2, !tbaa !3 *)
mov v741 mem0_420;
(*   %sub.2.18.3 = sub i16 %741, %call.i.2.18.3 *)
sub v_sub_2_18_3 v741 v_call_i_2_18_3;
(*   store i16 %sub.2.18.3, i16* %arrayidx9.2.18.3, align 2, !tbaa !3 *)
mov mem0_484 v_sub_2_18_3;
(*   %add21.2.18.3 = add i16 %741, %call.i.2.18.3 *)
add v_add21_2_18_3 v741 v_call_i_2_18_3;
(*   store i16 %add21.2.18.3, i16* %arrayidx11.2.18.3, align 2, !tbaa !3 *)
mov mem0_420 v_add21_2_18_3;
(*   %arrayidx9.2.19.3 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %742 = load i16, i16* %arrayidx9.2.19.3, align 2, !tbaa !3 *)
mov v742 mem0_486;
(*   %conv1.i.2.19.3 = sext i16 %742 to i32 *)
cast v_conv1_i_2_19_3@sint32 v742@sint16;
(*   %mul.i.2.19.3 = mul nsw i32 %conv1.i.2.19.3, 202 *)
mul v_mul_i_2_19_3 v_conv1_i_2_19_3 (202)@sint32;
(*   %call.i.2.19.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.19.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_19_3, v_call_i_2_19_3);
(*   %arrayidx11.2.19.3 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %743 = load i16, i16* %arrayidx11.2.19.3, align 2, !tbaa !3 *)
mov v743 mem0_422;
(*   %sub.2.19.3 = sub i16 %743, %call.i.2.19.3 *)
sub v_sub_2_19_3 v743 v_call_i_2_19_3;
(*   store i16 %sub.2.19.3, i16* %arrayidx9.2.19.3, align 2, !tbaa !3 *)
mov mem0_486 v_sub_2_19_3;
(*   %add21.2.19.3 = add i16 %743, %call.i.2.19.3 *)
add v_add21_2_19_3 v743 v_call_i_2_19_3;
(*   store i16 %add21.2.19.3, i16* %arrayidx11.2.19.3, align 2, !tbaa !3 *)
mov mem0_422 v_add21_2_19_3;
(*   %arrayidx9.2.20.3 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %744 = load i16, i16* %arrayidx9.2.20.3, align 2, !tbaa !3 *)
mov v744 mem0_488;
(*   %conv1.i.2.20.3 = sext i16 %744 to i32 *)
cast v_conv1_i_2_20_3@sint32 v744@sint16;
(*   %mul.i.2.20.3 = mul nsw i32 %conv1.i.2.20.3, 202 *)
mul v_mul_i_2_20_3 v_conv1_i_2_20_3 (202)@sint32;
(*   %call.i.2.20.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.20.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_20_3, v_call_i_2_20_3);
(*   %arrayidx11.2.20.3 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %745 = load i16, i16* %arrayidx11.2.20.3, align 2, !tbaa !3 *)
mov v745 mem0_424;
(*   %sub.2.20.3 = sub i16 %745, %call.i.2.20.3 *)
sub v_sub_2_20_3 v745 v_call_i_2_20_3;
(*   store i16 %sub.2.20.3, i16* %arrayidx9.2.20.3, align 2, !tbaa !3 *)
mov mem0_488 v_sub_2_20_3;
(*   %add21.2.20.3 = add i16 %745, %call.i.2.20.3 *)
add v_add21_2_20_3 v745 v_call_i_2_20_3;
(*   store i16 %add21.2.20.3, i16* %arrayidx11.2.20.3, align 2, !tbaa !3 *)
mov mem0_424 v_add21_2_20_3;
(*   %arrayidx9.2.21.3 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %746 = load i16, i16* %arrayidx9.2.21.3, align 2, !tbaa !3 *)
mov v746 mem0_490;
(*   %conv1.i.2.21.3 = sext i16 %746 to i32 *)
cast v_conv1_i_2_21_3@sint32 v746@sint16;
(*   %mul.i.2.21.3 = mul nsw i32 %conv1.i.2.21.3, 202 *)
mul v_mul_i_2_21_3 v_conv1_i_2_21_3 (202)@sint32;
(*   %call.i.2.21.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.21.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_21_3, v_call_i_2_21_3);
(*   %arrayidx11.2.21.3 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %747 = load i16, i16* %arrayidx11.2.21.3, align 2, !tbaa !3 *)
mov v747 mem0_426;
(*   %sub.2.21.3 = sub i16 %747, %call.i.2.21.3 *)
sub v_sub_2_21_3 v747 v_call_i_2_21_3;
(*   store i16 %sub.2.21.3, i16* %arrayidx9.2.21.3, align 2, !tbaa !3 *)
mov mem0_490 v_sub_2_21_3;
(*   %add21.2.21.3 = add i16 %747, %call.i.2.21.3 *)
add v_add21_2_21_3 v747 v_call_i_2_21_3;
(*   store i16 %add21.2.21.3, i16* %arrayidx11.2.21.3, align 2, !tbaa !3 *)
mov mem0_426 v_add21_2_21_3;
(*   %arrayidx9.2.22.3 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %748 = load i16, i16* %arrayidx9.2.22.3, align 2, !tbaa !3 *)
mov v748 mem0_492;
(*   %conv1.i.2.22.3 = sext i16 %748 to i32 *)
cast v_conv1_i_2_22_3@sint32 v748@sint16;
(*   %mul.i.2.22.3 = mul nsw i32 %conv1.i.2.22.3, 202 *)
mul v_mul_i_2_22_3 v_conv1_i_2_22_3 (202)@sint32;
(*   %call.i.2.22.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.22.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_22_3, v_call_i_2_22_3);
(*   %arrayidx11.2.22.3 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %749 = load i16, i16* %arrayidx11.2.22.3, align 2, !tbaa !3 *)
mov v749 mem0_428;
(*   %sub.2.22.3 = sub i16 %749, %call.i.2.22.3 *)
sub v_sub_2_22_3 v749 v_call_i_2_22_3;
(*   store i16 %sub.2.22.3, i16* %arrayidx9.2.22.3, align 2, !tbaa !3 *)
mov mem0_492 v_sub_2_22_3;
(*   %add21.2.22.3 = add i16 %749, %call.i.2.22.3 *)
add v_add21_2_22_3 v749 v_call_i_2_22_3;
(*   store i16 %add21.2.22.3, i16* %arrayidx11.2.22.3, align 2, !tbaa !3 *)
mov mem0_428 v_add21_2_22_3;
(*   %arrayidx9.2.23.3 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %750 = load i16, i16* %arrayidx9.2.23.3, align 2, !tbaa !3 *)
mov v750 mem0_494;
(*   %conv1.i.2.23.3 = sext i16 %750 to i32 *)
cast v_conv1_i_2_23_3@sint32 v750@sint16;
(*   %mul.i.2.23.3 = mul nsw i32 %conv1.i.2.23.3, 202 *)
mul v_mul_i_2_23_3 v_conv1_i_2_23_3 (202)@sint32;
(*   %call.i.2.23.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.23.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_23_3, v_call_i_2_23_3);
(*   %arrayidx11.2.23.3 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %751 = load i16, i16* %arrayidx11.2.23.3, align 2, !tbaa !3 *)
mov v751 mem0_430;
(*   %sub.2.23.3 = sub i16 %751, %call.i.2.23.3 *)
sub v_sub_2_23_3 v751 v_call_i_2_23_3;
(*   store i16 %sub.2.23.3, i16* %arrayidx9.2.23.3, align 2, !tbaa !3 *)
mov mem0_494 v_sub_2_23_3;
(*   %add21.2.23.3 = add i16 %751, %call.i.2.23.3 *)
add v_add21_2_23_3 v751 v_call_i_2_23_3;
(*   store i16 %add21.2.23.3, i16* %arrayidx11.2.23.3, align 2, !tbaa !3 *)
mov mem0_430 v_add21_2_23_3;
(*   %arrayidx9.2.24.3 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %752 = load i16, i16* %arrayidx9.2.24.3, align 2, !tbaa !3 *)
mov v752 mem0_496;
(*   %conv1.i.2.24.3 = sext i16 %752 to i32 *)
cast v_conv1_i_2_24_3@sint32 v752@sint16;
(*   %mul.i.2.24.3 = mul nsw i32 %conv1.i.2.24.3, 202 *)
mul v_mul_i_2_24_3 v_conv1_i_2_24_3 (202)@sint32;
(*   %call.i.2.24.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.24.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_24_3, v_call_i_2_24_3);
(*   %arrayidx11.2.24.3 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %753 = load i16, i16* %arrayidx11.2.24.3, align 2, !tbaa !3 *)
mov v753 mem0_432;
(*   %sub.2.24.3 = sub i16 %753, %call.i.2.24.3 *)
sub v_sub_2_24_3 v753 v_call_i_2_24_3;
(*   store i16 %sub.2.24.3, i16* %arrayidx9.2.24.3, align 2, !tbaa !3 *)
mov mem0_496 v_sub_2_24_3;
(*   %add21.2.24.3 = add i16 %753, %call.i.2.24.3 *)
add v_add21_2_24_3 v753 v_call_i_2_24_3;
(*   store i16 %add21.2.24.3, i16* %arrayidx11.2.24.3, align 2, !tbaa !3 *)
mov mem0_432 v_add21_2_24_3;
(*   %arrayidx9.2.25.3 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %754 = load i16, i16* %arrayidx9.2.25.3, align 2, !tbaa !3 *)
mov v754 mem0_498;
(*   %conv1.i.2.25.3 = sext i16 %754 to i32 *)
cast v_conv1_i_2_25_3@sint32 v754@sint16;
(*   %mul.i.2.25.3 = mul nsw i32 %conv1.i.2.25.3, 202 *)
mul v_mul_i_2_25_3 v_conv1_i_2_25_3 (202)@sint32;
(*   %call.i.2.25.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.25.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_25_3, v_call_i_2_25_3);
(*   %arrayidx11.2.25.3 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %755 = load i16, i16* %arrayidx11.2.25.3, align 2, !tbaa !3 *)
mov v755 mem0_434;
(*   %sub.2.25.3 = sub i16 %755, %call.i.2.25.3 *)
sub v_sub_2_25_3 v755 v_call_i_2_25_3;
(*   store i16 %sub.2.25.3, i16* %arrayidx9.2.25.3, align 2, !tbaa !3 *)
mov mem0_498 v_sub_2_25_3;
(*   %add21.2.25.3 = add i16 %755, %call.i.2.25.3 *)
add v_add21_2_25_3 v755 v_call_i_2_25_3;
(*   store i16 %add21.2.25.3, i16* %arrayidx11.2.25.3, align 2, !tbaa !3 *)
mov mem0_434 v_add21_2_25_3;
(*   %arrayidx9.2.26.3 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %756 = load i16, i16* %arrayidx9.2.26.3, align 2, !tbaa !3 *)
mov v756 mem0_500;
(*   %conv1.i.2.26.3 = sext i16 %756 to i32 *)
cast v_conv1_i_2_26_3@sint32 v756@sint16;
(*   %mul.i.2.26.3 = mul nsw i32 %conv1.i.2.26.3, 202 *)
mul v_mul_i_2_26_3 v_conv1_i_2_26_3 (202)@sint32;
(*   %call.i.2.26.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.26.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_26_3, v_call_i_2_26_3);
(*   %arrayidx11.2.26.3 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %757 = load i16, i16* %arrayidx11.2.26.3, align 2, !tbaa !3 *)
mov v757 mem0_436;
(*   %sub.2.26.3 = sub i16 %757, %call.i.2.26.3 *)
sub v_sub_2_26_3 v757 v_call_i_2_26_3;
(*   store i16 %sub.2.26.3, i16* %arrayidx9.2.26.3, align 2, !tbaa !3 *)
mov mem0_500 v_sub_2_26_3;
(*   %add21.2.26.3 = add i16 %757, %call.i.2.26.3 *)
add v_add21_2_26_3 v757 v_call_i_2_26_3;
(*   store i16 %add21.2.26.3, i16* %arrayidx11.2.26.3, align 2, !tbaa !3 *)
mov mem0_436 v_add21_2_26_3;
(*   %arrayidx9.2.27.3 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %758 = load i16, i16* %arrayidx9.2.27.3, align 2, !tbaa !3 *)
mov v758 mem0_502;
(*   %conv1.i.2.27.3 = sext i16 %758 to i32 *)
cast v_conv1_i_2_27_3@sint32 v758@sint16;
(*   %mul.i.2.27.3 = mul nsw i32 %conv1.i.2.27.3, 202 *)
mul v_mul_i_2_27_3 v_conv1_i_2_27_3 (202)@sint32;
(*   %call.i.2.27.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.27.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_27_3, v_call_i_2_27_3);
(*   %arrayidx11.2.27.3 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %759 = load i16, i16* %arrayidx11.2.27.3, align 2, !tbaa !3 *)
mov v759 mem0_438;
(*   %sub.2.27.3 = sub i16 %759, %call.i.2.27.3 *)
sub v_sub_2_27_3 v759 v_call_i_2_27_3;
(*   store i16 %sub.2.27.3, i16* %arrayidx9.2.27.3, align 2, !tbaa !3 *)
mov mem0_502 v_sub_2_27_3;
(*   %add21.2.27.3 = add i16 %759, %call.i.2.27.3 *)
add v_add21_2_27_3 v759 v_call_i_2_27_3;
(*   store i16 %add21.2.27.3, i16* %arrayidx11.2.27.3, align 2, !tbaa !3 *)
mov mem0_438 v_add21_2_27_3;
(*   %arrayidx9.2.28.3 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %760 = load i16, i16* %arrayidx9.2.28.3, align 2, !tbaa !3 *)
mov v760 mem0_504;
(*   %conv1.i.2.28.3 = sext i16 %760 to i32 *)
cast v_conv1_i_2_28_3@sint32 v760@sint16;
(*   %mul.i.2.28.3 = mul nsw i32 %conv1.i.2.28.3, 202 *)
mul v_mul_i_2_28_3 v_conv1_i_2_28_3 (202)@sint32;
(*   %call.i.2.28.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.28.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_28_3, v_call_i_2_28_3);
(*   %arrayidx11.2.28.3 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %761 = load i16, i16* %arrayidx11.2.28.3, align 2, !tbaa !3 *)
mov v761 mem0_440;
(*   %sub.2.28.3 = sub i16 %761, %call.i.2.28.3 *)
sub v_sub_2_28_3 v761 v_call_i_2_28_3;
(*   store i16 %sub.2.28.3, i16* %arrayidx9.2.28.3, align 2, !tbaa !3 *)
mov mem0_504 v_sub_2_28_3;
(*   %add21.2.28.3 = add i16 %761, %call.i.2.28.3 *)
add v_add21_2_28_3 v761 v_call_i_2_28_3;
(*   store i16 %add21.2.28.3, i16* %arrayidx11.2.28.3, align 2, !tbaa !3 *)
mov mem0_440 v_add21_2_28_3;
(*   %arrayidx9.2.29.3 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %762 = load i16, i16* %arrayidx9.2.29.3, align 2, !tbaa !3 *)
mov v762 mem0_506;
(*   %conv1.i.2.29.3 = sext i16 %762 to i32 *)
cast v_conv1_i_2_29_3@sint32 v762@sint16;
(*   %mul.i.2.29.3 = mul nsw i32 %conv1.i.2.29.3, 202 *)
mul v_mul_i_2_29_3 v_conv1_i_2_29_3 (202)@sint32;
(*   %call.i.2.29.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.29.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_29_3, v_call_i_2_29_3);
(*   %arrayidx11.2.29.3 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %763 = load i16, i16* %arrayidx11.2.29.3, align 2, !tbaa !3 *)
mov v763 mem0_442;
(*   %sub.2.29.3 = sub i16 %763, %call.i.2.29.3 *)
sub v_sub_2_29_3 v763 v_call_i_2_29_3;
(*   store i16 %sub.2.29.3, i16* %arrayidx9.2.29.3, align 2, !tbaa !3 *)
mov mem0_506 v_sub_2_29_3;
(*   %add21.2.29.3 = add i16 %763, %call.i.2.29.3 *)
add v_add21_2_29_3 v763 v_call_i_2_29_3;
(*   store i16 %add21.2.29.3, i16* %arrayidx11.2.29.3, align 2, !tbaa !3 *)
mov mem0_442 v_add21_2_29_3;
(*   %arrayidx9.2.30.3 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %764 = load i16, i16* %arrayidx9.2.30.3, align 2, !tbaa !3 *)
mov v764 mem0_508;
(*   %conv1.i.2.30.3 = sext i16 %764 to i32 *)
cast v_conv1_i_2_30_3@sint32 v764@sint16;
(*   %mul.i.2.30.3 = mul nsw i32 %conv1.i.2.30.3, 202 *)
mul v_mul_i_2_30_3 v_conv1_i_2_30_3 (202)@sint32;
(*   %call.i.2.30.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.30.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_30_3, v_call_i_2_30_3);
(*   %arrayidx11.2.30.3 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %765 = load i16, i16* %arrayidx11.2.30.3, align 2, !tbaa !3 *)
mov v765 mem0_444;
(*   %sub.2.30.3 = sub i16 %765, %call.i.2.30.3 *)
sub v_sub_2_30_3 v765 v_call_i_2_30_3;
(*   store i16 %sub.2.30.3, i16* %arrayidx9.2.30.3, align 2, !tbaa !3 *)
mov mem0_508 v_sub_2_30_3;
(*   %add21.2.30.3 = add i16 %765, %call.i.2.30.3 *)
add v_add21_2_30_3 v765 v_call_i_2_30_3;
(*   store i16 %add21.2.30.3, i16* %arrayidx11.2.30.3, align 2, !tbaa !3 *)
mov mem0_444 v_add21_2_30_3;
(*   %arrayidx9.2.31.3 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %766 = load i16, i16* %arrayidx9.2.31.3, align 2, !tbaa !3 *)
mov v766 mem0_510;
(*   %conv1.i.2.31.3 = sext i16 %766 to i32 *)
cast v_conv1_i_2_31_3@sint32 v766@sint16;
(*   %mul.i.2.31.3 = mul nsw i32 %conv1.i.2.31.3, 202 *)
mul v_mul_i_2_31_3 v_conv1_i_2_31_3 (202)@sint32;
(*   %call.i.2.31.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.2.31.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_2_31_3, v_call_i_2_31_3);
(*   %arrayidx11.2.31.3 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %767 = load i16, i16* %arrayidx11.2.31.3, align 2, !tbaa !3 *)
mov v767 mem0_446;
(*   %sub.2.31.3 = sub i16 %767, %call.i.2.31.3 *)
sub v_sub_2_31_3 v767 v_call_i_2_31_3;
(*   store i16 %sub.2.31.3, i16* %arrayidx9.2.31.3, align 2, !tbaa !3 *)
mov mem0_510 v_sub_2_31_3;
(*   %add21.2.31.3 = add i16 %767, %call.i.2.31.3 *)
add v_add21_2_31_3 v767 v_call_i_2_31_3;
(*   store i16 %add21.2.31.3, i16* %arrayidx11.2.31.3, align 2, !tbaa !3 *)
mov mem0_446 v_add21_2_31_3;

cut and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3) + 
mem0_8*(x**4) + mem0_10*(x**5) + mem0_12*(x**6) + mem0_14*(x**7) + 
mem0_16*(x**8) + mem0_18*(x**9) + mem0_20*(x**10) + mem0_22*(x**11) + 
mem0_24*(x**12) + mem0_26*(x**13) + mem0_28*(x**14) + mem0_30*(x**15) + 
mem0_32*(x**16) + mem0_34*(x**17) + mem0_36*(x**18) + mem0_38*(x**19) + 
mem0_40*(x**20) + mem0_42*(x**21) + mem0_44*(x**22) + mem0_46*(x**23) + 
mem0_48*(x**24) + mem0_50*(x**25) + mem0_52*(x**26) + mem0_54*(x**27) + 
mem0_56*(x**28) + mem0_58*(x**29) + mem0_60*(x**30) + mem0_62*(x**31)
)
[3329, x**32 - 2642],
eqmod 
input_polynomial * input_polynomial
(
mem0_64*(x**0) + mem0_66*(x**1) + mem0_68*(x**2) + mem0_70*(x**3) + 
mem0_72*(x**4) + mem0_74*(x**5) + mem0_76*(x**6) + mem0_78*(x**7) + 
mem0_80*(x**8) + mem0_82*(x**9) + mem0_84*(x**10) + mem0_86*(x**11) + 
mem0_88*(x**12) + mem0_90*(x**13) + mem0_92*(x**14) + mem0_94*(x**15) + 
mem0_96*(x**16) + mem0_98*(x**17) + mem0_100*(x**18) + mem0_102*(x**19) + 
mem0_104*(x**20) + mem0_106*(x**21) + mem0_108*(x**22) + mem0_110*(x**23) + 
mem0_112*(x**24) + mem0_114*(x**25) + mem0_116*(x**26) + mem0_118*(x**27) + 
mem0_120*(x**28) + mem0_122*(x**29) + mem0_124*(x**30) + mem0_126*(x**31)
)
[3329, x**32 - 687],
eqmod 
input_polynomial * input_polynomial
(
mem0_128*(x**0) + mem0_130*(x**1) + mem0_132*(x**2) + mem0_134*(x**3) + 
mem0_136*(x**4) + mem0_138*(x**5) + mem0_140*(x**6) + mem0_142*(x**7) + 
mem0_144*(x**8) + mem0_146*(x**9) + mem0_148*(x**10) + mem0_150*(x**11) + 
mem0_152*(x**12) + mem0_154*(x**13) + mem0_156*(x**14) + mem0_158*(x**15) + 
mem0_160*(x**16) + mem0_162*(x**17) + mem0_164*(x**18) + mem0_166*(x**19) + 
mem0_168*(x**20) + mem0_170*(x**21) + mem0_172*(x**22) + mem0_174*(x**23) + 
mem0_176*(x**24) + mem0_178*(x**25) + mem0_180*(x**26) + mem0_182*(x**27) + 
mem0_184*(x**28) + mem0_186*(x**29) + mem0_188*(x**30) + mem0_190*(x**31)
)
[3329, x**32 - 630],
eqmod 
input_polynomial * input_polynomial
(
mem0_192*(x**0) + mem0_194*(x**1) + mem0_196*(x**2) + mem0_198*(x**3) + 
mem0_200*(x**4) + mem0_202*(x**5) + mem0_204*(x**6) + mem0_206*(x**7) + 
mem0_208*(x**8) + mem0_210*(x**9) + mem0_212*(x**10) + mem0_214*(x**11) + 
mem0_216*(x**12) + mem0_218*(x**13) + mem0_220*(x**14) + mem0_222*(x**15) + 
mem0_224*(x**16) + mem0_226*(x**17) + mem0_228*(x**18) + mem0_230*(x**19) + 
mem0_232*(x**20) + mem0_234*(x**21) + mem0_236*(x**22) + mem0_238*(x**23) + 
mem0_240*(x**24) + mem0_242*(x**25) + mem0_244*(x**26) + mem0_246*(x**27) + 
mem0_248*(x**28) + mem0_250*(x**29) + mem0_252*(x**30) + mem0_254*(x**31)
)
[3329, x**32 - 2699],
eqmod 
input_polynomial * input_polynomial
(
mem0_256*(x**0) + mem0_258*(x**1) + mem0_260*(x**2) + mem0_262*(x**3) + 
mem0_264*(x**4) + mem0_266*(x**5) + mem0_268*(x**6) + mem0_270*(x**7) + 
mem0_272*(x**8) + mem0_274*(x**9) + mem0_276*(x**10) + mem0_278*(x**11) + 
mem0_280*(x**12) + mem0_282*(x**13) + mem0_284*(x**14) + mem0_286*(x**15) + 
mem0_288*(x**16) + mem0_290*(x**17) + mem0_292*(x**18) + mem0_294*(x**19) + 
mem0_296*(x**20) + mem0_298*(x**21) + mem0_300*(x**22) + mem0_302*(x**23) + 
mem0_304*(x**24) + mem0_306*(x**25) + mem0_308*(x**26) + mem0_310*(x**27) + 
mem0_312*(x**28) + mem0_314*(x**29) + mem0_316*(x**30) + mem0_318*(x**31)
)
[3329, x**32 - 1897],
eqmod 
input_polynomial * input_polynomial
(
mem0_320*(x**0) + mem0_322*(x**1) + mem0_324*(x**2) + mem0_326*(x**3) + 
mem0_328*(x**4) + mem0_330*(x**5) + mem0_332*(x**6) + mem0_334*(x**7) + 
mem0_336*(x**8) + mem0_338*(x**9) + mem0_340*(x**10) + mem0_342*(x**11) + 
mem0_344*(x**12) + mem0_346*(x**13) + mem0_348*(x**14) + mem0_350*(x**15) + 
mem0_352*(x**16) + mem0_354*(x**17) + mem0_356*(x**18) + mem0_358*(x**19) + 
mem0_360*(x**20) + mem0_362*(x**21) + mem0_364*(x**22) + mem0_366*(x**23) + 
mem0_368*(x**24) + mem0_370*(x**25) + mem0_372*(x**26) + mem0_374*(x**27) + 
mem0_376*(x**28) + mem0_378*(x**29) + mem0_380*(x**30) + mem0_382*(x**31)
)
[3329, x**32 - 1432],
eqmod 
input_polynomial * input_polynomial
(
mem0_384*(x**0) + mem0_386*(x**1) + mem0_388*(x**2) + mem0_390*(x**3) + 
mem0_392*(x**4) + mem0_394*(x**5) + mem0_396*(x**6) + mem0_398*(x**7) + 
mem0_400*(x**8) + mem0_402*(x**9) + mem0_404*(x**10) + mem0_406*(x**11) + 
mem0_408*(x**12) + mem0_410*(x**13) + mem0_412*(x**14) + mem0_414*(x**15) + 
mem0_416*(x**16) + mem0_418*(x**17) + mem0_420*(x**18) + mem0_422*(x**19) + 
mem0_424*(x**20) + mem0_426*(x**21) + mem0_428*(x**22) + mem0_430*(x**23) + 
mem0_432*(x**24) + mem0_434*(x**25) + mem0_436*(x**26) + mem0_438*(x**27) + 
mem0_440*(x**28) + mem0_442*(x**29) + mem0_444*(x**30) + mem0_446*(x**31)
)
[3329, x**32 - 848],
eqmod 
input_polynomial * input_polynomial
(
mem0_448*(x**0) + mem0_450*(x**1) + mem0_452*(x**2) + mem0_454*(x**3) + 
mem0_456*(x**4) + mem0_458*(x**5) + mem0_460*(x**6) + mem0_462*(x**7) + 
mem0_464*(x**8) + mem0_466*(x**9) + mem0_468*(x**10) + mem0_470*(x**11) + 
mem0_472*(x**12) + mem0_474*(x**13) + mem0_476*(x**14) + mem0_478*(x**15) + 
mem0_480*(x**16) + mem0_482*(x**17) + mem0_484*(x**18) + mem0_486*(x**19) + 
mem0_488*(x**20) + mem0_490*(x**21) + mem0_492*(x**22) + mem0_494*(x**23) + 
mem0_496*(x**24) + mem0_498*(x**25) + mem0_500*(x**26) + mem0_502*(x**27) + 
mem0_504*(x**28) + mem0_506*(x**29) + mem0_508*(x**30) + mem0_510*(x**31)
)
[3329, x**32 - 2481]
] && and [
   (-5)@16 * 3329@16 <s mem0_0, mem0_0 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_2, mem0_2 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_4, mem0_4 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_6, mem0_6 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_8, mem0_8 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_10, mem0_10 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_12, mem0_12 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_14, mem0_14 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_16, mem0_16 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_18, mem0_18 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_20, mem0_20 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_22, mem0_22 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_24, mem0_24 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_26, mem0_26 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_28, mem0_28 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_30, mem0_30 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_32, mem0_32 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_34, mem0_34 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_36, mem0_36 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_38, mem0_38 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_40, mem0_40 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_42, mem0_42 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_44, mem0_44 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_46, mem0_46 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_48, mem0_48 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_50, mem0_50 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_52, mem0_52 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_54, mem0_54 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_56, mem0_56 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_58, mem0_58 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_60, mem0_60 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_62, mem0_62 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_64, mem0_64 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_66, mem0_66 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_68, mem0_68 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_70, mem0_70 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_72, mem0_72 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_74, mem0_74 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_76, mem0_76 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_78, mem0_78 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_80, mem0_80 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_82, mem0_82 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_84, mem0_84 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_86, mem0_86 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_88, mem0_88 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_90, mem0_90 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_92, mem0_92 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_94, mem0_94 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_96, mem0_96 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_98, mem0_98 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_100, mem0_100 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_102, mem0_102 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_104, mem0_104 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_106, mem0_106 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_108, mem0_108 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_110, mem0_110 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_112, mem0_112 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_114, mem0_114 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_116, mem0_116 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_118, mem0_118 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_120, mem0_120 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_122, mem0_122 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_124, mem0_124 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_126, mem0_126 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_128, mem0_128 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_130, mem0_130 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_132, mem0_132 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_134, mem0_134 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_136, mem0_136 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_138, mem0_138 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_140, mem0_140 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_142, mem0_142 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_144, mem0_144 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_146, mem0_146 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_148, mem0_148 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_150, mem0_150 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_152, mem0_152 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_154, mem0_154 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_156, mem0_156 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_158, mem0_158 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_160, mem0_160 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_162, mem0_162 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_164, mem0_164 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_166, mem0_166 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_168, mem0_168 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_170, mem0_170 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_172, mem0_172 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_174, mem0_174 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_176, mem0_176 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_178, mem0_178 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_180, mem0_180 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_182, mem0_182 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_184, mem0_184 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_186, mem0_186 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_188, mem0_188 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_190, mem0_190 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_192, mem0_192 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_194, mem0_194 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_196, mem0_196 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_198, mem0_198 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_200, mem0_200 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_202, mem0_202 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_204, mem0_204 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_206, mem0_206 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_208, mem0_208 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_210, mem0_210 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_212, mem0_212 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_214, mem0_214 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_216, mem0_216 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_218, mem0_218 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_220, mem0_220 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_222, mem0_222 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_224, mem0_224 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_226, mem0_226 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_228, mem0_228 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_230, mem0_230 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_232, mem0_232 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_234, mem0_234 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_236, mem0_236 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_238, mem0_238 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_240, mem0_240 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_242, mem0_242 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_244, mem0_244 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_246, mem0_246 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_248, mem0_248 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_250, mem0_250 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_252, mem0_252 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_254, mem0_254 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_256, mem0_256 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_258, mem0_258 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_260, mem0_260 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_262, mem0_262 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_264, mem0_264 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_266, mem0_266 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_268, mem0_268 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_270, mem0_270 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_272, mem0_272 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_274, mem0_274 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_276, mem0_276 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_278, mem0_278 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_280, mem0_280 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_282, mem0_282 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_284, mem0_284 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_286, mem0_286 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_288, mem0_288 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_290, mem0_290 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_292, mem0_292 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_294, mem0_294 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_296, mem0_296 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_298, mem0_298 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_300, mem0_300 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_302, mem0_302 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_304, mem0_304 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_306, mem0_306 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_308, mem0_308 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_310, mem0_310 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_312, mem0_312 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_314, mem0_314 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_316, mem0_316 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_318, mem0_318 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_320, mem0_320 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_322, mem0_322 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_324, mem0_324 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_326, mem0_326 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_328, mem0_328 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_330, mem0_330 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_332, mem0_332 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_334, mem0_334 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_336, mem0_336 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_338, mem0_338 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_340, mem0_340 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_342, mem0_342 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_344, mem0_344 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_346, mem0_346 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_348, mem0_348 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_350, mem0_350 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_352, mem0_352 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_354, mem0_354 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_356, mem0_356 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_358, mem0_358 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_360, mem0_360 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_362, mem0_362 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_364, mem0_364 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_366, mem0_366 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_368, mem0_368 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_370, mem0_370 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_372, mem0_372 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_374, mem0_374 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_376, mem0_376 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_378, mem0_378 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_380, mem0_380 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_382, mem0_382 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_384, mem0_384 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_386, mem0_386 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_388, mem0_388 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_390, mem0_390 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_392, mem0_392 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_394, mem0_394 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_396, mem0_396 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_398, mem0_398 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_400, mem0_400 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_402, mem0_402 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_404, mem0_404 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_406, mem0_406 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_408, mem0_408 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_410, mem0_410 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_412, mem0_412 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_414, mem0_414 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_416, mem0_416 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_418, mem0_418 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_420, mem0_420 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_422, mem0_422 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_424, mem0_424 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_426, mem0_426 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_428, mem0_428 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_430, mem0_430 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_432, mem0_432 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_434, mem0_434 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_436, mem0_436 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_438, mem0_438 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_440, mem0_440 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_442, mem0_442 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_444, mem0_444 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_446, mem0_446 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_448, mem0_448 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_450, mem0_450 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_452, mem0_452 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_454, mem0_454 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_456, mem0_456 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_458, mem0_458 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_460, mem0_460 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_462, mem0_462 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_464, mem0_464 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_466, mem0_466 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_468, mem0_468 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_470, mem0_470 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_472, mem0_472 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_474, mem0_474 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_476, mem0_476 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_478, mem0_478 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_480, mem0_480 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_482, mem0_482 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_484, mem0_484 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_486, mem0_486 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_488, mem0_488 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_490, mem0_490 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_492, mem0_492 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_494, mem0_494 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_496, mem0_496 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_498, mem0_498 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_500, mem0_500 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_502, mem0_502 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_504, mem0_504 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_506, mem0_506 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_508, mem0_508 <s 5@16 * 3329@16,
   (-5)@16 * 3329@16 <s mem0_510, mem0_510 <s 5@16 * 3329@16
];


(* NOTE: k = 8 *)

(*   %arrayidx9.3 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %768 = load i16, i16* %arrayidx9.3, align 2, !tbaa !3 *)
mov v768 mem0_32;
(*   %conv1.i.3 = sext i16 %768 to i32 *)
cast v_conv1_i_3@sint32 v768@sint16;
(*   %mul.i.3 = mul nsw i32 %conv1.i.3, -171 *)
mul v_mul_i_3 v_conv1_i_3 (-171)@sint32;
(*   %call.i.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3, v_call_i_3);
(*   %769 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v769 mem0_0;
(*   %sub.3 = sub i16 %769, %call.i.3 *)
sub v_sub_3 v769 v_call_i_3;
(*   store i16 %sub.3, i16* %arrayidx9.3, align 2, !tbaa !3 *)
mov mem0_32 v_sub_3;
(*   %add21.3 = add i16 %769, %call.i.3 *)
add v_add21_3 v769 v_call_i_3;
(*   store i16 %add21.3, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21_3;
(*   %arrayidx9.3.1 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %770 = load i16, i16* %arrayidx9.3.1, align 2, !tbaa !3 *)
mov v770 mem0_34;
(*   %conv1.i.3.1 = sext i16 %770 to i32 *)
cast v_conv1_i_3_1@sint32 v770@sint16;
(*   %mul.i.3.1 = mul nsw i32 %conv1.i.3.1, -171 *)
mul v_mul_i_3_1 v_conv1_i_3_1 (-171)@sint32;
(*   %call.i.3.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1, v_call_i_3_1);
(*   %arrayidx11.3.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %771 = load i16, i16* %arrayidx11.3.1, align 2, !tbaa !3 *)
mov v771 mem0_2;
(*   %sub.3.1 = sub i16 %771, %call.i.3.1 *)
sub v_sub_3_1 v771 v_call_i_3_1;
(*   store i16 %sub.3.1, i16* %arrayidx9.3.1, align 2, !tbaa !3 *)
mov mem0_34 v_sub_3_1;
(*   %add21.3.1 = add i16 %771, %call.i.3.1 *)
add v_add21_3_1 v771 v_call_i_3_1;
(*   store i16 %add21.3.1, i16* %arrayidx11.3.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_3_1;
(*   %arrayidx9.3.2 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %772 = load i16, i16* %arrayidx9.3.2, align 2, !tbaa !3 *)
mov v772 mem0_36;
(*   %conv1.i.3.2 = sext i16 %772 to i32 *)
cast v_conv1_i_3_2@sint32 v772@sint16;
(*   %mul.i.3.2 = mul nsw i32 %conv1.i.3.2, -171 *)
mul v_mul_i_3_2 v_conv1_i_3_2 (-171)@sint32;
(*   %call.i.3.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2, v_call_i_3_2);
(*   %arrayidx11.3.2 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %773 = load i16, i16* %arrayidx11.3.2, align 2, !tbaa !3 *)
mov v773 mem0_4;
(*   %sub.3.2 = sub i16 %773, %call.i.3.2 *)
sub v_sub_3_2 v773 v_call_i_3_2;
(*   store i16 %sub.3.2, i16* %arrayidx9.3.2, align 2, !tbaa !3 *)
mov mem0_36 v_sub_3_2;
(*   %add21.3.2 = add i16 %773, %call.i.3.2 *)
add v_add21_3_2 v773 v_call_i_3_2;
(*   store i16 %add21.3.2, i16* %arrayidx11.3.2, align 2, !tbaa !3 *)
mov mem0_4 v_add21_3_2;
(*   %arrayidx9.3.3 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %774 = load i16, i16* %arrayidx9.3.3, align 2, !tbaa !3 *)
mov v774 mem0_38;
(*   %conv1.i.3.3 = sext i16 %774 to i32 *)
cast v_conv1_i_3_3@sint32 v774@sint16;
(*   %mul.i.3.3 = mul nsw i32 %conv1.i.3.3, -171 *)
mul v_mul_i_3_3 v_conv1_i_3_3 (-171)@sint32;
(*   %call.i.3.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3, v_call_i_3_3);
(*   %arrayidx11.3.3 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %775 = load i16, i16* %arrayidx11.3.3, align 2, !tbaa !3 *)
mov v775 mem0_6;
(*   %sub.3.3 = sub i16 %775, %call.i.3.3 *)
sub v_sub_3_3 v775 v_call_i_3_3;
(*   store i16 %sub.3.3, i16* %arrayidx9.3.3, align 2, !tbaa !3 *)
mov mem0_38 v_sub_3_3;
(*   %add21.3.3 = add i16 %775, %call.i.3.3 *)
add v_add21_3_3 v775 v_call_i_3_3;
(*   store i16 %add21.3.3, i16* %arrayidx11.3.3, align 2, !tbaa !3 *)
mov mem0_6 v_add21_3_3;
(*   %arrayidx9.3.4 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %776 = load i16, i16* %arrayidx9.3.4, align 2, !tbaa !3 *)
mov v776 mem0_40;
(*   %conv1.i.3.4 = sext i16 %776 to i32 *)
cast v_conv1_i_3_4@sint32 v776@sint16;
(*   %mul.i.3.4 = mul nsw i32 %conv1.i.3.4, -171 *)
mul v_mul_i_3_4 v_conv1_i_3_4 (-171)@sint32;
(*   %call.i.3.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4, v_call_i_3_4);
(*   %arrayidx11.3.4 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %777 = load i16, i16* %arrayidx11.3.4, align 2, !tbaa !3 *)
mov v777 mem0_8;
(*   %sub.3.4 = sub i16 %777, %call.i.3.4 *)
sub v_sub_3_4 v777 v_call_i_3_4;
(*   store i16 %sub.3.4, i16* %arrayidx9.3.4, align 2, !tbaa !3 *)
mov mem0_40 v_sub_3_4;
(*   %add21.3.4 = add i16 %777, %call.i.3.4 *)
add v_add21_3_4 v777 v_call_i_3_4;
(*   store i16 %add21.3.4, i16* %arrayidx11.3.4, align 2, !tbaa !3 *)
mov mem0_8 v_add21_3_4;
(*   %arrayidx9.3.5 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %778 = load i16, i16* %arrayidx9.3.5, align 2, !tbaa !3 *)
mov v778 mem0_42;
(*   %conv1.i.3.5 = sext i16 %778 to i32 *)
cast v_conv1_i_3_5@sint32 v778@sint16;
(*   %mul.i.3.5 = mul nsw i32 %conv1.i.3.5, -171 *)
mul v_mul_i_3_5 v_conv1_i_3_5 (-171)@sint32;
(*   %call.i.3.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5, v_call_i_3_5);
(*   %arrayidx11.3.5 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %779 = load i16, i16* %arrayidx11.3.5, align 2, !tbaa !3 *)
mov v779 mem0_10;
(*   %sub.3.5 = sub i16 %779, %call.i.3.5 *)
sub v_sub_3_5 v779 v_call_i_3_5;
(*   store i16 %sub.3.5, i16* %arrayidx9.3.5, align 2, !tbaa !3 *)
mov mem0_42 v_sub_3_5;
(*   %add21.3.5 = add i16 %779, %call.i.3.5 *)
add v_add21_3_5 v779 v_call_i_3_5;
(*   store i16 %add21.3.5, i16* %arrayidx11.3.5, align 2, !tbaa !3 *)
mov mem0_10 v_add21_3_5;
(*   %arrayidx9.3.6 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %780 = load i16, i16* %arrayidx9.3.6, align 2, !tbaa !3 *)
mov v780 mem0_44;
(*   %conv1.i.3.6 = sext i16 %780 to i32 *)
cast v_conv1_i_3_6@sint32 v780@sint16;
(*   %mul.i.3.6 = mul nsw i32 %conv1.i.3.6, -171 *)
mul v_mul_i_3_6 v_conv1_i_3_6 (-171)@sint32;
(*   %call.i.3.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6, v_call_i_3_6);
(*   %arrayidx11.3.6 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %781 = load i16, i16* %arrayidx11.3.6, align 2, !tbaa !3 *)
mov v781 mem0_12;
(*   %sub.3.6 = sub i16 %781, %call.i.3.6 *)
sub v_sub_3_6 v781 v_call_i_3_6;
(*   store i16 %sub.3.6, i16* %arrayidx9.3.6, align 2, !tbaa !3 *)
mov mem0_44 v_sub_3_6;
(*   %add21.3.6 = add i16 %781, %call.i.3.6 *)
add v_add21_3_6 v781 v_call_i_3_6;
(*   store i16 %add21.3.6, i16* %arrayidx11.3.6, align 2, !tbaa !3 *)
mov mem0_12 v_add21_3_6;
(*   %arrayidx9.3.7 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %782 = load i16, i16* %arrayidx9.3.7, align 2, !tbaa !3 *)
mov v782 mem0_46;
(*   %conv1.i.3.7 = sext i16 %782 to i32 *)
cast v_conv1_i_3_7@sint32 v782@sint16;
(*   %mul.i.3.7 = mul nsw i32 %conv1.i.3.7, -171 *)
mul v_mul_i_3_7 v_conv1_i_3_7 (-171)@sint32;
(*   %call.i.3.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7, v_call_i_3_7);
(*   %arrayidx11.3.7 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %783 = load i16, i16* %arrayidx11.3.7, align 2, !tbaa !3 *)
mov v783 mem0_14;
(*   %sub.3.7 = sub i16 %783, %call.i.3.7 *)
sub v_sub_3_7 v783 v_call_i_3_7;
(*   store i16 %sub.3.7, i16* %arrayidx9.3.7, align 2, !tbaa !3 *)
mov mem0_46 v_sub_3_7;
(*   %add21.3.7 = add i16 %783, %call.i.3.7 *)
add v_add21_3_7 v783 v_call_i_3_7;
(*   store i16 %add21.3.7, i16* %arrayidx11.3.7, align 2, !tbaa !3 *)
mov mem0_14 v_add21_3_7;
(*   %arrayidx9.3.8 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %784 = load i16, i16* %arrayidx9.3.8, align 2, !tbaa !3 *)
mov v784 mem0_48;
(*   %conv1.i.3.8 = sext i16 %784 to i32 *)
cast v_conv1_i_3_8@sint32 v784@sint16;
(*   %mul.i.3.8 = mul nsw i32 %conv1.i.3.8, -171 *)
mul v_mul_i_3_8 v_conv1_i_3_8 (-171)@sint32;
(*   %call.i.3.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8, v_call_i_3_8);
(*   %arrayidx11.3.8 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %785 = load i16, i16* %arrayidx11.3.8, align 2, !tbaa !3 *)
mov v785 mem0_16;
(*   %sub.3.8 = sub i16 %785, %call.i.3.8 *)
sub v_sub_3_8 v785 v_call_i_3_8;
(*   store i16 %sub.3.8, i16* %arrayidx9.3.8, align 2, !tbaa !3 *)
mov mem0_48 v_sub_3_8;
(*   %add21.3.8 = add i16 %785, %call.i.3.8 *)
add v_add21_3_8 v785 v_call_i_3_8;
(*   store i16 %add21.3.8, i16* %arrayidx11.3.8, align 2, !tbaa !3 *)
mov mem0_16 v_add21_3_8;
(*   %arrayidx9.3.9 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %786 = load i16, i16* %arrayidx9.3.9, align 2, !tbaa !3 *)
mov v786 mem0_50;
(*   %conv1.i.3.9 = sext i16 %786 to i32 *)
cast v_conv1_i_3_9@sint32 v786@sint16;
(*   %mul.i.3.9 = mul nsw i32 %conv1.i.3.9, -171 *)
mul v_mul_i_3_9 v_conv1_i_3_9 (-171)@sint32;
(*   %call.i.3.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9, v_call_i_3_9);
(*   %arrayidx11.3.9 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %787 = load i16, i16* %arrayidx11.3.9, align 2, !tbaa !3 *)
mov v787 mem0_18;
(*   %sub.3.9 = sub i16 %787, %call.i.3.9 *)
sub v_sub_3_9 v787 v_call_i_3_9;
(*   store i16 %sub.3.9, i16* %arrayidx9.3.9, align 2, !tbaa !3 *)
mov mem0_50 v_sub_3_9;
(*   %add21.3.9 = add i16 %787, %call.i.3.9 *)
add v_add21_3_9 v787 v_call_i_3_9;
(*   store i16 %add21.3.9, i16* %arrayidx11.3.9, align 2, !tbaa !3 *)
mov mem0_18 v_add21_3_9;
(*   %arrayidx9.3.10 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %788 = load i16, i16* %arrayidx9.3.10, align 2, !tbaa !3 *)
mov v788 mem0_52;
(*   %conv1.i.3.10 = sext i16 %788 to i32 *)
cast v_conv1_i_3_10@sint32 v788@sint16;
(*   %mul.i.3.10 = mul nsw i32 %conv1.i.3.10, -171 *)
mul v_mul_i_3_10 v_conv1_i_3_10 (-171)@sint32;
(*   %call.i.3.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10, v_call_i_3_10);
(*   %arrayidx11.3.10 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %789 = load i16, i16* %arrayidx11.3.10, align 2, !tbaa !3 *)
mov v789 mem0_20;
(*   %sub.3.10 = sub i16 %789, %call.i.3.10 *)
sub v_sub_3_10 v789 v_call_i_3_10;
(*   store i16 %sub.3.10, i16* %arrayidx9.3.10, align 2, !tbaa !3 *)
mov mem0_52 v_sub_3_10;
(*   %add21.3.10 = add i16 %789, %call.i.3.10 *)
add v_add21_3_10 v789 v_call_i_3_10;
(*   store i16 %add21.3.10, i16* %arrayidx11.3.10, align 2, !tbaa !3 *)
mov mem0_20 v_add21_3_10;
(*   %arrayidx9.3.11 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %790 = load i16, i16* %arrayidx9.3.11, align 2, !tbaa !3 *)
mov v790 mem0_54;
(*   %conv1.i.3.11 = sext i16 %790 to i32 *)
cast v_conv1_i_3_11@sint32 v790@sint16;
(*   %mul.i.3.11 = mul nsw i32 %conv1.i.3.11, -171 *)
mul v_mul_i_3_11 v_conv1_i_3_11 (-171)@sint32;
(*   %call.i.3.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11, v_call_i_3_11);
(*   %arrayidx11.3.11 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %791 = load i16, i16* %arrayidx11.3.11, align 2, !tbaa !3 *)
mov v791 mem0_22;
(*   %sub.3.11 = sub i16 %791, %call.i.3.11 *)
sub v_sub_3_11 v791 v_call_i_3_11;
(*   store i16 %sub.3.11, i16* %arrayidx9.3.11, align 2, !tbaa !3 *)
mov mem0_54 v_sub_3_11;
(*   %add21.3.11 = add i16 %791, %call.i.3.11 *)
add v_add21_3_11 v791 v_call_i_3_11;
(*   store i16 %add21.3.11, i16* %arrayidx11.3.11, align 2, !tbaa !3 *)
mov mem0_22 v_add21_3_11;
(*   %arrayidx9.3.12 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %792 = load i16, i16* %arrayidx9.3.12, align 2, !tbaa !3 *)
mov v792 mem0_56;
(*   %conv1.i.3.12 = sext i16 %792 to i32 *)
cast v_conv1_i_3_12@sint32 v792@sint16;
(*   %mul.i.3.12 = mul nsw i32 %conv1.i.3.12, -171 *)
mul v_mul_i_3_12 v_conv1_i_3_12 (-171)@sint32;
(*   %call.i.3.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12, v_call_i_3_12);
(*   %arrayidx11.3.12 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %793 = load i16, i16* %arrayidx11.3.12, align 2, !tbaa !3 *)
mov v793 mem0_24;
(*   %sub.3.12 = sub i16 %793, %call.i.3.12 *)
sub v_sub_3_12 v793 v_call_i_3_12;
(*   store i16 %sub.3.12, i16* %arrayidx9.3.12, align 2, !tbaa !3 *)
mov mem0_56 v_sub_3_12;
(*   %add21.3.12 = add i16 %793, %call.i.3.12 *)
add v_add21_3_12 v793 v_call_i_3_12;
(*   store i16 %add21.3.12, i16* %arrayidx11.3.12, align 2, !tbaa !3 *)
mov mem0_24 v_add21_3_12;
(*   %arrayidx9.3.13 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %794 = load i16, i16* %arrayidx9.3.13, align 2, !tbaa !3 *)
mov v794 mem0_58;
(*   %conv1.i.3.13 = sext i16 %794 to i32 *)
cast v_conv1_i_3_13@sint32 v794@sint16;
(*   %mul.i.3.13 = mul nsw i32 %conv1.i.3.13, -171 *)
mul v_mul_i_3_13 v_conv1_i_3_13 (-171)@sint32;
(*   %call.i.3.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13, v_call_i_3_13);
(*   %arrayidx11.3.13 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %795 = load i16, i16* %arrayidx11.3.13, align 2, !tbaa !3 *)
mov v795 mem0_26;
(*   %sub.3.13 = sub i16 %795, %call.i.3.13 *)
sub v_sub_3_13 v795 v_call_i_3_13;
(*   store i16 %sub.3.13, i16* %arrayidx9.3.13, align 2, !tbaa !3 *)
mov mem0_58 v_sub_3_13;
(*   %add21.3.13 = add i16 %795, %call.i.3.13 *)
add v_add21_3_13 v795 v_call_i_3_13;
(*   store i16 %add21.3.13, i16* %arrayidx11.3.13, align 2, !tbaa !3 *)
mov mem0_26 v_add21_3_13;
(*   %arrayidx9.3.14 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %796 = load i16, i16* %arrayidx9.3.14, align 2, !tbaa !3 *)
mov v796 mem0_60;
(*   %conv1.i.3.14 = sext i16 %796 to i32 *)
cast v_conv1_i_3_14@sint32 v796@sint16;
(*   %mul.i.3.14 = mul nsw i32 %conv1.i.3.14, -171 *)
mul v_mul_i_3_14 v_conv1_i_3_14 (-171)@sint32;
(*   %call.i.3.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14, v_call_i_3_14);
(*   %arrayidx11.3.14 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %797 = load i16, i16* %arrayidx11.3.14, align 2, !tbaa !3 *)
mov v797 mem0_28;
(*   %sub.3.14 = sub i16 %797, %call.i.3.14 *)
sub v_sub_3_14 v797 v_call_i_3_14;
(*   store i16 %sub.3.14, i16* %arrayidx9.3.14, align 2, !tbaa !3 *)
mov mem0_60 v_sub_3_14;
(*   %add21.3.14 = add i16 %797, %call.i.3.14 *)
add v_add21_3_14 v797 v_call_i_3_14;
(*   store i16 %add21.3.14, i16* %arrayidx11.3.14, align 2, !tbaa !3 *)
mov mem0_28 v_add21_3_14;
(*   %arrayidx9.3.15 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %798 = load i16, i16* %arrayidx9.3.15, align 2, !tbaa !3 *)
mov v798 mem0_62;
(*   %conv1.i.3.15 = sext i16 %798 to i32 *)
cast v_conv1_i_3_15@sint32 v798@sint16;
(*   %mul.i.3.15 = mul nsw i32 %conv1.i.3.15, -171 *)
mul v_mul_i_3_15 v_conv1_i_3_15 (-171)@sint32;
(*   %call.i.3.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15, v_call_i_3_15);
(*   %arrayidx11.3.15 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %799 = load i16, i16* %arrayidx11.3.15, align 2, !tbaa !3 *)
mov v799 mem0_30;
(*   %sub.3.15 = sub i16 %799, %call.i.3.15 *)
sub v_sub_3_15 v799 v_call_i_3_15;
(*   store i16 %sub.3.15, i16* %arrayidx9.3.15, align 2, !tbaa !3 *)
mov mem0_62 v_sub_3_15;
(*   %add21.3.15 = add i16 %799, %call.i.3.15 *)
add v_add21_3_15 v799 v_call_i_3_15;
(*   store i16 %add21.3.15, i16* %arrayidx11.3.15, align 2, !tbaa !3 *)
mov mem0_30 v_add21_3_15;

(* NOTE: k = 9 *)

(*   %arrayidx9.3.1178 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %800 = load i16, i16* %arrayidx9.3.1178, align 2, !tbaa !3 *)
mov v800 mem0_96;
(*   %conv1.i.3.1179 = sext i16 %800 to i32 *)
cast v_conv1_i_3_1179@sint32 v800@sint16;
(*   %mul.i.3.1180 = mul nsw i32 %conv1.i.3.1179, 622 *)
mul v_mul_i_3_1180 v_conv1_i_3_1179 (622)@sint32;
(*   %call.i.3.1181 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1180) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1180, v_call_i_3_1181);
(*   %arrayidx11.3.1182 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %801 = load i16, i16* %arrayidx11.3.1182, align 2, !tbaa !3 *)
mov v801 mem0_64;
(*   %sub.3.1183 = sub i16 %801, %call.i.3.1181 *)
sub v_sub_3_1183 v801 v_call_i_3_1181;
(*   store i16 %sub.3.1183, i16* %arrayidx9.3.1178, align 2, !tbaa !3 *)
mov mem0_96 v_sub_3_1183;
(*   %add21.3.1184 = add i16 %801, %call.i.3.1181 *)
add v_add21_3_1184 v801 v_call_i_3_1181;
(*   store i16 %add21.3.1184, i16* %arrayidx11.3.1182, align 2, !tbaa !3 *)
mov mem0_64 v_add21_3_1184;
(*   %arrayidx9.3.1.1 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %802 = load i16, i16* %arrayidx9.3.1.1, align 2, !tbaa !3 *)
mov v802 mem0_98;
(*   %conv1.i.3.1.1 = sext i16 %802 to i32 *)
cast v_conv1_i_3_1_1@sint32 v802@sint16;
(*   %mul.i.3.1.1 = mul nsw i32 %conv1.i.3.1.1, 622 *)
mul v_mul_i_3_1_1 v_conv1_i_3_1_1 (622)@sint32;
(*   %call.i.3.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_1, v_call_i_3_1_1);
(*   %arrayidx11.3.1.1 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %803 = load i16, i16* %arrayidx11.3.1.1, align 2, !tbaa !3 *)
mov v803 mem0_66;
(*   %sub.3.1.1 = sub i16 %803, %call.i.3.1.1 *)
sub v_sub_3_1_1 v803 v_call_i_3_1_1;
(*   store i16 %sub.3.1.1, i16* %arrayidx9.3.1.1, align 2, !tbaa !3 *)
mov mem0_98 v_sub_3_1_1;
(*   %add21.3.1.1 = add i16 %803, %call.i.3.1.1 *)
add v_add21_3_1_1 v803 v_call_i_3_1_1;
(*   store i16 %add21.3.1.1, i16* %arrayidx11.3.1.1, align 2, !tbaa !3 *)
mov mem0_66 v_add21_3_1_1;
(*   %arrayidx9.3.2.1 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %804 = load i16, i16* %arrayidx9.3.2.1, align 2, !tbaa !3 *)
mov v804 mem0_100;
(*   %conv1.i.3.2.1 = sext i16 %804 to i32 *)
cast v_conv1_i_3_2_1@sint32 v804@sint16;
(*   %mul.i.3.2.1 = mul nsw i32 %conv1.i.3.2.1, 622 *)
mul v_mul_i_3_2_1 v_conv1_i_3_2_1 (622)@sint32;
(*   %call.i.3.2.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_1, v_call_i_3_2_1);
(*   %arrayidx11.3.2.1 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %805 = load i16, i16* %arrayidx11.3.2.1, align 2, !tbaa !3 *)
mov v805 mem0_68;
(*   %sub.3.2.1 = sub i16 %805, %call.i.3.2.1 *)
sub v_sub_3_2_1 v805 v_call_i_3_2_1;
(*   store i16 %sub.3.2.1, i16* %arrayidx9.3.2.1, align 2, !tbaa !3 *)
mov mem0_100 v_sub_3_2_1;
(*   %add21.3.2.1 = add i16 %805, %call.i.3.2.1 *)
add v_add21_3_2_1 v805 v_call_i_3_2_1;
(*   store i16 %add21.3.2.1, i16* %arrayidx11.3.2.1, align 2, !tbaa !3 *)
mov mem0_68 v_add21_3_2_1;
(*   %arrayidx9.3.3.1 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %806 = load i16, i16* %arrayidx9.3.3.1, align 2, !tbaa !3 *)
mov v806 mem0_102;
(*   %conv1.i.3.3.1 = sext i16 %806 to i32 *)
cast v_conv1_i_3_3_1@sint32 v806@sint16;
(*   %mul.i.3.3.1 = mul nsw i32 %conv1.i.3.3.1, 622 *)
mul v_mul_i_3_3_1 v_conv1_i_3_3_1 (622)@sint32;
(*   %call.i.3.3.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_1, v_call_i_3_3_1);
(*   %arrayidx11.3.3.1 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %807 = load i16, i16* %arrayidx11.3.3.1, align 2, !tbaa !3 *)
mov v807 mem0_70;
(*   %sub.3.3.1 = sub i16 %807, %call.i.3.3.1 *)
sub v_sub_3_3_1 v807 v_call_i_3_3_1;
(*   store i16 %sub.3.3.1, i16* %arrayidx9.3.3.1, align 2, !tbaa !3 *)
mov mem0_102 v_sub_3_3_1;
(*   %add21.3.3.1 = add i16 %807, %call.i.3.3.1 *)
add v_add21_3_3_1 v807 v_call_i_3_3_1;
(*   store i16 %add21.3.3.1, i16* %arrayidx11.3.3.1, align 2, !tbaa !3 *)
mov mem0_70 v_add21_3_3_1;
(*   %arrayidx9.3.4.1 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %808 = load i16, i16* %arrayidx9.3.4.1, align 2, !tbaa !3 *)
mov v808 mem0_104;
(*   %conv1.i.3.4.1 = sext i16 %808 to i32 *)
cast v_conv1_i_3_4_1@sint32 v808@sint16;
(*   %mul.i.3.4.1 = mul nsw i32 %conv1.i.3.4.1, 622 *)
mul v_mul_i_3_4_1 v_conv1_i_3_4_1 (622)@sint32;
(*   %call.i.3.4.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_1, v_call_i_3_4_1);
(*   %arrayidx11.3.4.1 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %809 = load i16, i16* %arrayidx11.3.4.1, align 2, !tbaa !3 *)
mov v809 mem0_72;
(*   %sub.3.4.1 = sub i16 %809, %call.i.3.4.1 *)
sub v_sub_3_4_1 v809 v_call_i_3_4_1;
(*   store i16 %sub.3.4.1, i16* %arrayidx9.3.4.1, align 2, !tbaa !3 *)
mov mem0_104 v_sub_3_4_1;
(*   %add21.3.4.1 = add i16 %809, %call.i.3.4.1 *)
add v_add21_3_4_1 v809 v_call_i_3_4_1;
(*   store i16 %add21.3.4.1, i16* %arrayidx11.3.4.1, align 2, !tbaa !3 *)
mov mem0_72 v_add21_3_4_1;
(*   %arrayidx9.3.5.1 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %810 = load i16, i16* %arrayidx9.3.5.1, align 2, !tbaa !3 *)
mov v810 mem0_106;
(*   %conv1.i.3.5.1 = sext i16 %810 to i32 *)
cast v_conv1_i_3_5_1@sint32 v810@sint16;
(*   %mul.i.3.5.1 = mul nsw i32 %conv1.i.3.5.1, 622 *)
mul v_mul_i_3_5_1 v_conv1_i_3_5_1 (622)@sint32;
(*   %call.i.3.5.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_1, v_call_i_3_5_1);
(*   %arrayidx11.3.5.1 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %811 = load i16, i16* %arrayidx11.3.5.1, align 2, !tbaa !3 *)
mov v811 mem0_74;
(*   %sub.3.5.1 = sub i16 %811, %call.i.3.5.1 *)
sub v_sub_3_5_1 v811 v_call_i_3_5_1;
(*   store i16 %sub.3.5.1, i16* %arrayidx9.3.5.1, align 2, !tbaa !3 *)
mov mem0_106 v_sub_3_5_1;
(*   %add21.3.5.1 = add i16 %811, %call.i.3.5.1 *)
add v_add21_3_5_1 v811 v_call_i_3_5_1;
(*   store i16 %add21.3.5.1, i16* %arrayidx11.3.5.1, align 2, !tbaa !3 *)
mov mem0_74 v_add21_3_5_1;
(*   %arrayidx9.3.6.1 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %812 = load i16, i16* %arrayidx9.3.6.1, align 2, !tbaa !3 *)
mov v812 mem0_108;
(*   %conv1.i.3.6.1 = sext i16 %812 to i32 *)
cast v_conv1_i_3_6_1@sint32 v812@sint16;
(*   %mul.i.3.6.1 = mul nsw i32 %conv1.i.3.6.1, 622 *)
mul v_mul_i_3_6_1 v_conv1_i_3_6_1 (622)@sint32;
(*   %call.i.3.6.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_1, v_call_i_3_6_1);
(*   %arrayidx11.3.6.1 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %813 = load i16, i16* %arrayidx11.3.6.1, align 2, !tbaa !3 *)
mov v813 mem0_76;
(*   %sub.3.6.1 = sub i16 %813, %call.i.3.6.1 *)
sub v_sub_3_6_1 v813 v_call_i_3_6_1;
(*   store i16 %sub.3.6.1, i16* %arrayidx9.3.6.1, align 2, !tbaa !3 *)
mov mem0_108 v_sub_3_6_1;
(*   %add21.3.6.1 = add i16 %813, %call.i.3.6.1 *)
add v_add21_3_6_1 v813 v_call_i_3_6_1;
(*   store i16 %add21.3.6.1, i16* %arrayidx11.3.6.1, align 2, !tbaa !3 *)
mov mem0_76 v_add21_3_6_1;
(*   %arrayidx9.3.7.1 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %814 = load i16, i16* %arrayidx9.3.7.1, align 2, !tbaa !3 *)
mov v814 mem0_110;
(*   %conv1.i.3.7.1 = sext i16 %814 to i32 *)
cast v_conv1_i_3_7_1@sint32 v814@sint16;
(*   %mul.i.3.7.1 = mul nsw i32 %conv1.i.3.7.1, 622 *)
mul v_mul_i_3_7_1 v_conv1_i_3_7_1 (622)@sint32;
(*   %call.i.3.7.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_1, v_call_i_3_7_1);
(*   %arrayidx11.3.7.1 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %815 = load i16, i16* %arrayidx11.3.7.1, align 2, !tbaa !3 *)
mov v815 mem0_78;
(*   %sub.3.7.1 = sub i16 %815, %call.i.3.7.1 *)
sub v_sub_3_7_1 v815 v_call_i_3_7_1;
(*   store i16 %sub.3.7.1, i16* %arrayidx9.3.7.1, align 2, !tbaa !3 *)
mov mem0_110 v_sub_3_7_1;
(*   %add21.3.7.1 = add i16 %815, %call.i.3.7.1 *)
add v_add21_3_7_1 v815 v_call_i_3_7_1;
(*   store i16 %add21.3.7.1, i16* %arrayidx11.3.7.1, align 2, !tbaa !3 *)
mov mem0_78 v_add21_3_7_1;
(*   %arrayidx9.3.8.1 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %816 = load i16, i16* %arrayidx9.3.8.1, align 2, !tbaa !3 *)
mov v816 mem0_112;
(*   %conv1.i.3.8.1 = sext i16 %816 to i32 *)
cast v_conv1_i_3_8_1@sint32 v816@sint16;
(*   %mul.i.3.8.1 = mul nsw i32 %conv1.i.3.8.1, 622 *)
mul v_mul_i_3_8_1 v_conv1_i_3_8_1 (622)@sint32;
(*   %call.i.3.8.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_1, v_call_i_3_8_1);
(*   %arrayidx11.3.8.1 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %817 = load i16, i16* %arrayidx11.3.8.1, align 2, !tbaa !3 *)
mov v817 mem0_80;
(*   %sub.3.8.1 = sub i16 %817, %call.i.3.8.1 *)
sub v_sub_3_8_1 v817 v_call_i_3_8_1;
(*   store i16 %sub.3.8.1, i16* %arrayidx9.3.8.1, align 2, !tbaa !3 *)
mov mem0_112 v_sub_3_8_1;
(*   %add21.3.8.1 = add i16 %817, %call.i.3.8.1 *)
add v_add21_3_8_1 v817 v_call_i_3_8_1;
(*   store i16 %add21.3.8.1, i16* %arrayidx11.3.8.1, align 2, !tbaa !3 *)
mov mem0_80 v_add21_3_8_1;
(*   %arrayidx9.3.9.1 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %818 = load i16, i16* %arrayidx9.3.9.1, align 2, !tbaa !3 *)
mov v818 mem0_114;
(*   %conv1.i.3.9.1 = sext i16 %818 to i32 *)
cast v_conv1_i_3_9_1@sint32 v818@sint16;
(*   %mul.i.3.9.1 = mul nsw i32 %conv1.i.3.9.1, 622 *)
mul v_mul_i_3_9_1 v_conv1_i_3_9_1 (622)@sint32;
(*   %call.i.3.9.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_1, v_call_i_3_9_1);
(*   %arrayidx11.3.9.1 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %819 = load i16, i16* %arrayidx11.3.9.1, align 2, !tbaa !3 *)
mov v819 mem0_82;
(*   %sub.3.9.1 = sub i16 %819, %call.i.3.9.1 *)
sub v_sub_3_9_1 v819 v_call_i_3_9_1;
(*   store i16 %sub.3.9.1, i16* %arrayidx9.3.9.1, align 2, !tbaa !3 *)
mov mem0_114 v_sub_3_9_1;
(*   %add21.3.9.1 = add i16 %819, %call.i.3.9.1 *)
add v_add21_3_9_1 v819 v_call_i_3_9_1;
(*   store i16 %add21.3.9.1, i16* %arrayidx11.3.9.1, align 2, !tbaa !3 *)
mov mem0_82 v_add21_3_9_1;
(*   %arrayidx9.3.10.1 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %820 = load i16, i16* %arrayidx9.3.10.1, align 2, !tbaa !3 *)
mov v820 mem0_116;
(*   %conv1.i.3.10.1 = sext i16 %820 to i32 *)
cast v_conv1_i_3_10_1@sint32 v820@sint16;
(*   %mul.i.3.10.1 = mul nsw i32 %conv1.i.3.10.1, 622 *)
mul v_mul_i_3_10_1 v_conv1_i_3_10_1 (622)@sint32;
(*   %call.i.3.10.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_1, v_call_i_3_10_1);
(*   %arrayidx11.3.10.1 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %821 = load i16, i16* %arrayidx11.3.10.1, align 2, !tbaa !3 *)
mov v821 mem0_84;
(*   %sub.3.10.1 = sub i16 %821, %call.i.3.10.1 *)
sub v_sub_3_10_1 v821 v_call_i_3_10_1;
(*   store i16 %sub.3.10.1, i16* %arrayidx9.3.10.1, align 2, !tbaa !3 *)
mov mem0_116 v_sub_3_10_1;
(*   %add21.3.10.1 = add i16 %821, %call.i.3.10.1 *)
add v_add21_3_10_1 v821 v_call_i_3_10_1;
(*   store i16 %add21.3.10.1, i16* %arrayidx11.3.10.1, align 2, !tbaa !3 *)
mov mem0_84 v_add21_3_10_1;
(*   %arrayidx9.3.11.1 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %822 = load i16, i16* %arrayidx9.3.11.1, align 2, !tbaa !3 *)
mov v822 mem0_118;
(*   %conv1.i.3.11.1 = sext i16 %822 to i32 *)
cast v_conv1_i_3_11_1@sint32 v822@sint16;
(*   %mul.i.3.11.1 = mul nsw i32 %conv1.i.3.11.1, 622 *)
mul v_mul_i_3_11_1 v_conv1_i_3_11_1 (622)@sint32;
(*   %call.i.3.11.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_1, v_call_i_3_11_1);
(*   %arrayidx11.3.11.1 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %823 = load i16, i16* %arrayidx11.3.11.1, align 2, !tbaa !3 *)
mov v823 mem0_86;
(*   %sub.3.11.1 = sub i16 %823, %call.i.3.11.1 *)
sub v_sub_3_11_1 v823 v_call_i_3_11_1;
(*   store i16 %sub.3.11.1, i16* %arrayidx9.3.11.1, align 2, !tbaa !3 *)
mov mem0_118 v_sub_3_11_1;
(*   %add21.3.11.1 = add i16 %823, %call.i.3.11.1 *)
add v_add21_3_11_1 v823 v_call_i_3_11_1;
(*   store i16 %add21.3.11.1, i16* %arrayidx11.3.11.1, align 2, !tbaa !3 *)
mov mem0_86 v_add21_3_11_1;
(*   %arrayidx9.3.12.1 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %824 = load i16, i16* %arrayidx9.3.12.1, align 2, !tbaa !3 *)
mov v824 mem0_120;
(*   %conv1.i.3.12.1 = sext i16 %824 to i32 *)
cast v_conv1_i_3_12_1@sint32 v824@sint16;
(*   %mul.i.3.12.1 = mul nsw i32 %conv1.i.3.12.1, 622 *)
mul v_mul_i_3_12_1 v_conv1_i_3_12_1 (622)@sint32;
(*   %call.i.3.12.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_1, v_call_i_3_12_1);
(*   %arrayidx11.3.12.1 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %825 = load i16, i16* %arrayidx11.3.12.1, align 2, !tbaa !3 *)
mov v825 mem0_88;
(*   %sub.3.12.1 = sub i16 %825, %call.i.3.12.1 *)
sub v_sub_3_12_1 v825 v_call_i_3_12_1;
(*   store i16 %sub.3.12.1, i16* %arrayidx9.3.12.1, align 2, !tbaa !3 *)
mov mem0_120 v_sub_3_12_1;
(*   %add21.3.12.1 = add i16 %825, %call.i.3.12.1 *)
add v_add21_3_12_1 v825 v_call_i_3_12_1;
(*   store i16 %add21.3.12.1, i16* %arrayidx11.3.12.1, align 2, !tbaa !3 *)
mov mem0_88 v_add21_3_12_1;
(*   %arrayidx9.3.13.1 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %826 = load i16, i16* %arrayidx9.3.13.1, align 2, !tbaa !3 *)
mov v826 mem0_122;
(*   %conv1.i.3.13.1 = sext i16 %826 to i32 *)
cast v_conv1_i_3_13_1@sint32 v826@sint16;
(*   %mul.i.3.13.1 = mul nsw i32 %conv1.i.3.13.1, 622 *)
mul v_mul_i_3_13_1 v_conv1_i_3_13_1 (622)@sint32;
(*   %call.i.3.13.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_1, v_call_i_3_13_1);
(*   %arrayidx11.3.13.1 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %827 = load i16, i16* %arrayidx11.3.13.1, align 2, !tbaa !3 *)
mov v827 mem0_90;
(*   %sub.3.13.1 = sub i16 %827, %call.i.3.13.1 *)
sub v_sub_3_13_1 v827 v_call_i_3_13_1;
(*   store i16 %sub.3.13.1, i16* %arrayidx9.3.13.1, align 2, !tbaa !3 *)
mov mem0_122 v_sub_3_13_1;
(*   %add21.3.13.1 = add i16 %827, %call.i.3.13.1 *)
add v_add21_3_13_1 v827 v_call_i_3_13_1;
(*   store i16 %add21.3.13.1, i16* %arrayidx11.3.13.1, align 2, !tbaa !3 *)
mov mem0_90 v_add21_3_13_1;
(*   %arrayidx9.3.14.1 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %828 = load i16, i16* %arrayidx9.3.14.1, align 2, !tbaa !3 *)
mov v828 mem0_124;
(*   %conv1.i.3.14.1 = sext i16 %828 to i32 *)
cast v_conv1_i_3_14_1@sint32 v828@sint16;
(*   %mul.i.3.14.1 = mul nsw i32 %conv1.i.3.14.1, 622 *)
mul v_mul_i_3_14_1 v_conv1_i_3_14_1 (622)@sint32;
(*   %call.i.3.14.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_1, v_call_i_3_14_1);
(*   %arrayidx11.3.14.1 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %829 = load i16, i16* %arrayidx11.3.14.1, align 2, !tbaa !3 *)
mov v829 mem0_92;
(*   %sub.3.14.1 = sub i16 %829, %call.i.3.14.1 *)
sub v_sub_3_14_1 v829 v_call_i_3_14_1;
(*   store i16 %sub.3.14.1, i16* %arrayidx9.3.14.1, align 2, !tbaa !3 *)
mov mem0_124 v_sub_3_14_1;
(*   %add21.3.14.1 = add i16 %829, %call.i.3.14.1 *)
add v_add21_3_14_1 v829 v_call_i_3_14_1;
(*   store i16 %add21.3.14.1, i16* %arrayidx11.3.14.1, align 2, !tbaa !3 *)
mov mem0_92 v_add21_3_14_1;
(*   %arrayidx9.3.15.1 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %830 = load i16, i16* %arrayidx9.3.15.1, align 2, !tbaa !3 *)
mov v830 mem0_126;
(*   %conv1.i.3.15.1 = sext i16 %830 to i32 *)
cast v_conv1_i_3_15_1@sint32 v830@sint16;
(*   %mul.i.3.15.1 = mul nsw i32 %conv1.i.3.15.1, 622 *)
mul v_mul_i_3_15_1 v_conv1_i_3_15_1 (622)@sint32;
(*   %call.i.3.15.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_1, v_call_i_3_15_1);
(*   %arrayidx11.3.15.1 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %831 = load i16, i16* %arrayidx11.3.15.1, align 2, !tbaa !3 *)
mov v831 mem0_94;
(*   %sub.3.15.1 = sub i16 %831, %call.i.3.15.1 *)
sub v_sub_3_15_1 v831 v_call_i_3_15_1;
(*   store i16 %sub.3.15.1, i16* %arrayidx9.3.15.1, align 2, !tbaa !3 *)
mov mem0_126 v_sub_3_15_1;
(*   %add21.3.15.1 = add i16 %831, %call.i.3.15.1 *)
add v_add21_3_15_1 v831 v_call_i_3_15_1;
(*   store i16 %add21.3.15.1, i16* %arrayidx11.3.15.1, align 2, !tbaa !3 *)
mov mem0_94 v_add21_3_15_1;

(* NOTE: k = 10 *)

(*   %arrayidx9.3.2188 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %832 = load i16, i16* %arrayidx9.3.2188, align 2, !tbaa !3 *)
mov v832 mem0_160;
(*   %conv1.i.3.2189 = sext i16 %832 to i32 *)
cast v_conv1_i_3_2189@sint32 v832@sint16;
(*   %mul.i.3.2190 = mul nsw i32 %conv1.i.3.2189, 1577 *)
mul v_mul_i_3_2190 v_conv1_i_3_2189 (1577)@sint32;
(*   %call.i.3.2191 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2190) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2190, v_call_i_3_2191);
(*   %arrayidx11.3.2192 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %833 = load i16, i16* %arrayidx11.3.2192, align 2, !tbaa !3 *)
mov v833 mem0_128;
(*   %sub.3.2193 = sub i16 %833, %call.i.3.2191 *)
sub v_sub_3_2193 v833 v_call_i_3_2191;
(*   store i16 %sub.3.2193, i16* %arrayidx9.3.2188, align 2, !tbaa !3 *)
mov mem0_160 v_sub_3_2193;
(*   %add21.3.2194 = add i16 %833, %call.i.3.2191 *)
add v_add21_3_2194 v833 v_call_i_3_2191;
(*   store i16 %add21.3.2194, i16* %arrayidx11.3.2192, align 2, !tbaa !3 *)
mov mem0_128 v_add21_3_2194;
(*   %arrayidx9.3.1.2 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %834 = load i16, i16* %arrayidx9.3.1.2, align 2, !tbaa !3 *)
mov v834 mem0_162;
(*   %conv1.i.3.1.2 = sext i16 %834 to i32 *)
cast v_conv1_i_3_1_2@sint32 v834@sint16;
(*   %mul.i.3.1.2 = mul nsw i32 %conv1.i.3.1.2, 1577 *)
mul v_mul_i_3_1_2 v_conv1_i_3_1_2 (1577)@sint32;
(*   %call.i.3.1.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_2, v_call_i_3_1_2);
(*   %arrayidx11.3.1.2 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %835 = load i16, i16* %arrayidx11.3.1.2, align 2, !tbaa !3 *)
mov v835 mem0_130;
(*   %sub.3.1.2 = sub i16 %835, %call.i.3.1.2 *)
sub v_sub_3_1_2 v835 v_call_i_3_1_2;
(*   store i16 %sub.3.1.2, i16* %arrayidx9.3.1.2, align 2, !tbaa !3 *)
mov mem0_162 v_sub_3_1_2;
(*   %add21.3.1.2 = add i16 %835, %call.i.3.1.2 *)
add v_add21_3_1_2 v835 v_call_i_3_1_2;
(*   store i16 %add21.3.1.2, i16* %arrayidx11.3.1.2, align 2, !tbaa !3 *)
mov mem0_130 v_add21_3_1_2;
(*   %arrayidx9.3.2.2 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %836 = load i16, i16* %arrayidx9.3.2.2, align 2, !tbaa !3 *)
mov v836 mem0_164;
(*   %conv1.i.3.2.2 = sext i16 %836 to i32 *)
cast v_conv1_i_3_2_2@sint32 v836@sint16;
(*   %mul.i.3.2.2 = mul nsw i32 %conv1.i.3.2.2, 1577 *)
mul v_mul_i_3_2_2 v_conv1_i_3_2_2 (1577)@sint32;
(*   %call.i.3.2.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_2, v_call_i_3_2_2);
(*   %arrayidx11.3.2.2 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %837 = load i16, i16* %arrayidx11.3.2.2, align 2, !tbaa !3 *)
mov v837 mem0_132;
(*   %sub.3.2.2 = sub i16 %837, %call.i.3.2.2 *)
sub v_sub_3_2_2 v837 v_call_i_3_2_2;
(*   store i16 %sub.3.2.2, i16* %arrayidx9.3.2.2, align 2, !tbaa !3 *)
mov mem0_164 v_sub_3_2_2;
(*   %add21.3.2.2 = add i16 %837, %call.i.3.2.2 *)
add v_add21_3_2_2 v837 v_call_i_3_2_2;
(*   store i16 %add21.3.2.2, i16* %arrayidx11.3.2.2, align 2, !tbaa !3 *)
mov mem0_132 v_add21_3_2_2;
(*   %arrayidx9.3.3.2 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %838 = load i16, i16* %arrayidx9.3.3.2, align 2, !tbaa !3 *)
mov v838 mem0_166;
(*   %conv1.i.3.3.2 = sext i16 %838 to i32 *)
cast v_conv1_i_3_3_2@sint32 v838@sint16;
(*   %mul.i.3.3.2 = mul nsw i32 %conv1.i.3.3.2, 1577 *)
mul v_mul_i_3_3_2 v_conv1_i_3_3_2 (1577)@sint32;
(*   %call.i.3.3.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_2, v_call_i_3_3_2);
(*   %arrayidx11.3.3.2 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %839 = load i16, i16* %arrayidx11.3.3.2, align 2, !tbaa !3 *)
mov v839 mem0_134;
(*   %sub.3.3.2 = sub i16 %839, %call.i.3.3.2 *)
sub v_sub_3_3_2 v839 v_call_i_3_3_2;
(*   store i16 %sub.3.3.2, i16* %arrayidx9.3.3.2, align 2, !tbaa !3 *)
mov mem0_166 v_sub_3_3_2;
(*   %add21.3.3.2 = add i16 %839, %call.i.3.3.2 *)
add v_add21_3_3_2 v839 v_call_i_3_3_2;
(*   store i16 %add21.3.3.2, i16* %arrayidx11.3.3.2, align 2, !tbaa !3 *)
mov mem0_134 v_add21_3_3_2;
(*   %arrayidx9.3.4.2 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %840 = load i16, i16* %arrayidx9.3.4.2, align 2, !tbaa !3 *)
mov v840 mem0_168;
(*   %conv1.i.3.4.2 = sext i16 %840 to i32 *)
cast v_conv1_i_3_4_2@sint32 v840@sint16;
(*   %mul.i.3.4.2 = mul nsw i32 %conv1.i.3.4.2, 1577 *)
mul v_mul_i_3_4_2 v_conv1_i_3_4_2 (1577)@sint32;
(*   %call.i.3.4.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_2, v_call_i_3_4_2);
(*   %arrayidx11.3.4.2 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %841 = load i16, i16* %arrayidx11.3.4.2, align 2, !tbaa !3 *)
mov v841 mem0_136;
(*   %sub.3.4.2 = sub i16 %841, %call.i.3.4.2 *)
sub v_sub_3_4_2 v841 v_call_i_3_4_2;
(*   store i16 %sub.3.4.2, i16* %arrayidx9.3.4.2, align 2, !tbaa !3 *)
mov mem0_168 v_sub_3_4_2;
(*   %add21.3.4.2 = add i16 %841, %call.i.3.4.2 *)
add v_add21_3_4_2 v841 v_call_i_3_4_2;
(*   store i16 %add21.3.4.2, i16* %arrayidx11.3.4.2, align 2, !tbaa !3 *)
mov mem0_136 v_add21_3_4_2;
(*   %arrayidx9.3.5.2 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %842 = load i16, i16* %arrayidx9.3.5.2, align 2, !tbaa !3 *)
mov v842 mem0_170;
(*   %conv1.i.3.5.2 = sext i16 %842 to i32 *)
cast v_conv1_i_3_5_2@sint32 v842@sint16;
(*   %mul.i.3.5.2 = mul nsw i32 %conv1.i.3.5.2, 1577 *)
mul v_mul_i_3_5_2 v_conv1_i_3_5_2 (1577)@sint32;
(*   %call.i.3.5.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_2, v_call_i_3_5_2);
(*   %arrayidx11.3.5.2 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %843 = load i16, i16* %arrayidx11.3.5.2, align 2, !tbaa !3 *)
mov v843 mem0_138;
(*   %sub.3.5.2 = sub i16 %843, %call.i.3.5.2 *)
sub v_sub_3_5_2 v843 v_call_i_3_5_2;
(*   store i16 %sub.3.5.2, i16* %arrayidx9.3.5.2, align 2, !tbaa !3 *)
mov mem0_170 v_sub_3_5_2;
(*   %add21.3.5.2 = add i16 %843, %call.i.3.5.2 *)
add v_add21_3_5_2 v843 v_call_i_3_5_2;
(*   store i16 %add21.3.5.2, i16* %arrayidx11.3.5.2, align 2, !tbaa !3 *)
mov mem0_138 v_add21_3_5_2;
(*   %arrayidx9.3.6.2 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %844 = load i16, i16* %arrayidx9.3.6.2, align 2, !tbaa !3 *)
mov v844 mem0_172;
(*   %conv1.i.3.6.2 = sext i16 %844 to i32 *)
cast v_conv1_i_3_6_2@sint32 v844@sint16;
(*   %mul.i.3.6.2 = mul nsw i32 %conv1.i.3.6.2, 1577 *)
mul v_mul_i_3_6_2 v_conv1_i_3_6_2 (1577)@sint32;
(*   %call.i.3.6.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_2, v_call_i_3_6_2);
(*   %arrayidx11.3.6.2 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %845 = load i16, i16* %arrayidx11.3.6.2, align 2, !tbaa !3 *)
mov v845 mem0_140;
(*   %sub.3.6.2 = sub i16 %845, %call.i.3.6.2 *)
sub v_sub_3_6_2 v845 v_call_i_3_6_2;
(*   store i16 %sub.3.6.2, i16* %arrayidx9.3.6.2, align 2, !tbaa !3 *)
mov mem0_172 v_sub_3_6_2;
(*   %add21.3.6.2 = add i16 %845, %call.i.3.6.2 *)
add v_add21_3_6_2 v845 v_call_i_3_6_2;
(*   store i16 %add21.3.6.2, i16* %arrayidx11.3.6.2, align 2, !tbaa !3 *)
mov mem0_140 v_add21_3_6_2;
(*   %arrayidx9.3.7.2 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %846 = load i16, i16* %arrayidx9.3.7.2, align 2, !tbaa !3 *)
mov v846 mem0_174;
(*   %conv1.i.3.7.2 = sext i16 %846 to i32 *)
cast v_conv1_i_3_7_2@sint32 v846@sint16;
(*   %mul.i.3.7.2 = mul nsw i32 %conv1.i.3.7.2, 1577 *)
mul v_mul_i_3_7_2 v_conv1_i_3_7_2 (1577)@sint32;
(*   %call.i.3.7.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_2, v_call_i_3_7_2);
(*   %arrayidx11.3.7.2 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %847 = load i16, i16* %arrayidx11.3.7.2, align 2, !tbaa !3 *)
mov v847 mem0_142;
(*   %sub.3.7.2 = sub i16 %847, %call.i.3.7.2 *)
sub v_sub_3_7_2 v847 v_call_i_3_7_2;
(*   store i16 %sub.3.7.2, i16* %arrayidx9.3.7.2, align 2, !tbaa !3 *)
mov mem0_174 v_sub_3_7_2;
(*   %add21.3.7.2 = add i16 %847, %call.i.3.7.2 *)
add v_add21_3_7_2 v847 v_call_i_3_7_2;
(*   store i16 %add21.3.7.2, i16* %arrayidx11.3.7.2, align 2, !tbaa !3 *)
mov mem0_142 v_add21_3_7_2;
(*   %arrayidx9.3.8.2 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %848 = load i16, i16* %arrayidx9.3.8.2, align 2, !tbaa !3 *)
mov v848 mem0_176;
(*   %conv1.i.3.8.2 = sext i16 %848 to i32 *)
cast v_conv1_i_3_8_2@sint32 v848@sint16;
(*   %mul.i.3.8.2 = mul nsw i32 %conv1.i.3.8.2, 1577 *)
mul v_mul_i_3_8_2 v_conv1_i_3_8_2 (1577)@sint32;
(*   %call.i.3.8.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_2, v_call_i_3_8_2);
(*   %arrayidx11.3.8.2 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %849 = load i16, i16* %arrayidx11.3.8.2, align 2, !tbaa !3 *)
mov v849 mem0_144;
(*   %sub.3.8.2 = sub i16 %849, %call.i.3.8.2 *)
sub v_sub_3_8_2 v849 v_call_i_3_8_2;
(*   store i16 %sub.3.8.2, i16* %arrayidx9.3.8.2, align 2, !tbaa !3 *)
mov mem0_176 v_sub_3_8_2;
(*   %add21.3.8.2 = add i16 %849, %call.i.3.8.2 *)
add v_add21_3_8_2 v849 v_call_i_3_8_2;
(*   store i16 %add21.3.8.2, i16* %arrayidx11.3.8.2, align 2, !tbaa !3 *)
mov mem0_144 v_add21_3_8_2;
(*   %arrayidx9.3.9.2 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %850 = load i16, i16* %arrayidx9.3.9.2, align 2, !tbaa !3 *)
mov v850 mem0_178;
(*   %conv1.i.3.9.2 = sext i16 %850 to i32 *)
cast v_conv1_i_3_9_2@sint32 v850@sint16;
(*   %mul.i.3.9.2 = mul nsw i32 %conv1.i.3.9.2, 1577 *)
mul v_mul_i_3_9_2 v_conv1_i_3_9_2 (1577)@sint32;
(*   %call.i.3.9.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_2, v_call_i_3_9_2);
(*   %arrayidx11.3.9.2 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %851 = load i16, i16* %arrayidx11.3.9.2, align 2, !tbaa !3 *)
mov v851 mem0_146;
(*   %sub.3.9.2 = sub i16 %851, %call.i.3.9.2 *)
sub v_sub_3_9_2 v851 v_call_i_3_9_2;
(*   store i16 %sub.3.9.2, i16* %arrayidx9.3.9.2, align 2, !tbaa !3 *)
mov mem0_178 v_sub_3_9_2;
(*   %add21.3.9.2 = add i16 %851, %call.i.3.9.2 *)
add v_add21_3_9_2 v851 v_call_i_3_9_2;
(*   store i16 %add21.3.9.2, i16* %arrayidx11.3.9.2, align 2, !tbaa !3 *)
mov mem0_146 v_add21_3_9_2;
(*   %arrayidx9.3.10.2 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %852 = load i16, i16* %arrayidx9.3.10.2, align 2, !tbaa !3 *)
mov v852 mem0_180;
(*   %conv1.i.3.10.2 = sext i16 %852 to i32 *)
cast v_conv1_i_3_10_2@sint32 v852@sint16;
(*   %mul.i.3.10.2 = mul nsw i32 %conv1.i.3.10.2, 1577 *)
mul v_mul_i_3_10_2 v_conv1_i_3_10_2 (1577)@sint32;
(*   %call.i.3.10.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_2, v_call_i_3_10_2);
(*   %arrayidx11.3.10.2 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %853 = load i16, i16* %arrayidx11.3.10.2, align 2, !tbaa !3 *)
mov v853 mem0_148;
(*   %sub.3.10.2 = sub i16 %853, %call.i.3.10.2 *)
sub v_sub_3_10_2 v853 v_call_i_3_10_2;
(*   store i16 %sub.3.10.2, i16* %arrayidx9.3.10.2, align 2, !tbaa !3 *)
mov mem0_180 v_sub_3_10_2;
(*   %add21.3.10.2 = add i16 %853, %call.i.3.10.2 *)
add v_add21_3_10_2 v853 v_call_i_3_10_2;
(*   store i16 %add21.3.10.2, i16* %arrayidx11.3.10.2, align 2, !tbaa !3 *)
mov mem0_148 v_add21_3_10_2;
(*   %arrayidx9.3.11.2 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %854 = load i16, i16* %arrayidx9.3.11.2, align 2, !tbaa !3 *)
mov v854 mem0_182;
(*   %conv1.i.3.11.2 = sext i16 %854 to i32 *)
cast v_conv1_i_3_11_2@sint32 v854@sint16;
(*   %mul.i.3.11.2 = mul nsw i32 %conv1.i.3.11.2, 1577 *)
mul v_mul_i_3_11_2 v_conv1_i_3_11_2 (1577)@sint32;
(*   %call.i.3.11.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_2, v_call_i_3_11_2);
(*   %arrayidx11.3.11.2 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %855 = load i16, i16* %arrayidx11.3.11.2, align 2, !tbaa !3 *)
mov v855 mem0_150;
(*   %sub.3.11.2 = sub i16 %855, %call.i.3.11.2 *)
sub v_sub_3_11_2 v855 v_call_i_3_11_2;
(*   store i16 %sub.3.11.2, i16* %arrayidx9.3.11.2, align 2, !tbaa !3 *)
mov mem0_182 v_sub_3_11_2;
(*   %add21.3.11.2 = add i16 %855, %call.i.3.11.2 *)
add v_add21_3_11_2 v855 v_call_i_3_11_2;
(*   store i16 %add21.3.11.2, i16* %arrayidx11.3.11.2, align 2, !tbaa !3 *)
mov mem0_150 v_add21_3_11_2;
(*   %arrayidx9.3.12.2 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %856 = load i16, i16* %arrayidx9.3.12.2, align 2, !tbaa !3 *)
mov v856 mem0_184;
(*   %conv1.i.3.12.2 = sext i16 %856 to i32 *)
cast v_conv1_i_3_12_2@sint32 v856@sint16;
(*   %mul.i.3.12.2 = mul nsw i32 %conv1.i.3.12.2, 1577 *)
mul v_mul_i_3_12_2 v_conv1_i_3_12_2 (1577)@sint32;
(*   %call.i.3.12.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_2, v_call_i_3_12_2);
(*   %arrayidx11.3.12.2 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %857 = load i16, i16* %arrayidx11.3.12.2, align 2, !tbaa !3 *)
mov v857 mem0_152;
(*   %sub.3.12.2 = sub i16 %857, %call.i.3.12.2 *)
sub v_sub_3_12_2 v857 v_call_i_3_12_2;
(*   store i16 %sub.3.12.2, i16* %arrayidx9.3.12.2, align 2, !tbaa !3 *)
mov mem0_184 v_sub_3_12_2;
(*   %add21.3.12.2 = add i16 %857, %call.i.3.12.2 *)
add v_add21_3_12_2 v857 v_call_i_3_12_2;
(*   store i16 %add21.3.12.2, i16* %arrayidx11.3.12.2, align 2, !tbaa !3 *)
mov mem0_152 v_add21_3_12_2;
(*   %arrayidx9.3.13.2 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %858 = load i16, i16* %arrayidx9.3.13.2, align 2, !tbaa !3 *)
mov v858 mem0_186;
(*   %conv1.i.3.13.2 = sext i16 %858 to i32 *)
cast v_conv1_i_3_13_2@sint32 v858@sint16;
(*   %mul.i.3.13.2 = mul nsw i32 %conv1.i.3.13.2, 1577 *)
mul v_mul_i_3_13_2 v_conv1_i_3_13_2 (1577)@sint32;
(*   %call.i.3.13.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_2, v_call_i_3_13_2);
(*   %arrayidx11.3.13.2 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %859 = load i16, i16* %arrayidx11.3.13.2, align 2, !tbaa !3 *)
mov v859 mem0_154;
(*   %sub.3.13.2 = sub i16 %859, %call.i.3.13.2 *)
sub v_sub_3_13_2 v859 v_call_i_3_13_2;
(*   store i16 %sub.3.13.2, i16* %arrayidx9.3.13.2, align 2, !tbaa !3 *)
mov mem0_186 v_sub_3_13_2;
(*   %add21.3.13.2 = add i16 %859, %call.i.3.13.2 *)
add v_add21_3_13_2 v859 v_call_i_3_13_2;
(*   store i16 %add21.3.13.2, i16* %arrayidx11.3.13.2, align 2, !tbaa !3 *)
mov mem0_154 v_add21_3_13_2;
(*   %arrayidx9.3.14.2 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %860 = load i16, i16* %arrayidx9.3.14.2, align 2, !tbaa !3 *)
mov v860 mem0_188;
(*   %conv1.i.3.14.2 = sext i16 %860 to i32 *)
cast v_conv1_i_3_14_2@sint32 v860@sint16;
(*   %mul.i.3.14.2 = mul nsw i32 %conv1.i.3.14.2, 1577 *)
mul v_mul_i_3_14_2 v_conv1_i_3_14_2 (1577)@sint32;
(*   %call.i.3.14.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_2, v_call_i_3_14_2);
(*   %arrayidx11.3.14.2 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %861 = load i16, i16* %arrayidx11.3.14.2, align 2, !tbaa !3 *)
mov v861 mem0_156;
(*   %sub.3.14.2 = sub i16 %861, %call.i.3.14.2 *)
sub v_sub_3_14_2 v861 v_call_i_3_14_2;
(*   store i16 %sub.3.14.2, i16* %arrayidx9.3.14.2, align 2, !tbaa !3 *)
mov mem0_188 v_sub_3_14_2;
(*   %add21.3.14.2 = add i16 %861, %call.i.3.14.2 *)
add v_add21_3_14_2 v861 v_call_i_3_14_2;
(*   store i16 %add21.3.14.2, i16* %arrayidx11.3.14.2, align 2, !tbaa !3 *)
mov mem0_156 v_add21_3_14_2;
(*   %arrayidx9.3.15.2 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %862 = load i16, i16* %arrayidx9.3.15.2, align 2, !tbaa !3 *)
mov v862 mem0_190;
(*   %conv1.i.3.15.2 = sext i16 %862 to i32 *)
cast v_conv1_i_3_15_2@sint32 v862@sint16;
(*   %mul.i.3.15.2 = mul nsw i32 %conv1.i.3.15.2, 1577 *)
mul v_mul_i_3_15_2 v_conv1_i_3_15_2 (1577)@sint32;
(*   %call.i.3.15.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_2, v_call_i_3_15_2);
(*   %arrayidx11.3.15.2 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %863 = load i16, i16* %arrayidx11.3.15.2, align 2, !tbaa !3 *)
mov v863 mem0_158;
(*   %sub.3.15.2 = sub i16 %863, %call.i.3.15.2 *)
sub v_sub_3_15_2 v863 v_call_i_3_15_2;
(*   store i16 %sub.3.15.2, i16* %arrayidx9.3.15.2, align 2, !tbaa !3 *)
mov mem0_190 v_sub_3_15_2;
(*   %add21.3.15.2 = add i16 %863, %call.i.3.15.2 *)
add v_add21_3_15_2 v863 v_call_i_3_15_2;
(*   store i16 %add21.3.15.2, i16* %arrayidx11.3.15.2, align 2, !tbaa !3 *)
mov mem0_158 v_add21_3_15_2;

(* NOTE: k = 11 *)

(*   %arrayidx9.3.3198 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %864 = load i16, i16* %arrayidx9.3.3198, align 2, !tbaa !3 *)
mov v864 mem0_224;
(*   %conv1.i.3.3199 = sext i16 %864 to i32 *)
cast v_conv1_i_3_3199@sint32 v864@sint16;
(*   %mul.i.3.3200 = mul nsw i32 %conv1.i.3.3199, 182 *)
mul v_mul_i_3_3200 v_conv1_i_3_3199 (182)@sint32;
(*   %call.i.3.3201 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3200) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3200, v_call_i_3_3201);
(*   %arrayidx11.3.3202 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %865 = load i16, i16* %arrayidx11.3.3202, align 2, !tbaa !3 *)
mov v865 mem0_192;
(*   %sub.3.3203 = sub i16 %865, %call.i.3.3201 *)
sub v_sub_3_3203 v865 v_call_i_3_3201;
(*   store i16 %sub.3.3203, i16* %arrayidx9.3.3198, align 2, !tbaa !3 *)
mov mem0_224 v_sub_3_3203;
(*   %add21.3.3204 = add i16 %865, %call.i.3.3201 *)
add v_add21_3_3204 v865 v_call_i_3_3201;
(*   store i16 %add21.3.3204, i16* %arrayidx11.3.3202, align 2, !tbaa !3 *)
mov mem0_192 v_add21_3_3204;
(*   %arrayidx9.3.1.3 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %866 = load i16, i16* %arrayidx9.3.1.3, align 2, !tbaa !3 *)
mov v866 mem0_226;
(*   %conv1.i.3.1.3 = sext i16 %866 to i32 *)
cast v_conv1_i_3_1_3@sint32 v866@sint16;
(*   %mul.i.3.1.3 = mul nsw i32 %conv1.i.3.1.3, 182 *)
mul v_mul_i_3_1_3 v_conv1_i_3_1_3 (182)@sint32;
(*   %call.i.3.1.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_3, v_call_i_3_1_3);
(*   %arrayidx11.3.1.3 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %867 = load i16, i16* %arrayidx11.3.1.3, align 2, !tbaa !3 *)
mov v867 mem0_194;
(*   %sub.3.1.3 = sub i16 %867, %call.i.3.1.3 *)
sub v_sub_3_1_3 v867 v_call_i_3_1_3;
(*   store i16 %sub.3.1.3, i16* %arrayidx9.3.1.3, align 2, !tbaa !3 *)
mov mem0_226 v_sub_3_1_3;
(*   %add21.3.1.3 = add i16 %867, %call.i.3.1.3 *)
add v_add21_3_1_3 v867 v_call_i_3_1_3;
(*   store i16 %add21.3.1.3, i16* %arrayidx11.3.1.3, align 2, !tbaa !3 *)
mov mem0_194 v_add21_3_1_3;
(*   %arrayidx9.3.2.3 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %868 = load i16, i16* %arrayidx9.3.2.3, align 2, !tbaa !3 *)
mov v868 mem0_228;
(*   %conv1.i.3.2.3 = sext i16 %868 to i32 *)
cast v_conv1_i_3_2_3@sint32 v868@sint16;
(*   %mul.i.3.2.3 = mul nsw i32 %conv1.i.3.2.3, 182 *)
mul v_mul_i_3_2_3 v_conv1_i_3_2_3 (182)@sint32;
(*   %call.i.3.2.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_3, v_call_i_3_2_3);
(*   %arrayidx11.3.2.3 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %869 = load i16, i16* %arrayidx11.3.2.3, align 2, !tbaa !3 *)
mov v869 mem0_196;
(*   %sub.3.2.3 = sub i16 %869, %call.i.3.2.3 *)
sub v_sub_3_2_3 v869 v_call_i_3_2_3;
(*   store i16 %sub.3.2.3, i16* %arrayidx9.3.2.3, align 2, !tbaa !3 *)
mov mem0_228 v_sub_3_2_3;
(*   %add21.3.2.3 = add i16 %869, %call.i.3.2.3 *)
add v_add21_3_2_3 v869 v_call_i_3_2_3;
(*   store i16 %add21.3.2.3, i16* %arrayidx11.3.2.3, align 2, !tbaa !3 *)
mov mem0_196 v_add21_3_2_3;
(*   %arrayidx9.3.3.3 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %870 = load i16, i16* %arrayidx9.3.3.3, align 2, !tbaa !3 *)
mov v870 mem0_230;
(*   %conv1.i.3.3.3 = sext i16 %870 to i32 *)
cast v_conv1_i_3_3_3@sint32 v870@sint16;
(*   %mul.i.3.3.3 = mul nsw i32 %conv1.i.3.3.3, 182 *)
mul v_mul_i_3_3_3 v_conv1_i_3_3_3 (182)@sint32;
(*   %call.i.3.3.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_3, v_call_i_3_3_3);
(*   %arrayidx11.3.3.3 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %871 = load i16, i16* %arrayidx11.3.3.3, align 2, !tbaa !3 *)
mov v871 mem0_198;
(*   %sub.3.3.3 = sub i16 %871, %call.i.3.3.3 *)
sub v_sub_3_3_3 v871 v_call_i_3_3_3;
(*   store i16 %sub.3.3.3, i16* %arrayidx9.3.3.3, align 2, !tbaa !3 *)
mov mem0_230 v_sub_3_3_3;
(*   %add21.3.3.3 = add i16 %871, %call.i.3.3.3 *)
add v_add21_3_3_3 v871 v_call_i_3_3_3;
(*   store i16 %add21.3.3.3, i16* %arrayidx11.3.3.3, align 2, !tbaa !3 *)
mov mem0_198 v_add21_3_3_3;
(*   %arrayidx9.3.4.3 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %872 = load i16, i16* %arrayidx9.3.4.3, align 2, !tbaa !3 *)
mov v872 mem0_232;
(*   %conv1.i.3.4.3 = sext i16 %872 to i32 *)
cast v_conv1_i_3_4_3@sint32 v872@sint16;
(*   %mul.i.3.4.3 = mul nsw i32 %conv1.i.3.4.3, 182 *)
mul v_mul_i_3_4_3 v_conv1_i_3_4_3 (182)@sint32;
(*   %call.i.3.4.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_3, v_call_i_3_4_3);
(*   %arrayidx11.3.4.3 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %873 = load i16, i16* %arrayidx11.3.4.3, align 2, !tbaa !3 *)
mov v873 mem0_200;
(*   %sub.3.4.3 = sub i16 %873, %call.i.3.4.3 *)
sub v_sub_3_4_3 v873 v_call_i_3_4_3;
(*   store i16 %sub.3.4.3, i16* %arrayidx9.3.4.3, align 2, !tbaa !3 *)
mov mem0_232 v_sub_3_4_3;
(*   %add21.3.4.3 = add i16 %873, %call.i.3.4.3 *)
add v_add21_3_4_3 v873 v_call_i_3_4_3;
(*   store i16 %add21.3.4.3, i16* %arrayidx11.3.4.3, align 2, !tbaa !3 *)
mov mem0_200 v_add21_3_4_3;
(*   %arrayidx9.3.5.3 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %874 = load i16, i16* %arrayidx9.3.5.3, align 2, !tbaa !3 *)
mov v874 mem0_234;
(*   %conv1.i.3.5.3 = sext i16 %874 to i32 *)
cast v_conv1_i_3_5_3@sint32 v874@sint16;
(*   %mul.i.3.5.3 = mul nsw i32 %conv1.i.3.5.3, 182 *)
mul v_mul_i_3_5_3 v_conv1_i_3_5_3 (182)@sint32;
(*   %call.i.3.5.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_3, v_call_i_3_5_3);
(*   %arrayidx11.3.5.3 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %875 = load i16, i16* %arrayidx11.3.5.3, align 2, !tbaa !3 *)
mov v875 mem0_202;
(*   %sub.3.5.3 = sub i16 %875, %call.i.3.5.3 *)
sub v_sub_3_5_3 v875 v_call_i_3_5_3;
(*   store i16 %sub.3.5.3, i16* %arrayidx9.3.5.3, align 2, !tbaa !3 *)
mov mem0_234 v_sub_3_5_3;
(*   %add21.3.5.3 = add i16 %875, %call.i.3.5.3 *)
add v_add21_3_5_3 v875 v_call_i_3_5_3;
(*   store i16 %add21.3.5.3, i16* %arrayidx11.3.5.3, align 2, !tbaa !3 *)
mov mem0_202 v_add21_3_5_3;
(*   %arrayidx9.3.6.3 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %876 = load i16, i16* %arrayidx9.3.6.3, align 2, !tbaa !3 *)
mov v876 mem0_236;
(*   %conv1.i.3.6.3 = sext i16 %876 to i32 *)
cast v_conv1_i_3_6_3@sint32 v876@sint16;
(*   %mul.i.3.6.3 = mul nsw i32 %conv1.i.3.6.3, 182 *)
mul v_mul_i_3_6_3 v_conv1_i_3_6_3 (182)@sint32;
(*   %call.i.3.6.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_3, v_call_i_3_6_3);
(*   %arrayidx11.3.6.3 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %877 = load i16, i16* %arrayidx11.3.6.3, align 2, !tbaa !3 *)
mov v877 mem0_204;
(*   %sub.3.6.3 = sub i16 %877, %call.i.3.6.3 *)
sub v_sub_3_6_3 v877 v_call_i_3_6_3;
(*   store i16 %sub.3.6.3, i16* %arrayidx9.3.6.3, align 2, !tbaa !3 *)
mov mem0_236 v_sub_3_6_3;
(*   %add21.3.6.3 = add i16 %877, %call.i.3.6.3 *)
add v_add21_3_6_3 v877 v_call_i_3_6_3;
(*   store i16 %add21.3.6.3, i16* %arrayidx11.3.6.3, align 2, !tbaa !3 *)
mov mem0_204 v_add21_3_6_3;
(*   %arrayidx9.3.7.3 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %878 = load i16, i16* %arrayidx9.3.7.3, align 2, !tbaa !3 *)
mov v878 mem0_238;
(*   %conv1.i.3.7.3 = sext i16 %878 to i32 *)
cast v_conv1_i_3_7_3@sint32 v878@sint16;
(*   %mul.i.3.7.3 = mul nsw i32 %conv1.i.3.7.3, 182 *)
mul v_mul_i_3_7_3 v_conv1_i_3_7_3 (182)@sint32;
(*   %call.i.3.7.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_3, v_call_i_3_7_3);
(*   %arrayidx11.3.7.3 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %879 = load i16, i16* %arrayidx11.3.7.3, align 2, !tbaa !3 *)
mov v879 mem0_206;
(*   %sub.3.7.3 = sub i16 %879, %call.i.3.7.3 *)
sub v_sub_3_7_3 v879 v_call_i_3_7_3;
(*   store i16 %sub.3.7.3, i16* %arrayidx9.3.7.3, align 2, !tbaa !3 *)
mov mem0_238 v_sub_3_7_3;
(*   %add21.3.7.3 = add i16 %879, %call.i.3.7.3 *)
add v_add21_3_7_3 v879 v_call_i_3_7_3;
(*   store i16 %add21.3.7.3, i16* %arrayidx11.3.7.3, align 2, !tbaa !3 *)
mov mem0_206 v_add21_3_7_3;
(*   %arrayidx9.3.8.3 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %880 = load i16, i16* %arrayidx9.3.8.3, align 2, !tbaa !3 *)
mov v880 mem0_240;
(*   %conv1.i.3.8.3 = sext i16 %880 to i32 *)
cast v_conv1_i_3_8_3@sint32 v880@sint16;
(*   %mul.i.3.8.3 = mul nsw i32 %conv1.i.3.8.3, 182 *)
mul v_mul_i_3_8_3 v_conv1_i_3_8_3 (182)@sint32;
(*   %call.i.3.8.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_3, v_call_i_3_8_3);
(*   %arrayidx11.3.8.3 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %881 = load i16, i16* %arrayidx11.3.8.3, align 2, !tbaa !3 *)
mov v881 mem0_208;
(*   %sub.3.8.3 = sub i16 %881, %call.i.3.8.3 *)
sub v_sub_3_8_3 v881 v_call_i_3_8_3;
(*   store i16 %sub.3.8.3, i16* %arrayidx9.3.8.3, align 2, !tbaa !3 *)
mov mem0_240 v_sub_3_8_3;
(*   %add21.3.8.3 = add i16 %881, %call.i.3.8.3 *)
add v_add21_3_8_3 v881 v_call_i_3_8_3;
(*   store i16 %add21.3.8.3, i16* %arrayidx11.3.8.3, align 2, !tbaa !3 *)
mov mem0_208 v_add21_3_8_3;
(*   %arrayidx9.3.9.3 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %882 = load i16, i16* %arrayidx9.3.9.3, align 2, !tbaa !3 *)
mov v882 mem0_242;
(*   %conv1.i.3.9.3 = sext i16 %882 to i32 *)
cast v_conv1_i_3_9_3@sint32 v882@sint16;
(*   %mul.i.3.9.3 = mul nsw i32 %conv1.i.3.9.3, 182 *)
mul v_mul_i_3_9_3 v_conv1_i_3_9_3 (182)@sint32;
(*   %call.i.3.9.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_3, v_call_i_3_9_3);
(*   %arrayidx11.3.9.3 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %883 = load i16, i16* %arrayidx11.3.9.3, align 2, !tbaa !3 *)
mov v883 mem0_210;
(*   %sub.3.9.3 = sub i16 %883, %call.i.3.9.3 *)
sub v_sub_3_9_3 v883 v_call_i_3_9_3;
(*   store i16 %sub.3.9.3, i16* %arrayidx9.3.9.3, align 2, !tbaa !3 *)
mov mem0_242 v_sub_3_9_3;
(*   %add21.3.9.3 = add i16 %883, %call.i.3.9.3 *)
add v_add21_3_9_3 v883 v_call_i_3_9_3;
(*   store i16 %add21.3.9.3, i16* %arrayidx11.3.9.3, align 2, !tbaa !3 *)
mov mem0_210 v_add21_3_9_3;
(*   %arrayidx9.3.10.3 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %884 = load i16, i16* %arrayidx9.3.10.3, align 2, !tbaa !3 *)
mov v884 mem0_244;
(*   %conv1.i.3.10.3 = sext i16 %884 to i32 *)
cast v_conv1_i_3_10_3@sint32 v884@sint16;
(*   %mul.i.3.10.3 = mul nsw i32 %conv1.i.3.10.3, 182 *)
mul v_mul_i_3_10_3 v_conv1_i_3_10_3 (182)@sint32;
(*   %call.i.3.10.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_3, v_call_i_3_10_3);
(*   %arrayidx11.3.10.3 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %885 = load i16, i16* %arrayidx11.3.10.3, align 2, !tbaa !3 *)
mov v885 mem0_212;
(*   %sub.3.10.3 = sub i16 %885, %call.i.3.10.3 *)
sub v_sub_3_10_3 v885 v_call_i_3_10_3;
(*   store i16 %sub.3.10.3, i16* %arrayidx9.3.10.3, align 2, !tbaa !3 *)
mov mem0_244 v_sub_3_10_3;
(*   %add21.3.10.3 = add i16 %885, %call.i.3.10.3 *)
add v_add21_3_10_3 v885 v_call_i_3_10_3;
(*   store i16 %add21.3.10.3, i16* %arrayidx11.3.10.3, align 2, !tbaa !3 *)
mov mem0_212 v_add21_3_10_3;
(*   %arrayidx9.3.11.3 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %886 = load i16, i16* %arrayidx9.3.11.3, align 2, !tbaa !3 *)
mov v886 mem0_246;
(*   %conv1.i.3.11.3 = sext i16 %886 to i32 *)
cast v_conv1_i_3_11_3@sint32 v886@sint16;
(*   %mul.i.3.11.3 = mul nsw i32 %conv1.i.3.11.3, 182 *)
mul v_mul_i_3_11_3 v_conv1_i_3_11_3 (182)@sint32;
(*   %call.i.3.11.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_3, v_call_i_3_11_3);
(*   %arrayidx11.3.11.3 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %887 = load i16, i16* %arrayidx11.3.11.3, align 2, !tbaa !3 *)
mov v887 mem0_214;
(*   %sub.3.11.3 = sub i16 %887, %call.i.3.11.3 *)
sub v_sub_3_11_3 v887 v_call_i_3_11_3;
(*   store i16 %sub.3.11.3, i16* %arrayidx9.3.11.3, align 2, !tbaa !3 *)
mov mem0_246 v_sub_3_11_3;
(*   %add21.3.11.3 = add i16 %887, %call.i.3.11.3 *)
add v_add21_3_11_3 v887 v_call_i_3_11_3;
(*   store i16 %add21.3.11.3, i16* %arrayidx11.3.11.3, align 2, !tbaa !3 *)
mov mem0_214 v_add21_3_11_3;
(*   %arrayidx9.3.12.3 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %888 = load i16, i16* %arrayidx9.3.12.3, align 2, !tbaa !3 *)
mov v888 mem0_248;
(*   %conv1.i.3.12.3 = sext i16 %888 to i32 *)
cast v_conv1_i_3_12_3@sint32 v888@sint16;
(*   %mul.i.3.12.3 = mul nsw i32 %conv1.i.3.12.3, 182 *)
mul v_mul_i_3_12_3 v_conv1_i_3_12_3 (182)@sint32;
(*   %call.i.3.12.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_3, v_call_i_3_12_3);
(*   %arrayidx11.3.12.3 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %889 = load i16, i16* %arrayidx11.3.12.3, align 2, !tbaa !3 *)
mov v889 mem0_216;
(*   %sub.3.12.3 = sub i16 %889, %call.i.3.12.3 *)
sub v_sub_3_12_3 v889 v_call_i_3_12_3;
(*   store i16 %sub.3.12.3, i16* %arrayidx9.3.12.3, align 2, !tbaa !3 *)
mov mem0_248 v_sub_3_12_3;
(*   %add21.3.12.3 = add i16 %889, %call.i.3.12.3 *)
add v_add21_3_12_3 v889 v_call_i_3_12_3;
(*   store i16 %add21.3.12.3, i16* %arrayidx11.3.12.3, align 2, !tbaa !3 *)
mov mem0_216 v_add21_3_12_3;
(*   %arrayidx9.3.13.3 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %890 = load i16, i16* %arrayidx9.3.13.3, align 2, !tbaa !3 *)
mov v890 mem0_250;
(*   %conv1.i.3.13.3 = sext i16 %890 to i32 *)
cast v_conv1_i_3_13_3@sint32 v890@sint16;
(*   %mul.i.3.13.3 = mul nsw i32 %conv1.i.3.13.3, 182 *)
mul v_mul_i_3_13_3 v_conv1_i_3_13_3 (182)@sint32;
(*   %call.i.3.13.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_3, v_call_i_3_13_3);
(*   %arrayidx11.3.13.3 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %891 = load i16, i16* %arrayidx11.3.13.3, align 2, !tbaa !3 *)
mov v891 mem0_218;
(*   %sub.3.13.3 = sub i16 %891, %call.i.3.13.3 *)
sub v_sub_3_13_3 v891 v_call_i_3_13_3;
(*   store i16 %sub.3.13.3, i16* %arrayidx9.3.13.3, align 2, !tbaa !3 *)
mov mem0_250 v_sub_3_13_3;
(*   %add21.3.13.3 = add i16 %891, %call.i.3.13.3 *)
add v_add21_3_13_3 v891 v_call_i_3_13_3;
(*   store i16 %add21.3.13.3, i16* %arrayidx11.3.13.3, align 2, !tbaa !3 *)
mov mem0_218 v_add21_3_13_3;
(*   %arrayidx9.3.14.3 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %892 = load i16, i16* %arrayidx9.3.14.3, align 2, !tbaa !3 *)
mov v892 mem0_252;
(*   %conv1.i.3.14.3 = sext i16 %892 to i32 *)
cast v_conv1_i_3_14_3@sint32 v892@sint16;
(*   %mul.i.3.14.3 = mul nsw i32 %conv1.i.3.14.3, 182 *)
mul v_mul_i_3_14_3 v_conv1_i_3_14_3 (182)@sint32;
(*   %call.i.3.14.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_3, v_call_i_3_14_3);
(*   %arrayidx11.3.14.3 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %893 = load i16, i16* %arrayidx11.3.14.3, align 2, !tbaa !3 *)
mov v893 mem0_220;
(*   %sub.3.14.3 = sub i16 %893, %call.i.3.14.3 *)
sub v_sub_3_14_3 v893 v_call_i_3_14_3;
(*   store i16 %sub.3.14.3, i16* %arrayidx9.3.14.3, align 2, !tbaa !3 *)
mov mem0_252 v_sub_3_14_3;
(*   %add21.3.14.3 = add i16 %893, %call.i.3.14.3 *)
add v_add21_3_14_3 v893 v_call_i_3_14_3;
(*   store i16 %add21.3.14.3, i16* %arrayidx11.3.14.3, align 2, !tbaa !3 *)
mov mem0_220 v_add21_3_14_3;
(*   %arrayidx9.3.15.3 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %894 = load i16, i16* %arrayidx9.3.15.3, align 2, !tbaa !3 *)
mov v894 mem0_254;
(*   %conv1.i.3.15.3 = sext i16 %894 to i32 *)
cast v_conv1_i_3_15_3@sint32 v894@sint16;
(*   %mul.i.3.15.3 = mul nsw i32 %conv1.i.3.15.3, 182 *)
mul v_mul_i_3_15_3 v_conv1_i_3_15_3 (182)@sint32;
(*   %call.i.3.15.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_3, v_call_i_3_15_3);
(*   %arrayidx11.3.15.3 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %895 = load i16, i16* %arrayidx11.3.15.3, align 2, !tbaa !3 *)
mov v895 mem0_222;
(*   %sub.3.15.3 = sub i16 %895, %call.i.3.15.3 *)
sub v_sub_3_15_3 v895 v_call_i_3_15_3;
(*   store i16 %sub.3.15.3, i16* %arrayidx9.3.15.3, align 2, !tbaa !3 *)
mov mem0_254 v_sub_3_15_3;
(*   %add21.3.15.3 = add i16 %895, %call.i.3.15.3 *)
add v_add21_3_15_3 v895 v_call_i_3_15_3;
(*   store i16 %add21.3.15.3, i16* %arrayidx11.3.15.3, align 2, !tbaa !3 *)
mov mem0_222 v_add21_3_15_3;

(* NOTE: k = 12 *)

(*   %arrayidx9.3.4208 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %896 = load i16, i16* %arrayidx9.3.4208, align 2, !tbaa !3 *)
mov v896 mem0_288;
(*   %conv1.i.3.4209 = sext i16 %896 to i32 *)
cast v_conv1_i_3_4209@sint32 v896@sint16;
(*   %mul.i.3.4210 = mul nsw i32 %conv1.i.3.4209, 962 *)
mul v_mul_i_3_4210 v_conv1_i_3_4209 (962)@sint32;
(*   %call.i.3.4211 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4210) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4210, v_call_i_3_4211);
(*   %arrayidx11.3.4212 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %897 = load i16, i16* %arrayidx11.3.4212, align 2, !tbaa !3 *)
mov v897 mem0_256;
(*   %sub.3.4213 = sub i16 %897, %call.i.3.4211 *)
sub v_sub_3_4213 v897 v_call_i_3_4211;
(*   store i16 %sub.3.4213, i16* %arrayidx9.3.4208, align 2, !tbaa !3 *)
mov mem0_288 v_sub_3_4213;
(*   %add21.3.4214 = add i16 %897, %call.i.3.4211 *)
add v_add21_3_4214 v897 v_call_i_3_4211;
(*   store i16 %add21.3.4214, i16* %arrayidx11.3.4212, align 2, !tbaa !3 *)
mov mem0_256 v_add21_3_4214;
(*   %arrayidx9.3.1.4 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %898 = load i16, i16* %arrayidx9.3.1.4, align 2, !tbaa !3 *)
mov v898 mem0_290;
(*   %conv1.i.3.1.4 = sext i16 %898 to i32 *)
cast v_conv1_i_3_1_4@sint32 v898@sint16;
(*   %mul.i.3.1.4 = mul nsw i32 %conv1.i.3.1.4, 962 *)
mul v_mul_i_3_1_4 v_conv1_i_3_1_4 (962)@sint32;
(*   %call.i.3.1.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_4, v_call_i_3_1_4);
(*   %arrayidx11.3.1.4 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %899 = load i16, i16* %arrayidx11.3.1.4, align 2, !tbaa !3 *)
mov v899 mem0_258;
(*   %sub.3.1.4 = sub i16 %899, %call.i.3.1.4 *)
sub v_sub_3_1_4 v899 v_call_i_3_1_4;
(*   store i16 %sub.3.1.4, i16* %arrayidx9.3.1.4, align 2, !tbaa !3 *)
mov mem0_290 v_sub_3_1_4;
(*   %add21.3.1.4 = add i16 %899, %call.i.3.1.4 *)
add v_add21_3_1_4 v899 v_call_i_3_1_4;
(*   store i16 %add21.3.1.4, i16* %arrayidx11.3.1.4, align 2, !tbaa !3 *)
mov mem0_258 v_add21_3_1_4;
(*   %arrayidx9.3.2.4 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %900 = load i16, i16* %arrayidx9.3.2.4, align 2, !tbaa !3 *)
mov v900 mem0_292;
(*   %conv1.i.3.2.4 = sext i16 %900 to i32 *)
cast v_conv1_i_3_2_4@sint32 v900@sint16;
(*   %mul.i.3.2.4 = mul nsw i32 %conv1.i.3.2.4, 962 *)
mul v_mul_i_3_2_4 v_conv1_i_3_2_4 (962)@sint32;
(*   %call.i.3.2.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_4, v_call_i_3_2_4);
(*   %arrayidx11.3.2.4 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %901 = load i16, i16* %arrayidx11.3.2.4, align 2, !tbaa !3 *)
mov v901 mem0_260;
(*   %sub.3.2.4 = sub i16 %901, %call.i.3.2.4 *)
sub v_sub_3_2_4 v901 v_call_i_3_2_4;
(*   store i16 %sub.3.2.4, i16* %arrayidx9.3.2.4, align 2, !tbaa !3 *)
mov mem0_292 v_sub_3_2_4;
(*   %add21.3.2.4 = add i16 %901, %call.i.3.2.4 *)
add v_add21_3_2_4 v901 v_call_i_3_2_4;
(*   store i16 %add21.3.2.4, i16* %arrayidx11.3.2.4, align 2, !tbaa !3 *)
mov mem0_260 v_add21_3_2_4;
(*   %arrayidx9.3.3.4 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %902 = load i16, i16* %arrayidx9.3.3.4, align 2, !tbaa !3 *)
mov v902 mem0_294;
(*   %conv1.i.3.3.4 = sext i16 %902 to i32 *)
cast v_conv1_i_3_3_4@sint32 v902@sint16;
(*   %mul.i.3.3.4 = mul nsw i32 %conv1.i.3.3.4, 962 *)
mul v_mul_i_3_3_4 v_conv1_i_3_3_4 (962)@sint32;
(*   %call.i.3.3.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_4, v_call_i_3_3_4);
(*   %arrayidx11.3.3.4 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %903 = load i16, i16* %arrayidx11.3.3.4, align 2, !tbaa !3 *)
mov v903 mem0_262;
(*   %sub.3.3.4 = sub i16 %903, %call.i.3.3.4 *)
sub v_sub_3_3_4 v903 v_call_i_3_3_4;
(*   store i16 %sub.3.3.4, i16* %arrayidx9.3.3.4, align 2, !tbaa !3 *)
mov mem0_294 v_sub_3_3_4;
(*   %add21.3.3.4 = add i16 %903, %call.i.3.3.4 *)
add v_add21_3_3_4 v903 v_call_i_3_3_4;
(*   store i16 %add21.3.3.4, i16* %arrayidx11.3.3.4, align 2, !tbaa !3 *)
mov mem0_262 v_add21_3_3_4;
(*   %arrayidx9.3.4.4 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %904 = load i16, i16* %arrayidx9.3.4.4, align 2, !tbaa !3 *)
mov v904 mem0_296;
(*   %conv1.i.3.4.4 = sext i16 %904 to i32 *)
cast v_conv1_i_3_4_4@sint32 v904@sint16;
(*   %mul.i.3.4.4 = mul nsw i32 %conv1.i.3.4.4, 962 *)
mul v_mul_i_3_4_4 v_conv1_i_3_4_4 (962)@sint32;
(*   %call.i.3.4.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_4, v_call_i_3_4_4);
(*   %arrayidx11.3.4.4 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %905 = load i16, i16* %arrayidx11.3.4.4, align 2, !tbaa !3 *)
mov v905 mem0_264;
(*   %sub.3.4.4 = sub i16 %905, %call.i.3.4.4 *)
sub v_sub_3_4_4 v905 v_call_i_3_4_4;
(*   store i16 %sub.3.4.4, i16* %arrayidx9.3.4.4, align 2, !tbaa !3 *)
mov mem0_296 v_sub_3_4_4;
(*   %add21.3.4.4 = add i16 %905, %call.i.3.4.4 *)
add v_add21_3_4_4 v905 v_call_i_3_4_4;
(*   store i16 %add21.3.4.4, i16* %arrayidx11.3.4.4, align 2, !tbaa !3 *)
mov mem0_264 v_add21_3_4_4;
(*   %arrayidx9.3.5.4 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %906 = load i16, i16* %arrayidx9.3.5.4, align 2, !tbaa !3 *)
mov v906 mem0_298;
(*   %conv1.i.3.5.4 = sext i16 %906 to i32 *)
cast v_conv1_i_3_5_4@sint32 v906@sint16;
(*   %mul.i.3.5.4 = mul nsw i32 %conv1.i.3.5.4, 962 *)
mul v_mul_i_3_5_4 v_conv1_i_3_5_4 (962)@sint32;
(*   %call.i.3.5.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_4, v_call_i_3_5_4);
(*   %arrayidx11.3.5.4 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %907 = load i16, i16* %arrayidx11.3.5.4, align 2, !tbaa !3 *)
mov v907 mem0_266;
(*   %sub.3.5.4 = sub i16 %907, %call.i.3.5.4 *)
sub v_sub_3_5_4 v907 v_call_i_3_5_4;
(*   store i16 %sub.3.5.4, i16* %arrayidx9.3.5.4, align 2, !tbaa !3 *)
mov mem0_298 v_sub_3_5_4;
(*   %add21.3.5.4 = add i16 %907, %call.i.3.5.4 *)
add v_add21_3_5_4 v907 v_call_i_3_5_4;
(*   store i16 %add21.3.5.4, i16* %arrayidx11.3.5.4, align 2, !tbaa !3 *)
mov mem0_266 v_add21_3_5_4;
(*   %arrayidx9.3.6.4 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %908 = load i16, i16* %arrayidx9.3.6.4, align 2, !tbaa !3 *)
mov v908 mem0_300;
(*   %conv1.i.3.6.4 = sext i16 %908 to i32 *)
cast v_conv1_i_3_6_4@sint32 v908@sint16;
(*   %mul.i.3.6.4 = mul nsw i32 %conv1.i.3.6.4, 962 *)
mul v_mul_i_3_6_4 v_conv1_i_3_6_4 (962)@sint32;
(*   %call.i.3.6.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_4, v_call_i_3_6_4);
(*   %arrayidx11.3.6.4 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %909 = load i16, i16* %arrayidx11.3.6.4, align 2, !tbaa !3 *)
mov v909 mem0_268;
(*   %sub.3.6.4 = sub i16 %909, %call.i.3.6.4 *)
sub v_sub_3_6_4 v909 v_call_i_3_6_4;
(*   store i16 %sub.3.6.4, i16* %arrayidx9.3.6.4, align 2, !tbaa !3 *)
mov mem0_300 v_sub_3_6_4;
(*   %add21.3.6.4 = add i16 %909, %call.i.3.6.4 *)
add v_add21_3_6_4 v909 v_call_i_3_6_4;
(*   store i16 %add21.3.6.4, i16* %arrayidx11.3.6.4, align 2, !tbaa !3 *)
mov mem0_268 v_add21_3_6_4;
(*   %arrayidx9.3.7.4 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %910 = load i16, i16* %arrayidx9.3.7.4, align 2, !tbaa !3 *)
mov v910 mem0_302;
(*   %conv1.i.3.7.4 = sext i16 %910 to i32 *)
cast v_conv1_i_3_7_4@sint32 v910@sint16;
(*   %mul.i.3.7.4 = mul nsw i32 %conv1.i.3.7.4, 962 *)
mul v_mul_i_3_7_4 v_conv1_i_3_7_4 (962)@sint32;
(*   %call.i.3.7.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_4, v_call_i_3_7_4);
(*   %arrayidx11.3.7.4 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %911 = load i16, i16* %arrayidx11.3.7.4, align 2, !tbaa !3 *)
mov v911 mem0_270;
(*   %sub.3.7.4 = sub i16 %911, %call.i.3.7.4 *)
sub v_sub_3_7_4 v911 v_call_i_3_7_4;
(*   store i16 %sub.3.7.4, i16* %arrayidx9.3.7.4, align 2, !tbaa !3 *)
mov mem0_302 v_sub_3_7_4;
(*   %add21.3.7.4 = add i16 %911, %call.i.3.7.4 *)
add v_add21_3_7_4 v911 v_call_i_3_7_4;
(*   store i16 %add21.3.7.4, i16* %arrayidx11.3.7.4, align 2, !tbaa !3 *)
mov mem0_270 v_add21_3_7_4;
(*   %arrayidx9.3.8.4 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %912 = load i16, i16* %arrayidx9.3.8.4, align 2, !tbaa !3 *)
mov v912 mem0_304;
(*   %conv1.i.3.8.4 = sext i16 %912 to i32 *)
cast v_conv1_i_3_8_4@sint32 v912@sint16;
(*   %mul.i.3.8.4 = mul nsw i32 %conv1.i.3.8.4, 962 *)
mul v_mul_i_3_8_4 v_conv1_i_3_8_4 (962)@sint32;
(*   %call.i.3.8.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_4, v_call_i_3_8_4);
(*   %arrayidx11.3.8.4 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %913 = load i16, i16* %arrayidx11.3.8.4, align 2, !tbaa !3 *)
mov v913 mem0_272;
(*   %sub.3.8.4 = sub i16 %913, %call.i.3.8.4 *)
sub v_sub_3_8_4 v913 v_call_i_3_8_4;
(*   store i16 %sub.3.8.4, i16* %arrayidx9.3.8.4, align 2, !tbaa !3 *)
mov mem0_304 v_sub_3_8_4;
(*   %add21.3.8.4 = add i16 %913, %call.i.3.8.4 *)
add v_add21_3_8_4 v913 v_call_i_3_8_4;
(*   store i16 %add21.3.8.4, i16* %arrayidx11.3.8.4, align 2, !tbaa !3 *)
mov mem0_272 v_add21_3_8_4;
(*   %arrayidx9.3.9.4 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %914 = load i16, i16* %arrayidx9.3.9.4, align 2, !tbaa !3 *)
mov v914 mem0_306;
(*   %conv1.i.3.9.4 = sext i16 %914 to i32 *)
cast v_conv1_i_3_9_4@sint32 v914@sint16;
(*   %mul.i.3.9.4 = mul nsw i32 %conv1.i.3.9.4, 962 *)
mul v_mul_i_3_9_4 v_conv1_i_3_9_4 (962)@sint32;
(*   %call.i.3.9.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_4, v_call_i_3_9_4);
(*   %arrayidx11.3.9.4 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %915 = load i16, i16* %arrayidx11.3.9.4, align 2, !tbaa !3 *)
mov v915 mem0_274;
(*   %sub.3.9.4 = sub i16 %915, %call.i.3.9.4 *)
sub v_sub_3_9_4 v915 v_call_i_3_9_4;
(*   store i16 %sub.3.9.4, i16* %arrayidx9.3.9.4, align 2, !tbaa !3 *)
mov mem0_306 v_sub_3_9_4;
(*   %add21.3.9.4 = add i16 %915, %call.i.3.9.4 *)
add v_add21_3_9_4 v915 v_call_i_3_9_4;
(*   store i16 %add21.3.9.4, i16* %arrayidx11.3.9.4, align 2, !tbaa !3 *)
mov mem0_274 v_add21_3_9_4;
(*   %arrayidx9.3.10.4 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %916 = load i16, i16* %arrayidx9.3.10.4, align 2, !tbaa !3 *)
mov v916 mem0_308;
(*   %conv1.i.3.10.4 = sext i16 %916 to i32 *)
cast v_conv1_i_3_10_4@sint32 v916@sint16;
(*   %mul.i.3.10.4 = mul nsw i32 %conv1.i.3.10.4, 962 *)
mul v_mul_i_3_10_4 v_conv1_i_3_10_4 (962)@sint32;
(*   %call.i.3.10.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_4, v_call_i_3_10_4);
(*   %arrayidx11.3.10.4 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %917 = load i16, i16* %arrayidx11.3.10.4, align 2, !tbaa !3 *)
mov v917 mem0_276;
(*   %sub.3.10.4 = sub i16 %917, %call.i.3.10.4 *)
sub v_sub_3_10_4 v917 v_call_i_3_10_4;
(*   store i16 %sub.3.10.4, i16* %arrayidx9.3.10.4, align 2, !tbaa !3 *)
mov mem0_308 v_sub_3_10_4;
(*   %add21.3.10.4 = add i16 %917, %call.i.3.10.4 *)
add v_add21_3_10_4 v917 v_call_i_3_10_4;
(*   store i16 %add21.3.10.4, i16* %arrayidx11.3.10.4, align 2, !tbaa !3 *)
mov mem0_276 v_add21_3_10_4;
(*   %arrayidx9.3.11.4 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %918 = load i16, i16* %arrayidx9.3.11.4, align 2, !tbaa !3 *)
mov v918 mem0_310;
(*   %conv1.i.3.11.4 = sext i16 %918 to i32 *)
cast v_conv1_i_3_11_4@sint32 v918@sint16;
(*   %mul.i.3.11.4 = mul nsw i32 %conv1.i.3.11.4, 962 *)
mul v_mul_i_3_11_4 v_conv1_i_3_11_4 (962)@sint32;
(*   %call.i.3.11.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_4, v_call_i_3_11_4);
(*   %arrayidx11.3.11.4 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %919 = load i16, i16* %arrayidx11.3.11.4, align 2, !tbaa !3 *)
mov v919 mem0_278;
(*   %sub.3.11.4 = sub i16 %919, %call.i.3.11.4 *)
sub v_sub_3_11_4 v919 v_call_i_3_11_4;
(*   store i16 %sub.3.11.4, i16* %arrayidx9.3.11.4, align 2, !tbaa !3 *)
mov mem0_310 v_sub_3_11_4;
(*   %add21.3.11.4 = add i16 %919, %call.i.3.11.4 *)
add v_add21_3_11_4 v919 v_call_i_3_11_4;
(*   store i16 %add21.3.11.4, i16* %arrayidx11.3.11.4, align 2, !tbaa !3 *)
mov mem0_278 v_add21_3_11_4;
(*   %arrayidx9.3.12.4 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %920 = load i16, i16* %arrayidx9.3.12.4, align 2, !tbaa !3 *)
mov v920 mem0_312;
(*   %conv1.i.3.12.4 = sext i16 %920 to i32 *)
cast v_conv1_i_3_12_4@sint32 v920@sint16;
(*   %mul.i.3.12.4 = mul nsw i32 %conv1.i.3.12.4, 962 *)
mul v_mul_i_3_12_4 v_conv1_i_3_12_4 (962)@sint32;
(*   %call.i.3.12.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_4, v_call_i_3_12_4);
(*   %arrayidx11.3.12.4 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %921 = load i16, i16* %arrayidx11.3.12.4, align 2, !tbaa !3 *)
mov v921 mem0_280;
(*   %sub.3.12.4 = sub i16 %921, %call.i.3.12.4 *)
sub v_sub_3_12_4 v921 v_call_i_3_12_4;
(*   store i16 %sub.3.12.4, i16* %arrayidx9.3.12.4, align 2, !tbaa !3 *)
mov mem0_312 v_sub_3_12_4;
(*   %add21.3.12.4 = add i16 %921, %call.i.3.12.4 *)
add v_add21_3_12_4 v921 v_call_i_3_12_4;
(*   store i16 %add21.3.12.4, i16* %arrayidx11.3.12.4, align 2, !tbaa !3 *)
mov mem0_280 v_add21_3_12_4;
(*   %arrayidx9.3.13.4 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %922 = load i16, i16* %arrayidx9.3.13.4, align 2, !tbaa !3 *)
mov v922 mem0_314;
(*   %conv1.i.3.13.4 = sext i16 %922 to i32 *)
cast v_conv1_i_3_13_4@sint32 v922@sint16;
(*   %mul.i.3.13.4 = mul nsw i32 %conv1.i.3.13.4, 962 *)
mul v_mul_i_3_13_4 v_conv1_i_3_13_4 (962)@sint32;
(*   %call.i.3.13.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_4, v_call_i_3_13_4);
(*   %arrayidx11.3.13.4 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %923 = load i16, i16* %arrayidx11.3.13.4, align 2, !tbaa !3 *)
mov v923 mem0_282;
(*   %sub.3.13.4 = sub i16 %923, %call.i.3.13.4 *)
sub v_sub_3_13_4 v923 v_call_i_3_13_4;
(*   store i16 %sub.3.13.4, i16* %arrayidx9.3.13.4, align 2, !tbaa !3 *)
mov mem0_314 v_sub_3_13_4;
(*   %add21.3.13.4 = add i16 %923, %call.i.3.13.4 *)
add v_add21_3_13_4 v923 v_call_i_3_13_4;
(*   store i16 %add21.3.13.4, i16* %arrayidx11.3.13.4, align 2, !tbaa !3 *)
mov mem0_282 v_add21_3_13_4;
(*   %arrayidx9.3.14.4 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %924 = load i16, i16* %arrayidx9.3.14.4, align 2, !tbaa !3 *)
mov v924 mem0_316;
(*   %conv1.i.3.14.4 = sext i16 %924 to i32 *)
cast v_conv1_i_3_14_4@sint32 v924@sint16;
(*   %mul.i.3.14.4 = mul nsw i32 %conv1.i.3.14.4, 962 *)
mul v_mul_i_3_14_4 v_conv1_i_3_14_4 (962)@sint32;
(*   %call.i.3.14.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_4, v_call_i_3_14_4);
(*   %arrayidx11.3.14.4 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %925 = load i16, i16* %arrayidx11.3.14.4, align 2, !tbaa !3 *)
mov v925 mem0_284;
(*   %sub.3.14.4 = sub i16 %925, %call.i.3.14.4 *)
sub v_sub_3_14_4 v925 v_call_i_3_14_4;
(*   store i16 %sub.3.14.4, i16* %arrayidx9.3.14.4, align 2, !tbaa !3 *)
mov mem0_316 v_sub_3_14_4;
(*   %add21.3.14.4 = add i16 %925, %call.i.3.14.4 *)
add v_add21_3_14_4 v925 v_call_i_3_14_4;
(*   store i16 %add21.3.14.4, i16* %arrayidx11.3.14.4, align 2, !tbaa !3 *)
mov mem0_284 v_add21_3_14_4;
(*   %arrayidx9.3.15.4 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %926 = load i16, i16* %arrayidx9.3.15.4, align 2, !tbaa !3 *)
mov v926 mem0_318;
(*   %conv1.i.3.15.4 = sext i16 %926 to i32 *)
cast v_conv1_i_3_15_4@sint32 v926@sint16;
(*   %mul.i.3.15.4 = mul nsw i32 %conv1.i.3.15.4, 962 *)
mul v_mul_i_3_15_4 v_conv1_i_3_15_4 (962)@sint32;
(*   %call.i.3.15.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_4, v_call_i_3_15_4);
(*   %arrayidx11.3.15.4 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %927 = load i16, i16* %arrayidx11.3.15.4, align 2, !tbaa !3 *)
mov v927 mem0_286;
(*   %sub.3.15.4 = sub i16 %927, %call.i.3.15.4 *)
sub v_sub_3_15_4 v927 v_call_i_3_15_4;
(*   store i16 %sub.3.15.4, i16* %arrayidx9.3.15.4, align 2, !tbaa !3 *)
mov mem0_318 v_sub_3_15_4;
(*   %add21.3.15.4 = add i16 %927, %call.i.3.15.4 *)
add v_add21_3_15_4 v927 v_call_i_3_15_4;
(*   store i16 %add21.3.15.4, i16* %arrayidx11.3.15.4, align 2, !tbaa !3 *)
mov mem0_286 v_add21_3_15_4;

(* NOTE: k = 13 *)

(*   %arrayidx9.3.5218 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %928 = load i16, i16* %arrayidx9.3.5218, align 2, !tbaa !3 *)
mov v928 mem0_352;
(*   %conv1.i.3.5219 = sext i16 %928 to i32 *)
cast v_conv1_i_3_5219@sint32 v928@sint16;
(*   %mul.i.3.5220 = mul nsw i32 %conv1.i.3.5219, -1202 *)
mul v_mul_i_3_5220 v_conv1_i_3_5219 (-1202)@sint32;
(*   %call.i.3.5221 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5220) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5220, v_call_i_3_5221);
(*   %arrayidx11.3.5222 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %929 = load i16, i16* %arrayidx11.3.5222, align 2, !tbaa !3 *)
mov v929 mem0_320;
(*   %sub.3.5223 = sub i16 %929, %call.i.3.5221 *)
sub v_sub_3_5223 v929 v_call_i_3_5221;
(*   store i16 %sub.3.5223, i16* %arrayidx9.3.5218, align 2, !tbaa !3 *)
mov mem0_352 v_sub_3_5223;
(*   %add21.3.5224 = add i16 %929, %call.i.3.5221 *)
add v_add21_3_5224 v929 v_call_i_3_5221;
(*   store i16 %add21.3.5224, i16* %arrayidx11.3.5222, align 2, !tbaa !3 *)
mov mem0_320 v_add21_3_5224;
(*   %arrayidx9.3.1.5 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %930 = load i16, i16* %arrayidx9.3.1.5, align 2, !tbaa !3 *)
mov v930 mem0_354;
(*   %conv1.i.3.1.5 = sext i16 %930 to i32 *)
cast v_conv1_i_3_1_5@sint32 v930@sint16;
(*   %mul.i.3.1.5 = mul nsw i32 %conv1.i.3.1.5, -1202 *)
mul v_mul_i_3_1_5 v_conv1_i_3_1_5 (-1202)@sint32;
(*   %call.i.3.1.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_5, v_call_i_3_1_5);
(*   %arrayidx11.3.1.5 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %931 = load i16, i16* %arrayidx11.3.1.5, align 2, !tbaa !3 *)
mov v931 mem0_322;
(*   %sub.3.1.5 = sub i16 %931, %call.i.3.1.5 *)
sub v_sub_3_1_5 v931 v_call_i_3_1_5;
(*   store i16 %sub.3.1.5, i16* %arrayidx9.3.1.5, align 2, !tbaa !3 *)
mov mem0_354 v_sub_3_1_5;
(*   %add21.3.1.5 = add i16 %931, %call.i.3.1.5 *)
add v_add21_3_1_5 v931 v_call_i_3_1_5;
(*   store i16 %add21.3.1.5, i16* %arrayidx11.3.1.5, align 2, !tbaa !3 *)
mov mem0_322 v_add21_3_1_5;
(*   %arrayidx9.3.2.5 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %932 = load i16, i16* %arrayidx9.3.2.5, align 2, !tbaa !3 *)
mov v932 mem0_356;
(*   %conv1.i.3.2.5 = sext i16 %932 to i32 *)
cast v_conv1_i_3_2_5@sint32 v932@sint16;
(*   %mul.i.3.2.5 = mul nsw i32 %conv1.i.3.2.5, -1202 *)
mul v_mul_i_3_2_5 v_conv1_i_3_2_5 (-1202)@sint32;
(*   %call.i.3.2.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_5, v_call_i_3_2_5);
(*   %arrayidx11.3.2.5 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %933 = load i16, i16* %arrayidx11.3.2.5, align 2, !tbaa !3 *)
mov v933 mem0_324;
(*   %sub.3.2.5 = sub i16 %933, %call.i.3.2.5 *)
sub v_sub_3_2_5 v933 v_call_i_3_2_5;
(*   store i16 %sub.3.2.5, i16* %arrayidx9.3.2.5, align 2, !tbaa !3 *)
mov mem0_356 v_sub_3_2_5;
(*   %add21.3.2.5 = add i16 %933, %call.i.3.2.5 *)
add v_add21_3_2_5 v933 v_call_i_3_2_5;
(*   store i16 %add21.3.2.5, i16* %arrayidx11.3.2.5, align 2, !tbaa !3 *)
mov mem0_324 v_add21_3_2_5;
(*   %arrayidx9.3.3.5 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %934 = load i16, i16* %arrayidx9.3.3.5, align 2, !tbaa !3 *)
mov v934 mem0_358;
(*   %conv1.i.3.3.5 = sext i16 %934 to i32 *)
cast v_conv1_i_3_3_5@sint32 v934@sint16;
(*   %mul.i.3.3.5 = mul nsw i32 %conv1.i.3.3.5, -1202 *)
mul v_mul_i_3_3_5 v_conv1_i_3_3_5 (-1202)@sint32;
(*   %call.i.3.3.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_5, v_call_i_3_3_5);
(*   %arrayidx11.3.3.5 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %935 = load i16, i16* %arrayidx11.3.3.5, align 2, !tbaa !3 *)
mov v935 mem0_326;
(*   %sub.3.3.5 = sub i16 %935, %call.i.3.3.5 *)
sub v_sub_3_3_5 v935 v_call_i_3_3_5;
(*   store i16 %sub.3.3.5, i16* %arrayidx9.3.3.5, align 2, !tbaa !3 *)
mov mem0_358 v_sub_3_3_5;
(*   %add21.3.3.5 = add i16 %935, %call.i.3.3.5 *)
add v_add21_3_3_5 v935 v_call_i_3_3_5;
(*   store i16 %add21.3.3.5, i16* %arrayidx11.3.3.5, align 2, !tbaa !3 *)
mov mem0_326 v_add21_3_3_5;
(*   %arrayidx9.3.4.5 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %936 = load i16, i16* %arrayidx9.3.4.5, align 2, !tbaa !3 *)
mov v936 mem0_360;
(*   %conv1.i.3.4.5 = sext i16 %936 to i32 *)
cast v_conv1_i_3_4_5@sint32 v936@sint16;
(*   %mul.i.3.4.5 = mul nsw i32 %conv1.i.3.4.5, -1202 *)
mul v_mul_i_3_4_5 v_conv1_i_3_4_5 (-1202)@sint32;
(*   %call.i.3.4.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_5, v_call_i_3_4_5);
(*   %arrayidx11.3.4.5 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %937 = load i16, i16* %arrayidx11.3.4.5, align 2, !tbaa !3 *)
mov v937 mem0_328;
(*   %sub.3.4.5 = sub i16 %937, %call.i.3.4.5 *)
sub v_sub_3_4_5 v937 v_call_i_3_4_5;
(*   store i16 %sub.3.4.5, i16* %arrayidx9.3.4.5, align 2, !tbaa !3 *)
mov mem0_360 v_sub_3_4_5;
(*   %add21.3.4.5 = add i16 %937, %call.i.3.4.5 *)
add v_add21_3_4_5 v937 v_call_i_3_4_5;
(*   store i16 %add21.3.4.5, i16* %arrayidx11.3.4.5, align 2, !tbaa !3 *)
mov mem0_328 v_add21_3_4_5;
(*   %arrayidx9.3.5.5 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %938 = load i16, i16* %arrayidx9.3.5.5, align 2, !tbaa !3 *)
mov v938 mem0_362;
(*   %conv1.i.3.5.5 = sext i16 %938 to i32 *)
cast v_conv1_i_3_5_5@sint32 v938@sint16;
(*   %mul.i.3.5.5 = mul nsw i32 %conv1.i.3.5.5, -1202 *)
mul v_mul_i_3_5_5 v_conv1_i_3_5_5 (-1202)@sint32;
(*   %call.i.3.5.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_5, v_call_i_3_5_5);
(*   %arrayidx11.3.5.5 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %939 = load i16, i16* %arrayidx11.3.5.5, align 2, !tbaa !3 *)
mov v939 mem0_330;
(*   %sub.3.5.5 = sub i16 %939, %call.i.3.5.5 *)
sub v_sub_3_5_5 v939 v_call_i_3_5_5;
(*   store i16 %sub.3.5.5, i16* %arrayidx9.3.5.5, align 2, !tbaa !3 *)
mov mem0_362 v_sub_3_5_5;
(*   %add21.3.5.5 = add i16 %939, %call.i.3.5.5 *)
add v_add21_3_5_5 v939 v_call_i_3_5_5;
(*   store i16 %add21.3.5.5, i16* %arrayidx11.3.5.5, align 2, !tbaa !3 *)
mov mem0_330 v_add21_3_5_5;
(*   %arrayidx9.3.6.5 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %940 = load i16, i16* %arrayidx9.3.6.5, align 2, !tbaa !3 *)
mov v940 mem0_364;
(*   %conv1.i.3.6.5 = sext i16 %940 to i32 *)
cast v_conv1_i_3_6_5@sint32 v940@sint16;
(*   %mul.i.3.6.5 = mul nsw i32 %conv1.i.3.6.5, -1202 *)
mul v_mul_i_3_6_5 v_conv1_i_3_6_5 (-1202)@sint32;
(*   %call.i.3.6.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_5, v_call_i_3_6_5);
(*   %arrayidx11.3.6.5 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %941 = load i16, i16* %arrayidx11.3.6.5, align 2, !tbaa !3 *)
mov v941 mem0_332;
(*   %sub.3.6.5 = sub i16 %941, %call.i.3.6.5 *)
sub v_sub_3_6_5 v941 v_call_i_3_6_5;
(*   store i16 %sub.3.6.5, i16* %arrayidx9.3.6.5, align 2, !tbaa !3 *)
mov mem0_364 v_sub_3_6_5;
(*   %add21.3.6.5 = add i16 %941, %call.i.3.6.5 *)
add v_add21_3_6_5 v941 v_call_i_3_6_5;
(*   store i16 %add21.3.6.5, i16* %arrayidx11.3.6.5, align 2, !tbaa !3 *)
mov mem0_332 v_add21_3_6_5;
(*   %arrayidx9.3.7.5 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %942 = load i16, i16* %arrayidx9.3.7.5, align 2, !tbaa !3 *)
mov v942 mem0_366;
(*   %conv1.i.3.7.5 = sext i16 %942 to i32 *)
cast v_conv1_i_3_7_5@sint32 v942@sint16;
(*   %mul.i.3.7.5 = mul nsw i32 %conv1.i.3.7.5, -1202 *)
mul v_mul_i_3_7_5 v_conv1_i_3_7_5 (-1202)@sint32;
(*   %call.i.3.7.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_5, v_call_i_3_7_5);
(*   %arrayidx11.3.7.5 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %943 = load i16, i16* %arrayidx11.3.7.5, align 2, !tbaa !3 *)
mov v943 mem0_334;
(*   %sub.3.7.5 = sub i16 %943, %call.i.3.7.5 *)
sub v_sub_3_7_5 v943 v_call_i_3_7_5;
(*   store i16 %sub.3.7.5, i16* %arrayidx9.3.7.5, align 2, !tbaa !3 *)
mov mem0_366 v_sub_3_7_5;
(*   %add21.3.7.5 = add i16 %943, %call.i.3.7.5 *)
add v_add21_3_7_5 v943 v_call_i_3_7_5;
(*   store i16 %add21.3.7.5, i16* %arrayidx11.3.7.5, align 2, !tbaa !3 *)
mov mem0_334 v_add21_3_7_5;
(*   %arrayidx9.3.8.5 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %944 = load i16, i16* %arrayidx9.3.8.5, align 2, !tbaa !3 *)
mov v944 mem0_368;
(*   %conv1.i.3.8.5 = sext i16 %944 to i32 *)
cast v_conv1_i_3_8_5@sint32 v944@sint16;
(*   %mul.i.3.8.5 = mul nsw i32 %conv1.i.3.8.5, -1202 *)
mul v_mul_i_3_8_5 v_conv1_i_3_8_5 (-1202)@sint32;
(*   %call.i.3.8.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_5, v_call_i_3_8_5);
(*   %arrayidx11.3.8.5 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %945 = load i16, i16* %arrayidx11.3.8.5, align 2, !tbaa !3 *)
mov v945 mem0_336;
(*   %sub.3.8.5 = sub i16 %945, %call.i.3.8.5 *)
sub v_sub_3_8_5 v945 v_call_i_3_8_5;
(*   store i16 %sub.3.8.5, i16* %arrayidx9.3.8.5, align 2, !tbaa !3 *)
mov mem0_368 v_sub_3_8_5;
(*   %add21.3.8.5 = add i16 %945, %call.i.3.8.5 *)
add v_add21_3_8_5 v945 v_call_i_3_8_5;
(*   store i16 %add21.3.8.5, i16* %arrayidx11.3.8.5, align 2, !tbaa !3 *)
mov mem0_336 v_add21_3_8_5;
(*   %arrayidx9.3.9.5 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %946 = load i16, i16* %arrayidx9.3.9.5, align 2, !tbaa !3 *)
mov v946 mem0_370;
(*   %conv1.i.3.9.5 = sext i16 %946 to i32 *)
cast v_conv1_i_3_9_5@sint32 v946@sint16;
(*   %mul.i.3.9.5 = mul nsw i32 %conv1.i.3.9.5, -1202 *)
mul v_mul_i_3_9_5 v_conv1_i_3_9_5 (-1202)@sint32;
(*   %call.i.3.9.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_5, v_call_i_3_9_5);
(*   %arrayidx11.3.9.5 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %947 = load i16, i16* %arrayidx11.3.9.5, align 2, !tbaa !3 *)
mov v947 mem0_338;
(*   %sub.3.9.5 = sub i16 %947, %call.i.3.9.5 *)
sub v_sub_3_9_5 v947 v_call_i_3_9_5;
(*   store i16 %sub.3.9.5, i16* %arrayidx9.3.9.5, align 2, !tbaa !3 *)
mov mem0_370 v_sub_3_9_5;
(*   %add21.3.9.5 = add i16 %947, %call.i.3.9.5 *)
add v_add21_3_9_5 v947 v_call_i_3_9_5;
(*   store i16 %add21.3.9.5, i16* %arrayidx11.3.9.5, align 2, !tbaa !3 *)
mov mem0_338 v_add21_3_9_5;
(*   %arrayidx9.3.10.5 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %948 = load i16, i16* %arrayidx9.3.10.5, align 2, !tbaa !3 *)
mov v948 mem0_372;
(*   %conv1.i.3.10.5 = sext i16 %948 to i32 *)
cast v_conv1_i_3_10_5@sint32 v948@sint16;
(*   %mul.i.3.10.5 = mul nsw i32 %conv1.i.3.10.5, -1202 *)
mul v_mul_i_3_10_5 v_conv1_i_3_10_5 (-1202)@sint32;
(*   %call.i.3.10.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_5, v_call_i_3_10_5);
(*   %arrayidx11.3.10.5 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %949 = load i16, i16* %arrayidx11.3.10.5, align 2, !tbaa !3 *)
mov v949 mem0_340;
(*   %sub.3.10.5 = sub i16 %949, %call.i.3.10.5 *)
sub v_sub_3_10_5 v949 v_call_i_3_10_5;
(*   store i16 %sub.3.10.5, i16* %arrayidx9.3.10.5, align 2, !tbaa !3 *)
mov mem0_372 v_sub_3_10_5;
(*   %add21.3.10.5 = add i16 %949, %call.i.3.10.5 *)
add v_add21_3_10_5 v949 v_call_i_3_10_5;
(*   store i16 %add21.3.10.5, i16* %arrayidx11.3.10.5, align 2, !tbaa !3 *)
mov mem0_340 v_add21_3_10_5;
(*   %arrayidx9.3.11.5 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %950 = load i16, i16* %arrayidx9.3.11.5, align 2, !tbaa !3 *)
mov v950 mem0_374;
(*   %conv1.i.3.11.5 = sext i16 %950 to i32 *)
cast v_conv1_i_3_11_5@sint32 v950@sint16;
(*   %mul.i.3.11.5 = mul nsw i32 %conv1.i.3.11.5, -1202 *)
mul v_mul_i_3_11_5 v_conv1_i_3_11_5 (-1202)@sint32;
(*   %call.i.3.11.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_5, v_call_i_3_11_5);
(*   %arrayidx11.3.11.5 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %951 = load i16, i16* %arrayidx11.3.11.5, align 2, !tbaa !3 *)
mov v951 mem0_342;
(*   %sub.3.11.5 = sub i16 %951, %call.i.3.11.5 *)
sub v_sub_3_11_5 v951 v_call_i_3_11_5;
(*   store i16 %sub.3.11.5, i16* %arrayidx9.3.11.5, align 2, !tbaa !3 *)
mov mem0_374 v_sub_3_11_5;
(*   %add21.3.11.5 = add i16 %951, %call.i.3.11.5 *)
add v_add21_3_11_5 v951 v_call_i_3_11_5;
(*   store i16 %add21.3.11.5, i16* %arrayidx11.3.11.5, align 2, !tbaa !3 *)
mov mem0_342 v_add21_3_11_5;
(*   %arrayidx9.3.12.5 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %952 = load i16, i16* %arrayidx9.3.12.5, align 2, !tbaa !3 *)
mov v952 mem0_376;
(*   %conv1.i.3.12.5 = sext i16 %952 to i32 *)
cast v_conv1_i_3_12_5@sint32 v952@sint16;
(*   %mul.i.3.12.5 = mul nsw i32 %conv1.i.3.12.5, -1202 *)
mul v_mul_i_3_12_5 v_conv1_i_3_12_5 (-1202)@sint32;
(*   %call.i.3.12.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_5, v_call_i_3_12_5);
(*   %arrayidx11.3.12.5 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %953 = load i16, i16* %arrayidx11.3.12.5, align 2, !tbaa !3 *)
mov v953 mem0_344;
(*   %sub.3.12.5 = sub i16 %953, %call.i.3.12.5 *)
sub v_sub_3_12_5 v953 v_call_i_3_12_5;
(*   store i16 %sub.3.12.5, i16* %arrayidx9.3.12.5, align 2, !tbaa !3 *)
mov mem0_376 v_sub_3_12_5;
(*   %add21.3.12.5 = add i16 %953, %call.i.3.12.5 *)
add v_add21_3_12_5 v953 v_call_i_3_12_5;
(*   store i16 %add21.3.12.5, i16* %arrayidx11.3.12.5, align 2, !tbaa !3 *)
mov mem0_344 v_add21_3_12_5;
(*   %arrayidx9.3.13.5 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %954 = load i16, i16* %arrayidx9.3.13.5, align 2, !tbaa !3 *)
mov v954 mem0_378;
(*   %conv1.i.3.13.5 = sext i16 %954 to i32 *)
cast v_conv1_i_3_13_5@sint32 v954@sint16;
(*   %mul.i.3.13.5 = mul nsw i32 %conv1.i.3.13.5, -1202 *)
mul v_mul_i_3_13_5 v_conv1_i_3_13_5 (-1202)@sint32;
(*   %call.i.3.13.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_5, v_call_i_3_13_5);
(*   %arrayidx11.3.13.5 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %955 = load i16, i16* %arrayidx11.3.13.5, align 2, !tbaa !3 *)
mov v955 mem0_346;
(*   %sub.3.13.5 = sub i16 %955, %call.i.3.13.5 *)
sub v_sub_3_13_5 v955 v_call_i_3_13_5;
(*   store i16 %sub.3.13.5, i16* %arrayidx9.3.13.5, align 2, !tbaa !3 *)
mov mem0_378 v_sub_3_13_5;
(*   %add21.3.13.5 = add i16 %955, %call.i.3.13.5 *)
add v_add21_3_13_5 v955 v_call_i_3_13_5;
(*   store i16 %add21.3.13.5, i16* %arrayidx11.3.13.5, align 2, !tbaa !3 *)
mov mem0_346 v_add21_3_13_5;
(*   %arrayidx9.3.14.5 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %956 = load i16, i16* %arrayidx9.3.14.5, align 2, !tbaa !3 *)
mov v956 mem0_380;
(*   %conv1.i.3.14.5 = sext i16 %956 to i32 *)
cast v_conv1_i_3_14_5@sint32 v956@sint16;
(*   %mul.i.3.14.5 = mul nsw i32 %conv1.i.3.14.5, -1202 *)
mul v_mul_i_3_14_5 v_conv1_i_3_14_5 (-1202)@sint32;
(*   %call.i.3.14.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_5, v_call_i_3_14_5);
(*   %arrayidx11.3.14.5 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %957 = load i16, i16* %arrayidx11.3.14.5, align 2, !tbaa !3 *)
mov v957 mem0_348;
(*   %sub.3.14.5 = sub i16 %957, %call.i.3.14.5 *)
sub v_sub_3_14_5 v957 v_call_i_3_14_5;
(*   store i16 %sub.3.14.5, i16* %arrayidx9.3.14.5, align 2, !tbaa !3 *)
mov mem0_380 v_sub_3_14_5;
(*   %add21.3.14.5 = add i16 %957, %call.i.3.14.5 *)
add v_add21_3_14_5 v957 v_call_i_3_14_5;
(*   store i16 %add21.3.14.5, i16* %arrayidx11.3.14.5, align 2, !tbaa !3 *)
mov mem0_348 v_add21_3_14_5;
(*   %arrayidx9.3.15.5 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %958 = load i16, i16* %arrayidx9.3.15.5, align 2, !tbaa !3 *)
mov v958 mem0_382;
(*   %conv1.i.3.15.5 = sext i16 %958 to i32 *)
cast v_conv1_i_3_15_5@sint32 v958@sint16;
(*   %mul.i.3.15.5 = mul nsw i32 %conv1.i.3.15.5, -1202 *)
mul v_mul_i_3_15_5 v_conv1_i_3_15_5 (-1202)@sint32;
(*   %call.i.3.15.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_5, v_call_i_3_15_5);
(*   %arrayidx11.3.15.5 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %959 = load i16, i16* %arrayidx11.3.15.5, align 2, !tbaa !3 *)
mov v959 mem0_350;
(*   %sub.3.15.5 = sub i16 %959, %call.i.3.15.5 *)
sub v_sub_3_15_5 v959 v_call_i_3_15_5;
(*   store i16 %sub.3.15.5, i16* %arrayidx9.3.15.5, align 2, !tbaa !3 *)
mov mem0_382 v_sub_3_15_5;
(*   %add21.3.15.5 = add i16 %959, %call.i.3.15.5 *)
add v_add21_3_15_5 v959 v_call_i_3_15_5;
(*   store i16 %add21.3.15.5, i16* %arrayidx11.3.15.5, align 2, !tbaa !3 *)
mov mem0_350 v_add21_3_15_5;

(* NOTE: k = 14 *)

(*   %arrayidx9.3.6228 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %960 = load i16, i16* %arrayidx9.3.6228, align 2, !tbaa !3 *)
mov v960 mem0_416;
(*   %conv1.i.3.6229 = sext i16 %960 to i32 *)
cast v_conv1_i_3_6229@sint32 v960@sint16;
(*   %mul.i.3.6230 = mul nsw i32 %conv1.i.3.6229, -1474 *)
mul v_mul_i_3_6230 v_conv1_i_3_6229 (-1474)@sint32;
(*   %call.i.3.6231 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6230) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6230, v_call_i_3_6231);
(*   %arrayidx11.3.6232 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %961 = load i16, i16* %arrayidx11.3.6232, align 2, !tbaa !3 *)
mov v961 mem0_384;
(*   %sub.3.6233 = sub i16 %961, %call.i.3.6231 *)
sub v_sub_3_6233 v961 v_call_i_3_6231;
(*   store i16 %sub.3.6233, i16* %arrayidx9.3.6228, align 2, !tbaa !3 *)
mov mem0_416 v_sub_3_6233;
(*   %add21.3.6234 = add i16 %961, %call.i.3.6231 *)
add v_add21_3_6234 v961 v_call_i_3_6231;
(*   store i16 %add21.3.6234, i16* %arrayidx11.3.6232, align 2, !tbaa !3 *)
mov mem0_384 v_add21_3_6234;
(*   %arrayidx9.3.1.6 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %962 = load i16, i16* %arrayidx9.3.1.6, align 2, !tbaa !3 *)
mov v962 mem0_418;
(*   %conv1.i.3.1.6 = sext i16 %962 to i32 *)
cast v_conv1_i_3_1_6@sint32 v962@sint16;
(*   %mul.i.3.1.6 = mul nsw i32 %conv1.i.3.1.6, -1474 *)
mul v_mul_i_3_1_6 v_conv1_i_3_1_6 (-1474)@sint32;
(*   %call.i.3.1.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_6, v_call_i_3_1_6);
(*   %arrayidx11.3.1.6 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %963 = load i16, i16* %arrayidx11.3.1.6, align 2, !tbaa !3 *)
mov v963 mem0_386;
(*   %sub.3.1.6 = sub i16 %963, %call.i.3.1.6 *)
sub v_sub_3_1_6 v963 v_call_i_3_1_6;
(*   store i16 %sub.3.1.6, i16* %arrayidx9.3.1.6, align 2, !tbaa !3 *)
mov mem0_418 v_sub_3_1_6;
(*   %add21.3.1.6 = add i16 %963, %call.i.3.1.6 *)
add v_add21_3_1_6 v963 v_call_i_3_1_6;
(*   store i16 %add21.3.1.6, i16* %arrayidx11.3.1.6, align 2, !tbaa !3 *)
mov mem0_386 v_add21_3_1_6;
(*   %arrayidx9.3.2.6 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %964 = load i16, i16* %arrayidx9.3.2.6, align 2, !tbaa !3 *)
mov v964 mem0_420;
(*   %conv1.i.3.2.6 = sext i16 %964 to i32 *)
cast v_conv1_i_3_2_6@sint32 v964@sint16;
(*   %mul.i.3.2.6 = mul nsw i32 %conv1.i.3.2.6, -1474 *)
mul v_mul_i_3_2_6 v_conv1_i_3_2_6 (-1474)@sint32;
(*   %call.i.3.2.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_6, v_call_i_3_2_6);
(*   %arrayidx11.3.2.6 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %965 = load i16, i16* %arrayidx11.3.2.6, align 2, !tbaa !3 *)
mov v965 mem0_388;
(*   %sub.3.2.6 = sub i16 %965, %call.i.3.2.6 *)
sub v_sub_3_2_6 v965 v_call_i_3_2_6;
(*   store i16 %sub.3.2.6, i16* %arrayidx9.3.2.6, align 2, !tbaa !3 *)
mov mem0_420 v_sub_3_2_6;
(*   %add21.3.2.6 = add i16 %965, %call.i.3.2.6 *)
add v_add21_3_2_6 v965 v_call_i_3_2_6;
(*   store i16 %add21.3.2.6, i16* %arrayidx11.3.2.6, align 2, !tbaa !3 *)
mov mem0_388 v_add21_3_2_6;
(*   %arrayidx9.3.3.6 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %966 = load i16, i16* %arrayidx9.3.3.6, align 2, !tbaa !3 *)
mov v966 mem0_422;
(*   %conv1.i.3.3.6 = sext i16 %966 to i32 *)
cast v_conv1_i_3_3_6@sint32 v966@sint16;
(*   %mul.i.3.3.6 = mul nsw i32 %conv1.i.3.3.6, -1474 *)
mul v_mul_i_3_3_6 v_conv1_i_3_3_6 (-1474)@sint32;
(*   %call.i.3.3.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_6, v_call_i_3_3_6);
(*   %arrayidx11.3.3.6 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %967 = load i16, i16* %arrayidx11.3.3.6, align 2, !tbaa !3 *)
mov v967 mem0_390;
(*   %sub.3.3.6 = sub i16 %967, %call.i.3.3.6 *)
sub v_sub_3_3_6 v967 v_call_i_3_3_6;
(*   store i16 %sub.3.3.6, i16* %arrayidx9.3.3.6, align 2, !tbaa !3 *)
mov mem0_422 v_sub_3_3_6;
(*   %add21.3.3.6 = add i16 %967, %call.i.3.3.6 *)
add v_add21_3_3_6 v967 v_call_i_3_3_6;
(*   store i16 %add21.3.3.6, i16* %arrayidx11.3.3.6, align 2, !tbaa !3 *)
mov mem0_390 v_add21_3_3_6;
(*   %arrayidx9.3.4.6 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %968 = load i16, i16* %arrayidx9.3.4.6, align 2, !tbaa !3 *)
mov v968 mem0_424;
(*   %conv1.i.3.4.6 = sext i16 %968 to i32 *)
cast v_conv1_i_3_4_6@sint32 v968@sint16;
(*   %mul.i.3.4.6 = mul nsw i32 %conv1.i.3.4.6, -1474 *)
mul v_mul_i_3_4_6 v_conv1_i_3_4_6 (-1474)@sint32;
(*   %call.i.3.4.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_6, v_call_i_3_4_6);
(*   %arrayidx11.3.4.6 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %969 = load i16, i16* %arrayidx11.3.4.6, align 2, !tbaa !3 *)
mov v969 mem0_392;
(*   %sub.3.4.6 = sub i16 %969, %call.i.3.4.6 *)
sub v_sub_3_4_6 v969 v_call_i_3_4_6;
(*   store i16 %sub.3.4.6, i16* %arrayidx9.3.4.6, align 2, !tbaa !3 *)
mov mem0_424 v_sub_3_4_6;
(*   %add21.3.4.6 = add i16 %969, %call.i.3.4.6 *)
add v_add21_3_4_6 v969 v_call_i_3_4_6;
(*   store i16 %add21.3.4.6, i16* %arrayidx11.3.4.6, align 2, !tbaa !3 *)
mov mem0_392 v_add21_3_4_6;
(*   %arrayidx9.3.5.6 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %970 = load i16, i16* %arrayidx9.3.5.6, align 2, !tbaa !3 *)
mov v970 mem0_426;
(*   %conv1.i.3.5.6 = sext i16 %970 to i32 *)
cast v_conv1_i_3_5_6@sint32 v970@sint16;
(*   %mul.i.3.5.6 = mul nsw i32 %conv1.i.3.5.6, -1474 *)
mul v_mul_i_3_5_6 v_conv1_i_3_5_6 (-1474)@sint32;
(*   %call.i.3.5.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_6, v_call_i_3_5_6);
(*   %arrayidx11.3.5.6 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %971 = load i16, i16* %arrayidx11.3.5.6, align 2, !tbaa !3 *)
mov v971 mem0_394;
(*   %sub.3.5.6 = sub i16 %971, %call.i.3.5.6 *)
sub v_sub_3_5_6 v971 v_call_i_3_5_6;
(*   store i16 %sub.3.5.6, i16* %arrayidx9.3.5.6, align 2, !tbaa !3 *)
mov mem0_426 v_sub_3_5_6;
(*   %add21.3.5.6 = add i16 %971, %call.i.3.5.6 *)
add v_add21_3_5_6 v971 v_call_i_3_5_6;
(*   store i16 %add21.3.5.6, i16* %arrayidx11.3.5.6, align 2, !tbaa !3 *)
mov mem0_394 v_add21_3_5_6;
(*   %arrayidx9.3.6.6 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %972 = load i16, i16* %arrayidx9.3.6.6, align 2, !tbaa !3 *)
mov v972 mem0_428;
(*   %conv1.i.3.6.6 = sext i16 %972 to i32 *)
cast v_conv1_i_3_6_6@sint32 v972@sint16;
(*   %mul.i.3.6.6 = mul nsw i32 %conv1.i.3.6.6, -1474 *)
mul v_mul_i_3_6_6 v_conv1_i_3_6_6 (-1474)@sint32;
(*   %call.i.3.6.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_6, v_call_i_3_6_6);
(*   %arrayidx11.3.6.6 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %973 = load i16, i16* %arrayidx11.3.6.6, align 2, !tbaa !3 *)
mov v973 mem0_396;
(*   %sub.3.6.6 = sub i16 %973, %call.i.3.6.6 *)
sub v_sub_3_6_6 v973 v_call_i_3_6_6;
(*   store i16 %sub.3.6.6, i16* %arrayidx9.3.6.6, align 2, !tbaa !3 *)
mov mem0_428 v_sub_3_6_6;
(*   %add21.3.6.6 = add i16 %973, %call.i.3.6.6 *)
add v_add21_3_6_6 v973 v_call_i_3_6_6;
(*   store i16 %add21.3.6.6, i16* %arrayidx11.3.6.6, align 2, !tbaa !3 *)
mov mem0_396 v_add21_3_6_6;
(*   %arrayidx9.3.7.6 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %974 = load i16, i16* %arrayidx9.3.7.6, align 2, !tbaa !3 *)
mov v974 mem0_430;
(*   %conv1.i.3.7.6 = sext i16 %974 to i32 *)
cast v_conv1_i_3_7_6@sint32 v974@sint16;
(*   %mul.i.3.7.6 = mul nsw i32 %conv1.i.3.7.6, -1474 *)
mul v_mul_i_3_7_6 v_conv1_i_3_7_6 (-1474)@sint32;
(*   %call.i.3.7.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_6, v_call_i_3_7_6);
(*   %arrayidx11.3.7.6 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %975 = load i16, i16* %arrayidx11.3.7.6, align 2, !tbaa !3 *)
mov v975 mem0_398;
(*   %sub.3.7.6 = sub i16 %975, %call.i.3.7.6 *)
sub v_sub_3_7_6 v975 v_call_i_3_7_6;
(*   store i16 %sub.3.7.6, i16* %arrayidx9.3.7.6, align 2, !tbaa !3 *)
mov mem0_430 v_sub_3_7_6;
(*   %add21.3.7.6 = add i16 %975, %call.i.3.7.6 *)
add v_add21_3_7_6 v975 v_call_i_3_7_6;
(*   store i16 %add21.3.7.6, i16* %arrayidx11.3.7.6, align 2, !tbaa !3 *)
mov mem0_398 v_add21_3_7_6;
(*   %arrayidx9.3.8.6 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %976 = load i16, i16* %arrayidx9.3.8.6, align 2, !tbaa !3 *)
mov v976 mem0_432;
(*   %conv1.i.3.8.6 = sext i16 %976 to i32 *)
cast v_conv1_i_3_8_6@sint32 v976@sint16;
(*   %mul.i.3.8.6 = mul nsw i32 %conv1.i.3.8.6, -1474 *)
mul v_mul_i_3_8_6 v_conv1_i_3_8_6 (-1474)@sint32;
(*   %call.i.3.8.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_6, v_call_i_3_8_6);
(*   %arrayidx11.3.8.6 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %977 = load i16, i16* %arrayidx11.3.8.6, align 2, !tbaa !3 *)
mov v977 mem0_400;
(*   %sub.3.8.6 = sub i16 %977, %call.i.3.8.6 *)
sub v_sub_3_8_6 v977 v_call_i_3_8_6;
(*   store i16 %sub.3.8.6, i16* %arrayidx9.3.8.6, align 2, !tbaa !3 *)
mov mem0_432 v_sub_3_8_6;
(*   %add21.3.8.6 = add i16 %977, %call.i.3.8.6 *)
add v_add21_3_8_6 v977 v_call_i_3_8_6;
(*   store i16 %add21.3.8.6, i16* %arrayidx11.3.8.6, align 2, !tbaa !3 *)
mov mem0_400 v_add21_3_8_6;
(*   %arrayidx9.3.9.6 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %978 = load i16, i16* %arrayidx9.3.9.6, align 2, !tbaa !3 *)
mov v978 mem0_434;
(*   %conv1.i.3.9.6 = sext i16 %978 to i32 *)
cast v_conv1_i_3_9_6@sint32 v978@sint16;
(*   %mul.i.3.9.6 = mul nsw i32 %conv1.i.3.9.6, -1474 *)
mul v_mul_i_3_9_6 v_conv1_i_3_9_6 (-1474)@sint32;
(*   %call.i.3.9.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_6, v_call_i_3_9_6);
(*   %arrayidx11.3.9.6 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %979 = load i16, i16* %arrayidx11.3.9.6, align 2, !tbaa !3 *)
mov v979 mem0_402;
(*   %sub.3.9.6 = sub i16 %979, %call.i.3.9.6 *)
sub v_sub_3_9_6 v979 v_call_i_3_9_6;
(*   store i16 %sub.3.9.6, i16* %arrayidx9.3.9.6, align 2, !tbaa !3 *)
mov mem0_434 v_sub_3_9_6;
(*   %add21.3.9.6 = add i16 %979, %call.i.3.9.6 *)
add v_add21_3_9_6 v979 v_call_i_3_9_6;
(*   store i16 %add21.3.9.6, i16* %arrayidx11.3.9.6, align 2, !tbaa !3 *)
mov mem0_402 v_add21_3_9_6;
(*   %arrayidx9.3.10.6 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %980 = load i16, i16* %arrayidx9.3.10.6, align 2, !tbaa !3 *)
mov v980 mem0_436;
(*   %conv1.i.3.10.6 = sext i16 %980 to i32 *)
cast v_conv1_i_3_10_6@sint32 v980@sint16;
(*   %mul.i.3.10.6 = mul nsw i32 %conv1.i.3.10.6, -1474 *)
mul v_mul_i_3_10_6 v_conv1_i_3_10_6 (-1474)@sint32;
(*   %call.i.3.10.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_6, v_call_i_3_10_6);
(*   %arrayidx11.3.10.6 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %981 = load i16, i16* %arrayidx11.3.10.6, align 2, !tbaa !3 *)
mov v981 mem0_404;
(*   %sub.3.10.6 = sub i16 %981, %call.i.3.10.6 *)
sub v_sub_3_10_6 v981 v_call_i_3_10_6;
(*   store i16 %sub.3.10.6, i16* %arrayidx9.3.10.6, align 2, !tbaa !3 *)
mov mem0_436 v_sub_3_10_6;
(*   %add21.3.10.6 = add i16 %981, %call.i.3.10.6 *)
add v_add21_3_10_6 v981 v_call_i_3_10_6;
(*   store i16 %add21.3.10.6, i16* %arrayidx11.3.10.6, align 2, !tbaa !3 *)
mov mem0_404 v_add21_3_10_6;
(*   %arrayidx9.3.11.6 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %982 = load i16, i16* %arrayidx9.3.11.6, align 2, !tbaa !3 *)
mov v982 mem0_438;
(*   %conv1.i.3.11.6 = sext i16 %982 to i32 *)
cast v_conv1_i_3_11_6@sint32 v982@sint16;
(*   %mul.i.3.11.6 = mul nsw i32 %conv1.i.3.11.6, -1474 *)
mul v_mul_i_3_11_6 v_conv1_i_3_11_6 (-1474)@sint32;
(*   %call.i.3.11.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_6, v_call_i_3_11_6);
(*   %arrayidx11.3.11.6 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %983 = load i16, i16* %arrayidx11.3.11.6, align 2, !tbaa !3 *)
mov v983 mem0_406;
(*   %sub.3.11.6 = sub i16 %983, %call.i.3.11.6 *)
sub v_sub_3_11_6 v983 v_call_i_3_11_6;
(*   store i16 %sub.3.11.6, i16* %arrayidx9.3.11.6, align 2, !tbaa !3 *)
mov mem0_438 v_sub_3_11_6;
(*   %add21.3.11.6 = add i16 %983, %call.i.3.11.6 *)
add v_add21_3_11_6 v983 v_call_i_3_11_6;
(*   store i16 %add21.3.11.6, i16* %arrayidx11.3.11.6, align 2, !tbaa !3 *)
mov mem0_406 v_add21_3_11_6;
(*   %arrayidx9.3.12.6 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %984 = load i16, i16* %arrayidx9.3.12.6, align 2, !tbaa !3 *)
mov v984 mem0_440;
(*   %conv1.i.3.12.6 = sext i16 %984 to i32 *)
cast v_conv1_i_3_12_6@sint32 v984@sint16;
(*   %mul.i.3.12.6 = mul nsw i32 %conv1.i.3.12.6, -1474 *)
mul v_mul_i_3_12_6 v_conv1_i_3_12_6 (-1474)@sint32;
(*   %call.i.3.12.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_6, v_call_i_3_12_6);
(*   %arrayidx11.3.12.6 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %985 = load i16, i16* %arrayidx11.3.12.6, align 2, !tbaa !3 *)
mov v985 mem0_408;
(*   %sub.3.12.6 = sub i16 %985, %call.i.3.12.6 *)
sub v_sub_3_12_6 v985 v_call_i_3_12_6;
(*   store i16 %sub.3.12.6, i16* %arrayidx9.3.12.6, align 2, !tbaa !3 *)
mov mem0_440 v_sub_3_12_6;
(*   %add21.3.12.6 = add i16 %985, %call.i.3.12.6 *)
add v_add21_3_12_6 v985 v_call_i_3_12_6;
(*   store i16 %add21.3.12.6, i16* %arrayidx11.3.12.6, align 2, !tbaa !3 *)
mov mem0_408 v_add21_3_12_6;
(*   %arrayidx9.3.13.6 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %986 = load i16, i16* %arrayidx9.3.13.6, align 2, !tbaa !3 *)
mov v986 mem0_442;
(*   %conv1.i.3.13.6 = sext i16 %986 to i32 *)
cast v_conv1_i_3_13_6@sint32 v986@sint16;
(*   %mul.i.3.13.6 = mul nsw i32 %conv1.i.3.13.6, -1474 *)
mul v_mul_i_3_13_6 v_conv1_i_3_13_6 (-1474)@sint32;
(*   %call.i.3.13.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_6, v_call_i_3_13_6);
(*   %arrayidx11.3.13.6 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %987 = load i16, i16* %arrayidx11.3.13.6, align 2, !tbaa !3 *)
mov v987 mem0_410;
(*   %sub.3.13.6 = sub i16 %987, %call.i.3.13.6 *)
sub v_sub_3_13_6 v987 v_call_i_3_13_6;
(*   store i16 %sub.3.13.6, i16* %arrayidx9.3.13.6, align 2, !tbaa !3 *)
mov mem0_442 v_sub_3_13_6;
(*   %add21.3.13.6 = add i16 %987, %call.i.3.13.6 *)
add v_add21_3_13_6 v987 v_call_i_3_13_6;
(*   store i16 %add21.3.13.6, i16* %arrayidx11.3.13.6, align 2, !tbaa !3 *)
mov mem0_410 v_add21_3_13_6;
(*   %arrayidx9.3.14.6 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %988 = load i16, i16* %arrayidx9.3.14.6, align 2, !tbaa !3 *)
mov v988 mem0_444;
(*   %conv1.i.3.14.6 = sext i16 %988 to i32 *)
cast v_conv1_i_3_14_6@sint32 v988@sint16;
(*   %mul.i.3.14.6 = mul nsw i32 %conv1.i.3.14.6, -1474 *)
mul v_mul_i_3_14_6 v_conv1_i_3_14_6 (-1474)@sint32;
(*   %call.i.3.14.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_6, v_call_i_3_14_6);
(*   %arrayidx11.3.14.6 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %989 = load i16, i16* %arrayidx11.3.14.6, align 2, !tbaa !3 *)
mov v989 mem0_412;
(*   %sub.3.14.6 = sub i16 %989, %call.i.3.14.6 *)
sub v_sub_3_14_6 v989 v_call_i_3_14_6;
(*   store i16 %sub.3.14.6, i16* %arrayidx9.3.14.6, align 2, !tbaa !3 *)
mov mem0_444 v_sub_3_14_6;
(*   %add21.3.14.6 = add i16 %989, %call.i.3.14.6 *)
add v_add21_3_14_6 v989 v_call_i_3_14_6;
(*   store i16 %add21.3.14.6, i16* %arrayidx11.3.14.6, align 2, !tbaa !3 *)
mov mem0_412 v_add21_3_14_6;
(*   %arrayidx9.3.15.6 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %990 = load i16, i16* %arrayidx9.3.15.6, align 2, !tbaa !3 *)
mov v990 mem0_446;
(*   %conv1.i.3.15.6 = sext i16 %990 to i32 *)
cast v_conv1_i_3_15_6@sint32 v990@sint16;
(*   %mul.i.3.15.6 = mul nsw i32 %conv1.i.3.15.6, -1474 *)
mul v_mul_i_3_15_6 v_conv1_i_3_15_6 (-1474)@sint32;
(*   %call.i.3.15.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_6, v_call_i_3_15_6);
(*   %arrayidx11.3.15.6 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %991 = load i16, i16* %arrayidx11.3.15.6, align 2, !tbaa !3 *)
mov v991 mem0_414;
(*   %sub.3.15.6 = sub i16 %991, %call.i.3.15.6 *)
sub v_sub_3_15_6 v991 v_call_i_3_15_6;
(*   store i16 %sub.3.15.6, i16* %arrayidx9.3.15.6, align 2, !tbaa !3 *)
mov mem0_446 v_sub_3_15_6;
(*   %add21.3.15.6 = add i16 %991, %call.i.3.15.6 *)
add v_add21_3_15_6 v991 v_call_i_3_15_6;
(*   store i16 %add21.3.15.6, i16* %arrayidx11.3.15.6, align 2, !tbaa !3 *)
mov mem0_414 v_add21_3_15_6;

(* NOTE: k = 15 *)

(*   %arrayidx9.3.7238 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %992 = load i16, i16* %arrayidx9.3.7238, align 2, !tbaa !3 *)
mov v992 mem0_480;
(*   %conv1.i.3.7239 = sext i16 %992 to i32 *)
cast v_conv1_i_3_7239@sint32 v992@sint16;
(*   %mul.i.3.7240 = mul nsw i32 %conv1.i.3.7239, 1468 *)
mul v_mul_i_3_7240 v_conv1_i_3_7239 (1468)@sint32;
(*   %call.i.3.7241 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7240) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7240, v_call_i_3_7241);
(*   %arrayidx11.3.7242 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %993 = load i16, i16* %arrayidx11.3.7242, align 2, !tbaa !3 *)
mov v993 mem0_448;
(*   %sub.3.7243 = sub i16 %993, %call.i.3.7241 *)
sub v_sub_3_7243 v993 v_call_i_3_7241;
(*   store i16 %sub.3.7243, i16* %arrayidx9.3.7238, align 2, !tbaa !3 *)
mov mem0_480 v_sub_3_7243;
(*   %add21.3.7244 = add i16 %993, %call.i.3.7241 *)
add v_add21_3_7244 v993 v_call_i_3_7241;
(*   store i16 %add21.3.7244, i16* %arrayidx11.3.7242, align 2, !tbaa !3 *)
mov mem0_448 v_add21_3_7244;
(*   %arrayidx9.3.1.7 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %994 = load i16, i16* %arrayidx9.3.1.7, align 2, !tbaa !3 *)
mov v994 mem0_482;
(*   %conv1.i.3.1.7 = sext i16 %994 to i32 *)
cast v_conv1_i_3_1_7@sint32 v994@sint16;
(*   %mul.i.3.1.7 = mul nsw i32 %conv1.i.3.1.7, 1468 *)
mul v_mul_i_3_1_7 v_conv1_i_3_1_7 (1468)@sint32;
(*   %call.i.3.1.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.1.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_1_7, v_call_i_3_1_7);
(*   %arrayidx11.3.1.7 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %995 = load i16, i16* %arrayidx11.3.1.7, align 2, !tbaa !3 *)
mov v995 mem0_450;
(*   %sub.3.1.7 = sub i16 %995, %call.i.3.1.7 *)
sub v_sub_3_1_7 v995 v_call_i_3_1_7;
(*   store i16 %sub.3.1.7, i16* %arrayidx9.3.1.7, align 2, !tbaa !3 *)
mov mem0_482 v_sub_3_1_7;
(*   %add21.3.1.7 = add i16 %995, %call.i.3.1.7 *)
add v_add21_3_1_7 v995 v_call_i_3_1_7;
(*   store i16 %add21.3.1.7, i16* %arrayidx11.3.1.7, align 2, !tbaa !3 *)
mov mem0_450 v_add21_3_1_7;
(*   %arrayidx9.3.2.7 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %996 = load i16, i16* %arrayidx9.3.2.7, align 2, !tbaa !3 *)
mov v996 mem0_484;
(*   %conv1.i.3.2.7 = sext i16 %996 to i32 *)
cast v_conv1_i_3_2_7@sint32 v996@sint16;
(*   %mul.i.3.2.7 = mul nsw i32 %conv1.i.3.2.7, 1468 *)
mul v_mul_i_3_2_7 v_conv1_i_3_2_7 (1468)@sint32;
(*   %call.i.3.2.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.2.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_2_7, v_call_i_3_2_7);
(*   %arrayidx11.3.2.7 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %997 = load i16, i16* %arrayidx11.3.2.7, align 2, !tbaa !3 *)
mov v997 mem0_452;
(*   %sub.3.2.7 = sub i16 %997, %call.i.3.2.7 *)
sub v_sub_3_2_7 v997 v_call_i_3_2_7;
(*   store i16 %sub.3.2.7, i16* %arrayidx9.3.2.7, align 2, !tbaa !3 *)
mov mem0_484 v_sub_3_2_7;
(*   %add21.3.2.7 = add i16 %997, %call.i.3.2.7 *)
add v_add21_3_2_7 v997 v_call_i_3_2_7;
(*   store i16 %add21.3.2.7, i16* %arrayidx11.3.2.7, align 2, !tbaa !3 *)
mov mem0_452 v_add21_3_2_7;
(*   %arrayidx9.3.3.7 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %998 = load i16, i16* %arrayidx9.3.3.7, align 2, !tbaa !3 *)
mov v998 mem0_486;
(*   %conv1.i.3.3.7 = sext i16 %998 to i32 *)
cast v_conv1_i_3_3_7@sint32 v998@sint16;
(*   %mul.i.3.3.7 = mul nsw i32 %conv1.i.3.3.7, 1468 *)
mul v_mul_i_3_3_7 v_conv1_i_3_3_7 (1468)@sint32;
(*   %call.i.3.3.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.3.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_3_7, v_call_i_3_3_7);
(*   %arrayidx11.3.3.7 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %999 = load i16, i16* %arrayidx11.3.3.7, align 2, !tbaa !3 *)
mov v999 mem0_454;
(*   %sub.3.3.7 = sub i16 %999, %call.i.3.3.7 *)
sub v_sub_3_3_7 v999 v_call_i_3_3_7;
(*   store i16 %sub.3.3.7, i16* %arrayidx9.3.3.7, align 2, !tbaa !3 *)
mov mem0_486 v_sub_3_3_7;
(*   %add21.3.3.7 = add i16 %999, %call.i.3.3.7 *)
add v_add21_3_3_7 v999 v_call_i_3_3_7;
(*   store i16 %add21.3.3.7, i16* %arrayidx11.3.3.7, align 2, !tbaa !3 *)
mov mem0_454 v_add21_3_3_7;
(*   %arrayidx9.3.4.7 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %1000 = load i16, i16* %arrayidx9.3.4.7, align 2, !tbaa !3 *)
mov v1000 mem0_488;
(*   %conv1.i.3.4.7 = sext i16 %1000 to i32 *)
cast v_conv1_i_3_4_7@sint32 v1000@sint16;
(*   %mul.i.3.4.7 = mul nsw i32 %conv1.i.3.4.7, 1468 *)
mul v_mul_i_3_4_7 v_conv1_i_3_4_7 (1468)@sint32;
(*   %call.i.3.4.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.4.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_4_7, v_call_i_3_4_7);
(*   %arrayidx11.3.4.7 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %1001 = load i16, i16* %arrayidx11.3.4.7, align 2, !tbaa !3 *)
mov v1001 mem0_456;
(*   %sub.3.4.7 = sub i16 %1001, %call.i.3.4.7 *)
sub v_sub_3_4_7 v1001 v_call_i_3_4_7;
(*   store i16 %sub.3.4.7, i16* %arrayidx9.3.4.7, align 2, !tbaa !3 *)
mov mem0_488 v_sub_3_4_7;
(*   %add21.3.4.7 = add i16 %1001, %call.i.3.4.7 *)
add v_add21_3_4_7 v1001 v_call_i_3_4_7;
(*   store i16 %add21.3.4.7, i16* %arrayidx11.3.4.7, align 2, !tbaa !3 *)
mov mem0_456 v_add21_3_4_7;
(*   %arrayidx9.3.5.7 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %1002 = load i16, i16* %arrayidx9.3.5.7, align 2, !tbaa !3 *)
mov v1002 mem0_490;
(*   %conv1.i.3.5.7 = sext i16 %1002 to i32 *)
cast v_conv1_i_3_5_7@sint32 v1002@sint16;
(*   %mul.i.3.5.7 = mul nsw i32 %conv1.i.3.5.7, 1468 *)
mul v_mul_i_3_5_7 v_conv1_i_3_5_7 (1468)@sint32;
(*   %call.i.3.5.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.5.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_5_7, v_call_i_3_5_7);
(*   %arrayidx11.3.5.7 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %1003 = load i16, i16* %arrayidx11.3.5.7, align 2, !tbaa !3 *)
mov v1003 mem0_458;
(*   %sub.3.5.7 = sub i16 %1003, %call.i.3.5.7 *)
sub v_sub_3_5_7 v1003 v_call_i_3_5_7;
(*   store i16 %sub.3.5.7, i16* %arrayidx9.3.5.7, align 2, !tbaa !3 *)
mov mem0_490 v_sub_3_5_7;
(*   %add21.3.5.7 = add i16 %1003, %call.i.3.5.7 *)
add v_add21_3_5_7 v1003 v_call_i_3_5_7;
(*   store i16 %add21.3.5.7, i16* %arrayidx11.3.5.7, align 2, !tbaa !3 *)
mov mem0_458 v_add21_3_5_7;
(*   %arrayidx9.3.6.7 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %1004 = load i16, i16* %arrayidx9.3.6.7, align 2, !tbaa !3 *)
mov v1004 mem0_492;
(*   %conv1.i.3.6.7 = sext i16 %1004 to i32 *)
cast v_conv1_i_3_6_7@sint32 v1004@sint16;
(*   %mul.i.3.6.7 = mul nsw i32 %conv1.i.3.6.7, 1468 *)
mul v_mul_i_3_6_7 v_conv1_i_3_6_7 (1468)@sint32;
(*   %call.i.3.6.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.6.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_6_7, v_call_i_3_6_7);
(*   %arrayidx11.3.6.7 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %1005 = load i16, i16* %arrayidx11.3.6.7, align 2, !tbaa !3 *)
mov v1005 mem0_460;
(*   %sub.3.6.7 = sub i16 %1005, %call.i.3.6.7 *)
sub v_sub_3_6_7 v1005 v_call_i_3_6_7;
(*   store i16 %sub.3.6.7, i16* %arrayidx9.3.6.7, align 2, !tbaa !3 *)
mov mem0_492 v_sub_3_6_7;
(*   %add21.3.6.7 = add i16 %1005, %call.i.3.6.7 *)
add v_add21_3_6_7 v1005 v_call_i_3_6_7;
(*   store i16 %add21.3.6.7, i16* %arrayidx11.3.6.7, align 2, !tbaa !3 *)
mov mem0_460 v_add21_3_6_7;
(*   %arrayidx9.3.7.7 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %1006 = load i16, i16* %arrayidx9.3.7.7, align 2, !tbaa !3 *)
mov v1006 mem0_494;
(*   %conv1.i.3.7.7 = sext i16 %1006 to i32 *)
cast v_conv1_i_3_7_7@sint32 v1006@sint16;
(*   %mul.i.3.7.7 = mul nsw i32 %conv1.i.3.7.7, 1468 *)
mul v_mul_i_3_7_7 v_conv1_i_3_7_7 (1468)@sint32;
(*   %call.i.3.7.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.7.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_7_7, v_call_i_3_7_7);
(*   %arrayidx11.3.7.7 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %1007 = load i16, i16* %arrayidx11.3.7.7, align 2, !tbaa !3 *)
mov v1007 mem0_462;
(*   %sub.3.7.7 = sub i16 %1007, %call.i.3.7.7 *)
sub v_sub_3_7_7 v1007 v_call_i_3_7_7;
(*   store i16 %sub.3.7.7, i16* %arrayidx9.3.7.7, align 2, !tbaa !3 *)
mov mem0_494 v_sub_3_7_7;
(*   %add21.3.7.7 = add i16 %1007, %call.i.3.7.7 *)
add v_add21_3_7_7 v1007 v_call_i_3_7_7;
(*   store i16 %add21.3.7.7, i16* %arrayidx11.3.7.7, align 2, !tbaa !3 *)
mov mem0_462 v_add21_3_7_7;
(*   %arrayidx9.3.8.7 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %1008 = load i16, i16* %arrayidx9.3.8.7, align 2, !tbaa !3 *)
mov v1008 mem0_496;
(*   %conv1.i.3.8.7 = sext i16 %1008 to i32 *)
cast v_conv1_i_3_8_7@sint32 v1008@sint16;
(*   %mul.i.3.8.7 = mul nsw i32 %conv1.i.3.8.7, 1468 *)
mul v_mul_i_3_8_7 v_conv1_i_3_8_7 (1468)@sint32;
(*   %call.i.3.8.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.8.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_8_7, v_call_i_3_8_7);
(*   %arrayidx11.3.8.7 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %1009 = load i16, i16* %arrayidx11.3.8.7, align 2, !tbaa !3 *)
mov v1009 mem0_464;
(*   %sub.3.8.7 = sub i16 %1009, %call.i.3.8.7 *)
sub v_sub_3_8_7 v1009 v_call_i_3_8_7;
(*   store i16 %sub.3.8.7, i16* %arrayidx9.3.8.7, align 2, !tbaa !3 *)
mov mem0_496 v_sub_3_8_7;
(*   %add21.3.8.7 = add i16 %1009, %call.i.3.8.7 *)
add v_add21_3_8_7 v1009 v_call_i_3_8_7;
(*   store i16 %add21.3.8.7, i16* %arrayidx11.3.8.7, align 2, !tbaa !3 *)
mov mem0_464 v_add21_3_8_7;
(*   %arrayidx9.3.9.7 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %1010 = load i16, i16* %arrayidx9.3.9.7, align 2, !tbaa !3 *)
mov v1010 mem0_498;
(*   %conv1.i.3.9.7 = sext i16 %1010 to i32 *)
cast v_conv1_i_3_9_7@sint32 v1010@sint16;
(*   %mul.i.3.9.7 = mul nsw i32 %conv1.i.3.9.7, 1468 *)
mul v_mul_i_3_9_7 v_conv1_i_3_9_7 (1468)@sint32;
(*   %call.i.3.9.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.9.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_9_7, v_call_i_3_9_7);
(*   %arrayidx11.3.9.7 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %1011 = load i16, i16* %arrayidx11.3.9.7, align 2, !tbaa !3 *)
mov v1011 mem0_466;
(*   %sub.3.9.7 = sub i16 %1011, %call.i.3.9.7 *)
sub v_sub_3_9_7 v1011 v_call_i_3_9_7;
(*   store i16 %sub.3.9.7, i16* %arrayidx9.3.9.7, align 2, !tbaa !3 *)
mov mem0_498 v_sub_3_9_7;
(*   %add21.3.9.7 = add i16 %1011, %call.i.3.9.7 *)
add v_add21_3_9_7 v1011 v_call_i_3_9_7;
(*   store i16 %add21.3.9.7, i16* %arrayidx11.3.9.7, align 2, !tbaa !3 *)
mov mem0_466 v_add21_3_9_7;
(*   %arrayidx9.3.10.7 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %1012 = load i16, i16* %arrayidx9.3.10.7, align 2, !tbaa !3 *)
mov v1012 mem0_500;
(*   %conv1.i.3.10.7 = sext i16 %1012 to i32 *)
cast v_conv1_i_3_10_7@sint32 v1012@sint16;
(*   %mul.i.3.10.7 = mul nsw i32 %conv1.i.3.10.7, 1468 *)
mul v_mul_i_3_10_7 v_conv1_i_3_10_7 (1468)@sint32;
(*   %call.i.3.10.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.10.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_10_7, v_call_i_3_10_7);
(*   %arrayidx11.3.10.7 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %1013 = load i16, i16* %arrayidx11.3.10.7, align 2, !tbaa !3 *)
mov v1013 mem0_468;
(*   %sub.3.10.7 = sub i16 %1013, %call.i.3.10.7 *)
sub v_sub_3_10_7 v1013 v_call_i_3_10_7;
(*   store i16 %sub.3.10.7, i16* %arrayidx9.3.10.7, align 2, !tbaa !3 *)
mov mem0_500 v_sub_3_10_7;
(*   %add21.3.10.7 = add i16 %1013, %call.i.3.10.7 *)
add v_add21_3_10_7 v1013 v_call_i_3_10_7;
(*   store i16 %add21.3.10.7, i16* %arrayidx11.3.10.7, align 2, !tbaa !3 *)
mov mem0_468 v_add21_3_10_7;
(*   %arrayidx9.3.11.7 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %1014 = load i16, i16* %arrayidx9.3.11.7, align 2, !tbaa !3 *)
mov v1014 mem0_502;
(*   %conv1.i.3.11.7 = sext i16 %1014 to i32 *)
cast v_conv1_i_3_11_7@sint32 v1014@sint16;
(*   %mul.i.3.11.7 = mul nsw i32 %conv1.i.3.11.7, 1468 *)
mul v_mul_i_3_11_7 v_conv1_i_3_11_7 (1468)@sint32;
(*   %call.i.3.11.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.11.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_11_7, v_call_i_3_11_7);
(*   %arrayidx11.3.11.7 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %1015 = load i16, i16* %arrayidx11.3.11.7, align 2, !tbaa !3 *)
mov v1015 mem0_470;
(*   %sub.3.11.7 = sub i16 %1015, %call.i.3.11.7 *)
sub v_sub_3_11_7 v1015 v_call_i_3_11_7;
(*   store i16 %sub.3.11.7, i16* %arrayidx9.3.11.7, align 2, !tbaa !3 *)
mov mem0_502 v_sub_3_11_7;
(*   %add21.3.11.7 = add i16 %1015, %call.i.3.11.7 *)
add v_add21_3_11_7 v1015 v_call_i_3_11_7;
(*   store i16 %add21.3.11.7, i16* %arrayidx11.3.11.7, align 2, !tbaa !3 *)
mov mem0_470 v_add21_3_11_7;
(*   %arrayidx9.3.12.7 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %1016 = load i16, i16* %arrayidx9.3.12.7, align 2, !tbaa !3 *)
mov v1016 mem0_504;
(*   %conv1.i.3.12.7 = sext i16 %1016 to i32 *)
cast v_conv1_i_3_12_7@sint32 v1016@sint16;
(*   %mul.i.3.12.7 = mul nsw i32 %conv1.i.3.12.7, 1468 *)
mul v_mul_i_3_12_7 v_conv1_i_3_12_7 (1468)@sint32;
(*   %call.i.3.12.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.12.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_12_7, v_call_i_3_12_7);
(*   %arrayidx11.3.12.7 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %1017 = load i16, i16* %arrayidx11.3.12.7, align 2, !tbaa !3 *)
mov v1017 mem0_472;
(*   %sub.3.12.7 = sub i16 %1017, %call.i.3.12.7 *)
sub v_sub_3_12_7 v1017 v_call_i_3_12_7;
(*   store i16 %sub.3.12.7, i16* %arrayidx9.3.12.7, align 2, !tbaa !3 *)
mov mem0_504 v_sub_3_12_7;
(*   %add21.3.12.7 = add i16 %1017, %call.i.3.12.7 *)
add v_add21_3_12_7 v1017 v_call_i_3_12_7;
(*   store i16 %add21.3.12.7, i16* %arrayidx11.3.12.7, align 2, !tbaa !3 *)
mov mem0_472 v_add21_3_12_7;
(*   %arrayidx9.3.13.7 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %1018 = load i16, i16* %arrayidx9.3.13.7, align 2, !tbaa !3 *)
mov v1018 mem0_506;
(*   %conv1.i.3.13.7 = sext i16 %1018 to i32 *)
cast v_conv1_i_3_13_7@sint32 v1018@sint16;
(*   %mul.i.3.13.7 = mul nsw i32 %conv1.i.3.13.7, 1468 *)
mul v_mul_i_3_13_7 v_conv1_i_3_13_7 (1468)@sint32;
(*   %call.i.3.13.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.13.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_13_7, v_call_i_3_13_7);
(*   %arrayidx11.3.13.7 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %1019 = load i16, i16* %arrayidx11.3.13.7, align 2, !tbaa !3 *)
mov v1019 mem0_474;
(*   %sub.3.13.7 = sub i16 %1019, %call.i.3.13.7 *)
sub v_sub_3_13_7 v1019 v_call_i_3_13_7;
(*   store i16 %sub.3.13.7, i16* %arrayidx9.3.13.7, align 2, !tbaa !3 *)
mov mem0_506 v_sub_3_13_7;
(*   %add21.3.13.7 = add i16 %1019, %call.i.3.13.7 *)
add v_add21_3_13_7 v1019 v_call_i_3_13_7;
(*   store i16 %add21.3.13.7, i16* %arrayidx11.3.13.7, align 2, !tbaa !3 *)
mov mem0_474 v_add21_3_13_7;
(*   %arrayidx9.3.14.7 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %1020 = load i16, i16* %arrayidx9.3.14.7, align 2, !tbaa !3 *)
mov v1020 mem0_508;
(*   %conv1.i.3.14.7 = sext i16 %1020 to i32 *)
cast v_conv1_i_3_14_7@sint32 v1020@sint16;
(*   %mul.i.3.14.7 = mul nsw i32 %conv1.i.3.14.7, 1468 *)
mul v_mul_i_3_14_7 v_conv1_i_3_14_7 (1468)@sint32;
(*   %call.i.3.14.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.14.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_14_7, v_call_i_3_14_7);
(*   %arrayidx11.3.14.7 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %1021 = load i16, i16* %arrayidx11.3.14.7, align 2, !tbaa !3 *)
mov v1021 mem0_476;
(*   %sub.3.14.7 = sub i16 %1021, %call.i.3.14.7 *)
sub v_sub_3_14_7 v1021 v_call_i_3_14_7;
(*   store i16 %sub.3.14.7, i16* %arrayidx9.3.14.7, align 2, !tbaa !3 *)
mov mem0_508 v_sub_3_14_7;
(*   %add21.3.14.7 = add i16 %1021, %call.i.3.14.7 *)
add v_add21_3_14_7 v1021 v_call_i_3_14_7;
(*   store i16 %add21.3.14.7, i16* %arrayidx11.3.14.7, align 2, !tbaa !3 *)
mov mem0_476 v_add21_3_14_7;
(*   %arrayidx9.3.15.7 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %1022 = load i16, i16* %arrayidx9.3.15.7, align 2, !tbaa !3 *)
mov v1022 mem0_510;
(*   %conv1.i.3.15.7 = sext i16 %1022 to i32 *)
cast v_conv1_i_3_15_7@sint32 v1022@sint16;
(*   %mul.i.3.15.7 = mul nsw i32 %conv1.i.3.15.7, 1468 *)
mul v_mul_i_3_15_7 v_conv1_i_3_15_7 (1468)@sint32;
(*   %call.i.3.15.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.3.15.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_3_15_7, v_call_i_3_15_7);
(*   %arrayidx11.3.15.7 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %1023 = load i16, i16* %arrayidx11.3.15.7, align 2, !tbaa !3 *)
mov v1023 mem0_478;
(*   %sub.3.15.7 = sub i16 %1023, %call.i.3.15.7 *)
sub v_sub_3_15_7 v1023 v_call_i_3_15_7;
(*   store i16 %sub.3.15.7, i16* %arrayidx9.3.15.7, align 2, !tbaa !3 *)
mov mem0_510 v_sub_3_15_7;
(*   %add21.3.15.7 = add i16 %1023, %call.i.3.15.7 *)
add v_add21_3_15_7 v1023 v_call_i_3_15_7;
(*   store i16 %add21.3.15.7, i16* %arrayidx11.3.15.7, align 2, !tbaa !3 *)
mov mem0_478 v_add21_3_15_7;

cut and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3) + 
mem0_8*(x**4) + mem0_10*(x**5) + mem0_12*(x**6) + mem0_14*(x**7) + 
mem0_16*(x**8) + mem0_18*(x**9) + mem0_20*(x**10) + mem0_22*(x**11) + 
mem0_24*(x**12) + mem0_26*(x**13) + mem0_28*(x**14) + mem0_30*(x**15)
)
[3329, x**16 - 1062],
eqmod 
input_polynomial * input_polynomial
(
mem0_32*(x**0) + mem0_34*(x**1) + mem0_36*(x**2) + mem0_38*(x**3) + 
mem0_40*(x**4) + mem0_42*(x**5) + mem0_44*(x**6) + mem0_46*(x**7) + 
mem0_48*(x**8) + mem0_50*(x**9) + mem0_52*(x**10) + mem0_54*(x**11) + 
mem0_56*(x**12) + mem0_58*(x**13) + mem0_60*(x**14) + mem0_62*(x**15)
)
[3329, x**16 - 2267],
eqmod 
input_polynomial * input_polynomial
(
mem0_64*(x**0) + mem0_66*(x**1) + mem0_68*(x**2) + mem0_70*(x**3) + 
mem0_72*(x**4) + mem0_74*(x**5) + mem0_76*(x**6) + mem0_78*(x**7) + 
mem0_80*(x**8) + mem0_82*(x**9) + mem0_84*(x**10) + mem0_86*(x**11) + 
mem0_88*(x**12) + mem0_90*(x**13) + mem0_92*(x**14) + mem0_94*(x**15)
)
[3329, x**16 - 1919],
eqmod 
input_polynomial * input_polynomial
(
mem0_96*(x**0) + mem0_98*(x**1) + mem0_100*(x**2) + mem0_102*(x**3) + 
mem0_104*(x**4) + mem0_106*(x**5) + mem0_108*(x**6) + mem0_110*(x**7) + 
mem0_112*(x**8) + mem0_114*(x**9) + mem0_116*(x**10) + mem0_118*(x**11) + 
mem0_120*(x**12) + mem0_122*(x**13) + mem0_124*(x**14) + mem0_126*(x**15)
)
[3329, x**16 - 1410],
eqmod 
input_polynomial * input_polynomial
(
mem0_128*(x**0) + mem0_130*(x**1) + mem0_132*(x**2) + mem0_134*(x**3) + 
mem0_136*(x**4) + mem0_138*(x**5) + mem0_140*(x**6) + mem0_142*(x**7) + 
mem0_144*(x**8) + mem0_146*(x**9) + mem0_148*(x**10) + mem0_150*(x**11) + 
mem0_152*(x**12) + mem0_154*(x**13) + mem0_156*(x**14) + mem0_158*(x**15)
)
[3329, x**16 - 193],
eqmod 
input_polynomial * input_polynomial
(
mem0_160*(x**0) + mem0_162*(x**1) + mem0_164*(x**2) + mem0_166*(x**3) + 
mem0_168*(x**4) + mem0_170*(x**5) + mem0_172*(x**6) + mem0_174*(x**7) + 
mem0_176*(x**8) + mem0_178*(x**9) + mem0_180*(x**10) + mem0_182*(x**11) + 
mem0_184*(x**12) + mem0_186*(x**13) + mem0_188*(x**14) + mem0_190*(x**15)
)
[3329, x**16 - 3136],
eqmod 
input_polynomial * input_polynomial
(
mem0_192*(x**0) + mem0_194*(x**1) + mem0_196*(x**2) + mem0_198*(x**3) + 
mem0_200*(x**4) + mem0_202*(x**5) + mem0_204*(x**6) + mem0_206*(x**7) + 
mem0_208*(x**8) + mem0_210*(x**9) + mem0_212*(x**10) + mem0_214*(x**11) + 
mem0_216*(x**12) + mem0_218*(x**13) + mem0_220*(x**14) + mem0_222*(x**15)
)
[3329, x**16 - 797],
eqmod 
input_polynomial * input_polynomial
(
mem0_224*(x**0) + mem0_226*(x**1) + mem0_228*(x**2) + mem0_230*(x**3) + 
mem0_232*(x**4) + mem0_234*(x**5) + mem0_236*(x**6) + mem0_238*(x**7) + 
mem0_240*(x**8) + mem0_242*(x**9) + mem0_244*(x**10) + mem0_246*(x**11) + 
mem0_248*(x**12) + mem0_250*(x**13) + mem0_252*(x**14) + mem0_254*(x**15)
)
[3329, x**16 - 2532],
eqmod 
input_polynomial * input_polynomial
(
mem0_256*(x**0) + mem0_258*(x**1) + mem0_260*(x**2) + mem0_262*(x**3) + 
mem0_264*(x**4) + mem0_266*(x**5) + mem0_268*(x**6) + mem0_270*(x**7) + 
mem0_272*(x**8) + mem0_274*(x**9) + mem0_276*(x**10) + mem0_278*(x**11) + 
mem0_280*(x**12) + mem0_282*(x**13) + mem0_284*(x**14) + mem0_286*(x**15)
)
[3329, x**16 - 2786],
eqmod 
input_polynomial * input_polynomial
(
mem0_288*(x**0) + mem0_290*(x**1) + mem0_292*(x**2) + mem0_294*(x**3) + 
mem0_296*(x**4) + mem0_298*(x**5) + mem0_300*(x**6) + mem0_302*(x**7) + 
mem0_304*(x**8) + mem0_306*(x**9) + mem0_308*(x**10) + mem0_310*(x**11) + 
mem0_312*(x**12) + mem0_314*(x**13) + mem0_316*(x**14) + mem0_318*(x**15)
)
[3329, x**16 - 543],
eqmod 
input_polynomial * input_polynomial
(
mem0_320*(x**0) + mem0_322*(x**1) + mem0_324*(x**2) + mem0_326*(x**3) + 
mem0_328*(x**4) + mem0_330*(x**5) + mem0_332*(x**6) + mem0_334*(x**7) + 
mem0_336*(x**8) + mem0_338*(x**9) + mem0_340*(x**10) + mem0_342*(x**11) + 
mem0_344*(x**12) + mem0_346*(x**13) + mem0_348*(x**14) + mem0_350*(x**15)
)
[3329, x**16 - 3260],
eqmod 
input_polynomial * input_polynomial
(
mem0_352*(x**0) + mem0_354*(x**1) + mem0_356*(x**2) + mem0_358*(x**3) + 
mem0_360*(x**4) + mem0_362*(x**5) + mem0_364*(x**6) + mem0_366*(x**7) + 
mem0_368*(x**8) + mem0_370*(x**9) + mem0_372*(x**10) + mem0_374*(x**11) + 
mem0_376*(x**12) + mem0_378*(x**13) + mem0_380*(x**14) + mem0_382*(x**15)
)
[3329, x**16 - 69],
eqmod 
input_polynomial * input_polynomial
(
mem0_384*(x**0) + mem0_386*(x**1) + mem0_388*(x**2) + mem0_390*(x**3) + 
mem0_392*(x**4) + mem0_394*(x**5) + mem0_396*(x**6) + mem0_398*(x**7) + 
mem0_400*(x**8) + mem0_402*(x**9) + mem0_404*(x**10) + mem0_406*(x**11) + 
mem0_408*(x**12) + mem0_410*(x**13) + mem0_412*(x**14) + mem0_414*(x**15)
)
[3329, x**16 - 569],
eqmod 
input_polynomial * input_polynomial
(
mem0_416*(x**0) + mem0_418*(x**1) + mem0_420*(x**2) + mem0_422*(x**3) + 
mem0_424*(x**4) + mem0_426*(x**5) + mem0_428*(x**6) + mem0_430*(x**7) + 
mem0_432*(x**8) + mem0_434*(x**9) + mem0_436*(x**10) + mem0_438*(x**11) + 
mem0_440*(x**12) + mem0_442*(x**13) + mem0_444*(x**14) + mem0_446*(x**15)
)
[3329, x**16 - 2760],
eqmod 
input_polynomial * input_polynomial
(
mem0_448*(x**0) + mem0_450*(x**1) + mem0_452*(x**2) + mem0_454*(x**3) + 
mem0_456*(x**4) + mem0_458*(x**5) + mem0_460*(x**6) + mem0_462*(x**7) + 
mem0_464*(x**8) + mem0_466*(x**9) + mem0_468*(x**10) + mem0_470*(x**11) + 
mem0_472*(x**12) + mem0_474*(x**13) + mem0_476*(x**14) + mem0_478*(x**15)
)
[3329, x**16 - 1746],
eqmod 
input_polynomial * input_polynomial
(
mem0_480*(x**0) + mem0_482*(x**1) + mem0_484*(x**2) + mem0_486*(x**3) + 
mem0_488*(x**4) + mem0_490*(x**5) + mem0_492*(x**6) + mem0_494*(x**7) + 
mem0_496*(x**8) + mem0_498*(x**9) + mem0_500*(x**10) + mem0_502*(x**11) + 
mem0_504*(x**12) + mem0_506*(x**13) + mem0_508*(x**14) + mem0_510*(x**15)
)
[3329, x**16 - 1583]
] && and [
   (-6)@16 * 3329@16 <s mem0_0, mem0_0 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_2, mem0_2 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_4, mem0_4 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_6, mem0_6 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_8, mem0_8 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_10, mem0_10 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_12, mem0_12 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_14, mem0_14 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_16, mem0_16 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_18, mem0_18 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_20, mem0_20 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_22, mem0_22 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_24, mem0_24 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_26, mem0_26 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_28, mem0_28 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_30, mem0_30 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_32, mem0_32 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_34, mem0_34 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_36, mem0_36 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_38, mem0_38 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_40, mem0_40 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_42, mem0_42 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_44, mem0_44 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_46, mem0_46 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_48, mem0_48 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_50, mem0_50 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_52, mem0_52 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_54, mem0_54 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_56, mem0_56 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_58, mem0_58 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_60, mem0_60 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_62, mem0_62 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_64, mem0_64 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_66, mem0_66 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_68, mem0_68 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_70, mem0_70 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_72, mem0_72 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_74, mem0_74 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_76, mem0_76 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_78, mem0_78 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_80, mem0_80 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_82, mem0_82 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_84, mem0_84 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_86, mem0_86 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_88, mem0_88 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_90, mem0_90 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_92, mem0_92 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_94, mem0_94 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_96, mem0_96 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_98, mem0_98 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_100, mem0_100 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_102, mem0_102 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_104, mem0_104 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_106, mem0_106 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_108, mem0_108 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_110, mem0_110 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_112, mem0_112 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_114, mem0_114 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_116, mem0_116 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_118, mem0_118 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_120, mem0_120 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_122, mem0_122 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_124, mem0_124 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_126, mem0_126 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_128, mem0_128 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_130, mem0_130 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_132, mem0_132 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_134, mem0_134 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_136, mem0_136 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_138, mem0_138 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_140, mem0_140 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_142, mem0_142 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_144, mem0_144 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_146, mem0_146 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_148, mem0_148 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_150, mem0_150 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_152, mem0_152 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_154, mem0_154 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_156, mem0_156 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_158, mem0_158 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_160, mem0_160 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_162, mem0_162 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_164, mem0_164 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_166, mem0_166 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_168, mem0_168 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_170, mem0_170 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_172, mem0_172 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_174, mem0_174 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_176, mem0_176 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_178, mem0_178 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_180, mem0_180 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_182, mem0_182 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_184, mem0_184 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_186, mem0_186 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_188, mem0_188 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_190, mem0_190 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_192, mem0_192 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_194, mem0_194 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_196, mem0_196 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_198, mem0_198 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_200, mem0_200 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_202, mem0_202 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_204, mem0_204 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_206, mem0_206 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_208, mem0_208 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_210, mem0_210 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_212, mem0_212 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_214, mem0_214 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_216, mem0_216 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_218, mem0_218 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_220, mem0_220 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_222, mem0_222 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_224, mem0_224 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_226, mem0_226 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_228, mem0_228 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_230, mem0_230 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_232, mem0_232 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_234, mem0_234 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_236, mem0_236 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_238, mem0_238 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_240, mem0_240 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_242, mem0_242 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_244, mem0_244 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_246, mem0_246 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_248, mem0_248 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_250, mem0_250 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_252, mem0_252 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_254, mem0_254 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_256, mem0_256 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_258, mem0_258 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_260, mem0_260 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_262, mem0_262 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_264, mem0_264 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_266, mem0_266 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_268, mem0_268 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_270, mem0_270 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_272, mem0_272 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_274, mem0_274 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_276, mem0_276 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_278, mem0_278 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_280, mem0_280 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_282, mem0_282 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_284, mem0_284 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_286, mem0_286 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_288, mem0_288 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_290, mem0_290 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_292, mem0_292 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_294, mem0_294 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_296, mem0_296 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_298, mem0_298 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_300, mem0_300 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_302, mem0_302 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_304, mem0_304 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_306, mem0_306 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_308, mem0_308 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_310, mem0_310 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_312, mem0_312 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_314, mem0_314 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_316, mem0_316 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_318, mem0_318 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_320, mem0_320 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_322, mem0_322 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_324, mem0_324 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_326, mem0_326 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_328, mem0_328 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_330, mem0_330 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_332, mem0_332 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_334, mem0_334 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_336, mem0_336 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_338, mem0_338 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_340, mem0_340 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_342, mem0_342 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_344, mem0_344 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_346, mem0_346 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_348, mem0_348 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_350, mem0_350 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_352, mem0_352 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_354, mem0_354 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_356, mem0_356 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_358, mem0_358 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_360, mem0_360 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_362, mem0_362 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_364, mem0_364 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_366, mem0_366 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_368, mem0_368 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_370, mem0_370 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_372, mem0_372 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_374, mem0_374 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_376, mem0_376 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_378, mem0_378 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_380, mem0_380 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_382, mem0_382 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_384, mem0_384 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_386, mem0_386 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_388, mem0_388 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_390, mem0_390 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_392, mem0_392 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_394, mem0_394 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_396, mem0_396 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_398, mem0_398 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_400, mem0_400 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_402, mem0_402 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_404, mem0_404 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_406, mem0_406 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_408, mem0_408 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_410, mem0_410 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_412, mem0_412 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_414, mem0_414 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_416, mem0_416 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_418, mem0_418 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_420, mem0_420 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_422, mem0_422 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_424, mem0_424 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_426, mem0_426 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_428, mem0_428 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_430, mem0_430 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_432, mem0_432 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_434, mem0_434 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_436, mem0_436 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_438, mem0_438 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_440, mem0_440 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_442, mem0_442 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_444, mem0_444 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_446, mem0_446 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_448, mem0_448 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_450, mem0_450 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_452, mem0_452 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_454, mem0_454 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_456, mem0_456 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_458, mem0_458 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_460, mem0_460 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_462, mem0_462 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_464, mem0_464 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_466, mem0_466 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_468, mem0_468 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_470, mem0_470 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_472, mem0_472 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_474, mem0_474 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_476, mem0_476 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_478, mem0_478 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_480, mem0_480 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_482, mem0_482 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_484, mem0_484 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_486, mem0_486 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_488, mem0_488 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_490, mem0_490 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_492, mem0_492 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_494, mem0_494 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_496, mem0_496 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_498, mem0_498 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_500, mem0_500 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_502, mem0_502 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_504, mem0_504 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_506, mem0_506 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_508, mem0_508 <s 6@16 * 3329@16,
   (-6)@16 * 3329@16 <s mem0_510, mem0_510 <s 6@16 * 3329@16
];


(* NOTE: k = 16 *)

(*    Z_q[x]/(x**256 - zeta**128)
          ~= Z_q[x]/( x**8 -   zeta**4) * Z_q[x]/(x** 8 - zeta**132) *
             Z_q[x]/( x**8 -  zeta**68) * Z_q[x]/(x** 8 - zeta**196) *
             Z_q[x]/( x**8 -  zeta**36) * Z_q[x]/(x** 8 - zeta**164) *
             Z_q[x]/( x**8 - zeta**100) * Z_q[x]/(x** 8 - zeta**228) *
             Z_q[x]/( x**8 -  zeta**20) * Z_q[x]/(x** 8 - zeta**148) *
             Z_q[x]/( x**8 -  zeta**84) * Z_q[x]/(x** 8 - zeta**212) *
             Z_q[x]/( x**8 -  zeta**52) * Z_q[x]/(x** 8 - zeta**180) *
             Z_q[x]/( x**8 - zeta**116) * Z_q[x]/(x** 8 - zeta**244) *
             Z_q[x]/( x**8 -  zeta**12) * Z_q[x]/(x** 8 - zeta**140) *
             Z_q[x]/( x**8 -  zeta**76) * Z_q[x]/(x** 8 - zeta**204) *
             Z_q[x]/( x**8 -  zeta**44) * Z_q[x]/(x** 8 - zeta**172) *
             Z_q[x]/( x**8 - zeta**108) * Z_q[x]/(x** 8 - zeta**236) *
             Z_q[x]/( x**8 -  zeta**28) * Z_q[x]/(x** 8 - zeta**156) *
             Z_q[x]/( x**8 -  zeta**92) * Z_q[x]/(x** 8 - zeta**220) *
             Z_q[x]/( x**8 -  zeta**60) * Z_q[x]/(x** 8 - zeta**188) *
             Z_q[x]/( x**8 - zeta**124) * Z_q[x]/(x** 8 - zeta**252)
*)

(*   %arrayidx9.4 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %1024 = load i16, i16* %arrayidx9.4, align 2, !tbaa !3 *)
mov v1024 mem0_16;
(*   %conv1.i.4 = sext i16 %1024 to i32 *)
cast v_conv1_i_4@sint32 v1024@sint16;
(*   %mul.i.4 = mul nsw i32 %conv1.i.4, 573 *)
mul v_mul_i_4 v_conv1_i_4 (573)@sint32;
(*   %call.i.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4, v_call_i_4);
(*   %1025 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v1025 mem0_0;
(*   %sub.4 = sub i16 %1025, %call.i.4 *)
sub v_sub_4 v1025 v_call_i_4;
(*   store i16 %sub.4, i16* %arrayidx9.4, align 2, !tbaa !3 *)
mov mem0_16 v_sub_4;
(*   %add21.4 = add i16 %1025, %call.i.4 *)
add v_add21_4 v1025 v_call_i_4;
(*   store i16 %add21.4, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21_4;
(*   %arrayidx9.4.1 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %1026 = load i16, i16* %arrayidx9.4.1, align 2, !tbaa !3 *)
mov v1026 mem0_18;
(*   %conv1.i.4.1 = sext i16 %1026 to i32 *)
cast v_conv1_i_4_1@sint32 v1026@sint16;
(*   %mul.i.4.1 = mul nsw i32 %conv1.i.4.1, 573 *)
mul v_mul_i_4_1 v_conv1_i_4_1 (573)@sint32;
(*   %call.i.4.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1, v_call_i_4_1);
(*   %arrayidx11.4.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %1027 = load i16, i16* %arrayidx11.4.1, align 2, !tbaa !3 *)
mov v1027 mem0_2;
(*   %sub.4.1 = sub i16 %1027, %call.i.4.1 *)
sub v_sub_4_1 v1027 v_call_i_4_1;
(*   store i16 %sub.4.1, i16* %arrayidx9.4.1, align 2, !tbaa !3 *)
mov mem0_18 v_sub_4_1;
(*   %add21.4.1 = add i16 %1027, %call.i.4.1 *)
add v_add21_4_1 v1027 v_call_i_4_1;
(*   store i16 %add21.4.1, i16* %arrayidx11.4.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_4_1;
(*   %arrayidx9.4.2 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %1028 = load i16, i16* %arrayidx9.4.2, align 2, !tbaa !3 *)
mov v1028 mem0_20;
(*   %conv1.i.4.2 = sext i16 %1028 to i32 *)
cast v_conv1_i_4_2@sint32 v1028@sint16;
(*   %mul.i.4.2 = mul nsw i32 %conv1.i.4.2, 573 *)
mul v_mul_i_4_2 v_conv1_i_4_2 (573)@sint32;
(*   %call.i.4.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2, v_call_i_4_2);
(*   %arrayidx11.4.2 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %1029 = load i16, i16* %arrayidx11.4.2, align 2, !tbaa !3 *)
mov v1029 mem0_4;
(*   %sub.4.2 = sub i16 %1029, %call.i.4.2 *)
sub v_sub_4_2 v1029 v_call_i_4_2;
(*   store i16 %sub.4.2, i16* %arrayidx9.4.2, align 2, !tbaa !3 *)
mov mem0_20 v_sub_4_2;
(*   %add21.4.2 = add i16 %1029, %call.i.4.2 *)
add v_add21_4_2 v1029 v_call_i_4_2;
(*   store i16 %add21.4.2, i16* %arrayidx11.4.2, align 2, !tbaa !3 *)
mov mem0_4 v_add21_4_2;
(*   %arrayidx9.4.3 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %1030 = load i16, i16* %arrayidx9.4.3, align 2, !tbaa !3 *)
mov v1030 mem0_22;
(*   %conv1.i.4.3 = sext i16 %1030 to i32 *)
cast v_conv1_i_4_3@sint32 v1030@sint16;
(*   %mul.i.4.3 = mul nsw i32 %conv1.i.4.3, 573 *)
mul v_mul_i_4_3 v_conv1_i_4_3 (573)@sint32;
(*   %call.i.4.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3, v_call_i_4_3);
(*   %arrayidx11.4.3 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %1031 = load i16, i16* %arrayidx11.4.3, align 2, !tbaa !3 *)
mov v1031 mem0_6;
(*   %sub.4.3 = sub i16 %1031, %call.i.4.3 *)
sub v_sub_4_3 v1031 v_call_i_4_3;
(*   store i16 %sub.4.3, i16* %arrayidx9.4.3, align 2, !tbaa !3 *)
mov mem0_22 v_sub_4_3;
(*   %add21.4.3 = add i16 %1031, %call.i.4.3 *)
add v_add21_4_3 v1031 v_call_i_4_3;
(*   store i16 %add21.4.3, i16* %arrayidx11.4.3, align 2, !tbaa !3 *)
mov mem0_6 v_add21_4_3;
(*   %arrayidx9.4.4 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %1032 = load i16, i16* %arrayidx9.4.4, align 2, !tbaa !3 *)
mov v1032 mem0_24;
(*   %conv1.i.4.4 = sext i16 %1032 to i32 *)
cast v_conv1_i_4_4@sint32 v1032@sint16;
(*   %mul.i.4.4 = mul nsw i32 %conv1.i.4.4, 573 *)
mul v_mul_i_4_4 v_conv1_i_4_4 (573)@sint32;
(*   %call.i.4.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4, v_call_i_4_4);
(*   %arrayidx11.4.4 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %1033 = load i16, i16* %arrayidx11.4.4, align 2, !tbaa !3 *)
mov v1033 mem0_8;
(*   %sub.4.4 = sub i16 %1033, %call.i.4.4 *)
sub v_sub_4_4 v1033 v_call_i_4_4;
(*   store i16 %sub.4.4, i16* %arrayidx9.4.4, align 2, !tbaa !3 *)
mov mem0_24 v_sub_4_4;
(*   %add21.4.4 = add i16 %1033, %call.i.4.4 *)
add v_add21_4_4 v1033 v_call_i_4_4;
(*   store i16 %add21.4.4, i16* %arrayidx11.4.4, align 2, !tbaa !3 *)
mov mem0_8 v_add21_4_4;
(*   %arrayidx9.4.5 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %1034 = load i16, i16* %arrayidx9.4.5, align 2, !tbaa !3 *)
mov v1034 mem0_26;
(*   %conv1.i.4.5 = sext i16 %1034 to i32 *)
cast v_conv1_i_4_5@sint32 v1034@sint16;
(*   %mul.i.4.5 = mul nsw i32 %conv1.i.4.5, 573 *)
mul v_mul_i_4_5 v_conv1_i_4_5 (573)@sint32;
(*   %call.i.4.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5, v_call_i_4_5);
(*   %arrayidx11.4.5 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %1035 = load i16, i16* %arrayidx11.4.5, align 2, !tbaa !3 *)
mov v1035 mem0_10;
(*   %sub.4.5 = sub i16 %1035, %call.i.4.5 *)
sub v_sub_4_5 v1035 v_call_i_4_5;
(*   store i16 %sub.4.5, i16* %arrayidx9.4.5, align 2, !tbaa !3 *)
mov mem0_26 v_sub_4_5;
(*   %add21.4.5 = add i16 %1035, %call.i.4.5 *)
add v_add21_4_5 v1035 v_call_i_4_5;
(*   store i16 %add21.4.5, i16* %arrayidx11.4.5, align 2, !tbaa !3 *)
mov mem0_10 v_add21_4_5;
(*   %arrayidx9.4.6 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %1036 = load i16, i16* %arrayidx9.4.6, align 2, !tbaa !3 *)
mov v1036 mem0_28;
(*   %conv1.i.4.6 = sext i16 %1036 to i32 *)
cast v_conv1_i_4_6@sint32 v1036@sint16;
(*   %mul.i.4.6 = mul nsw i32 %conv1.i.4.6, 573 *)
mul v_mul_i_4_6 v_conv1_i_4_6 (573)@sint32;
(*   %call.i.4.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6, v_call_i_4_6);
(*   %arrayidx11.4.6 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %1037 = load i16, i16* %arrayidx11.4.6, align 2, !tbaa !3 *)
mov v1037 mem0_12;
(*   %sub.4.6 = sub i16 %1037, %call.i.4.6 *)
sub v_sub_4_6 v1037 v_call_i_4_6;
(*   store i16 %sub.4.6, i16* %arrayidx9.4.6, align 2, !tbaa !3 *)
mov mem0_28 v_sub_4_6;
(*   %add21.4.6 = add i16 %1037, %call.i.4.6 *)
add v_add21_4_6 v1037 v_call_i_4_6;
(*   store i16 %add21.4.6, i16* %arrayidx11.4.6, align 2, !tbaa !3 *)
mov mem0_12 v_add21_4_6;
(*   %arrayidx9.4.7 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %1038 = load i16, i16* %arrayidx9.4.7, align 2, !tbaa !3 *)
mov v1038 mem0_30;
(*   %conv1.i.4.7 = sext i16 %1038 to i32 *)
cast v_conv1_i_4_7@sint32 v1038@sint16;
(*   %mul.i.4.7 = mul nsw i32 %conv1.i.4.7, 573 *)
mul v_mul_i_4_7 v_conv1_i_4_7 (573)@sint32;
(*   %call.i.4.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7, v_call_i_4_7);
(*   %arrayidx11.4.7 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %1039 = load i16, i16* %arrayidx11.4.7, align 2, !tbaa !3 *)
mov v1039 mem0_14;
(*   %sub.4.7 = sub i16 %1039, %call.i.4.7 *)
sub v_sub_4_7 v1039 v_call_i_4_7;
(*   store i16 %sub.4.7, i16* %arrayidx9.4.7, align 2, !tbaa !3 *)
mov mem0_30 v_sub_4_7;
(*   %add21.4.7 = add i16 %1039, %call.i.4.7 *)
add v_add21_4_7 v1039 v_call_i_4_7;
(*   store i16 %add21.4.7, i16* %arrayidx11.4.7, align 2, !tbaa !3 *)
mov mem0_14 v_add21_4_7;

(* NOTE: k = 17 *)

(*   %arrayidx9.4.1108 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %1040 = load i16, i16* %arrayidx9.4.1108, align 2, !tbaa !3 *)
mov v1040 mem0_48;
(*   %conv1.i.4.1109 = sext i16 %1040 to i32 *)
cast v_conv1_i_4_1109@sint32 v1040@sint16;
(*   %mul.i.4.1110 = mul nsw i32 %conv1.i.4.1109, -1325 *)
mul v_mul_i_4_1110 v_conv1_i_4_1109 (-1325)@sint32;
(*   %call.i.4.1111 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1110) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1110, v_call_i_4_1111);
(*   %arrayidx11.4.1112 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %1041 = load i16, i16* %arrayidx11.4.1112, align 2, !tbaa !3 *)
mov v1041 mem0_32;
(*   %sub.4.1113 = sub i16 %1041, %call.i.4.1111 *)
sub v_sub_4_1113 v1041 v_call_i_4_1111;
(*   store i16 %sub.4.1113, i16* %arrayidx9.4.1108, align 2, !tbaa !3 *)
mov mem0_48 v_sub_4_1113;
(*   %add21.4.1114 = add i16 %1041, %call.i.4.1111 *)
add v_add21_4_1114 v1041 v_call_i_4_1111;
(*   store i16 %add21.4.1114, i16* %arrayidx11.4.1112, align 2, !tbaa !3 *)
mov mem0_32 v_add21_4_1114;
(*   %arrayidx9.4.1.1 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %1042 = load i16, i16* %arrayidx9.4.1.1, align 2, !tbaa !3 *)
mov v1042 mem0_50;
(*   %conv1.i.4.1.1 = sext i16 %1042 to i32 *)
cast v_conv1_i_4_1_1@sint32 v1042@sint16;
(*   %mul.i.4.1.1 = mul nsw i32 %conv1.i.4.1.1, -1325 *)
mul v_mul_i_4_1_1 v_conv1_i_4_1_1 (-1325)@sint32;
(*   %call.i.4.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_1, v_call_i_4_1_1);
(*   %arrayidx11.4.1.1 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %1043 = load i16, i16* %arrayidx11.4.1.1, align 2, !tbaa !3 *)
mov v1043 mem0_34;
(*   %sub.4.1.1 = sub i16 %1043, %call.i.4.1.1 *)
sub v_sub_4_1_1 v1043 v_call_i_4_1_1;
(*   store i16 %sub.4.1.1, i16* %arrayidx9.4.1.1, align 2, !tbaa !3 *)
mov mem0_50 v_sub_4_1_1;
(*   %add21.4.1.1 = add i16 %1043, %call.i.4.1.1 *)
add v_add21_4_1_1 v1043 v_call_i_4_1_1;
(*   store i16 %add21.4.1.1, i16* %arrayidx11.4.1.1, align 2, !tbaa !3 *)
mov mem0_34 v_add21_4_1_1;
(*   %arrayidx9.4.2.1 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %1044 = load i16, i16* %arrayidx9.4.2.1, align 2, !tbaa !3 *)
mov v1044 mem0_52;
(*   %conv1.i.4.2.1 = sext i16 %1044 to i32 *)
cast v_conv1_i_4_2_1@sint32 v1044@sint16;
(*   %mul.i.4.2.1 = mul nsw i32 %conv1.i.4.2.1, -1325 *)
mul v_mul_i_4_2_1 v_conv1_i_4_2_1 (-1325)@sint32;
(*   %call.i.4.2.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_1, v_call_i_4_2_1);
(*   %arrayidx11.4.2.1 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %1045 = load i16, i16* %arrayidx11.4.2.1, align 2, !tbaa !3 *)
mov v1045 mem0_36;
(*   %sub.4.2.1 = sub i16 %1045, %call.i.4.2.1 *)
sub v_sub_4_2_1 v1045 v_call_i_4_2_1;
(*   store i16 %sub.4.2.1, i16* %arrayidx9.4.2.1, align 2, !tbaa !3 *)
mov mem0_52 v_sub_4_2_1;
(*   %add21.4.2.1 = add i16 %1045, %call.i.4.2.1 *)
add v_add21_4_2_1 v1045 v_call_i_4_2_1;
(*   store i16 %add21.4.2.1, i16* %arrayidx11.4.2.1, align 2, !tbaa !3 *)
mov mem0_36 v_add21_4_2_1;
(*   %arrayidx9.4.3.1 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %1046 = load i16, i16* %arrayidx9.4.3.1, align 2, !tbaa !3 *)
mov v1046 mem0_54;
(*   %conv1.i.4.3.1 = sext i16 %1046 to i32 *)
cast v_conv1_i_4_3_1@sint32 v1046@sint16;
(*   %mul.i.4.3.1 = mul nsw i32 %conv1.i.4.3.1, -1325 *)
mul v_mul_i_4_3_1 v_conv1_i_4_3_1 (-1325)@sint32;
(*   %call.i.4.3.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_1, v_call_i_4_3_1);
(*   %arrayidx11.4.3.1 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %1047 = load i16, i16* %arrayidx11.4.3.1, align 2, !tbaa !3 *)
mov v1047 mem0_38;
(*   %sub.4.3.1 = sub i16 %1047, %call.i.4.3.1 *)
sub v_sub_4_3_1 v1047 v_call_i_4_3_1;
(*   store i16 %sub.4.3.1, i16* %arrayidx9.4.3.1, align 2, !tbaa !3 *)
mov mem0_54 v_sub_4_3_1;
(*   %add21.4.3.1 = add i16 %1047, %call.i.4.3.1 *)
add v_add21_4_3_1 v1047 v_call_i_4_3_1;
(*   store i16 %add21.4.3.1, i16* %arrayidx11.4.3.1, align 2, !tbaa !3 *)
mov mem0_38 v_add21_4_3_1;
(*   %arrayidx9.4.4.1 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %1048 = load i16, i16* %arrayidx9.4.4.1, align 2, !tbaa !3 *)
mov v1048 mem0_56;
(*   %conv1.i.4.4.1 = sext i16 %1048 to i32 *)
cast v_conv1_i_4_4_1@sint32 v1048@sint16;
(*   %mul.i.4.4.1 = mul nsw i32 %conv1.i.4.4.1, -1325 *)
mul v_mul_i_4_4_1 v_conv1_i_4_4_1 (-1325)@sint32;
(*   %call.i.4.4.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_1, v_call_i_4_4_1);
(*   %arrayidx11.4.4.1 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %1049 = load i16, i16* %arrayidx11.4.4.1, align 2, !tbaa !3 *)
mov v1049 mem0_40;
(*   %sub.4.4.1 = sub i16 %1049, %call.i.4.4.1 *)
sub v_sub_4_4_1 v1049 v_call_i_4_4_1;
(*   store i16 %sub.4.4.1, i16* %arrayidx9.4.4.1, align 2, !tbaa !3 *)
mov mem0_56 v_sub_4_4_1;
(*   %add21.4.4.1 = add i16 %1049, %call.i.4.4.1 *)
add v_add21_4_4_1 v1049 v_call_i_4_4_1;
(*   store i16 %add21.4.4.1, i16* %arrayidx11.4.4.1, align 2, !tbaa !3 *)
mov mem0_40 v_add21_4_4_1;
(*   %arrayidx9.4.5.1 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %1050 = load i16, i16* %arrayidx9.4.5.1, align 2, !tbaa !3 *)
mov v1050 mem0_58;
(*   %conv1.i.4.5.1 = sext i16 %1050 to i32 *)
cast v_conv1_i_4_5_1@sint32 v1050@sint16;
(*   %mul.i.4.5.1 = mul nsw i32 %conv1.i.4.5.1, -1325 *)
mul v_mul_i_4_5_1 v_conv1_i_4_5_1 (-1325)@sint32;
(*   %call.i.4.5.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_1, v_call_i_4_5_1);
(*   %arrayidx11.4.5.1 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %1051 = load i16, i16* %arrayidx11.4.5.1, align 2, !tbaa !3 *)
mov v1051 mem0_42;
(*   %sub.4.5.1 = sub i16 %1051, %call.i.4.5.1 *)
sub v_sub_4_5_1 v1051 v_call_i_4_5_1;
(*   store i16 %sub.4.5.1, i16* %arrayidx9.4.5.1, align 2, !tbaa !3 *)
mov mem0_58 v_sub_4_5_1;
(*   %add21.4.5.1 = add i16 %1051, %call.i.4.5.1 *)
add v_add21_4_5_1 v1051 v_call_i_4_5_1;
(*   store i16 %add21.4.5.1, i16* %arrayidx11.4.5.1, align 2, !tbaa !3 *)
mov mem0_42 v_add21_4_5_1;
(*   %arrayidx9.4.6.1 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %1052 = load i16, i16* %arrayidx9.4.6.1, align 2, !tbaa !3 *)
mov v1052 mem0_60;
(*   %conv1.i.4.6.1 = sext i16 %1052 to i32 *)
cast v_conv1_i_4_6_1@sint32 v1052@sint16;
(*   %mul.i.4.6.1 = mul nsw i32 %conv1.i.4.6.1, -1325 *)
mul v_mul_i_4_6_1 v_conv1_i_4_6_1 (-1325)@sint32;
(*   %call.i.4.6.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_1, v_call_i_4_6_1);
(*   %arrayidx11.4.6.1 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %1053 = load i16, i16* %arrayidx11.4.6.1, align 2, !tbaa !3 *)
mov v1053 mem0_44;
(*   %sub.4.6.1 = sub i16 %1053, %call.i.4.6.1 *)
sub v_sub_4_6_1 v1053 v_call_i_4_6_1;
(*   store i16 %sub.4.6.1, i16* %arrayidx9.4.6.1, align 2, !tbaa !3 *)
mov mem0_60 v_sub_4_6_1;
(*   %add21.4.6.1 = add i16 %1053, %call.i.4.6.1 *)
add v_add21_4_6_1 v1053 v_call_i_4_6_1;
(*   store i16 %add21.4.6.1, i16* %arrayidx11.4.6.1, align 2, !tbaa !3 *)
mov mem0_44 v_add21_4_6_1;
(*   %arrayidx9.4.7.1 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %1054 = load i16, i16* %arrayidx9.4.7.1, align 2, !tbaa !3 *)
mov v1054 mem0_62;
(*   %conv1.i.4.7.1 = sext i16 %1054 to i32 *)
cast v_conv1_i_4_7_1@sint32 v1054@sint16;
(*   %mul.i.4.7.1 = mul nsw i32 %conv1.i.4.7.1, -1325 *)
mul v_mul_i_4_7_1 v_conv1_i_4_7_1 (-1325)@sint32;
(*   %call.i.4.7.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_1, v_call_i_4_7_1);
(*   %arrayidx11.4.7.1 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %1055 = load i16, i16* %arrayidx11.4.7.1, align 2, !tbaa !3 *)
mov v1055 mem0_46;
(*   %sub.4.7.1 = sub i16 %1055, %call.i.4.7.1 *)
sub v_sub_4_7_1 v1055 v_call_i_4_7_1;
(*   store i16 %sub.4.7.1, i16* %arrayidx9.4.7.1, align 2, !tbaa !3 *)
mov mem0_62 v_sub_4_7_1;
(*   %add21.4.7.1 = add i16 %1055, %call.i.4.7.1 *)
add v_add21_4_7_1 v1055 v_call_i_4_7_1;
(*   store i16 %add21.4.7.1, i16* %arrayidx11.4.7.1, align 2, !tbaa !3 *)
mov mem0_46 v_add21_4_7_1;

(* NOTE: k = 18 *)

(*   %arrayidx9.4.2118 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %1056 = load i16, i16* %arrayidx9.4.2118, align 2, !tbaa !3 *)
mov v1056 mem0_80;
(*   %conv1.i.4.2119 = sext i16 %1056 to i32 *)
cast v_conv1_i_4_2119@sint32 v1056@sint16;
(*   %mul.i.4.2120 = mul nsw i32 %conv1.i.4.2119, 264 *)
mul v_mul_i_4_2120 v_conv1_i_4_2119 (264)@sint32;
(*   %call.i.4.2121 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2120) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2120, v_call_i_4_2121);
(*   %arrayidx11.4.2122 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %1057 = load i16, i16* %arrayidx11.4.2122, align 2, !tbaa !3 *)
mov v1057 mem0_64;
(*   %sub.4.2123 = sub i16 %1057, %call.i.4.2121 *)
sub v_sub_4_2123 v1057 v_call_i_4_2121;
(*   store i16 %sub.4.2123, i16* %arrayidx9.4.2118, align 2, !tbaa !3 *)
mov mem0_80 v_sub_4_2123;
(*   %add21.4.2124 = add i16 %1057, %call.i.4.2121 *)
add v_add21_4_2124 v1057 v_call_i_4_2121;
(*   store i16 %add21.4.2124, i16* %arrayidx11.4.2122, align 2, !tbaa !3 *)
mov mem0_64 v_add21_4_2124;
(*   %arrayidx9.4.1.2 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %1058 = load i16, i16* %arrayidx9.4.1.2, align 2, !tbaa !3 *)
mov v1058 mem0_82;
(*   %conv1.i.4.1.2 = sext i16 %1058 to i32 *)
cast v_conv1_i_4_1_2@sint32 v1058@sint16;
(*   %mul.i.4.1.2 = mul nsw i32 %conv1.i.4.1.2, 264 *)
mul v_mul_i_4_1_2 v_conv1_i_4_1_2 (264)@sint32;
(*   %call.i.4.1.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_2, v_call_i_4_1_2);
(*   %arrayidx11.4.1.2 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %1059 = load i16, i16* %arrayidx11.4.1.2, align 2, !tbaa !3 *)
mov v1059 mem0_66;
(*   %sub.4.1.2 = sub i16 %1059, %call.i.4.1.2 *)
sub v_sub_4_1_2 v1059 v_call_i_4_1_2;
(*   store i16 %sub.4.1.2, i16* %arrayidx9.4.1.2, align 2, !tbaa !3 *)
mov mem0_82 v_sub_4_1_2;
(*   %add21.4.1.2 = add i16 %1059, %call.i.4.1.2 *)
add v_add21_4_1_2 v1059 v_call_i_4_1_2;
(*   store i16 %add21.4.1.2, i16* %arrayidx11.4.1.2, align 2, !tbaa !3 *)
mov mem0_66 v_add21_4_1_2;
(*   %arrayidx9.4.2.2 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %1060 = load i16, i16* %arrayidx9.4.2.2, align 2, !tbaa !3 *)
mov v1060 mem0_84;
(*   %conv1.i.4.2.2 = sext i16 %1060 to i32 *)
cast v_conv1_i_4_2_2@sint32 v1060@sint16;
(*   %mul.i.4.2.2 = mul nsw i32 %conv1.i.4.2.2, 264 *)
mul v_mul_i_4_2_2 v_conv1_i_4_2_2 (264)@sint32;
(*   %call.i.4.2.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_2, v_call_i_4_2_2);
(*   %arrayidx11.4.2.2 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %1061 = load i16, i16* %arrayidx11.4.2.2, align 2, !tbaa !3 *)
mov v1061 mem0_68;
(*   %sub.4.2.2 = sub i16 %1061, %call.i.4.2.2 *)
sub v_sub_4_2_2 v1061 v_call_i_4_2_2;
(*   store i16 %sub.4.2.2, i16* %arrayidx9.4.2.2, align 2, !tbaa !3 *)
mov mem0_84 v_sub_4_2_2;
(*   %add21.4.2.2 = add i16 %1061, %call.i.4.2.2 *)
add v_add21_4_2_2 v1061 v_call_i_4_2_2;
(*   store i16 %add21.4.2.2, i16* %arrayidx11.4.2.2, align 2, !tbaa !3 *)
mov mem0_68 v_add21_4_2_2;
(*   %arrayidx9.4.3.2 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %1062 = load i16, i16* %arrayidx9.4.3.2, align 2, !tbaa !3 *)
mov v1062 mem0_86;
(*   %conv1.i.4.3.2 = sext i16 %1062 to i32 *)
cast v_conv1_i_4_3_2@sint32 v1062@sint16;
(*   %mul.i.4.3.2 = mul nsw i32 %conv1.i.4.3.2, 264 *)
mul v_mul_i_4_3_2 v_conv1_i_4_3_2 (264)@sint32;
(*   %call.i.4.3.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_2, v_call_i_4_3_2);
(*   %arrayidx11.4.3.2 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %1063 = load i16, i16* %arrayidx11.4.3.2, align 2, !tbaa !3 *)
mov v1063 mem0_70;
(*   %sub.4.3.2 = sub i16 %1063, %call.i.4.3.2 *)
sub v_sub_4_3_2 v1063 v_call_i_4_3_2;
(*   store i16 %sub.4.3.2, i16* %arrayidx9.4.3.2, align 2, !tbaa !3 *)
mov mem0_86 v_sub_4_3_2;
(*   %add21.4.3.2 = add i16 %1063, %call.i.4.3.2 *)
add v_add21_4_3_2 v1063 v_call_i_4_3_2;
(*   store i16 %add21.4.3.2, i16* %arrayidx11.4.3.2, align 2, !tbaa !3 *)
mov mem0_70 v_add21_4_3_2;
(*   %arrayidx9.4.4.2 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %1064 = load i16, i16* %arrayidx9.4.4.2, align 2, !tbaa !3 *)
mov v1064 mem0_88;
(*   %conv1.i.4.4.2 = sext i16 %1064 to i32 *)
cast v_conv1_i_4_4_2@sint32 v1064@sint16;
(*   %mul.i.4.4.2 = mul nsw i32 %conv1.i.4.4.2, 264 *)
mul v_mul_i_4_4_2 v_conv1_i_4_4_2 (264)@sint32;
(*   %call.i.4.4.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_2, v_call_i_4_4_2);
(*   %arrayidx11.4.4.2 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %1065 = load i16, i16* %arrayidx11.4.4.2, align 2, !tbaa !3 *)
mov v1065 mem0_72;
(*   %sub.4.4.2 = sub i16 %1065, %call.i.4.4.2 *)
sub v_sub_4_4_2 v1065 v_call_i_4_4_2;
(*   store i16 %sub.4.4.2, i16* %arrayidx9.4.4.2, align 2, !tbaa !3 *)
mov mem0_88 v_sub_4_4_2;
(*   %add21.4.4.2 = add i16 %1065, %call.i.4.4.2 *)
add v_add21_4_4_2 v1065 v_call_i_4_4_2;
(*   store i16 %add21.4.4.2, i16* %arrayidx11.4.4.2, align 2, !tbaa !3 *)
mov mem0_72 v_add21_4_4_2;
(*   %arrayidx9.4.5.2 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %1066 = load i16, i16* %arrayidx9.4.5.2, align 2, !tbaa !3 *)
mov v1066 mem0_90;
(*   %conv1.i.4.5.2 = sext i16 %1066 to i32 *)
cast v_conv1_i_4_5_2@sint32 v1066@sint16;
(*   %mul.i.4.5.2 = mul nsw i32 %conv1.i.4.5.2, 264 *)
mul v_mul_i_4_5_2 v_conv1_i_4_5_2 (264)@sint32;
(*   %call.i.4.5.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_2, v_call_i_4_5_2);
(*   %arrayidx11.4.5.2 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %1067 = load i16, i16* %arrayidx11.4.5.2, align 2, !tbaa !3 *)
mov v1067 mem0_74;
(*   %sub.4.5.2 = sub i16 %1067, %call.i.4.5.2 *)
sub v_sub_4_5_2 v1067 v_call_i_4_5_2;
(*   store i16 %sub.4.5.2, i16* %arrayidx9.4.5.2, align 2, !tbaa !3 *)
mov mem0_90 v_sub_4_5_2;
(*   %add21.4.5.2 = add i16 %1067, %call.i.4.5.2 *)
add v_add21_4_5_2 v1067 v_call_i_4_5_2;
(*   store i16 %add21.4.5.2, i16* %arrayidx11.4.5.2, align 2, !tbaa !3 *)
mov mem0_74 v_add21_4_5_2;
(*   %arrayidx9.4.6.2 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %1068 = load i16, i16* %arrayidx9.4.6.2, align 2, !tbaa !3 *)
mov v1068 mem0_92;
(*   %conv1.i.4.6.2 = sext i16 %1068 to i32 *)
cast v_conv1_i_4_6_2@sint32 v1068@sint16;
(*   %mul.i.4.6.2 = mul nsw i32 %conv1.i.4.6.2, 264 *)
mul v_mul_i_4_6_2 v_conv1_i_4_6_2 (264)@sint32;
(*   %call.i.4.6.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_2, v_call_i_4_6_2);
(*   %arrayidx11.4.6.2 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %1069 = load i16, i16* %arrayidx11.4.6.2, align 2, !tbaa !3 *)
mov v1069 mem0_76;
(*   %sub.4.6.2 = sub i16 %1069, %call.i.4.6.2 *)
sub v_sub_4_6_2 v1069 v_call_i_4_6_2;
(*   store i16 %sub.4.6.2, i16* %arrayidx9.4.6.2, align 2, !tbaa !3 *)
mov mem0_92 v_sub_4_6_2;
(*   %add21.4.6.2 = add i16 %1069, %call.i.4.6.2 *)
add v_add21_4_6_2 v1069 v_call_i_4_6_2;
(*   store i16 %add21.4.6.2, i16* %arrayidx11.4.6.2, align 2, !tbaa !3 *)
mov mem0_76 v_add21_4_6_2;
(*   %arrayidx9.4.7.2 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %1070 = load i16, i16* %arrayidx9.4.7.2, align 2, !tbaa !3 *)
mov v1070 mem0_94;
(*   %conv1.i.4.7.2 = sext i16 %1070 to i32 *)
cast v_conv1_i_4_7_2@sint32 v1070@sint16;
(*   %mul.i.4.7.2 = mul nsw i32 %conv1.i.4.7.2, 264 *)
mul v_mul_i_4_7_2 v_conv1_i_4_7_2 (264)@sint32;
(*   %call.i.4.7.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_2, v_call_i_4_7_2);
(*   %arrayidx11.4.7.2 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %1071 = load i16, i16* %arrayidx11.4.7.2, align 2, !tbaa !3 *)
mov v1071 mem0_78;
(*   %sub.4.7.2 = sub i16 %1071, %call.i.4.7.2 *)
sub v_sub_4_7_2 v1071 v_call_i_4_7_2;
(*   store i16 %sub.4.7.2, i16* %arrayidx9.4.7.2, align 2, !tbaa !3 *)
mov mem0_94 v_sub_4_7_2;
(*   %add21.4.7.2 = add i16 %1071, %call.i.4.7.2 *)
add v_add21_4_7_2 v1071 v_call_i_4_7_2;
(*   store i16 %add21.4.7.2, i16* %arrayidx11.4.7.2, align 2, !tbaa !3 *)
mov mem0_78 v_add21_4_7_2;

(* NOTE: k = 19 *)

(*   %arrayidx9.4.3128 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %1072 = load i16, i16* %arrayidx9.4.3128, align 2, !tbaa !3 *)
mov v1072 mem0_112;
(*   %conv1.i.4.3129 = sext i16 %1072 to i32 *)
cast v_conv1_i_4_3129@sint32 v1072@sint16;
(*   %mul.i.4.3130 = mul nsw i32 %conv1.i.4.3129, 383 *)
mul v_mul_i_4_3130 v_conv1_i_4_3129 (383)@sint32;
(*   %call.i.4.3131 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3130) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3130, v_call_i_4_3131);
(*   %arrayidx11.4.3132 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %1073 = load i16, i16* %arrayidx11.4.3132, align 2, !tbaa !3 *)
mov v1073 mem0_96;
(*   %sub.4.3133 = sub i16 %1073, %call.i.4.3131 *)
sub v_sub_4_3133 v1073 v_call_i_4_3131;
(*   store i16 %sub.4.3133, i16* %arrayidx9.4.3128, align 2, !tbaa !3 *)
mov mem0_112 v_sub_4_3133;
(*   %add21.4.3134 = add i16 %1073, %call.i.4.3131 *)
add v_add21_4_3134 v1073 v_call_i_4_3131;
(*   store i16 %add21.4.3134, i16* %arrayidx11.4.3132, align 2, !tbaa !3 *)
mov mem0_96 v_add21_4_3134;
(*   %arrayidx9.4.1.3 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %1074 = load i16, i16* %arrayidx9.4.1.3, align 2, !tbaa !3 *)
mov v1074 mem0_114;
(*   %conv1.i.4.1.3 = sext i16 %1074 to i32 *)
cast v_conv1_i_4_1_3@sint32 v1074@sint16;
(*   %mul.i.4.1.3 = mul nsw i32 %conv1.i.4.1.3, 383 *)
mul v_mul_i_4_1_3 v_conv1_i_4_1_3 (383)@sint32;
(*   %call.i.4.1.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_3, v_call_i_4_1_3);
(*   %arrayidx11.4.1.3 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %1075 = load i16, i16* %arrayidx11.4.1.3, align 2, !tbaa !3 *)
mov v1075 mem0_98;
(*   %sub.4.1.3 = sub i16 %1075, %call.i.4.1.3 *)
sub v_sub_4_1_3 v1075 v_call_i_4_1_3;
(*   store i16 %sub.4.1.3, i16* %arrayidx9.4.1.3, align 2, !tbaa !3 *)
mov mem0_114 v_sub_4_1_3;
(*   %add21.4.1.3 = add i16 %1075, %call.i.4.1.3 *)
add v_add21_4_1_3 v1075 v_call_i_4_1_3;
(*   store i16 %add21.4.1.3, i16* %arrayidx11.4.1.3, align 2, !tbaa !3 *)
mov mem0_98 v_add21_4_1_3;
(*   %arrayidx9.4.2.3 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %1076 = load i16, i16* %arrayidx9.4.2.3, align 2, !tbaa !3 *)
mov v1076 mem0_116;
(*   %conv1.i.4.2.3 = sext i16 %1076 to i32 *)
cast v_conv1_i_4_2_3@sint32 v1076@sint16;
(*   %mul.i.4.2.3 = mul nsw i32 %conv1.i.4.2.3, 383 *)
mul v_mul_i_4_2_3 v_conv1_i_4_2_3 (383)@sint32;
(*   %call.i.4.2.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_3, v_call_i_4_2_3);
(*   %arrayidx11.4.2.3 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %1077 = load i16, i16* %arrayidx11.4.2.3, align 2, !tbaa !3 *)
mov v1077 mem0_100;
(*   %sub.4.2.3 = sub i16 %1077, %call.i.4.2.3 *)
sub v_sub_4_2_3 v1077 v_call_i_4_2_3;
(*   store i16 %sub.4.2.3, i16* %arrayidx9.4.2.3, align 2, !tbaa !3 *)
mov mem0_116 v_sub_4_2_3;
(*   %add21.4.2.3 = add i16 %1077, %call.i.4.2.3 *)
add v_add21_4_2_3 v1077 v_call_i_4_2_3;
(*   store i16 %add21.4.2.3, i16* %arrayidx11.4.2.3, align 2, !tbaa !3 *)
mov mem0_100 v_add21_4_2_3;
(*   %arrayidx9.4.3.3 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %1078 = load i16, i16* %arrayidx9.4.3.3, align 2, !tbaa !3 *)
mov v1078 mem0_118;
(*   %conv1.i.4.3.3 = sext i16 %1078 to i32 *)
cast v_conv1_i_4_3_3@sint32 v1078@sint16;
(*   %mul.i.4.3.3 = mul nsw i32 %conv1.i.4.3.3, 383 *)
mul v_mul_i_4_3_3 v_conv1_i_4_3_3 (383)@sint32;
(*   %call.i.4.3.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_3, v_call_i_4_3_3);
(*   %arrayidx11.4.3.3 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %1079 = load i16, i16* %arrayidx11.4.3.3, align 2, !tbaa !3 *)
mov v1079 mem0_102;
(*   %sub.4.3.3 = sub i16 %1079, %call.i.4.3.3 *)
sub v_sub_4_3_3 v1079 v_call_i_4_3_3;
(*   store i16 %sub.4.3.3, i16* %arrayidx9.4.3.3, align 2, !tbaa !3 *)
mov mem0_118 v_sub_4_3_3;
(*   %add21.4.3.3 = add i16 %1079, %call.i.4.3.3 *)
add v_add21_4_3_3 v1079 v_call_i_4_3_3;
(*   store i16 %add21.4.3.3, i16* %arrayidx11.4.3.3, align 2, !tbaa !3 *)
mov mem0_102 v_add21_4_3_3;
(*   %arrayidx9.4.4.3 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %1080 = load i16, i16* %arrayidx9.4.4.3, align 2, !tbaa !3 *)
mov v1080 mem0_120;
(*   %conv1.i.4.4.3 = sext i16 %1080 to i32 *)
cast v_conv1_i_4_4_3@sint32 v1080@sint16;
(*   %mul.i.4.4.3 = mul nsw i32 %conv1.i.4.4.3, 383 *)
mul v_mul_i_4_4_3 v_conv1_i_4_4_3 (383)@sint32;
(*   %call.i.4.4.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_3, v_call_i_4_4_3);
(*   %arrayidx11.4.4.3 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %1081 = load i16, i16* %arrayidx11.4.4.3, align 2, !tbaa !3 *)
mov v1081 mem0_104;
(*   %sub.4.4.3 = sub i16 %1081, %call.i.4.4.3 *)
sub v_sub_4_4_3 v1081 v_call_i_4_4_3;
(*   store i16 %sub.4.4.3, i16* %arrayidx9.4.4.3, align 2, !tbaa !3 *)
mov mem0_120 v_sub_4_4_3;
(*   %add21.4.4.3 = add i16 %1081, %call.i.4.4.3 *)
add v_add21_4_4_3 v1081 v_call_i_4_4_3;
(*   store i16 %add21.4.4.3, i16* %arrayidx11.4.4.3, align 2, !tbaa !3 *)
mov mem0_104 v_add21_4_4_3;
(*   %arrayidx9.4.5.3 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %1082 = load i16, i16* %arrayidx9.4.5.3, align 2, !tbaa !3 *)
mov v1082 mem0_122;
(*   %conv1.i.4.5.3 = sext i16 %1082 to i32 *)
cast v_conv1_i_4_5_3@sint32 v1082@sint16;
(*   %mul.i.4.5.3 = mul nsw i32 %conv1.i.4.5.3, 383 *)
mul v_mul_i_4_5_3 v_conv1_i_4_5_3 (383)@sint32;
(*   %call.i.4.5.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_3, v_call_i_4_5_3);
(*   %arrayidx11.4.5.3 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %1083 = load i16, i16* %arrayidx11.4.5.3, align 2, !tbaa !3 *)
mov v1083 mem0_106;
(*   %sub.4.5.3 = sub i16 %1083, %call.i.4.5.3 *)
sub v_sub_4_5_3 v1083 v_call_i_4_5_3;
(*   store i16 %sub.4.5.3, i16* %arrayidx9.4.5.3, align 2, !tbaa !3 *)
mov mem0_122 v_sub_4_5_3;
(*   %add21.4.5.3 = add i16 %1083, %call.i.4.5.3 *)
add v_add21_4_5_3 v1083 v_call_i_4_5_3;
(*   store i16 %add21.4.5.3, i16* %arrayidx11.4.5.3, align 2, !tbaa !3 *)
mov mem0_106 v_add21_4_5_3;
(*   %arrayidx9.4.6.3 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %1084 = load i16, i16* %arrayidx9.4.6.3, align 2, !tbaa !3 *)
mov v1084 mem0_124;
(*   %conv1.i.4.6.3 = sext i16 %1084 to i32 *)
cast v_conv1_i_4_6_3@sint32 v1084@sint16;
(*   %mul.i.4.6.3 = mul nsw i32 %conv1.i.4.6.3, 383 *)
mul v_mul_i_4_6_3 v_conv1_i_4_6_3 (383)@sint32;
(*   %call.i.4.6.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_3, v_call_i_4_6_3);
(*   %arrayidx11.4.6.3 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %1085 = load i16, i16* %arrayidx11.4.6.3, align 2, !tbaa !3 *)
mov v1085 mem0_108;
(*   %sub.4.6.3 = sub i16 %1085, %call.i.4.6.3 *)
sub v_sub_4_6_3 v1085 v_call_i_4_6_3;
(*   store i16 %sub.4.6.3, i16* %arrayidx9.4.6.3, align 2, !tbaa !3 *)
mov mem0_124 v_sub_4_6_3;
(*   %add21.4.6.3 = add i16 %1085, %call.i.4.6.3 *)
add v_add21_4_6_3 v1085 v_call_i_4_6_3;
(*   store i16 %add21.4.6.3, i16* %arrayidx11.4.6.3, align 2, !tbaa !3 *)
mov mem0_108 v_add21_4_6_3;
(*   %arrayidx9.4.7.3 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %1086 = load i16, i16* %arrayidx9.4.7.3, align 2, !tbaa !3 *)
mov v1086 mem0_126;
(*   %conv1.i.4.7.3 = sext i16 %1086 to i32 *)
cast v_conv1_i_4_7_3@sint32 v1086@sint16;
(*   %mul.i.4.7.3 = mul nsw i32 %conv1.i.4.7.3, 383 *)
mul v_mul_i_4_7_3 v_conv1_i_4_7_3 (383)@sint32;
(*   %call.i.4.7.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_3, v_call_i_4_7_3);
(*   %arrayidx11.4.7.3 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %1087 = load i16, i16* %arrayidx11.4.7.3, align 2, !tbaa !3 *)
mov v1087 mem0_110;
(*   %sub.4.7.3 = sub i16 %1087, %call.i.4.7.3 *)
sub v_sub_4_7_3 v1087 v_call_i_4_7_3;
(*   store i16 %sub.4.7.3, i16* %arrayidx9.4.7.3, align 2, !tbaa !3 *)
mov mem0_126 v_sub_4_7_3;
(*   %add21.4.7.3 = add i16 %1087, %call.i.4.7.3 *)
add v_add21_4_7_3 v1087 v_call_i_4_7_3;
(*   store i16 %add21.4.7.3, i16* %arrayidx11.4.7.3, align 2, !tbaa !3 *)
mov mem0_110 v_add21_4_7_3;

(* NOTE: k = 20 *)

(*   %arrayidx9.4.4138 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %1088 = load i16, i16* %arrayidx9.4.4138, align 2, !tbaa !3 *)
mov v1088 mem0_144;
(*   %conv1.i.4.4139 = sext i16 %1088 to i32 *)
cast v_conv1_i_4_4139@sint32 v1088@sint16;
(*   %mul.i.4.4140 = mul nsw i32 %conv1.i.4.4139, -829 *)
mul v_mul_i_4_4140 v_conv1_i_4_4139 (-829)@sint32;
(*   %call.i.4.4141 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4140) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4140, v_call_i_4_4141);
(*   %arrayidx11.4.4142 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %1089 = load i16, i16* %arrayidx11.4.4142, align 2, !tbaa !3 *)
mov v1089 mem0_128;
(*   %sub.4.4143 = sub i16 %1089, %call.i.4.4141 *)
sub v_sub_4_4143 v1089 v_call_i_4_4141;
(*   store i16 %sub.4.4143, i16* %arrayidx9.4.4138, align 2, !tbaa !3 *)
mov mem0_144 v_sub_4_4143;
(*   %add21.4.4144 = add i16 %1089, %call.i.4.4141 *)
add v_add21_4_4144 v1089 v_call_i_4_4141;
(*   store i16 %add21.4.4144, i16* %arrayidx11.4.4142, align 2, !tbaa !3 *)
mov mem0_128 v_add21_4_4144;
(*   %arrayidx9.4.1.4 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %1090 = load i16, i16* %arrayidx9.4.1.4, align 2, !tbaa !3 *)
mov v1090 mem0_146;
(*   %conv1.i.4.1.4 = sext i16 %1090 to i32 *)
cast v_conv1_i_4_1_4@sint32 v1090@sint16;
(*   %mul.i.4.1.4 = mul nsw i32 %conv1.i.4.1.4, -829 *)
mul v_mul_i_4_1_4 v_conv1_i_4_1_4 (-829)@sint32;
(*   %call.i.4.1.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_4, v_call_i_4_1_4);
(*   %arrayidx11.4.1.4 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %1091 = load i16, i16* %arrayidx11.4.1.4, align 2, !tbaa !3 *)
mov v1091 mem0_130;
(*   %sub.4.1.4 = sub i16 %1091, %call.i.4.1.4 *)
sub v_sub_4_1_4 v1091 v_call_i_4_1_4;
(*   store i16 %sub.4.1.4, i16* %arrayidx9.4.1.4, align 2, !tbaa !3 *)
mov mem0_146 v_sub_4_1_4;
(*   %add21.4.1.4 = add i16 %1091, %call.i.4.1.4 *)
add v_add21_4_1_4 v1091 v_call_i_4_1_4;
(*   store i16 %add21.4.1.4, i16* %arrayidx11.4.1.4, align 2, !tbaa !3 *)
mov mem0_130 v_add21_4_1_4;
(*   %arrayidx9.4.2.4 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %1092 = load i16, i16* %arrayidx9.4.2.4, align 2, !tbaa !3 *)
mov v1092 mem0_148;
(*   %conv1.i.4.2.4 = sext i16 %1092 to i32 *)
cast v_conv1_i_4_2_4@sint32 v1092@sint16;
(*   %mul.i.4.2.4 = mul nsw i32 %conv1.i.4.2.4, -829 *)
mul v_mul_i_4_2_4 v_conv1_i_4_2_4 (-829)@sint32;
(*   %call.i.4.2.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_4, v_call_i_4_2_4);
(*   %arrayidx11.4.2.4 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %1093 = load i16, i16* %arrayidx11.4.2.4, align 2, !tbaa !3 *)
mov v1093 mem0_132;
(*   %sub.4.2.4 = sub i16 %1093, %call.i.4.2.4 *)
sub v_sub_4_2_4 v1093 v_call_i_4_2_4;
(*   store i16 %sub.4.2.4, i16* %arrayidx9.4.2.4, align 2, !tbaa !3 *)
mov mem0_148 v_sub_4_2_4;
(*   %add21.4.2.4 = add i16 %1093, %call.i.4.2.4 *)
add v_add21_4_2_4 v1093 v_call_i_4_2_4;
(*   store i16 %add21.4.2.4, i16* %arrayidx11.4.2.4, align 2, !tbaa !3 *)
mov mem0_132 v_add21_4_2_4;
(*   %arrayidx9.4.3.4 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %1094 = load i16, i16* %arrayidx9.4.3.4, align 2, !tbaa !3 *)
mov v1094 mem0_150;
(*   %conv1.i.4.3.4 = sext i16 %1094 to i32 *)
cast v_conv1_i_4_3_4@sint32 v1094@sint16;
(*   %mul.i.4.3.4 = mul nsw i32 %conv1.i.4.3.4, -829 *)
mul v_mul_i_4_3_4 v_conv1_i_4_3_4 (-829)@sint32;
(*   %call.i.4.3.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_4, v_call_i_4_3_4);
(*   %arrayidx11.4.3.4 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %1095 = load i16, i16* %arrayidx11.4.3.4, align 2, !tbaa !3 *)
mov v1095 mem0_134;
(*   %sub.4.3.4 = sub i16 %1095, %call.i.4.3.4 *)
sub v_sub_4_3_4 v1095 v_call_i_4_3_4;
(*   store i16 %sub.4.3.4, i16* %arrayidx9.4.3.4, align 2, !tbaa !3 *)
mov mem0_150 v_sub_4_3_4;
(*   %add21.4.3.4 = add i16 %1095, %call.i.4.3.4 *)
add v_add21_4_3_4 v1095 v_call_i_4_3_4;
(*   store i16 %add21.4.3.4, i16* %arrayidx11.4.3.4, align 2, !tbaa !3 *)
mov mem0_134 v_add21_4_3_4;
(*   %arrayidx9.4.4.4 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %1096 = load i16, i16* %arrayidx9.4.4.4, align 2, !tbaa !3 *)
mov v1096 mem0_152;
(*   %conv1.i.4.4.4 = sext i16 %1096 to i32 *)
cast v_conv1_i_4_4_4@sint32 v1096@sint16;
(*   %mul.i.4.4.4 = mul nsw i32 %conv1.i.4.4.4, -829 *)
mul v_mul_i_4_4_4 v_conv1_i_4_4_4 (-829)@sint32;
(*   %call.i.4.4.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_4, v_call_i_4_4_4);
(*   %arrayidx11.4.4.4 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %1097 = load i16, i16* %arrayidx11.4.4.4, align 2, !tbaa !3 *)
mov v1097 mem0_136;
(*   %sub.4.4.4 = sub i16 %1097, %call.i.4.4.4 *)
sub v_sub_4_4_4 v1097 v_call_i_4_4_4;
(*   store i16 %sub.4.4.4, i16* %arrayidx9.4.4.4, align 2, !tbaa !3 *)
mov mem0_152 v_sub_4_4_4;
(*   %add21.4.4.4 = add i16 %1097, %call.i.4.4.4 *)
add v_add21_4_4_4 v1097 v_call_i_4_4_4;
(*   store i16 %add21.4.4.4, i16* %arrayidx11.4.4.4, align 2, !tbaa !3 *)
mov mem0_136 v_add21_4_4_4;
(*   %arrayidx9.4.5.4 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %1098 = load i16, i16* %arrayidx9.4.5.4, align 2, !tbaa !3 *)
mov v1098 mem0_154;
(*   %conv1.i.4.5.4 = sext i16 %1098 to i32 *)
cast v_conv1_i_4_5_4@sint32 v1098@sint16;
(*   %mul.i.4.5.4 = mul nsw i32 %conv1.i.4.5.4, -829 *)
mul v_mul_i_4_5_4 v_conv1_i_4_5_4 (-829)@sint32;
(*   %call.i.4.5.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_4, v_call_i_4_5_4);
(*   %arrayidx11.4.5.4 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %1099 = load i16, i16* %arrayidx11.4.5.4, align 2, !tbaa !3 *)
mov v1099 mem0_138;
(*   %sub.4.5.4 = sub i16 %1099, %call.i.4.5.4 *)
sub v_sub_4_5_4 v1099 v_call_i_4_5_4;
(*   store i16 %sub.4.5.4, i16* %arrayidx9.4.5.4, align 2, !tbaa !3 *)
mov mem0_154 v_sub_4_5_4;
(*   %add21.4.5.4 = add i16 %1099, %call.i.4.5.4 *)
add v_add21_4_5_4 v1099 v_call_i_4_5_4;
(*   store i16 %add21.4.5.4, i16* %arrayidx11.4.5.4, align 2, !tbaa !3 *)
mov mem0_138 v_add21_4_5_4;
(*   %arrayidx9.4.6.4 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %1100 = load i16, i16* %arrayidx9.4.6.4, align 2, !tbaa !3 *)
mov v1100 mem0_156;
(*   %conv1.i.4.6.4 = sext i16 %1100 to i32 *)
cast v_conv1_i_4_6_4@sint32 v1100@sint16;
(*   %mul.i.4.6.4 = mul nsw i32 %conv1.i.4.6.4, -829 *)
mul v_mul_i_4_6_4 v_conv1_i_4_6_4 (-829)@sint32;
(*   %call.i.4.6.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_4, v_call_i_4_6_4);
(*   %arrayidx11.4.6.4 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %1101 = load i16, i16* %arrayidx11.4.6.4, align 2, !tbaa !3 *)
mov v1101 mem0_140;
(*   %sub.4.6.4 = sub i16 %1101, %call.i.4.6.4 *)
sub v_sub_4_6_4 v1101 v_call_i_4_6_4;
(*   store i16 %sub.4.6.4, i16* %arrayidx9.4.6.4, align 2, !tbaa !3 *)
mov mem0_156 v_sub_4_6_4;
(*   %add21.4.6.4 = add i16 %1101, %call.i.4.6.4 *)
add v_add21_4_6_4 v1101 v_call_i_4_6_4;
(*   store i16 %add21.4.6.4, i16* %arrayidx11.4.6.4, align 2, !tbaa !3 *)
mov mem0_140 v_add21_4_6_4;
(*   %arrayidx9.4.7.4 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %1102 = load i16, i16* %arrayidx9.4.7.4, align 2, !tbaa !3 *)
mov v1102 mem0_158;
(*   %conv1.i.4.7.4 = sext i16 %1102 to i32 *)
cast v_conv1_i_4_7_4@sint32 v1102@sint16;
(*   %mul.i.4.7.4 = mul nsw i32 %conv1.i.4.7.4, -829 *)
mul v_mul_i_4_7_4 v_conv1_i_4_7_4 (-829)@sint32;
(*   %call.i.4.7.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_4, v_call_i_4_7_4);
(*   %arrayidx11.4.7.4 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %1103 = load i16, i16* %arrayidx11.4.7.4, align 2, !tbaa !3 *)
mov v1103 mem0_142;
(*   %sub.4.7.4 = sub i16 %1103, %call.i.4.7.4 *)
sub v_sub_4_7_4 v1103 v_call_i_4_7_4;
(*   store i16 %sub.4.7.4, i16* %arrayidx9.4.7.4, align 2, !tbaa !3 *)
mov mem0_158 v_sub_4_7_4;
(*   %add21.4.7.4 = add i16 %1103, %call.i.4.7.4 *)
add v_add21_4_7_4 v1103 v_call_i_4_7_4;
(*   store i16 %add21.4.7.4, i16* %arrayidx11.4.7.4, align 2, !tbaa !3 *)
mov mem0_142 v_add21_4_7_4;

(* NOTE: k = 21 *)

(*   %arrayidx9.4.5148 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %1104 = load i16, i16* %arrayidx9.4.5148, align 2, !tbaa !3 *)
mov v1104 mem0_176;
(*   %conv1.i.4.5149 = sext i16 %1104 to i32 *)
cast v_conv1_i_4_5149@sint32 v1104@sint16;
(*   %mul.i.4.5150 = mul nsw i32 %conv1.i.4.5149, 1458 *)
mul v_mul_i_4_5150 v_conv1_i_4_5149 (1458)@sint32;
(*   %call.i.4.5151 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5150) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5150, v_call_i_4_5151);
(*   %arrayidx11.4.5152 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %1105 = load i16, i16* %arrayidx11.4.5152, align 2, !tbaa !3 *)
mov v1105 mem0_160;
(*   %sub.4.5153 = sub i16 %1105, %call.i.4.5151 *)
sub v_sub_4_5153 v1105 v_call_i_4_5151;
(*   store i16 %sub.4.5153, i16* %arrayidx9.4.5148, align 2, !tbaa !3 *)
mov mem0_176 v_sub_4_5153;
(*   %add21.4.5154 = add i16 %1105, %call.i.4.5151 *)
add v_add21_4_5154 v1105 v_call_i_4_5151;
(*   store i16 %add21.4.5154, i16* %arrayidx11.4.5152, align 2, !tbaa !3 *)
mov mem0_160 v_add21_4_5154;
(*   %arrayidx9.4.1.5 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %1106 = load i16, i16* %arrayidx9.4.1.5, align 2, !tbaa !3 *)
mov v1106 mem0_178;
(*   %conv1.i.4.1.5 = sext i16 %1106 to i32 *)
cast v_conv1_i_4_1_5@sint32 v1106@sint16;
(*   %mul.i.4.1.5 = mul nsw i32 %conv1.i.4.1.5, 1458 *)
mul v_mul_i_4_1_5 v_conv1_i_4_1_5 (1458)@sint32;
(*   %call.i.4.1.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_5, v_call_i_4_1_5);
(*   %arrayidx11.4.1.5 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %1107 = load i16, i16* %arrayidx11.4.1.5, align 2, !tbaa !3 *)
mov v1107 mem0_162;
(*   %sub.4.1.5 = sub i16 %1107, %call.i.4.1.5 *)
sub v_sub_4_1_5 v1107 v_call_i_4_1_5;
(*   store i16 %sub.4.1.5, i16* %arrayidx9.4.1.5, align 2, !tbaa !3 *)
mov mem0_178 v_sub_4_1_5;
(*   %add21.4.1.5 = add i16 %1107, %call.i.4.1.5 *)
add v_add21_4_1_5 v1107 v_call_i_4_1_5;
(*   store i16 %add21.4.1.5, i16* %arrayidx11.4.1.5, align 2, !tbaa !3 *)
mov mem0_162 v_add21_4_1_5;
(*   %arrayidx9.4.2.5 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %1108 = load i16, i16* %arrayidx9.4.2.5, align 2, !tbaa !3 *)
mov v1108 mem0_180;
(*   %conv1.i.4.2.5 = sext i16 %1108 to i32 *)
cast v_conv1_i_4_2_5@sint32 v1108@sint16;
(*   %mul.i.4.2.5 = mul nsw i32 %conv1.i.4.2.5, 1458 *)
mul v_mul_i_4_2_5 v_conv1_i_4_2_5 (1458)@sint32;
(*   %call.i.4.2.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_5, v_call_i_4_2_5);
(*   %arrayidx11.4.2.5 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %1109 = load i16, i16* %arrayidx11.4.2.5, align 2, !tbaa !3 *)
mov v1109 mem0_164;
(*   %sub.4.2.5 = sub i16 %1109, %call.i.4.2.5 *)
sub v_sub_4_2_5 v1109 v_call_i_4_2_5;
(*   store i16 %sub.4.2.5, i16* %arrayidx9.4.2.5, align 2, !tbaa !3 *)
mov mem0_180 v_sub_4_2_5;
(*   %add21.4.2.5 = add i16 %1109, %call.i.4.2.5 *)
add v_add21_4_2_5 v1109 v_call_i_4_2_5;
(*   store i16 %add21.4.2.5, i16* %arrayidx11.4.2.5, align 2, !tbaa !3 *)
mov mem0_164 v_add21_4_2_5;
(*   %arrayidx9.4.3.5 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %1110 = load i16, i16* %arrayidx9.4.3.5, align 2, !tbaa !3 *)
mov v1110 mem0_182;
(*   %conv1.i.4.3.5 = sext i16 %1110 to i32 *)
cast v_conv1_i_4_3_5@sint32 v1110@sint16;
(*   %mul.i.4.3.5 = mul nsw i32 %conv1.i.4.3.5, 1458 *)
mul v_mul_i_4_3_5 v_conv1_i_4_3_5 (1458)@sint32;
(*   %call.i.4.3.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_5, v_call_i_4_3_5);
(*   %arrayidx11.4.3.5 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %1111 = load i16, i16* %arrayidx11.4.3.5, align 2, !tbaa !3 *)
mov v1111 mem0_166;
(*   %sub.4.3.5 = sub i16 %1111, %call.i.4.3.5 *)
sub v_sub_4_3_5 v1111 v_call_i_4_3_5;
(*   store i16 %sub.4.3.5, i16* %arrayidx9.4.3.5, align 2, !tbaa !3 *)
mov mem0_182 v_sub_4_3_5;
(*   %add21.4.3.5 = add i16 %1111, %call.i.4.3.5 *)
add v_add21_4_3_5 v1111 v_call_i_4_3_5;
(*   store i16 %add21.4.3.5, i16* %arrayidx11.4.3.5, align 2, !tbaa !3 *)
mov mem0_166 v_add21_4_3_5;
(*   %arrayidx9.4.4.5 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %1112 = load i16, i16* %arrayidx9.4.4.5, align 2, !tbaa !3 *)
mov v1112 mem0_184;
(*   %conv1.i.4.4.5 = sext i16 %1112 to i32 *)
cast v_conv1_i_4_4_5@sint32 v1112@sint16;
(*   %mul.i.4.4.5 = mul nsw i32 %conv1.i.4.4.5, 1458 *)
mul v_mul_i_4_4_5 v_conv1_i_4_4_5 (1458)@sint32;
(*   %call.i.4.4.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_5, v_call_i_4_4_5);
(*   %arrayidx11.4.4.5 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %1113 = load i16, i16* %arrayidx11.4.4.5, align 2, !tbaa !3 *)
mov v1113 mem0_168;
(*   %sub.4.4.5 = sub i16 %1113, %call.i.4.4.5 *)
sub v_sub_4_4_5 v1113 v_call_i_4_4_5;
(*   store i16 %sub.4.4.5, i16* %arrayidx9.4.4.5, align 2, !tbaa !3 *)
mov mem0_184 v_sub_4_4_5;
(*   %add21.4.4.5 = add i16 %1113, %call.i.4.4.5 *)
add v_add21_4_4_5 v1113 v_call_i_4_4_5;
(*   store i16 %add21.4.4.5, i16* %arrayidx11.4.4.5, align 2, !tbaa !3 *)
mov mem0_168 v_add21_4_4_5;
(*   %arrayidx9.4.5.5 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %1114 = load i16, i16* %arrayidx9.4.5.5, align 2, !tbaa !3 *)
mov v1114 mem0_186;
(*   %conv1.i.4.5.5 = sext i16 %1114 to i32 *)
cast v_conv1_i_4_5_5@sint32 v1114@sint16;
(*   %mul.i.4.5.5 = mul nsw i32 %conv1.i.4.5.5, 1458 *)
mul v_mul_i_4_5_5 v_conv1_i_4_5_5 (1458)@sint32;
(*   %call.i.4.5.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_5, v_call_i_4_5_5);
(*   %arrayidx11.4.5.5 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %1115 = load i16, i16* %arrayidx11.4.5.5, align 2, !tbaa !3 *)
mov v1115 mem0_170;
(*   %sub.4.5.5 = sub i16 %1115, %call.i.4.5.5 *)
sub v_sub_4_5_5 v1115 v_call_i_4_5_5;
(*   store i16 %sub.4.5.5, i16* %arrayidx9.4.5.5, align 2, !tbaa !3 *)
mov mem0_186 v_sub_4_5_5;
(*   %add21.4.5.5 = add i16 %1115, %call.i.4.5.5 *)
add v_add21_4_5_5 v1115 v_call_i_4_5_5;
(*   store i16 %add21.4.5.5, i16* %arrayidx11.4.5.5, align 2, !tbaa !3 *)
mov mem0_170 v_add21_4_5_5;
(*   %arrayidx9.4.6.5 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %1116 = load i16, i16* %arrayidx9.4.6.5, align 2, !tbaa !3 *)
mov v1116 mem0_188;
(*   %conv1.i.4.6.5 = sext i16 %1116 to i32 *)
cast v_conv1_i_4_6_5@sint32 v1116@sint16;
(*   %mul.i.4.6.5 = mul nsw i32 %conv1.i.4.6.5, 1458 *)
mul v_mul_i_4_6_5 v_conv1_i_4_6_5 (1458)@sint32;
(*   %call.i.4.6.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_5, v_call_i_4_6_5);
(*   %arrayidx11.4.6.5 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %1117 = load i16, i16* %arrayidx11.4.6.5, align 2, !tbaa !3 *)
mov v1117 mem0_172;
(*   %sub.4.6.5 = sub i16 %1117, %call.i.4.6.5 *)
sub v_sub_4_6_5 v1117 v_call_i_4_6_5;
(*   store i16 %sub.4.6.5, i16* %arrayidx9.4.6.5, align 2, !tbaa !3 *)
mov mem0_188 v_sub_4_6_5;
(*   %add21.4.6.5 = add i16 %1117, %call.i.4.6.5 *)
add v_add21_4_6_5 v1117 v_call_i_4_6_5;
(*   store i16 %add21.4.6.5, i16* %arrayidx11.4.6.5, align 2, !tbaa !3 *)
mov mem0_172 v_add21_4_6_5;
(*   %arrayidx9.4.7.5 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %1118 = load i16, i16* %arrayidx9.4.7.5, align 2, !tbaa !3 *)
mov v1118 mem0_190;
(*   %conv1.i.4.7.5 = sext i16 %1118 to i32 *)
cast v_conv1_i_4_7_5@sint32 v1118@sint16;
(*   %mul.i.4.7.5 = mul nsw i32 %conv1.i.4.7.5, 1458 *)
mul v_mul_i_4_7_5 v_conv1_i_4_7_5 (1458)@sint32;
(*   %call.i.4.7.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_5, v_call_i_4_7_5);
(*   %arrayidx11.4.7.5 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %1119 = load i16, i16* %arrayidx11.4.7.5, align 2, !tbaa !3 *)
mov v1119 mem0_174;
(*   %sub.4.7.5 = sub i16 %1119, %call.i.4.7.5 *)
sub v_sub_4_7_5 v1119 v_call_i_4_7_5;
(*   store i16 %sub.4.7.5, i16* %arrayidx9.4.7.5, align 2, !tbaa !3 *)
mov mem0_190 v_sub_4_7_5;
(*   %add21.4.7.5 = add i16 %1119, %call.i.4.7.5 *)
add v_add21_4_7_5 v1119 v_call_i_4_7_5;
(*   store i16 %add21.4.7.5, i16* %arrayidx11.4.7.5, align 2, !tbaa !3 *)
mov mem0_174 v_add21_4_7_5;

(* NOTE: k = 22 *)

(*   %arrayidx9.4.6158 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %1120 = load i16, i16* %arrayidx9.4.6158, align 2, !tbaa !3 *)
mov v1120 mem0_208;
(*   %conv1.i.4.6159 = sext i16 %1120 to i32 *)
cast v_conv1_i_4_6159@sint32 v1120@sint16;
(*   %mul.i.4.6160 = mul nsw i32 %conv1.i.4.6159, -1602 *)
mul v_mul_i_4_6160 v_conv1_i_4_6159 (-1602)@sint32;
(*   %call.i.4.6161 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6160) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6160, v_call_i_4_6161);
(*   %arrayidx11.4.6162 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %1121 = load i16, i16* %arrayidx11.4.6162, align 2, !tbaa !3 *)
mov v1121 mem0_192;
(*   %sub.4.6163 = sub i16 %1121, %call.i.4.6161 *)
sub v_sub_4_6163 v1121 v_call_i_4_6161;
(*   store i16 %sub.4.6163, i16* %arrayidx9.4.6158, align 2, !tbaa !3 *)
mov mem0_208 v_sub_4_6163;
(*   %add21.4.6164 = add i16 %1121, %call.i.4.6161 *)
add v_add21_4_6164 v1121 v_call_i_4_6161;
(*   store i16 %add21.4.6164, i16* %arrayidx11.4.6162, align 2, !tbaa !3 *)
mov mem0_192 v_add21_4_6164;
(*   %arrayidx9.4.1.6 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %1122 = load i16, i16* %arrayidx9.4.1.6, align 2, !tbaa !3 *)
mov v1122 mem0_210;
(*   %conv1.i.4.1.6 = sext i16 %1122 to i32 *)
cast v_conv1_i_4_1_6@sint32 v1122@sint16;
(*   %mul.i.4.1.6 = mul nsw i32 %conv1.i.4.1.6, -1602 *)
mul v_mul_i_4_1_6 v_conv1_i_4_1_6 (-1602)@sint32;
(*   %call.i.4.1.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_6, v_call_i_4_1_6);
(*   %arrayidx11.4.1.6 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %1123 = load i16, i16* %arrayidx11.4.1.6, align 2, !tbaa !3 *)
mov v1123 mem0_194;
(*   %sub.4.1.6 = sub i16 %1123, %call.i.4.1.6 *)
sub v_sub_4_1_6 v1123 v_call_i_4_1_6;
(*   store i16 %sub.4.1.6, i16* %arrayidx9.4.1.6, align 2, !tbaa !3 *)
mov mem0_210 v_sub_4_1_6;
(*   %add21.4.1.6 = add i16 %1123, %call.i.4.1.6 *)
add v_add21_4_1_6 v1123 v_call_i_4_1_6;
(*   store i16 %add21.4.1.6, i16* %arrayidx11.4.1.6, align 2, !tbaa !3 *)
mov mem0_194 v_add21_4_1_6;
(*   %arrayidx9.4.2.6 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %1124 = load i16, i16* %arrayidx9.4.2.6, align 2, !tbaa !3 *)
mov v1124 mem0_212;
(*   %conv1.i.4.2.6 = sext i16 %1124 to i32 *)
cast v_conv1_i_4_2_6@sint32 v1124@sint16;
(*   %mul.i.4.2.6 = mul nsw i32 %conv1.i.4.2.6, -1602 *)
mul v_mul_i_4_2_6 v_conv1_i_4_2_6 (-1602)@sint32;
(*   %call.i.4.2.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_6, v_call_i_4_2_6);
(*   %arrayidx11.4.2.6 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %1125 = load i16, i16* %arrayidx11.4.2.6, align 2, !tbaa !3 *)
mov v1125 mem0_196;
(*   %sub.4.2.6 = sub i16 %1125, %call.i.4.2.6 *)
sub v_sub_4_2_6 v1125 v_call_i_4_2_6;
(*   store i16 %sub.4.2.6, i16* %arrayidx9.4.2.6, align 2, !tbaa !3 *)
mov mem0_212 v_sub_4_2_6;
(*   %add21.4.2.6 = add i16 %1125, %call.i.4.2.6 *)
add v_add21_4_2_6 v1125 v_call_i_4_2_6;
(*   store i16 %add21.4.2.6, i16* %arrayidx11.4.2.6, align 2, !tbaa !3 *)
mov mem0_196 v_add21_4_2_6;
(*   %arrayidx9.4.3.6 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %1126 = load i16, i16* %arrayidx9.4.3.6, align 2, !tbaa !3 *)
mov v1126 mem0_214;
(*   %conv1.i.4.3.6 = sext i16 %1126 to i32 *)
cast v_conv1_i_4_3_6@sint32 v1126@sint16;
(*   %mul.i.4.3.6 = mul nsw i32 %conv1.i.4.3.6, -1602 *)
mul v_mul_i_4_3_6 v_conv1_i_4_3_6 (-1602)@sint32;
(*   %call.i.4.3.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_6, v_call_i_4_3_6);
(*   %arrayidx11.4.3.6 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %1127 = load i16, i16* %arrayidx11.4.3.6, align 2, !tbaa !3 *)
mov v1127 mem0_198;
(*   %sub.4.3.6 = sub i16 %1127, %call.i.4.3.6 *)
sub v_sub_4_3_6 v1127 v_call_i_4_3_6;
(*   store i16 %sub.4.3.6, i16* %arrayidx9.4.3.6, align 2, !tbaa !3 *)
mov mem0_214 v_sub_4_3_6;
(*   %add21.4.3.6 = add i16 %1127, %call.i.4.3.6 *)
add v_add21_4_3_6 v1127 v_call_i_4_3_6;
(*   store i16 %add21.4.3.6, i16* %arrayidx11.4.3.6, align 2, !tbaa !3 *)
mov mem0_198 v_add21_4_3_6;
(*   %arrayidx9.4.4.6 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %1128 = load i16, i16* %arrayidx9.4.4.6, align 2, !tbaa !3 *)
mov v1128 mem0_216;
(*   %conv1.i.4.4.6 = sext i16 %1128 to i32 *)
cast v_conv1_i_4_4_6@sint32 v1128@sint16;
(*   %mul.i.4.4.6 = mul nsw i32 %conv1.i.4.4.6, -1602 *)
mul v_mul_i_4_4_6 v_conv1_i_4_4_6 (-1602)@sint32;
(*   %call.i.4.4.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_6, v_call_i_4_4_6);
(*   %arrayidx11.4.4.6 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %1129 = load i16, i16* %arrayidx11.4.4.6, align 2, !tbaa !3 *)
mov v1129 mem0_200;
(*   %sub.4.4.6 = sub i16 %1129, %call.i.4.4.6 *)
sub v_sub_4_4_6 v1129 v_call_i_4_4_6;
(*   store i16 %sub.4.4.6, i16* %arrayidx9.4.4.6, align 2, !tbaa !3 *)
mov mem0_216 v_sub_4_4_6;
(*   %add21.4.4.6 = add i16 %1129, %call.i.4.4.6 *)
add v_add21_4_4_6 v1129 v_call_i_4_4_6;
(*   store i16 %add21.4.4.6, i16* %arrayidx11.4.4.6, align 2, !tbaa !3 *)
mov mem0_200 v_add21_4_4_6;
(*   %arrayidx9.4.5.6 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %1130 = load i16, i16* %arrayidx9.4.5.6, align 2, !tbaa !3 *)
mov v1130 mem0_218;
(*   %conv1.i.4.5.6 = sext i16 %1130 to i32 *)
cast v_conv1_i_4_5_6@sint32 v1130@sint16;
(*   %mul.i.4.5.6 = mul nsw i32 %conv1.i.4.5.6, -1602 *)
mul v_mul_i_4_5_6 v_conv1_i_4_5_6 (-1602)@sint32;
(*   %call.i.4.5.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_6, v_call_i_4_5_6);
(*   %arrayidx11.4.5.6 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %1131 = load i16, i16* %arrayidx11.4.5.6, align 2, !tbaa !3 *)
mov v1131 mem0_202;
(*   %sub.4.5.6 = sub i16 %1131, %call.i.4.5.6 *)
sub v_sub_4_5_6 v1131 v_call_i_4_5_6;
(*   store i16 %sub.4.5.6, i16* %arrayidx9.4.5.6, align 2, !tbaa !3 *)
mov mem0_218 v_sub_4_5_6;
(*   %add21.4.5.6 = add i16 %1131, %call.i.4.5.6 *)
add v_add21_4_5_6 v1131 v_call_i_4_5_6;
(*   store i16 %add21.4.5.6, i16* %arrayidx11.4.5.6, align 2, !tbaa !3 *)
mov mem0_202 v_add21_4_5_6;
(*   %arrayidx9.4.6.6 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %1132 = load i16, i16* %arrayidx9.4.6.6, align 2, !tbaa !3 *)
mov v1132 mem0_220;
(*   %conv1.i.4.6.6 = sext i16 %1132 to i32 *)
cast v_conv1_i_4_6_6@sint32 v1132@sint16;
(*   %mul.i.4.6.6 = mul nsw i32 %conv1.i.4.6.6, -1602 *)
mul v_mul_i_4_6_6 v_conv1_i_4_6_6 (-1602)@sint32;
(*   %call.i.4.6.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_6, v_call_i_4_6_6);
(*   %arrayidx11.4.6.6 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %1133 = load i16, i16* %arrayidx11.4.6.6, align 2, !tbaa !3 *)
mov v1133 mem0_204;
(*   %sub.4.6.6 = sub i16 %1133, %call.i.4.6.6 *)
sub v_sub_4_6_6 v1133 v_call_i_4_6_6;
(*   store i16 %sub.4.6.6, i16* %arrayidx9.4.6.6, align 2, !tbaa !3 *)
mov mem0_220 v_sub_4_6_6;
(*   %add21.4.6.6 = add i16 %1133, %call.i.4.6.6 *)
add v_add21_4_6_6 v1133 v_call_i_4_6_6;
(*   store i16 %add21.4.6.6, i16* %arrayidx11.4.6.6, align 2, !tbaa !3 *)
mov mem0_204 v_add21_4_6_6;
(*   %arrayidx9.4.7.6 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %1134 = load i16, i16* %arrayidx9.4.7.6, align 2, !tbaa !3 *)
mov v1134 mem0_222;
(*   %conv1.i.4.7.6 = sext i16 %1134 to i32 *)
cast v_conv1_i_4_7_6@sint32 v1134@sint16;
(*   %mul.i.4.7.6 = mul nsw i32 %conv1.i.4.7.6, -1602 *)
mul v_mul_i_4_7_6 v_conv1_i_4_7_6 (-1602)@sint32;
(*   %call.i.4.7.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_6, v_call_i_4_7_6);
(*   %arrayidx11.4.7.6 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %1135 = load i16, i16* %arrayidx11.4.7.6, align 2, !tbaa !3 *)
mov v1135 mem0_206;
(*   %sub.4.7.6 = sub i16 %1135, %call.i.4.7.6 *)
sub v_sub_4_7_6 v1135 v_call_i_4_7_6;
(*   store i16 %sub.4.7.6, i16* %arrayidx9.4.7.6, align 2, !tbaa !3 *)
mov mem0_222 v_sub_4_7_6;
(*   %add21.4.7.6 = add i16 %1135, %call.i.4.7.6 *)
add v_add21_4_7_6 v1135 v_call_i_4_7_6;
(*   store i16 %add21.4.7.6, i16* %arrayidx11.4.7.6, align 2, !tbaa !3 *)
mov mem0_206 v_add21_4_7_6;

(* NOTE: k = 23 *)

(*   %arrayidx9.4.7168 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %1136 = load i16, i16* %arrayidx9.4.7168, align 2, !tbaa !3 *)
mov v1136 mem0_240;
(*   %conv1.i.4.7169 = sext i16 %1136 to i32 *)
cast v_conv1_i_4_7169@sint32 v1136@sint16;
(*   %mul.i.4.7170 = mul nsw i32 %conv1.i.4.7169, -130 *)
mul v_mul_i_4_7170 v_conv1_i_4_7169 (-130)@sint32;
(*   %call.i.4.7171 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7170) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7170, v_call_i_4_7171);
(*   %arrayidx11.4.7172 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %1137 = load i16, i16* %arrayidx11.4.7172, align 2, !tbaa !3 *)
mov v1137 mem0_224;
(*   %sub.4.7173 = sub i16 %1137, %call.i.4.7171 *)
sub v_sub_4_7173 v1137 v_call_i_4_7171;
(*   store i16 %sub.4.7173, i16* %arrayidx9.4.7168, align 2, !tbaa !3 *)
mov mem0_240 v_sub_4_7173;
(*   %add21.4.7174 = add i16 %1137, %call.i.4.7171 *)
add v_add21_4_7174 v1137 v_call_i_4_7171;
(*   store i16 %add21.4.7174, i16* %arrayidx11.4.7172, align 2, !tbaa !3 *)
mov mem0_224 v_add21_4_7174;
(*   %arrayidx9.4.1.7 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %1138 = load i16, i16* %arrayidx9.4.1.7, align 2, !tbaa !3 *)
mov v1138 mem0_242;
(*   %conv1.i.4.1.7 = sext i16 %1138 to i32 *)
cast v_conv1_i_4_1_7@sint32 v1138@sint16;
(*   %mul.i.4.1.7 = mul nsw i32 %conv1.i.4.1.7, -130 *)
mul v_mul_i_4_1_7 v_conv1_i_4_1_7 (-130)@sint32;
(*   %call.i.4.1.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_7, v_call_i_4_1_7);
(*   %arrayidx11.4.1.7 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %1139 = load i16, i16* %arrayidx11.4.1.7, align 2, !tbaa !3 *)
mov v1139 mem0_226;
(*   %sub.4.1.7 = sub i16 %1139, %call.i.4.1.7 *)
sub v_sub_4_1_7 v1139 v_call_i_4_1_7;
(*   store i16 %sub.4.1.7, i16* %arrayidx9.4.1.7, align 2, !tbaa !3 *)
mov mem0_242 v_sub_4_1_7;
(*   %add21.4.1.7 = add i16 %1139, %call.i.4.1.7 *)
add v_add21_4_1_7 v1139 v_call_i_4_1_7;
(*   store i16 %add21.4.1.7, i16* %arrayidx11.4.1.7, align 2, !tbaa !3 *)
mov mem0_226 v_add21_4_1_7;
(*   %arrayidx9.4.2.7 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %1140 = load i16, i16* %arrayidx9.4.2.7, align 2, !tbaa !3 *)
mov v1140 mem0_244;
(*   %conv1.i.4.2.7 = sext i16 %1140 to i32 *)
cast v_conv1_i_4_2_7@sint32 v1140@sint16;
(*   %mul.i.4.2.7 = mul nsw i32 %conv1.i.4.2.7, -130 *)
mul v_mul_i_4_2_7 v_conv1_i_4_2_7 (-130)@sint32;
(*   %call.i.4.2.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_7, v_call_i_4_2_7);
(*   %arrayidx11.4.2.7 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %1141 = load i16, i16* %arrayidx11.4.2.7, align 2, !tbaa !3 *)
mov v1141 mem0_228;
(*   %sub.4.2.7 = sub i16 %1141, %call.i.4.2.7 *)
sub v_sub_4_2_7 v1141 v_call_i_4_2_7;
(*   store i16 %sub.4.2.7, i16* %arrayidx9.4.2.7, align 2, !tbaa !3 *)
mov mem0_244 v_sub_4_2_7;
(*   %add21.4.2.7 = add i16 %1141, %call.i.4.2.7 *)
add v_add21_4_2_7 v1141 v_call_i_4_2_7;
(*   store i16 %add21.4.2.7, i16* %arrayidx11.4.2.7, align 2, !tbaa !3 *)
mov mem0_228 v_add21_4_2_7;
(*   %arrayidx9.4.3.7 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %1142 = load i16, i16* %arrayidx9.4.3.7, align 2, !tbaa !3 *)
mov v1142 mem0_246;
(*   %conv1.i.4.3.7 = sext i16 %1142 to i32 *)
cast v_conv1_i_4_3_7@sint32 v1142@sint16;
(*   %mul.i.4.3.7 = mul nsw i32 %conv1.i.4.3.7, -130 *)
mul v_mul_i_4_3_7 v_conv1_i_4_3_7 (-130)@sint32;
(*   %call.i.4.3.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_7, v_call_i_4_3_7);
(*   %arrayidx11.4.3.7 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %1143 = load i16, i16* %arrayidx11.4.3.7, align 2, !tbaa !3 *)
mov v1143 mem0_230;
(*   %sub.4.3.7 = sub i16 %1143, %call.i.4.3.7 *)
sub v_sub_4_3_7 v1143 v_call_i_4_3_7;
(*   store i16 %sub.4.3.7, i16* %arrayidx9.4.3.7, align 2, !tbaa !3 *)
mov mem0_246 v_sub_4_3_7;
(*   %add21.4.3.7 = add i16 %1143, %call.i.4.3.7 *)
add v_add21_4_3_7 v1143 v_call_i_4_3_7;
(*   store i16 %add21.4.3.7, i16* %arrayidx11.4.3.7, align 2, !tbaa !3 *)
mov mem0_230 v_add21_4_3_7;
(*   %arrayidx9.4.4.7 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %1144 = load i16, i16* %arrayidx9.4.4.7, align 2, !tbaa !3 *)
mov v1144 mem0_248;
(*   %conv1.i.4.4.7 = sext i16 %1144 to i32 *)
cast v_conv1_i_4_4_7@sint32 v1144@sint16;
(*   %mul.i.4.4.7 = mul nsw i32 %conv1.i.4.4.7, -130 *)
mul v_mul_i_4_4_7 v_conv1_i_4_4_7 (-130)@sint32;
(*   %call.i.4.4.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_7, v_call_i_4_4_7);
(*   %arrayidx11.4.4.7 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %1145 = load i16, i16* %arrayidx11.4.4.7, align 2, !tbaa !3 *)
mov v1145 mem0_232;
(*   %sub.4.4.7 = sub i16 %1145, %call.i.4.4.7 *)
sub v_sub_4_4_7 v1145 v_call_i_4_4_7;
(*   store i16 %sub.4.4.7, i16* %arrayidx9.4.4.7, align 2, !tbaa !3 *)
mov mem0_248 v_sub_4_4_7;
(*   %add21.4.4.7 = add i16 %1145, %call.i.4.4.7 *)
add v_add21_4_4_7 v1145 v_call_i_4_4_7;
(*   store i16 %add21.4.4.7, i16* %arrayidx11.4.4.7, align 2, !tbaa !3 *)
mov mem0_232 v_add21_4_4_7;
(*   %arrayidx9.4.5.7 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %1146 = load i16, i16* %arrayidx9.4.5.7, align 2, !tbaa !3 *)
mov v1146 mem0_250;
(*   %conv1.i.4.5.7 = sext i16 %1146 to i32 *)
cast v_conv1_i_4_5_7@sint32 v1146@sint16;
(*   %mul.i.4.5.7 = mul nsw i32 %conv1.i.4.5.7, -130 *)
mul v_mul_i_4_5_7 v_conv1_i_4_5_7 (-130)@sint32;
(*   %call.i.4.5.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_7, v_call_i_4_5_7);
(*   %arrayidx11.4.5.7 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %1147 = load i16, i16* %arrayidx11.4.5.7, align 2, !tbaa !3 *)
mov v1147 mem0_234;
(*   %sub.4.5.7 = sub i16 %1147, %call.i.4.5.7 *)
sub v_sub_4_5_7 v1147 v_call_i_4_5_7;
(*   store i16 %sub.4.5.7, i16* %arrayidx9.4.5.7, align 2, !tbaa !3 *)
mov mem0_250 v_sub_4_5_7;
(*   %add21.4.5.7 = add i16 %1147, %call.i.4.5.7 *)
add v_add21_4_5_7 v1147 v_call_i_4_5_7;
(*   store i16 %add21.4.5.7, i16* %arrayidx11.4.5.7, align 2, !tbaa !3 *)
mov mem0_234 v_add21_4_5_7;
(*   %arrayidx9.4.6.7 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %1148 = load i16, i16* %arrayidx9.4.6.7, align 2, !tbaa !3 *)
mov v1148 mem0_252;
(*   %conv1.i.4.6.7 = sext i16 %1148 to i32 *)
cast v_conv1_i_4_6_7@sint32 v1148@sint16;
(*   %mul.i.4.6.7 = mul nsw i32 %conv1.i.4.6.7, -130 *)
mul v_mul_i_4_6_7 v_conv1_i_4_6_7 (-130)@sint32;
(*   %call.i.4.6.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_7, v_call_i_4_6_7);
(*   %arrayidx11.4.6.7 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %1149 = load i16, i16* %arrayidx11.4.6.7, align 2, !tbaa !3 *)
mov v1149 mem0_236;
(*   %sub.4.6.7 = sub i16 %1149, %call.i.4.6.7 *)
sub v_sub_4_6_7 v1149 v_call_i_4_6_7;
(*   store i16 %sub.4.6.7, i16* %arrayidx9.4.6.7, align 2, !tbaa !3 *)
mov mem0_252 v_sub_4_6_7;
(*   %add21.4.6.7 = add i16 %1149, %call.i.4.6.7 *)
add v_add21_4_6_7 v1149 v_call_i_4_6_7;
(*   store i16 %add21.4.6.7, i16* %arrayidx11.4.6.7, align 2, !tbaa !3 *)
mov mem0_236 v_add21_4_6_7;
(*   %arrayidx9.4.7.7 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %1150 = load i16, i16* %arrayidx9.4.7.7, align 2, !tbaa !3 *)
mov v1150 mem0_254;
(*   %conv1.i.4.7.7 = sext i16 %1150 to i32 *)
cast v_conv1_i_4_7_7@sint32 v1150@sint16;
(*   %mul.i.4.7.7 = mul nsw i32 %conv1.i.4.7.7, -130 *)
mul v_mul_i_4_7_7 v_conv1_i_4_7_7 (-130)@sint32;
(*   %call.i.4.7.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_7, v_call_i_4_7_7);
(*   %arrayidx11.4.7.7 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %1151 = load i16, i16* %arrayidx11.4.7.7, align 2, !tbaa !3 *)
mov v1151 mem0_238;
(*   %sub.4.7.7 = sub i16 %1151, %call.i.4.7.7 *)
sub v_sub_4_7_7 v1151 v_call_i_4_7_7;
(*   store i16 %sub.4.7.7, i16* %arrayidx9.4.7.7, align 2, !tbaa !3 *)
mov mem0_254 v_sub_4_7_7;
(*   %add21.4.7.7 = add i16 %1151, %call.i.4.7.7 *)
add v_add21_4_7_7 v1151 v_call_i_4_7_7;
(*   store i16 %add21.4.7.7, i16* %arrayidx11.4.7.7, align 2, !tbaa !3 *)
mov mem0_238 v_add21_4_7_7;

(* NOTE: k = 24 *)

(*   %arrayidx9.4.8 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %1152 = load i16, i16* %arrayidx9.4.8, align 2, !tbaa !3 *)
mov v1152 mem0_272;
(*   %conv1.i.4.8 = sext i16 %1152 to i32 *)
cast v_conv1_i_4_8@sint32 v1152@sint16;
(*   %mul.i.4.8 = mul nsw i32 %conv1.i.4.8, -681 *)
mul v_mul_i_4_8 v_conv1_i_4_8 (-681)@sint32;
(*   %call.i.4.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_8, v_call_i_4_8);
(*   %arrayidx11.4.8 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %1153 = load i16, i16* %arrayidx11.4.8, align 2, !tbaa !3 *)
mov v1153 mem0_256;
(*   %sub.4.8 = sub i16 %1153, %call.i.4.8 *)
sub v_sub_4_8 v1153 v_call_i_4_8;
(*   store i16 %sub.4.8, i16* %arrayidx9.4.8, align 2, !tbaa !3 *)
mov mem0_272 v_sub_4_8;
(*   %add21.4.8 = add i16 %1153, %call.i.4.8 *)
add v_add21_4_8 v1153 v_call_i_4_8;
(*   store i16 %add21.4.8, i16* %arrayidx11.4.8, align 2, !tbaa !3 *)
mov mem0_256 v_add21_4_8;
(*   %arrayidx9.4.1.8 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %1154 = load i16, i16* %arrayidx9.4.1.8, align 2, !tbaa !3 *)
mov v1154 mem0_274;
(*   %conv1.i.4.1.8 = sext i16 %1154 to i32 *)
cast v_conv1_i_4_1_8@sint32 v1154@sint16;
(*   %mul.i.4.1.8 = mul nsw i32 %conv1.i.4.1.8, -681 *)
mul v_mul_i_4_1_8 v_conv1_i_4_1_8 (-681)@sint32;
(*   %call.i.4.1.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_8, v_call_i_4_1_8);
(*   %arrayidx11.4.1.8 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %1155 = load i16, i16* %arrayidx11.4.1.8, align 2, !tbaa !3 *)
mov v1155 mem0_258;
(*   %sub.4.1.8 = sub i16 %1155, %call.i.4.1.8 *)
sub v_sub_4_1_8 v1155 v_call_i_4_1_8;
(*   store i16 %sub.4.1.8, i16* %arrayidx9.4.1.8, align 2, !tbaa !3 *)
mov mem0_274 v_sub_4_1_8;
(*   %add21.4.1.8 = add i16 %1155, %call.i.4.1.8 *)
add v_add21_4_1_8 v1155 v_call_i_4_1_8;
(*   store i16 %add21.4.1.8, i16* %arrayidx11.4.1.8, align 2, !tbaa !3 *)
mov mem0_258 v_add21_4_1_8;
(*   %arrayidx9.4.2.8 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %1156 = load i16, i16* %arrayidx9.4.2.8, align 2, !tbaa !3 *)
mov v1156 mem0_276;
(*   %conv1.i.4.2.8 = sext i16 %1156 to i32 *)
cast v_conv1_i_4_2_8@sint32 v1156@sint16;
(*   %mul.i.4.2.8 = mul nsw i32 %conv1.i.4.2.8, -681 *)
mul v_mul_i_4_2_8 v_conv1_i_4_2_8 (-681)@sint32;
(*   %call.i.4.2.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_8, v_call_i_4_2_8);
(*   %arrayidx11.4.2.8 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %1157 = load i16, i16* %arrayidx11.4.2.8, align 2, !tbaa !3 *)
mov v1157 mem0_260;
(*   %sub.4.2.8 = sub i16 %1157, %call.i.4.2.8 *)
sub v_sub_4_2_8 v1157 v_call_i_4_2_8;
(*   store i16 %sub.4.2.8, i16* %arrayidx9.4.2.8, align 2, !tbaa !3 *)
mov mem0_276 v_sub_4_2_8;
(*   %add21.4.2.8 = add i16 %1157, %call.i.4.2.8 *)
add v_add21_4_2_8 v1157 v_call_i_4_2_8;
(*   store i16 %add21.4.2.8, i16* %arrayidx11.4.2.8, align 2, !tbaa !3 *)
mov mem0_260 v_add21_4_2_8;
(*   %arrayidx9.4.3.8 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %1158 = load i16, i16* %arrayidx9.4.3.8, align 2, !tbaa !3 *)
mov v1158 mem0_278;
(*   %conv1.i.4.3.8 = sext i16 %1158 to i32 *)
cast v_conv1_i_4_3_8@sint32 v1158@sint16;
(*   %mul.i.4.3.8 = mul nsw i32 %conv1.i.4.3.8, -681 *)
mul v_mul_i_4_3_8 v_conv1_i_4_3_8 (-681)@sint32;
(*   %call.i.4.3.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_8, v_call_i_4_3_8);
(*   %arrayidx11.4.3.8 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %1159 = load i16, i16* %arrayidx11.4.3.8, align 2, !tbaa !3 *)
mov v1159 mem0_262;
(*   %sub.4.3.8 = sub i16 %1159, %call.i.4.3.8 *)
sub v_sub_4_3_8 v1159 v_call_i_4_3_8;
(*   store i16 %sub.4.3.8, i16* %arrayidx9.4.3.8, align 2, !tbaa !3 *)
mov mem0_278 v_sub_4_3_8;
(*   %add21.4.3.8 = add i16 %1159, %call.i.4.3.8 *)
add v_add21_4_3_8 v1159 v_call_i_4_3_8;
(*   store i16 %add21.4.3.8, i16* %arrayidx11.4.3.8, align 2, !tbaa !3 *)
mov mem0_262 v_add21_4_3_8;
(*   %arrayidx9.4.4.8 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %1160 = load i16, i16* %arrayidx9.4.4.8, align 2, !tbaa !3 *)
mov v1160 mem0_280;
(*   %conv1.i.4.4.8 = sext i16 %1160 to i32 *)
cast v_conv1_i_4_4_8@sint32 v1160@sint16;
(*   %mul.i.4.4.8 = mul nsw i32 %conv1.i.4.4.8, -681 *)
mul v_mul_i_4_4_8 v_conv1_i_4_4_8 (-681)@sint32;
(*   %call.i.4.4.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_8, v_call_i_4_4_8);
(*   %arrayidx11.4.4.8 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %1161 = load i16, i16* %arrayidx11.4.4.8, align 2, !tbaa !3 *)
mov v1161 mem0_264;
(*   %sub.4.4.8 = sub i16 %1161, %call.i.4.4.8 *)
sub v_sub_4_4_8 v1161 v_call_i_4_4_8;
(*   store i16 %sub.4.4.8, i16* %arrayidx9.4.4.8, align 2, !tbaa !3 *)
mov mem0_280 v_sub_4_4_8;
(*   %add21.4.4.8 = add i16 %1161, %call.i.4.4.8 *)
add v_add21_4_4_8 v1161 v_call_i_4_4_8;
(*   store i16 %add21.4.4.8, i16* %arrayidx11.4.4.8, align 2, !tbaa !3 *)
mov mem0_264 v_add21_4_4_8;
(*   %arrayidx9.4.5.8 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %1162 = load i16, i16* %arrayidx9.4.5.8, align 2, !tbaa !3 *)
mov v1162 mem0_282;
(*   %conv1.i.4.5.8 = sext i16 %1162 to i32 *)
cast v_conv1_i_4_5_8@sint32 v1162@sint16;
(*   %mul.i.4.5.8 = mul nsw i32 %conv1.i.4.5.8, -681 *)
mul v_mul_i_4_5_8 v_conv1_i_4_5_8 (-681)@sint32;
(*   %call.i.4.5.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_8, v_call_i_4_5_8);
(*   %arrayidx11.4.5.8 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %1163 = load i16, i16* %arrayidx11.4.5.8, align 2, !tbaa !3 *)
mov v1163 mem0_266;
(*   %sub.4.5.8 = sub i16 %1163, %call.i.4.5.8 *)
sub v_sub_4_5_8 v1163 v_call_i_4_5_8;
(*   store i16 %sub.4.5.8, i16* %arrayidx9.4.5.8, align 2, !tbaa !3 *)
mov mem0_282 v_sub_4_5_8;
(*   %add21.4.5.8 = add i16 %1163, %call.i.4.5.8 *)
add v_add21_4_5_8 v1163 v_call_i_4_5_8;
(*   store i16 %add21.4.5.8, i16* %arrayidx11.4.5.8, align 2, !tbaa !3 *)
mov mem0_266 v_add21_4_5_8;
(*   %arrayidx9.4.6.8 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %1164 = load i16, i16* %arrayidx9.4.6.8, align 2, !tbaa !3 *)
mov v1164 mem0_284;
(*   %conv1.i.4.6.8 = sext i16 %1164 to i32 *)
cast v_conv1_i_4_6_8@sint32 v1164@sint16;
(*   %mul.i.4.6.8 = mul nsw i32 %conv1.i.4.6.8, -681 *)
mul v_mul_i_4_6_8 v_conv1_i_4_6_8 (-681)@sint32;
(*   %call.i.4.6.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_8, v_call_i_4_6_8);
(*   %arrayidx11.4.6.8 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %1165 = load i16, i16* %arrayidx11.4.6.8, align 2, !tbaa !3 *)
mov v1165 mem0_268;
(*   %sub.4.6.8 = sub i16 %1165, %call.i.4.6.8 *)
sub v_sub_4_6_8 v1165 v_call_i_4_6_8;
(*   store i16 %sub.4.6.8, i16* %arrayidx9.4.6.8, align 2, !tbaa !3 *)
mov mem0_284 v_sub_4_6_8;
(*   %add21.4.6.8 = add i16 %1165, %call.i.4.6.8 *)
add v_add21_4_6_8 v1165 v_call_i_4_6_8;
(*   store i16 %add21.4.6.8, i16* %arrayidx11.4.6.8, align 2, !tbaa !3 *)
mov mem0_268 v_add21_4_6_8;
(*   %arrayidx9.4.7.8 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %1166 = load i16, i16* %arrayidx9.4.7.8, align 2, !tbaa !3 *)
mov v1166 mem0_286;
(*   %conv1.i.4.7.8 = sext i16 %1166 to i32 *)
cast v_conv1_i_4_7_8@sint32 v1166@sint16;
(*   %mul.i.4.7.8 = mul nsw i32 %conv1.i.4.7.8, -681 *)
mul v_mul_i_4_7_8 v_conv1_i_4_7_8 (-681)@sint32;
(*   %call.i.4.7.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_8, v_call_i_4_7_8);
(*   %arrayidx11.4.7.8 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %1167 = load i16, i16* %arrayidx11.4.7.8, align 2, !tbaa !3 *)
mov v1167 mem0_270;
(*   %sub.4.7.8 = sub i16 %1167, %call.i.4.7.8 *)
sub v_sub_4_7_8 v1167 v_call_i_4_7_8;
(*   store i16 %sub.4.7.8, i16* %arrayidx9.4.7.8, align 2, !tbaa !3 *)
mov mem0_286 v_sub_4_7_8;
(*   %add21.4.7.8 = add i16 %1167, %call.i.4.7.8 *)
add v_add21_4_7_8 v1167 v_call_i_4_7_8;
(*   store i16 %add21.4.7.8, i16* %arrayidx11.4.7.8, align 2, !tbaa !3 *)
mov mem0_270 v_add21_4_7_8;

(* NOTE: k = 25 *)

(*   %arrayidx9.4.9 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %1168 = load i16, i16* %arrayidx9.4.9, align 2, !tbaa !3 *)
mov v1168 mem0_304;
(*   %conv1.i.4.9 = sext i16 %1168 to i32 *)
cast v_conv1_i_4_9@sint32 v1168@sint16;
(*   %mul.i.4.9 = mul nsw i32 %conv1.i.4.9, 1017 *)
mul v_mul_i_4_9 v_conv1_i_4_9 (1017)@sint32;
(*   %call.i.4.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_9, v_call_i_4_9);
(*   %arrayidx11.4.9 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %1169 = load i16, i16* %arrayidx11.4.9, align 2, !tbaa !3 *)
mov v1169 mem0_288;
(*   %sub.4.9 = sub i16 %1169, %call.i.4.9 *)
sub v_sub_4_9 v1169 v_call_i_4_9;
(*   store i16 %sub.4.9, i16* %arrayidx9.4.9, align 2, !tbaa !3 *)
mov mem0_304 v_sub_4_9;
(*   %add21.4.9 = add i16 %1169, %call.i.4.9 *)
add v_add21_4_9 v1169 v_call_i_4_9;
(*   store i16 %add21.4.9, i16* %arrayidx11.4.9, align 2, !tbaa !3 *)
mov mem0_288 v_add21_4_9;
(*   %arrayidx9.4.1.9 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %1170 = load i16, i16* %arrayidx9.4.1.9, align 2, !tbaa !3 *)
mov v1170 mem0_306;
(*   %conv1.i.4.1.9 = sext i16 %1170 to i32 *)
cast v_conv1_i_4_1_9@sint32 v1170@sint16;
(*   %mul.i.4.1.9 = mul nsw i32 %conv1.i.4.1.9, 1017 *)
mul v_mul_i_4_1_9 v_conv1_i_4_1_9 (1017)@sint32;
(*   %call.i.4.1.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_9, v_call_i_4_1_9);
(*   %arrayidx11.4.1.9 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %1171 = load i16, i16* %arrayidx11.4.1.9, align 2, !tbaa !3 *)
mov v1171 mem0_290;
(*   %sub.4.1.9 = sub i16 %1171, %call.i.4.1.9 *)
sub v_sub_4_1_9 v1171 v_call_i_4_1_9;
(*   store i16 %sub.4.1.9, i16* %arrayidx9.4.1.9, align 2, !tbaa !3 *)
mov mem0_306 v_sub_4_1_9;
(*   %add21.4.1.9 = add i16 %1171, %call.i.4.1.9 *)
add v_add21_4_1_9 v1171 v_call_i_4_1_9;
(*   store i16 %add21.4.1.9, i16* %arrayidx11.4.1.9, align 2, !tbaa !3 *)
mov mem0_290 v_add21_4_1_9;
(*   %arrayidx9.4.2.9 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %1172 = load i16, i16* %arrayidx9.4.2.9, align 2, !tbaa !3 *)
mov v1172 mem0_308;
(*   %conv1.i.4.2.9 = sext i16 %1172 to i32 *)
cast v_conv1_i_4_2_9@sint32 v1172@sint16;
(*   %mul.i.4.2.9 = mul nsw i32 %conv1.i.4.2.9, 1017 *)
mul v_mul_i_4_2_9 v_conv1_i_4_2_9 (1017)@sint32;
(*   %call.i.4.2.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_9, v_call_i_4_2_9);
(*   %arrayidx11.4.2.9 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %1173 = load i16, i16* %arrayidx11.4.2.9, align 2, !tbaa !3 *)
mov v1173 mem0_292;
(*   %sub.4.2.9 = sub i16 %1173, %call.i.4.2.9 *)
sub v_sub_4_2_9 v1173 v_call_i_4_2_9;
(*   store i16 %sub.4.2.9, i16* %arrayidx9.4.2.9, align 2, !tbaa !3 *)
mov mem0_308 v_sub_4_2_9;
(*   %add21.4.2.9 = add i16 %1173, %call.i.4.2.9 *)
add v_add21_4_2_9 v1173 v_call_i_4_2_9;
(*   store i16 %add21.4.2.9, i16* %arrayidx11.4.2.9, align 2, !tbaa !3 *)
mov mem0_292 v_add21_4_2_9;
(*   %arrayidx9.4.3.9 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %1174 = load i16, i16* %arrayidx9.4.3.9, align 2, !tbaa !3 *)
mov v1174 mem0_310;
(*   %conv1.i.4.3.9 = sext i16 %1174 to i32 *)
cast v_conv1_i_4_3_9@sint32 v1174@sint16;
(*   %mul.i.4.3.9 = mul nsw i32 %conv1.i.4.3.9, 1017 *)
mul v_mul_i_4_3_9 v_conv1_i_4_3_9 (1017)@sint32;
(*   %call.i.4.3.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_9, v_call_i_4_3_9);
(*   %arrayidx11.4.3.9 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %1175 = load i16, i16* %arrayidx11.4.3.9, align 2, !tbaa !3 *)
mov v1175 mem0_294;
(*   %sub.4.3.9 = sub i16 %1175, %call.i.4.3.9 *)
sub v_sub_4_3_9 v1175 v_call_i_4_3_9;
(*   store i16 %sub.4.3.9, i16* %arrayidx9.4.3.9, align 2, !tbaa !3 *)
mov mem0_310 v_sub_4_3_9;
(*   %add21.4.3.9 = add i16 %1175, %call.i.4.3.9 *)
add v_add21_4_3_9 v1175 v_call_i_4_3_9;
(*   store i16 %add21.4.3.9, i16* %arrayidx11.4.3.9, align 2, !tbaa !3 *)
mov mem0_294 v_add21_4_3_9;
(*   %arrayidx9.4.4.9 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %1176 = load i16, i16* %arrayidx9.4.4.9, align 2, !tbaa !3 *)
mov v1176 mem0_312;
(*   %conv1.i.4.4.9 = sext i16 %1176 to i32 *)
cast v_conv1_i_4_4_9@sint32 v1176@sint16;
(*   %mul.i.4.4.9 = mul nsw i32 %conv1.i.4.4.9, 1017 *)
mul v_mul_i_4_4_9 v_conv1_i_4_4_9 (1017)@sint32;
(*   %call.i.4.4.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_9, v_call_i_4_4_9);
(*   %arrayidx11.4.4.9 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %1177 = load i16, i16* %arrayidx11.4.4.9, align 2, !tbaa !3 *)
mov v1177 mem0_296;
(*   %sub.4.4.9 = sub i16 %1177, %call.i.4.4.9 *)
sub v_sub_4_4_9 v1177 v_call_i_4_4_9;
(*   store i16 %sub.4.4.9, i16* %arrayidx9.4.4.9, align 2, !tbaa !3 *)
mov mem0_312 v_sub_4_4_9;
(*   %add21.4.4.9 = add i16 %1177, %call.i.4.4.9 *)
add v_add21_4_4_9 v1177 v_call_i_4_4_9;
(*   store i16 %add21.4.4.9, i16* %arrayidx11.4.4.9, align 2, !tbaa !3 *)
mov mem0_296 v_add21_4_4_9;
(*   %arrayidx9.4.5.9 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %1178 = load i16, i16* %arrayidx9.4.5.9, align 2, !tbaa !3 *)
mov v1178 mem0_314;
(*   %conv1.i.4.5.9 = sext i16 %1178 to i32 *)
cast v_conv1_i_4_5_9@sint32 v1178@sint16;
(*   %mul.i.4.5.9 = mul nsw i32 %conv1.i.4.5.9, 1017 *)
mul v_mul_i_4_5_9 v_conv1_i_4_5_9 (1017)@sint32;
(*   %call.i.4.5.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_9, v_call_i_4_5_9);
(*   %arrayidx11.4.5.9 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %1179 = load i16, i16* %arrayidx11.4.5.9, align 2, !tbaa !3 *)
mov v1179 mem0_298;
(*   %sub.4.5.9 = sub i16 %1179, %call.i.4.5.9 *)
sub v_sub_4_5_9 v1179 v_call_i_4_5_9;
(*   store i16 %sub.4.5.9, i16* %arrayidx9.4.5.9, align 2, !tbaa !3 *)
mov mem0_314 v_sub_4_5_9;
(*   %add21.4.5.9 = add i16 %1179, %call.i.4.5.9 *)
add v_add21_4_5_9 v1179 v_call_i_4_5_9;
(*   store i16 %add21.4.5.9, i16* %arrayidx11.4.5.9, align 2, !tbaa !3 *)
mov mem0_298 v_add21_4_5_9;
(*   %arrayidx9.4.6.9 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %1180 = load i16, i16* %arrayidx9.4.6.9, align 2, !tbaa !3 *)
mov v1180 mem0_316;
(*   %conv1.i.4.6.9 = sext i16 %1180 to i32 *)
cast v_conv1_i_4_6_9@sint32 v1180@sint16;
(*   %mul.i.4.6.9 = mul nsw i32 %conv1.i.4.6.9, 1017 *)
mul v_mul_i_4_6_9 v_conv1_i_4_6_9 (1017)@sint32;
(*   %call.i.4.6.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_9, v_call_i_4_6_9);
(*   %arrayidx11.4.6.9 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %1181 = load i16, i16* %arrayidx11.4.6.9, align 2, !tbaa !3 *)
mov v1181 mem0_300;
(*   %sub.4.6.9 = sub i16 %1181, %call.i.4.6.9 *)
sub v_sub_4_6_9 v1181 v_call_i_4_6_9;
(*   store i16 %sub.4.6.9, i16* %arrayidx9.4.6.9, align 2, !tbaa !3 *)
mov mem0_316 v_sub_4_6_9;
(*   %add21.4.6.9 = add i16 %1181, %call.i.4.6.9 *)
add v_add21_4_6_9 v1181 v_call_i_4_6_9;
(*   store i16 %add21.4.6.9, i16* %arrayidx11.4.6.9, align 2, !tbaa !3 *)
mov mem0_300 v_add21_4_6_9;
(*   %arrayidx9.4.7.9 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %1182 = load i16, i16* %arrayidx9.4.7.9, align 2, !tbaa !3 *)
mov v1182 mem0_318;
(*   %conv1.i.4.7.9 = sext i16 %1182 to i32 *)
cast v_conv1_i_4_7_9@sint32 v1182@sint16;
(*   %mul.i.4.7.9 = mul nsw i32 %conv1.i.4.7.9, 1017 *)
mul v_mul_i_4_7_9 v_conv1_i_4_7_9 (1017)@sint32;
(*   %call.i.4.7.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_9, v_call_i_4_7_9);
(*   %arrayidx11.4.7.9 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %1183 = load i16, i16* %arrayidx11.4.7.9, align 2, !tbaa !3 *)
mov v1183 mem0_302;
(*   %sub.4.7.9 = sub i16 %1183, %call.i.4.7.9 *)
sub v_sub_4_7_9 v1183 v_call_i_4_7_9;
(*   store i16 %sub.4.7.9, i16* %arrayidx9.4.7.9, align 2, !tbaa !3 *)
mov mem0_318 v_sub_4_7_9;
(*   %add21.4.7.9 = add i16 %1183, %call.i.4.7.9 *)
add v_add21_4_7_9 v1183 v_call_i_4_7_9;
(*   store i16 %add21.4.7.9, i16* %arrayidx11.4.7.9, align 2, !tbaa !3 *)
mov mem0_302 v_add21_4_7_9;

(* NOTE: k = 26 *)

(*   %arrayidx9.4.10 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %1184 = load i16, i16* %arrayidx9.4.10, align 2, !tbaa !3 *)
mov v1184 mem0_336;
(*   %conv1.i.4.10 = sext i16 %1184 to i32 *)
cast v_conv1_i_4_10@sint32 v1184@sint16;
(*   %mul.i.4.10 = mul nsw i32 %conv1.i.4.10, 732 *)
mul v_mul_i_4_10 v_conv1_i_4_10 (732)@sint32;
(*   %call.i.4.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_10, v_call_i_4_10);
(*   %arrayidx11.4.10 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %1185 = load i16, i16* %arrayidx11.4.10, align 2, !tbaa !3 *)
mov v1185 mem0_320;
(*   %sub.4.10 = sub i16 %1185, %call.i.4.10 *)
sub v_sub_4_10 v1185 v_call_i_4_10;
(*   store i16 %sub.4.10, i16* %arrayidx9.4.10, align 2, !tbaa !3 *)
mov mem0_336 v_sub_4_10;
(*   %add21.4.10 = add i16 %1185, %call.i.4.10 *)
add v_add21_4_10 v1185 v_call_i_4_10;
(*   store i16 %add21.4.10, i16* %arrayidx11.4.10, align 2, !tbaa !3 *)
mov mem0_320 v_add21_4_10;
(*   %arrayidx9.4.1.10 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %1186 = load i16, i16* %arrayidx9.4.1.10, align 2, !tbaa !3 *)
mov v1186 mem0_338;
(*   %conv1.i.4.1.10 = sext i16 %1186 to i32 *)
cast v_conv1_i_4_1_10@sint32 v1186@sint16;
(*   %mul.i.4.1.10 = mul nsw i32 %conv1.i.4.1.10, 732 *)
mul v_mul_i_4_1_10 v_conv1_i_4_1_10 (732)@sint32;
(*   %call.i.4.1.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_10, v_call_i_4_1_10);
(*   %arrayidx11.4.1.10 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %1187 = load i16, i16* %arrayidx11.4.1.10, align 2, !tbaa !3 *)
mov v1187 mem0_322;
(*   %sub.4.1.10 = sub i16 %1187, %call.i.4.1.10 *)
sub v_sub_4_1_10 v1187 v_call_i_4_1_10;
(*   store i16 %sub.4.1.10, i16* %arrayidx9.4.1.10, align 2, !tbaa !3 *)
mov mem0_338 v_sub_4_1_10;
(*   %add21.4.1.10 = add i16 %1187, %call.i.4.1.10 *)
add v_add21_4_1_10 v1187 v_call_i_4_1_10;
(*   store i16 %add21.4.1.10, i16* %arrayidx11.4.1.10, align 2, !tbaa !3 *)
mov mem0_322 v_add21_4_1_10;
(*   %arrayidx9.4.2.10 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %1188 = load i16, i16* %arrayidx9.4.2.10, align 2, !tbaa !3 *)
mov v1188 mem0_340;
(*   %conv1.i.4.2.10 = sext i16 %1188 to i32 *)
cast v_conv1_i_4_2_10@sint32 v1188@sint16;
(*   %mul.i.4.2.10 = mul nsw i32 %conv1.i.4.2.10, 732 *)
mul v_mul_i_4_2_10 v_conv1_i_4_2_10 (732)@sint32;
(*   %call.i.4.2.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_10, v_call_i_4_2_10);
(*   %arrayidx11.4.2.10 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %1189 = load i16, i16* %arrayidx11.4.2.10, align 2, !tbaa !3 *)
mov v1189 mem0_324;
(*   %sub.4.2.10 = sub i16 %1189, %call.i.4.2.10 *)
sub v_sub_4_2_10 v1189 v_call_i_4_2_10;
(*   store i16 %sub.4.2.10, i16* %arrayidx9.4.2.10, align 2, !tbaa !3 *)
mov mem0_340 v_sub_4_2_10;
(*   %add21.4.2.10 = add i16 %1189, %call.i.4.2.10 *)
add v_add21_4_2_10 v1189 v_call_i_4_2_10;
(*   store i16 %add21.4.2.10, i16* %arrayidx11.4.2.10, align 2, !tbaa !3 *)
mov mem0_324 v_add21_4_2_10;
(*   %arrayidx9.4.3.10 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %1190 = load i16, i16* %arrayidx9.4.3.10, align 2, !tbaa !3 *)
mov v1190 mem0_342;
(*   %conv1.i.4.3.10 = sext i16 %1190 to i32 *)
cast v_conv1_i_4_3_10@sint32 v1190@sint16;
(*   %mul.i.4.3.10 = mul nsw i32 %conv1.i.4.3.10, 732 *)
mul v_mul_i_4_3_10 v_conv1_i_4_3_10 (732)@sint32;
(*   %call.i.4.3.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_10, v_call_i_4_3_10);
(*   %arrayidx11.4.3.10 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %1191 = load i16, i16* %arrayidx11.4.3.10, align 2, !tbaa !3 *)
mov v1191 mem0_326;
(*   %sub.4.3.10 = sub i16 %1191, %call.i.4.3.10 *)
sub v_sub_4_3_10 v1191 v_call_i_4_3_10;
(*   store i16 %sub.4.3.10, i16* %arrayidx9.4.3.10, align 2, !tbaa !3 *)
mov mem0_342 v_sub_4_3_10;
(*   %add21.4.3.10 = add i16 %1191, %call.i.4.3.10 *)
add v_add21_4_3_10 v1191 v_call_i_4_3_10;
(*   store i16 %add21.4.3.10, i16* %arrayidx11.4.3.10, align 2, !tbaa !3 *)
mov mem0_326 v_add21_4_3_10;
(*   %arrayidx9.4.4.10 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %1192 = load i16, i16* %arrayidx9.4.4.10, align 2, !tbaa !3 *)
mov v1192 mem0_344;
(*   %conv1.i.4.4.10 = sext i16 %1192 to i32 *)
cast v_conv1_i_4_4_10@sint32 v1192@sint16;
(*   %mul.i.4.4.10 = mul nsw i32 %conv1.i.4.4.10, 732 *)
mul v_mul_i_4_4_10 v_conv1_i_4_4_10 (732)@sint32;
(*   %call.i.4.4.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_10, v_call_i_4_4_10);
(*   %arrayidx11.4.4.10 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %1193 = load i16, i16* %arrayidx11.4.4.10, align 2, !tbaa !3 *)
mov v1193 mem0_328;
(*   %sub.4.4.10 = sub i16 %1193, %call.i.4.4.10 *)
sub v_sub_4_4_10 v1193 v_call_i_4_4_10;
(*   store i16 %sub.4.4.10, i16* %arrayidx9.4.4.10, align 2, !tbaa !3 *)
mov mem0_344 v_sub_4_4_10;
(*   %add21.4.4.10 = add i16 %1193, %call.i.4.4.10 *)
add v_add21_4_4_10 v1193 v_call_i_4_4_10;
(*   store i16 %add21.4.4.10, i16* %arrayidx11.4.4.10, align 2, !tbaa !3 *)
mov mem0_328 v_add21_4_4_10;
(*   %arrayidx9.4.5.10 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %1194 = load i16, i16* %arrayidx9.4.5.10, align 2, !tbaa !3 *)
mov v1194 mem0_346;
(*   %conv1.i.4.5.10 = sext i16 %1194 to i32 *)
cast v_conv1_i_4_5_10@sint32 v1194@sint16;
(*   %mul.i.4.5.10 = mul nsw i32 %conv1.i.4.5.10, 732 *)
mul v_mul_i_4_5_10 v_conv1_i_4_5_10 (732)@sint32;
(*   %call.i.4.5.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_10, v_call_i_4_5_10);
(*   %arrayidx11.4.5.10 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %1195 = load i16, i16* %arrayidx11.4.5.10, align 2, !tbaa !3 *)
mov v1195 mem0_330;
(*   %sub.4.5.10 = sub i16 %1195, %call.i.4.5.10 *)
sub v_sub_4_5_10 v1195 v_call_i_4_5_10;
(*   store i16 %sub.4.5.10, i16* %arrayidx9.4.5.10, align 2, !tbaa !3 *)
mov mem0_346 v_sub_4_5_10;
(*   %add21.4.5.10 = add i16 %1195, %call.i.4.5.10 *)
add v_add21_4_5_10 v1195 v_call_i_4_5_10;
(*   store i16 %add21.4.5.10, i16* %arrayidx11.4.5.10, align 2, !tbaa !3 *)
mov mem0_330 v_add21_4_5_10;
(*   %arrayidx9.4.6.10 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %1196 = load i16, i16* %arrayidx9.4.6.10, align 2, !tbaa !3 *)
mov v1196 mem0_348;
(*   %conv1.i.4.6.10 = sext i16 %1196 to i32 *)
cast v_conv1_i_4_6_10@sint32 v1196@sint16;
(*   %mul.i.4.6.10 = mul nsw i32 %conv1.i.4.6.10, 732 *)
mul v_mul_i_4_6_10 v_conv1_i_4_6_10 (732)@sint32;
(*   %call.i.4.6.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_10, v_call_i_4_6_10);
(*   %arrayidx11.4.6.10 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %1197 = load i16, i16* %arrayidx11.4.6.10, align 2, !tbaa !3 *)
mov v1197 mem0_332;
(*   %sub.4.6.10 = sub i16 %1197, %call.i.4.6.10 *)
sub v_sub_4_6_10 v1197 v_call_i_4_6_10;
(*   store i16 %sub.4.6.10, i16* %arrayidx9.4.6.10, align 2, !tbaa !3 *)
mov mem0_348 v_sub_4_6_10;
(*   %add21.4.6.10 = add i16 %1197, %call.i.4.6.10 *)
add v_add21_4_6_10 v1197 v_call_i_4_6_10;
(*   store i16 %add21.4.6.10, i16* %arrayidx11.4.6.10, align 2, !tbaa !3 *)
mov mem0_332 v_add21_4_6_10;
(*   %arrayidx9.4.7.10 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %1198 = load i16, i16* %arrayidx9.4.7.10, align 2, !tbaa !3 *)
mov v1198 mem0_350;
(*   %conv1.i.4.7.10 = sext i16 %1198 to i32 *)
cast v_conv1_i_4_7_10@sint32 v1198@sint16;
(*   %mul.i.4.7.10 = mul nsw i32 %conv1.i.4.7.10, 732 *)
mul v_mul_i_4_7_10 v_conv1_i_4_7_10 (732)@sint32;
(*   %call.i.4.7.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_10, v_call_i_4_7_10);
(*   %arrayidx11.4.7.10 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %1199 = load i16, i16* %arrayidx11.4.7.10, align 2, !tbaa !3 *)
mov v1199 mem0_334;
(*   %sub.4.7.10 = sub i16 %1199, %call.i.4.7.10 *)
sub v_sub_4_7_10 v1199 v_call_i_4_7_10;
(*   store i16 %sub.4.7.10, i16* %arrayidx9.4.7.10, align 2, !tbaa !3 *)
mov mem0_350 v_sub_4_7_10;
(*   %add21.4.7.10 = add i16 %1199, %call.i.4.7.10 *)
add v_add21_4_7_10 v1199 v_call_i_4_7_10;
(*   store i16 %add21.4.7.10, i16* %arrayidx11.4.7.10, align 2, !tbaa !3 *)
mov mem0_334 v_add21_4_7_10;

(* NOTE: k = 27 *)

(*   %arrayidx9.4.11 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %1200 = load i16, i16* %arrayidx9.4.11, align 2, !tbaa !3 *)
mov v1200 mem0_368;
(*   %conv1.i.4.11 = sext i16 %1200 to i32 *)
cast v_conv1_i_4_11@sint32 v1200@sint16;
(*   %mul.i.4.11 = mul nsw i32 %conv1.i.4.11, 608 *)
mul v_mul_i_4_11 v_conv1_i_4_11 (608)@sint32;
(*   %call.i.4.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_11, v_call_i_4_11);
(*   %arrayidx11.4.11 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %1201 = load i16, i16* %arrayidx11.4.11, align 2, !tbaa !3 *)
mov v1201 mem0_352;
(*   %sub.4.11 = sub i16 %1201, %call.i.4.11 *)
sub v_sub_4_11 v1201 v_call_i_4_11;
(*   store i16 %sub.4.11, i16* %arrayidx9.4.11, align 2, !tbaa !3 *)
mov mem0_368 v_sub_4_11;
(*   %add21.4.11 = add i16 %1201, %call.i.4.11 *)
add v_add21_4_11 v1201 v_call_i_4_11;
(*   store i16 %add21.4.11, i16* %arrayidx11.4.11, align 2, !tbaa !3 *)
mov mem0_352 v_add21_4_11;
(*   %arrayidx9.4.1.11 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %1202 = load i16, i16* %arrayidx9.4.1.11, align 2, !tbaa !3 *)
mov v1202 mem0_370;
(*   %conv1.i.4.1.11 = sext i16 %1202 to i32 *)
cast v_conv1_i_4_1_11@sint32 v1202@sint16;
(*   %mul.i.4.1.11 = mul nsw i32 %conv1.i.4.1.11, 608 *)
mul v_mul_i_4_1_11 v_conv1_i_4_1_11 (608)@sint32;
(*   %call.i.4.1.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_11, v_call_i_4_1_11);
(*   %arrayidx11.4.1.11 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %1203 = load i16, i16* %arrayidx11.4.1.11, align 2, !tbaa !3 *)
mov v1203 mem0_354;
(*   %sub.4.1.11 = sub i16 %1203, %call.i.4.1.11 *)
sub v_sub_4_1_11 v1203 v_call_i_4_1_11;
(*   store i16 %sub.4.1.11, i16* %arrayidx9.4.1.11, align 2, !tbaa !3 *)
mov mem0_370 v_sub_4_1_11;
(*   %add21.4.1.11 = add i16 %1203, %call.i.4.1.11 *)
add v_add21_4_1_11 v1203 v_call_i_4_1_11;
(*   store i16 %add21.4.1.11, i16* %arrayidx11.4.1.11, align 2, !tbaa !3 *)
mov mem0_354 v_add21_4_1_11;
(*   %arrayidx9.4.2.11 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %1204 = load i16, i16* %arrayidx9.4.2.11, align 2, !tbaa !3 *)
mov v1204 mem0_372;
(*   %conv1.i.4.2.11 = sext i16 %1204 to i32 *)
cast v_conv1_i_4_2_11@sint32 v1204@sint16;
(*   %mul.i.4.2.11 = mul nsw i32 %conv1.i.4.2.11, 608 *)
mul v_mul_i_4_2_11 v_conv1_i_4_2_11 (608)@sint32;
(*   %call.i.4.2.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_11, v_call_i_4_2_11);
(*   %arrayidx11.4.2.11 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %1205 = load i16, i16* %arrayidx11.4.2.11, align 2, !tbaa !3 *)
mov v1205 mem0_356;
(*   %sub.4.2.11 = sub i16 %1205, %call.i.4.2.11 *)
sub v_sub_4_2_11 v1205 v_call_i_4_2_11;
(*   store i16 %sub.4.2.11, i16* %arrayidx9.4.2.11, align 2, !tbaa !3 *)
mov mem0_372 v_sub_4_2_11;
(*   %add21.4.2.11 = add i16 %1205, %call.i.4.2.11 *)
add v_add21_4_2_11 v1205 v_call_i_4_2_11;
(*   store i16 %add21.4.2.11, i16* %arrayidx11.4.2.11, align 2, !tbaa !3 *)
mov mem0_356 v_add21_4_2_11;
(*   %arrayidx9.4.3.11 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %1206 = load i16, i16* %arrayidx9.4.3.11, align 2, !tbaa !3 *)
mov v1206 mem0_374;
(*   %conv1.i.4.3.11 = sext i16 %1206 to i32 *)
cast v_conv1_i_4_3_11@sint32 v1206@sint16;
(*   %mul.i.4.3.11 = mul nsw i32 %conv1.i.4.3.11, 608 *)
mul v_mul_i_4_3_11 v_conv1_i_4_3_11 (608)@sint32;
(*   %call.i.4.3.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_11, v_call_i_4_3_11);
(*   %arrayidx11.4.3.11 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %1207 = load i16, i16* %arrayidx11.4.3.11, align 2, !tbaa !3 *)
mov v1207 mem0_358;
(*   %sub.4.3.11 = sub i16 %1207, %call.i.4.3.11 *)
sub v_sub_4_3_11 v1207 v_call_i_4_3_11;
(*   store i16 %sub.4.3.11, i16* %arrayidx9.4.3.11, align 2, !tbaa !3 *)
mov mem0_374 v_sub_4_3_11;
(*   %add21.4.3.11 = add i16 %1207, %call.i.4.3.11 *)
add v_add21_4_3_11 v1207 v_call_i_4_3_11;
(*   store i16 %add21.4.3.11, i16* %arrayidx11.4.3.11, align 2, !tbaa !3 *)
mov mem0_358 v_add21_4_3_11;
(*   %arrayidx9.4.4.11 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %1208 = load i16, i16* %arrayidx9.4.4.11, align 2, !tbaa !3 *)
mov v1208 mem0_376;
(*   %conv1.i.4.4.11 = sext i16 %1208 to i32 *)
cast v_conv1_i_4_4_11@sint32 v1208@sint16;
(*   %mul.i.4.4.11 = mul nsw i32 %conv1.i.4.4.11, 608 *)
mul v_mul_i_4_4_11 v_conv1_i_4_4_11 (608)@sint32;
(*   %call.i.4.4.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_11, v_call_i_4_4_11);
(*   %arrayidx11.4.4.11 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %1209 = load i16, i16* %arrayidx11.4.4.11, align 2, !tbaa !3 *)
mov v1209 mem0_360;
(*   %sub.4.4.11 = sub i16 %1209, %call.i.4.4.11 *)
sub v_sub_4_4_11 v1209 v_call_i_4_4_11;
(*   store i16 %sub.4.4.11, i16* %arrayidx9.4.4.11, align 2, !tbaa !3 *)
mov mem0_376 v_sub_4_4_11;
(*   %add21.4.4.11 = add i16 %1209, %call.i.4.4.11 *)
add v_add21_4_4_11 v1209 v_call_i_4_4_11;
(*   store i16 %add21.4.4.11, i16* %arrayidx11.4.4.11, align 2, !tbaa !3 *)
mov mem0_360 v_add21_4_4_11;
(*   %arrayidx9.4.5.11 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %1210 = load i16, i16* %arrayidx9.4.5.11, align 2, !tbaa !3 *)
mov v1210 mem0_378;
(*   %conv1.i.4.5.11 = sext i16 %1210 to i32 *)
cast v_conv1_i_4_5_11@sint32 v1210@sint16;
(*   %mul.i.4.5.11 = mul nsw i32 %conv1.i.4.5.11, 608 *)
mul v_mul_i_4_5_11 v_conv1_i_4_5_11 (608)@sint32;
(*   %call.i.4.5.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_11, v_call_i_4_5_11);
(*   %arrayidx11.4.5.11 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %1211 = load i16, i16* %arrayidx11.4.5.11, align 2, !tbaa !3 *)
mov v1211 mem0_362;
(*   %sub.4.5.11 = sub i16 %1211, %call.i.4.5.11 *)
sub v_sub_4_5_11 v1211 v_call_i_4_5_11;
(*   store i16 %sub.4.5.11, i16* %arrayidx9.4.5.11, align 2, !tbaa !3 *)
mov mem0_378 v_sub_4_5_11;
(*   %add21.4.5.11 = add i16 %1211, %call.i.4.5.11 *)
add v_add21_4_5_11 v1211 v_call_i_4_5_11;
(*   store i16 %add21.4.5.11, i16* %arrayidx11.4.5.11, align 2, !tbaa !3 *)
mov mem0_362 v_add21_4_5_11;
(*   %arrayidx9.4.6.11 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %1212 = load i16, i16* %arrayidx9.4.6.11, align 2, !tbaa !3 *)
mov v1212 mem0_380;
(*   %conv1.i.4.6.11 = sext i16 %1212 to i32 *)
cast v_conv1_i_4_6_11@sint32 v1212@sint16;
(*   %mul.i.4.6.11 = mul nsw i32 %conv1.i.4.6.11, 608 *)
mul v_mul_i_4_6_11 v_conv1_i_4_6_11 (608)@sint32;
(*   %call.i.4.6.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_11, v_call_i_4_6_11);
(*   %arrayidx11.4.6.11 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %1213 = load i16, i16* %arrayidx11.4.6.11, align 2, !tbaa !3 *)
mov v1213 mem0_364;
(*   %sub.4.6.11 = sub i16 %1213, %call.i.4.6.11 *)
sub v_sub_4_6_11 v1213 v_call_i_4_6_11;
(*   store i16 %sub.4.6.11, i16* %arrayidx9.4.6.11, align 2, !tbaa !3 *)
mov mem0_380 v_sub_4_6_11;
(*   %add21.4.6.11 = add i16 %1213, %call.i.4.6.11 *)
add v_add21_4_6_11 v1213 v_call_i_4_6_11;
(*   store i16 %add21.4.6.11, i16* %arrayidx11.4.6.11, align 2, !tbaa !3 *)
mov mem0_364 v_add21_4_6_11;
(*   %arrayidx9.4.7.11 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %1214 = load i16, i16* %arrayidx9.4.7.11, align 2, !tbaa !3 *)
mov v1214 mem0_382;
(*   %conv1.i.4.7.11 = sext i16 %1214 to i32 *)
cast v_conv1_i_4_7_11@sint32 v1214@sint16;
(*   %mul.i.4.7.11 = mul nsw i32 %conv1.i.4.7.11, 608 *)
mul v_mul_i_4_7_11 v_conv1_i_4_7_11 (608)@sint32;
(*   %call.i.4.7.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_11, v_call_i_4_7_11);
(*   %arrayidx11.4.7.11 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %1215 = load i16, i16* %arrayidx11.4.7.11, align 2, !tbaa !3 *)
mov v1215 mem0_366;
(*   %sub.4.7.11 = sub i16 %1215, %call.i.4.7.11 *)
sub v_sub_4_7_11 v1215 v_call_i_4_7_11;
(*   store i16 %sub.4.7.11, i16* %arrayidx9.4.7.11, align 2, !tbaa !3 *)
mov mem0_382 v_sub_4_7_11;
(*   %add21.4.7.11 = add i16 %1215, %call.i.4.7.11 *)
add v_add21_4_7_11 v1215 v_call_i_4_7_11;
(*   store i16 %add21.4.7.11, i16* %arrayidx11.4.7.11, align 2, !tbaa !3 *)
mov mem0_366 v_add21_4_7_11;

(* NOTE: k = 28 *)

(*   %arrayidx9.4.12 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %1216 = load i16, i16* %arrayidx9.4.12, align 2, !tbaa !3 *)
mov v1216 mem0_400;
(*   %conv1.i.4.12 = sext i16 %1216 to i32 *)
cast v_conv1_i_4_12@sint32 v1216@sint16;
(*   %mul.i.4.12 = mul nsw i32 %conv1.i.4.12, -1542 *)
mul v_mul_i_4_12 v_conv1_i_4_12 (-1542)@sint32;
(*   %call.i.4.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_12, v_call_i_4_12);
(*   %arrayidx11.4.12 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %1217 = load i16, i16* %arrayidx11.4.12, align 2, !tbaa !3 *)
mov v1217 mem0_384;
(*   %sub.4.12 = sub i16 %1217, %call.i.4.12 *)
sub v_sub_4_12 v1217 v_call_i_4_12;
(*   store i16 %sub.4.12, i16* %arrayidx9.4.12, align 2, !tbaa !3 *)
mov mem0_400 v_sub_4_12;
(*   %add21.4.12 = add i16 %1217, %call.i.4.12 *)
add v_add21_4_12 v1217 v_call_i_4_12;
(*   store i16 %add21.4.12, i16* %arrayidx11.4.12, align 2, !tbaa !3 *)
mov mem0_384 v_add21_4_12;
(*   %arrayidx9.4.1.12 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %1218 = load i16, i16* %arrayidx9.4.1.12, align 2, !tbaa !3 *)
mov v1218 mem0_402;
(*   %conv1.i.4.1.12 = sext i16 %1218 to i32 *)
cast v_conv1_i_4_1_12@sint32 v1218@sint16;
(*   %mul.i.4.1.12 = mul nsw i32 %conv1.i.4.1.12, -1542 *)
mul v_mul_i_4_1_12 v_conv1_i_4_1_12 (-1542)@sint32;
(*   %call.i.4.1.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_12, v_call_i_4_1_12);
(*   %arrayidx11.4.1.12 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %1219 = load i16, i16* %arrayidx11.4.1.12, align 2, !tbaa !3 *)
mov v1219 mem0_386;
(*   %sub.4.1.12 = sub i16 %1219, %call.i.4.1.12 *)
sub v_sub_4_1_12 v1219 v_call_i_4_1_12;
(*   store i16 %sub.4.1.12, i16* %arrayidx9.4.1.12, align 2, !tbaa !3 *)
mov mem0_402 v_sub_4_1_12;
(*   %add21.4.1.12 = add i16 %1219, %call.i.4.1.12 *)
add v_add21_4_1_12 v1219 v_call_i_4_1_12;
(*   store i16 %add21.4.1.12, i16* %arrayidx11.4.1.12, align 2, !tbaa !3 *)
mov mem0_386 v_add21_4_1_12;
(*   %arrayidx9.4.2.12 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %1220 = load i16, i16* %arrayidx9.4.2.12, align 2, !tbaa !3 *)
mov v1220 mem0_404;
(*   %conv1.i.4.2.12 = sext i16 %1220 to i32 *)
cast v_conv1_i_4_2_12@sint32 v1220@sint16;
(*   %mul.i.4.2.12 = mul nsw i32 %conv1.i.4.2.12, -1542 *)
mul v_mul_i_4_2_12 v_conv1_i_4_2_12 (-1542)@sint32;
(*   %call.i.4.2.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_12, v_call_i_4_2_12);
(*   %arrayidx11.4.2.12 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %1221 = load i16, i16* %arrayidx11.4.2.12, align 2, !tbaa !3 *)
mov v1221 mem0_388;
(*   %sub.4.2.12 = sub i16 %1221, %call.i.4.2.12 *)
sub v_sub_4_2_12 v1221 v_call_i_4_2_12;
(*   store i16 %sub.4.2.12, i16* %arrayidx9.4.2.12, align 2, !tbaa !3 *)
mov mem0_404 v_sub_4_2_12;
(*   %add21.4.2.12 = add i16 %1221, %call.i.4.2.12 *)
add v_add21_4_2_12 v1221 v_call_i_4_2_12;
(*   store i16 %add21.4.2.12, i16* %arrayidx11.4.2.12, align 2, !tbaa !3 *)
mov mem0_388 v_add21_4_2_12;
(*   %arrayidx9.4.3.12 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %1222 = load i16, i16* %arrayidx9.4.3.12, align 2, !tbaa !3 *)
mov v1222 mem0_406;
(*   %conv1.i.4.3.12 = sext i16 %1222 to i32 *)
cast v_conv1_i_4_3_12@sint32 v1222@sint16;
(*   %mul.i.4.3.12 = mul nsw i32 %conv1.i.4.3.12, -1542 *)
mul v_mul_i_4_3_12 v_conv1_i_4_3_12 (-1542)@sint32;
(*   %call.i.4.3.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_12, v_call_i_4_3_12);
(*   %arrayidx11.4.3.12 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %1223 = load i16, i16* %arrayidx11.4.3.12, align 2, !tbaa !3 *)
mov v1223 mem0_390;
(*   %sub.4.3.12 = sub i16 %1223, %call.i.4.3.12 *)
sub v_sub_4_3_12 v1223 v_call_i_4_3_12;
(*   store i16 %sub.4.3.12, i16* %arrayidx9.4.3.12, align 2, !tbaa !3 *)
mov mem0_406 v_sub_4_3_12;
(*   %add21.4.3.12 = add i16 %1223, %call.i.4.3.12 *)
add v_add21_4_3_12 v1223 v_call_i_4_3_12;
(*   store i16 %add21.4.3.12, i16* %arrayidx11.4.3.12, align 2, !tbaa !3 *)
mov mem0_390 v_add21_4_3_12;
(*   %arrayidx9.4.4.12 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %1224 = load i16, i16* %arrayidx9.4.4.12, align 2, !tbaa !3 *)
mov v1224 mem0_408;
(*   %conv1.i.4.4.12 = sext i16 %1224 to i32 *)
cast v_conv1_i_4_4_12@sint32 v1224@sint16;
(*   %mul.i.4.4.12 = mul nsw i32 %conv1.i.4.4.12, -1542 *)
mul v_mul_i_4_4_12 v_conv1_i_4_4_12 (-1542)@sint32;
(*   %call.i.4.4.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_12, v_call_i_4_4_12);
(*   %arrayidx11.4.4.12 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %1225 = load i16, i16* %arrayidx11.4.4.12, align 2, !tbaa !3 *)
mov v1225 mem0_392;
(*   %sub.4.4.12 = sub i16 %1225, %call.i.4.4.12 *)
sub v_sub_4_4_12 v1225 v_call_i_4_4_12;
(*   store i16 %sub.4.4.12, i16* %arrayidx9.4.4.12, align 2, !tbaa !3 *)
mov mem0_408 v_sub_4_4_12;
(*   %add21.4.4.12 = add i16 %1225, %call.i.4.4.12 *)
add v_add21_4_4_12 v1225 v_call_i_4_4_12;
(*   store i16 %add21.4.4.12, i16* %arrayidx11.4.4.12, align 2, !tbaa !3 *)
mov mem0_392 v_add21_4_4_12;
(*   %arrayidx9.4.5.12 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %1226 = load i16, i16* %arrayidx9.4.5.12, align 2, !tbaa !3 *)
mov v1226 mem0_410;
(*   %conv1.i.4.5.12 = sext i16 %1226 to i32 *)
cast v_conv1_i_4_5_12@sint32 v1226@sint16;
(*   %mul.i.4.5.12 = mul nsw i32 %conv1.i.4.5.12, -1542 *)
mul v_mul_i_4_5_12 v_conv1_i_4_5_12 (-1542)@sint32;
(*   %call.i.4.5.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_12, v_call_i_4_5_12);
(*   %arrayidx11.4.5.12 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %1227 = load i16, i16* %arrayidx11.4.5.12, align 2, !tbaa !3 *)
mov v1227 mem0_394;
(*   %sub.4.5.12 = sub i16 %1227, %call.i.4.5.12 *)
sub v_sub_4_5_12 v1227 v_call_i_4_5_12;
(*   store i16 %sub.4.5.12, i16* %arrayidx9.4.5.12, align 2, !tbaa !3 *)
mov mem0_410 v_sub_4_5_12;
(*   %add21.4.5.12 = add i16 %1227, %call.i.4.5.12 *)
add v_add21_4_5_12 v1227 v_call_i_4_5_12;
(*   store i16 %add21.4.5.12, i16* %arrayidx11.4.5.12, align 2, !tbaa !3 *)
mov mem0_394 v_add21_4_5_12;
(*   %arrayidx9.4.6.12 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %1228 = load i16, i16* %arrayidx9.4.6.12, align 2, !tbaa !3 *)
mov v1228 mem0_412;
(*   %conv1.i.4.6.12 = sext i16 %1228 to i32 *)
cast v_conv1_i_4_6_12@sint32 v1228@sint16;
(*   %mul.i.4.6.12 = mul nsw i32 %conv1.i.4.6.12, -1542 *)
mul v_mul_i_4_6_12 v_conv1_i_4_6_12 (-1542)@sint32;
(*   %call.i.4.6.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_12, v_call_i_4_6_12);
(*   %arrayidx11.4.6.12 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %1229 = load i16, i16* %arrayidx11.4.6.12, align 2, !tbaa !3 *)
mov v1229 mem0_396;
(*   %sub.4.6.12 = sub i16 %1229, %call.i.4.6.12 *)
sub v_sub_4_6_12 v1229 v_call_i_4_6_12;
(*   store i16 %sub.4.6.12, i16* %arrayidx9.4.6.12, align 2, !tbaa !3 *)
mov mem0_412 v_sub_4_6_12;
(*   %add21.4.6.12 = add i16 %1229, %call.i.4.6.12 *)
add v_add21_4_6_12 v1229 v_call_i_4_6_12;
(*   store i16 %add21.4.6.12, i16* %arrayidx11.4.6.12, align 2, !tbaa !3 *)
mov mem0_396 v_add21_4_6_12;
(*   %arrayidx9.4.7.12 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %1230 = load i16, i16* %arrayidx9.4.7.12, align 2, !tbaa !3 *)
mov v1230 mem0_414;
(*   %conv1.i.4.7.12 = sext i16 %1230 to i32 *)
cast v_conv1_i_4_7_12@sint32 v1230@sint16;
(*   %mul.i.4.7.12 = mul nsw i32 %conv1.i.4.7.12, -1542 *)
mul v_mul_i_4_7_12 v_conv1_i_4_7_12 (-1542)@sint32;
(*   %call.i.4.7.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_12, v_call_i_4_7_12);
(*   %arrayidx11.4.7.12 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %1231 = load i16, i16* %arrayidx11.4.7.12, align 2, !tbaa !3 *)
mov v1231 mem0_398;
(*   %sub.4.7.12 = sub i16 %1231, %call.i.4.7.12 *)
sub v_sub_4_7_12 v1231 v_call_i_4_7_12;
(*   store i16 %sub.4.7.12, i16* %arrayidx9.4.7.12, align 2, !tbaa !3 *)
mov mem0_414 v_sub_4_7_12;
(*   %add21.4.7.12 = add i16 %1231, %call.i.4.7.12 *)
add v_add21_4_7_12 v1231 v_call_i_4_7_12;
(*   store i16 %add21.4.7.12, i16* %arrayidx11.4.7.12, align 2, !tbaa !3 *)
mov mem0_398 v_add21_4_7_12;

(* NOTE: k = 29 *)

(*   %arrayidx9.4.13 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %1232 = load i16, i16* %arrayidx9.4.13, align 2, !tbaa !3 *)
mov v1232 mem0_432;
(*   %conv1.i.4.13 = sext i16 %1232 to i32 *)
cast v_conv1_i_4_13@sint32 v1232@sint16;
(*   %mul.i.4.13 = mul nsw i32 %conv1.i.4.13, 411 *)
mul v_mul_i_4_13 v_conv1_i_4_13 (411)@sint32;
(*   %call.i.4.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_13, v_call_i_4_13);
(*   %arrayidx11.4.13 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %1233 = load i16, i16* %arrayidx11.4.13, align 2, !tbaa !3 *)
mov v1233 mem0_416;
(*   %sub.4.13 = sub i16 %1233, %call.i.4.13 *)
sub v_sub_4_13 v1233 v_call_i_4_13;
(*   store i16 %sub.4.13, i16* %arrayidx9.4.13, align 2, !tbaa !3 *)
mov mem0_432 v_sub_4_13;
(*   %add21.4.13 = add i16 %1233, %call.i.4.13 *)
add v_add21_4_13 v1233 v_call_i_4_13;
(*   store i16 %add21.4.13, i16* %arrayidx11.4.13, align 2, !tbaa !3 *)
mov mem0_416 v_add21_4_13;
(*   %arrayidx9.4.1.13 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %1234 = load i16, i16* %arrayidx9.4.1.13, align 2, !tbaa !3 *)
mov v1234 mem0_434;
(*   %conv1.i.4.1.13 = sext i16 %1234 to i32 *)
cast v_conv1_i_4_1_13@sint32 v1234@sint16;
(*   %mul.i.4.1.13 = mul nsw i32 %conv1.i.4.1.13, 411 *)
mul v_mul_i_4_1_13 v_conv1_i_4_1_13 (411)@sint32;
(*   %call.i.4.1.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_13, v_call_i_4_1_13);
(*   %arrayidx11.4.1.13 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %1235 = load i16, i16* %arrayidx11.4.1.13, align 2, !tbaa !3 *)
mov v1235 mem0_418;
(*   %sub.4.1.13 = sub i16 %1235, %call.i.4.1.13 *)
sub v_sub_4_1_13 v1235 v_call_i_4_1_13;
(*   store i16 %sub.4.1.13, i16* %arrayidx9.4.1.13, align 2, !tbaa !3 *)
mov mem0_434 v_sub_4_1_13;
(*   %add21.4.1.13 = add i16 %1235, %call.i.4.1.13 *)
add v_add21_4_1_13 v1235 v_call_i_4_1_13;
(*   store i16 %add21.4.1.13, i16* %arrayidx11.4.1.13, align 2, !tbaa !3 *)
mov mem0_418 v_add21_4_1_13;
(*   %arrayidx9.4.2.13 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %1236 = load i16, i16* %arrayidx9.4.2.13, align 2, !tbaa !3 *)
mov v1236 mem0_436;
(*   %conv1.i.4.2.13 = sext i16 %1236 to i32 *)
cast v_conv1_i_4_2_13@sint32 v1236@sint16;
(*   %mul.i.4.2.13 = mul nsw i32 %conv1.i.4.2.13, 411 *)
mul v_mul_i_4_2_13 v_conv1_i_4_2_13 (411)@sint32;
(*   %call.i.4.2.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_13, v_call_i_4_2_13);
(*   %arrayidx11.4.2.13 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %1237 = load i16, i16* %arrayidx11.4.2.13, align 2, !tbaa !3 *)
mov v1237 mem0_420;
(*   %sub.4.2.13 = sub i16 %1237, %call.i.4.2.13 *)
sub v_sub_4_2_13 v1237 v_call_i_4_2_13;
(*   store i16 %sub.4.2.13, i16* %arrayidx9.4.2.13, align 2, !tbaa !3 *)
mov mem0_436 v_sub_4_2_13;
(*   %add21.4.2.13 = add i16 %1237, %call.i.4.2.13 *)
add v_add21_4_2_13 v1237 v_call_i_4_2_13;
(*   store i16 %add21.4.2.13, i16* %arrayidx11.4.2.13, align 2, !tbaa !3 *)
mov mem0_420 v_add21_4_2_13;
(*   %arrayidx9.4.3.13 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %1238 = load i16, i16* %arrayidx9.4.3.13, align 2, !tbaa !3 *)
mov v1238 mem0_438;
(*   %conv1.i.4.3.13 = sext i16 %1238 to i32 *)
cast v_conv1_i_4_3_13@sint32 v1238@sint16;
(*   %mul.i.4.3.13 = mul nsw i32 %conv1.i.4.3.13, 411 *)
mul v_mul_i_4_3_13 v_conv1_i_4_3_13 (411)@sint32;
(*   %call.i.4.3.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_13, v_call_i_4_3_13);
(*   %arrayidx11.4.3.13 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %1239 = load i16, i16* %arrayidx11.4.3.13, align 2, !tbaa !3 *)
mov v1239 mem0_422;
(*   %sub.4.3.13 = sub i16 %1239, %call.i.4.3.13 *)
sub v_sub_4_3_13 v1239 v_call_i_4_3_13;
(*   store i16 %sub.4.3.13, i16* %arrayidx9.4.3.13, align 2, !tbaa !3 *)
mov mem0_438 v_sub_4_3_13;
(*   %add21.4.3.13 = add i16 %1239, %call.i.4.3.13 *)
add v_add21_4_3_13 v1239 v_call_i_4_3_13;
(*   store i16 %add21.4.3.13, i16* %arrayidx11.4.3.13, align 2, !tbaa !3 *)
mov mem0_422 v_add21_4_3_13;
(*   %arrayidx9.4.4.13 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %1240 = load i16, i16* %arrayidx9.4.4.13, align 2, !tbaa !3 *)
mov v1240 mem0_440;
(*   %conv1.i.4.4.13 = sext i16 %1240 to i32 *)
cast v_conv1_i_4_4_13@sint32 v1240@sint16;
(*   %mul.i.4.4.13 = mul nsw i32 %conv1.i.4.4.13, 411 *)
mul v_mul_i_4_4_13 v_conv1_i_4_4_13 (411)@sint32;
(*   %call.i.4.4.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_13, v_call_i_4_4_13);
(*   %arrayidx11.4.4.13 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %1241 = load i16, i16* %arrayidx11.4.4.13, align 2, !tbaa !3 *)
mov v1241 mem0_424;
(*   %sub.4.4.13 = sub i16 %1241, %call.i.4.4.13 *)
sub v_sub_4_4_13 v1241 v_call_i_4_4_13;
(*   store i16 %sub.4.4.13, i16* %arrayidx9.4.4.13, align 2, !tbaa !3 *)
mov mem0_440 v_sub_4_4_13;
(*   %add21.4.4.13 = add i16 %1241, %call.i.4.4.13 *)
add v_add21_4_4_13 v1241 v_call_i_4_4_13;
(*   store i16 %add21.4.4.13, i16* %arrayidx11.4.4.13, align 2, !tbaa !3 *)
mov mem0_424 v_add21_4_4_13;
(*   %arrayidx9.4.5.13 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %1242 = load i16, i16* %arrayidx9.4.5.13, align 2, !tbaa !3 *)
mov v1242 mem0_442;
(*   %conv1.i.4.5.13 = sext i16 %1242 to i32 *)
cast v_conv1_i_4_5_13@sint32 v1242@sint16;
(*   %mul.i.4.5.13 = mul nsw i32 %conv1.i.4.5.13, 411 *)
mul v_mul_i_4_5_13 v_conv1_i_4_5_13 (411)@sint32;
(*   %call.i.4.5.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_13, v_call_i_4_5_13);
(*   %arrayidx11.4.5.13 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %1243 = load i16, i16* %arrayidx11.4.5.13, align 2, !tbaa !3 *)
mov v1243 mem0_426;
(*   %sub.4.5.13 = sub i16 %1243, %call.i.4.5.13 *)
sub v_sub_4_5_13 v1243 v_call_i_4_5_13;
(*   store i16 %sub.4.5.13, i16* %arrayidx9.4.5.13, align 2, !tbaa !3 *)
mov mem0_442 v_sub_4_5_13;
(*   %add21.4.5.13 = add i16 %1243, %call.i.4.5.13 *)
add v_add21_4_5_13 v1243 v_call_i_4_5_13;
(*   store i16 %add21.4.5.13, i16* %arrayidx11.4.5.13, align 2, !tbaa !3 *)
mov mem0_426 v_add21_4_5_13;
(*   %arrayidx9.4.6.13 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %1244 = load i16, i16* %arrayidx9.4.6.13, align 2, !tbaa !3 *)
mov v1244 mem0_444;
(*   %conv1.i.4.6.13 = sext i16 %1244 to i32 *)
cast v_conv1_i_4_6_13@sint32 v1244@sint16;
(*   %mul.i.4.6.13 = mul nsw i32 %conv1.i.4.6.13, 411 *)
mul v_mul_i_4_6_13 v_conv1_i_4_6_13 (411)@sint32;
(*   %call.i.4.6.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_13, v_call_i_4_6_13);
(*   %arrayidx11.4.6.13 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %1245 = load i16, i16* %arrayidx11.4.6.13, align 2, !tbaa !3 *)
mov v1245 mem0_428;
(*   %sub.4.6.13 = sub i16 %1245, %call.i.4.6.13 *)
sub v_sub_4_6_13 v1245 v_call_i_4_6_13;
(*   store i16 %sub.4.6.13, i16* %arrayidx9.4.6.13, align 2, !tbaa !3 *)
mov mem0_444 v_sub_4_6_13;
(*   %add21.4.6.13 = add i16 %1245, %call.i.4.6.13 *)
add v_add21_4_6_13 v1245 v_call_i_4_6_13;
(*   store i16 %add21.4.6.13, i16* %arrayidx11.4.6.13, align 2, !tbaa !3 *)
mov mem0_428 v_add21_4_6_13;
(*   %arrayidx9.4.7.13 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %1246 = load i16, i16* %arrayidx9.4.7.13, align 2, !tbaa !3 *)
mov v1246 mem0_446;
(*   %conv1.i.4.7.13 = sext i16 %1246 to i32 *)
cast v_conv1_i_4_7_13@sint32 v1246@sint16;
(*   %mul.i.4.7.13 = mul nsw i32 %conv1.i.4.7.13, 411 *)
mul v_mul_i_4_7_13 v_conv1_i_4_7_13 (411)@sint32;
(*   %call.i.4.7.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_13, v_call_i_4_7_13);
(*   %arrayidx11.4.7.13 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %1247 = load i16, i16* %arrayidx11.4.7.13, align 2, !tbaa !3 *)
mov v1247 mem0_430;
(*   %sub.4.7.13 = sub i16 %1247, %call.i.4.7.13 *)
sub v_sub_4_7_13 v1247 v_call_i_4_7_13;
(*   store i16 %sub.4.7.13, i16* %arrayidx9.4.7.13, align 2, !tbaa !3 *)
mov mem0_446 v_sub_4_7_13;
(*   %add21.4.7.13 = add i16 %1247, %call.i.4.7.13 *)
add v_add21_4_7_13 v1247 v_call_i_4_7_13;
(*   store i16 %add21.4.7.13, i16* %arrayidx11.4.7.13, align 2, !tbaa !3 *)
mov mem0_430 v_add21_4_7_13;

(* NOTE: k = 30 *)

(*   %arrayidx9.4.14 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %1248 = load i16, i16* %arrayidx9.4.14, align 2, !tbaa !3 *)
mov v1248 mem0_464;
(*   %conv1.i.4.14 = sext i16 %1248 to i32 *)
cast v_conv1_i_4_14@sint32 v1248@sint16;
(*   %mul.i.4.14 = mul nsw i32 %conv1.i.4.14, -205 *)
mul v_mul_i_4_14 v_conv1_i_4_14 (-205)@sint32;
(*   %call.i.4.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_14, v_call_i_4_14);
(*   %arrayidx11.4.14 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %1249 = load i16, i16* %arrayidx11.4.14, align 2, !tbaa !3 *)
mov v1249 mem0_448;
(*   %sub.4.14 = sub i16 %1249, %call.i.4.14 *)
sub v_sub_4_14 v1249 v_call_i_4_14;
(*   store i16 %sub.4.14, i16* %arrayidx9.4.14, align 2, !tbaa !3 *)
mov mem0_464 v_sub_4_14;
(*   %add21.4.14 = add i16 %1249, %call.i.4.14 *)
add v_add21_4_14 v1249 v_call_i_4_14;
(*   store i16 %add21.4.14, i16* %arrayidx11.4.14, align 2, !tbaa !3 *)
mov mem0_448 v_add21_4_14;
(*   %arrayidx9.4.1.14 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %1250 = load i16, i16* %arrayidx9.4.1.14, align 2, !tbaa !3 *)
mov v1250 mem0_466;
(*   %conv1.i.4.1.14 = sext i16 %1250 to i32 *)
cast v_conv1_i_4_1_14@sint32 v1250@sint16;
(*   %mul.i.4.1.14 = mul nsw i32 %conv1.i.4.1.14, -205 *)
mul v_mul_i_4_1_14 v_conv1_i_4_1_14 (-205)@sint32;
(*   %call.i.4.1.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_14, v_call_i_4_1_14);
(*   %arrayidx11.4.1.14 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %1251 = load i16, i16* %arrayidx11.4.1.14, align 2, !tbaa !3 *)
mov v1251 mem0_450;
(*   %sub.4.1.14 = sub i16 %1251, %call.i.4.1.14 *)
sub v_sub_4_1_14 v1251 v_call_i_4_1_14;
(*   store i16 %sub.4.1.14, i16* %arrayidx9.4.1.14, align 2, !tbaa !3 *)
mov mem0_466 v_sub_4_1_14;
(*   %add21.4.1.14 = add i16 %1251, %call.i.4.1.14 *)
add v_add21_4_1_14 v1251 v_call_i_4_1_14;
(*   store i16 %add21.4.1.14, i16* %arrayidx11.4.1.14, align 2, !tbaa !3 *)
mov mem0_450 v_add21_4_1_14;
(*   %arrayidx9.4.2.14 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %1252 = load i16, i16* %arrayidx9.4.2.14, align 2, !tbaa !3 *)
mov v1252 mem0_468;
(*   %conv1.i.4.2.14 = sext i16 %1252 to i32 *)
cast v_conv1_i_4_2_14@sint32 v1252@sint16;
(*   %mul.i.4.2.14 = mul nsw i32 %conv1.i.4.2.14, -205 *)
mul v_mul_i_4_2_14 v_conv1_i_4_2_14 (-205)@sint32;
(*   %call.i.4.2.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_14, v_call_i_4_2_14);
(*   %arrayidx11.4.2.14 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %1253 = load i16, i16* %arrayidx11.4.2.14, align 2, !tbaa !3 *)
mov v1253 mem0_452;
(*   %sub.4.2.14 = sub i16 %1253, %call.i.4.2.14 *)
sub v_sub_4_2_14 v1253 v_call_i_4_2_14;
(*   store i16 %sub.4.2.14, i16* %arrayidx9.4.2.14, align 2, !tbaa !3 *)
mov mem0_468 v_sub_4_2_14;
(*   %add21.4.2.14 = add i16 %1253, %call.i.4.2.14 *)
add v_add21_4_2_14 v1253 v_call_i_4_2_14;
(*   store i16 %add21.4.2.14, i16* %arrayidx11.4.2.14, align 2, !tbaa !3 *)
mov mem0_452 v_add21_4_2_14;
(*   %arrayidx9.4.3.14 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %1254 = load i16, i16* %arrayidx9.4.3.14, align 2, !tbaa !3 *)
mov v1254 mem0_470;
(*   %conv1.i.4.3.14 = sext i16 %1254 to i32 *)
cast v_conv1_i_4_3_14@sint32 v1254@sint16;
(*   %mul.i.4.3.14 = mul nsw i32 %conv1.i.4.3.14, -205 *)
mul v_mul_i_4_3_14 v_conv1_i_4_3_14 (-205)@sint32;
(*   %call.i.4.3.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_14, v_call_i_4_3_14);
(*   %arrayidx11.4.3.14 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %1255 = load i16, i16* %arrayidx11.4.3.14, align 2, !tbaa !3 *)
mov v1255 mem0_454;
(*   %sub.4.3.14 = sub i16 %1255, %call.i.4.3.14 *)
sub v_sub_4_3_14 v1255 v_call_i_4_3_14;
(*   store i16 %sub.4.3.14, i16* %arrayidx9.4.3.14, align 2, !tbaa !3 *)
mov mem0_470 v_sub_4_3_14;
(*   %add21.4.3.14 = add i16 %1255, %call.i.4.3.14 *)
add v_add21_4_3_14 v1255 v_call_i_4_3_14;
(*   store i16 %add21.4.3.14, i16* %arrayidx11.4.3.14, align 2, !tbaa !3 *)
mov mem0_454 v_add21_4_3_14;
(*   %arrayidx9.4.4.14 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %1256 = load i16, i16* %arrayidx9.4.4.14, align 2, !tbaa !3 *)
mov v1256 mem0_472;
(*   %conv1.i.4.4.14 = sext i16 %1256 to i32 *)
cast v_conv1_i_4_4_14@sint32 v1256@sint16;
(*   %mul.i.4.4.14 = mul nsw i32 %conv1.i.4.4.14, -205 *)
mul v_mul_i_4_4_14 v_conv1_i_4_4_14 (-205)@sint32;
(*   %call.i.4.4.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_14, v_call_i_4_4_14);
(*   %arrayidx11.4.4.14 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %1257 = load i16, i16* %arrayidx11.4.4.14, align 2, !tbaa !3 *)
mov v1257 mem0_456;
(*   %sub.4.4.14 = sub i16 %1257, %call.i.4.4.14 *)
sub v_sub_4_4_14 v1257 v_call_i_4_4_14;
(*   store i16 %sub.4.4.14, i16* %arrayidx9.4.4.14, align 2, !tbaa !3 *)
mov mem0_472 v_sub_4_4_14;
(*   %add21.4.4.14 = add i16 %1257, %call.i.4.4.14 *)
add v_add21_4_4_14 v1257 v_call_i_4_4_14;
(*   store i16 %add21.4.4.14, i16* %arrayidx11.4.4.14, align 2, !tbaa !3 *)
mov mem0_456 v_add21_4_4_14;
(*   %arrayidx9.4.5.14 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %1258 = load i16, i16* %arrayidx9.4.5.14, align 2, !tbaa !3 *)
mov v1258 mem0_474;
(*   %conv1.i.4.5.14 = sext i16 %1258 to i32 *)
cast v_conv1_i_4_5_14@sint32 v1258@sint16;
(*   %mul.i.4.5.14 = mul nsw i32 %conv1.i.4.5.14, -205 *)
mul v_mul_i_4_5_14 v_conv1_i_4_5_14 (-205)@sint32;
(*   %call.i.4.5.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_14, v_call_i_4_5_14);
(*   %arrayidx11.4.5.14 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %1259 = load i16, i16* %arrayidx11.4.5.14, align 2, !tbaa !3 *)
mov v1259 mem0_458;
(*   %sub.4.5.14 = sub i16 %1259, %call.i.4.5.14 *)
sub v_sub_4_5_14 v1259 v_call_i_4_5_14;
(*   store i16 %sub.4.5.14, i16* %arrayidx9.4.5.14, align 2, !tbaa !3 *)
mov mem0_474 v_sub_4_5_14;
(*   %add21.4.5.14 = add i16 %1259, %call.i.4.5.14 *)
add v_add21_4_5_14 v1259 v_call_i_4_5_14;
(*   store i16 %add21.4.5.14, i16* %arrayidx11.4.5.14, align 2, !tbaa !3 *)
mov mem0_458 v_add21_4_5_14;
(*   %arrayidx9.4.6.14 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %1260 = load i16, i16* %arrayidx9.4.6.14, align 2, !tbaa !3 *)
mov v1260 mem0_476;
(*   %conv1.i.4.6.14 = sext i16 %1260 to i32 *)
cast v_conv1_i_4_6_14@sint32 v1260@sint16;
(*   %mul.i.4.6.14 = mul nsw i32 %conv1.i.4.6.14, -205 *)
mul v_mul_i_4_6_14 v_conv1_i_4_6_14 (-205)@sint32;
(*   %call.i.4.6.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_14, v_call_i_4_6_14);
(*   %arrayidx11.4.6.14 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %1261 = load i16, i16* %arrayidx11.4.6.14, align 2, !tbaa !3 *)
mov v1261 mem0_460;
(*   %sub.4.6.14 = sub i16 %1261, %call.i.4.6.14 *)
sub v_sub_4_6_14 v1261 v_call_i_4_6_14;
(*   store i16 %sub.4.6.14, i16* %arrayidx9.4.6.14, align 2, !tbaa !3 *)
mov mem0_476 v_sub_4_6_14;
(*   %add21.4.6.14 = add i16 %1261, %call.i.4.6.14 *)
add v_add21_4_6_14 v1261 v_call_i_4_6_14;
(*   store i16 %add21.4.6.14, i16* %arrayidx11.4.6.14, align 2, !tbaa !3 *)
mov mem0_460 v_add21_4_6_14;
(*   %arrayidx9.4.7.14 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %1262 = load i16, i16* %arrayidx9.4.7.14, align 2, !tbaa !3 *)
mov v1262 mem0_478;
(*   %conv1.i.4.7.14 = sext i16 %1262 to i32 *)
cast v_conv1_i_4_7_14@sint32 v1262@sint16;
(*   %mul.i.4.7.14 = mul nsw i32 %conv1.i.4.7.14, -205 *)
mul v_mul_i_4_7_14 v_conv1_i_4_7_14 (-205)@sint32;
(*   %call.i.4.7.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_14, v_call_i_4_7_14);
(*   %arrayidx11.4.7.14 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %1263 = load i16, i16* %arrayidx11.4.7.14, align 2, !tbaa !3 *)
mov v1263 mem0_462;
(*   %sub.4.7.14 = sub i16 %1263, %call.i.4.7.14 *)
sub v_sub_4_7_14 v1263 v_call_i_4_7_14;
(*   store i16 %sub.4.7.14, i16* %arrayidx9.4.7.14, align 2, !tbaa !3 *)
mov mem0_478 v_sub_4_7_14;
(*   %add21.4.7.14 = add i16 %1263, %call.i.4.7.14 *)
add v_add21_4_7_14 v1263 v_call_i_4_7_14;
(*   store i16 %add21.4.7.14, i16* %arrayidx11.4.7.14, align 2, !tbaa !3 *)
mov mem0_462 v_add21_4_7_14;

(* NOTE: k = 31 *)

(*   %arrayidx9.4.15 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %1264 = load i16, i16* %arrayidx9.4.15, align 2, !tbaa !3 *)
mov v1264 mem0_496;
(*   %conv1.i.4.15 = sext i16 %1264 to i32 *)
cast v_conv1_i_4_15@sint32 v1264@sint16;
(*   %mul.i.4.15 = mul nsw i32 %conv1.i.4.15, -1571 *)
mul v_mul_i_4_15 v_conv1_i_4_15 (-1571)@sint32;
(*   %call.i.4.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_15, v_call_i_4_15);
(*   %arrayidx11.4.15 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %1265 = load i16, i16* %arrayidx11.4.15, align 2, !tbaa !3 *)
mov v1265 mem0_480;
(*   %sub.4.15 = sub i16 %1265, %call.i.4.15 *)
sub v_sub_4_15 v1265 v_call_i_4_15;
(*   store i16 %sub.4.15, i16* %arrayidx9.4.15, align 2, !tbaa !3 *)
mov mem0_496 v_sub_4_15;
(*   %add21.4.15 = add i16 %1265, %call.i.4.15 *)
add v_add21_4_15 v1265 v_call_i_4_15;
(*   store i16 %add21.4.15, i16* %arrayidx11.4.15, align 2, !tbaa !3 *)
mov mem0_480 v_add21_4_15;
(*   %arrayidx9.4.1.15 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %1266 = load i16, i16* %arrayidx9.4.1.15, align 2, !tbaa !3 *)
mov v1266 mem0_498;
(*   %conv1.i.4.1.15 = sext i16 %1266 to i32 *)
cast v_conv1_i_4_1_15@sint32 v1266@sint16;
(*   %mul.i.4.1.15 = mul nsw i32 %conv1.i.4.1.15, -1571 *)
mul v_mul_i_4_1_15 v_conv1_i_4_1_15 (-1571)@sint32;
(*   %call.i.4.1.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.1.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_1_15, v_call_i_4_1_15);
(*   %arrayidx11.4.1.15 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %1267 = load i16, i16* %arrayidx11.4.1.15, align 2, !tbaa !3 *)
mov v1267 mem0_482;
(*   %sub.4.1.15 = sub i16 %1267, %call.i.4.1.15 *)
sub v_sub_4_1_15 v1267 v_call_i_4_1_15;
(*   store i16 %sub.4.1.15, i16* %arrayidx9.4.1.15, align 2, !tbaa !3 *)
mov mem0_498 v_sub_4_1_15;
(*   %add21.4.1.15 = add i16 %1267, %call.i.4.1.15 *)
add v_add21_4_1_15 v1267 v_call_i_4_1_15;
(*   store i16 %add21.4.1.15, i16* %arrayidx11.4.1.15, align 2, !tbaa !3 *)
mov mem0_482 v_add21_4_1_15;
(*   %arrayidx9.4.2.15 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %1268 = load i16, i16* %arrayidx9.4.2.15, align 2, !tbaa !3 *)
mov v1268 mem0_500;
(*   %conv1.i.4.2.15 = sext i16 %1268 to i32 *)
cast v_conv1_i_4_2_15@sint32 v1268@sint16;
(*   %mul.i.4.2.15 = mul nsw i32 %conv1.i.4.2.15, -1571 *)
mul v_mul_i_4_2_15 v_conv1_i_4_2_15 (-1571)@sint32;
(*   %call.i.4.2.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.2.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_2_15, v_call_i_4_2_15);
(*   %arrayidx11.4.2.15 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %1269 = load i16, i16* %arrayidx11.4.2.15, align 2, !tbaa !3 *)
mov v1269 mem0_484;
(*   %sub.4.2.15 = sub i16 %1269, %call.i.4.2.15 *)
sub v_sub_4_2_15 v1269 v_call_i_4_2_15;
(*   store i16 %sub.4.2.15, i16* %arrayidx9.4.2.15, align 2, !tbaa !3 *)
mov mem0_500 v_sub_4_2_15;
(*   %add21.4.2.15 = add i16 %1269, %call.i.4.2.15 *)
add v_add21_4_2_15 v1269 v_call_i_4_2_15;
(*   store i16 %add21.4.2.15, i16* %arrayidx11.4.2.15, align 2, !tbaa !3 *)
mov mem0_484 v_add21_4_2_15;
(*   %arrayidx9.4.3.15 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %1270 = load i16, i16* %arrayidx9.4.3.15, align 2, !tbaa !3 *)
mov v1270 mem0_502;
(*   %conv1.i.4.3.15 = sext i16 %1270 to i32 *)
cast v_conv1_i_4_3_15@sint32 v1270@sint16;
(*   %mul.i.4.3.15 = mul nsw i32 %conv1.i.4.3.15, -1571 *)
mul v_mul_i_4_3_15 v_conv1_i_4_3_15 (-1571)@sint32;
(*   %call.i.4.3.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.3.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_3_15, v_call_i_4_3_15);
(*   %arrayidx11.4.3.15 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %1271 = load i16, i16* %arrayidx11.4.3.15, align 2, !tbaa !3 *)
mov v1271 mem0_486;
(*   %sub.4.3.15 = sub i16 %1271, %call.i.4.3.15 *)
sub v_sub_4_3_15 v1271 v_call_i_4_3_15;
(*   store i16 %sub.4.3.15, i16* %arrayidx9.4.3.15, align 2, !tbaa !3 *)
mov mem0_502 v_sub_4_3_15;
(*   %add21.4.3.15 = add i16 %1271, %call.i.4.3.15 *)
add v_add21_4_3_15 v1271 v_call_i_4_3_15;
(*   store i16 %add21.4.3.15, i16* %arrayidx11.4.3.15, align 2, !tbaa !3 *)
mov mem0_486 v_add21_4_3_15;
(*   %arrayidx9.4.4.15 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %1272 = load i16, i16* %arrayidx9.4.4.15, align 2, !tbaa !3 *)
mov v1272 mem0_504;
(*   %conv1.i.4.4.15 = sext i16 %1272 to i32 *)
cast v_conv1_i_4_4_15@sint32 v1272@sint16;
(*   %mul.i.4.4.15 = mul nsw i32 %conv1.i.4.4.15, -1571 *)
mul v_mul_i_4_4_15 v_conv1_i_4_4_15 (-1571)@sint32;
(*   %call.i.4.4.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.4.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_4_15, v_call_i_4_4_15);
(*   %arrayidx11.4.4.15 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %1273 = load i16, i16* %arrayidx11.4.4.15, align 2, !tbaa !3 *)
mov v1273 mem0_488;
(*   %sub.4.4.15 = sub i16 %1273, %call.i.4.4.15 *)
sub v_sub_4_4_15 v1273 v_call_i_4_4_15;
(*   store i16 %sub.4.4.15, i16* %arrayidx9.4.4.15, align 2, !tbaa !3 *)
mov mem0_504 v_sub_4_4_15;
(*   %add21.4.4.15 = add i16 %1273, %call.i.4.4.15 *)
add v_add21_4_4_15 v1273 v_call_i_4_4_15;
(*   store i16 %add21.4.4.15, i16* %arrayidx11.4.4.15, align 2, !tbaa !3 *)
mov mem0_488 v_add21_4_4_15;
(*   %arrayidx9.4.5.15 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %1274 = load i16, i16* %arrayidx9.4.5.15, align 2, !tbaa !3 *)
mov v1274 mem0_506;
(*   %conv1.i.4.5.15 = sext i16 %1274 to i32 *)
cast v_conv1_i_4_5_15@sint32 v1274@sint16;
(*   %mul.i.4.5.15 = mul nsw i32 %conv1.i.4.5.15, -1571 *)
mul v_mul_i_4_5_15 v_conv1_i_4_5_15 (-1571)@sint32;
(*   %call.i.4.5.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.5.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_5_15, v_call_i_4_5_15);
(*   %arrayidx11.4.5.15 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %1275 = load i16, i16* %arrayidx11.4.5.15, align 2, !tbaa !3 *)
mov v1275 mem0_490;
(*   %sub.4.5.15 = sub i16 %1275, %call.i.4.5.15 *)
sub v_sub_4_5_15 v1275 v_call_i_4_5_15;
(*   store i16 %sub.4.5.15, i16* %arrayidx9.4.5.15, align 2, !tbaa !3 *)
mov mem0_506 v_sub_4_5_15;
(*   %add21.4.5.15 = add i16 %1275, %call.i.4.5.15 *)
add v_add21_4_5_15 v1275 v_call_i_4_5_15;
(*   store i16 %add21.4.5.15, i16* %arrayidx11.4.5.15, align 2, !tbaa !3 *)
mov mem0_490 v_add21_4_5_15;
(*   %arrayidx9.4.6.15 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %1276 = load i16, i16* %arrayidx9.4.6.15, align 2, !tbaa !3 *)
mov v1276 mem0_508;
(*   %conv1.i.4.6.15 = sext i16 %1276 to i32 *)
cast v_conv1_i_4_6_15@sint32 v1276@sint16;
(*   %mul.i.4.6.15 = mul nsw i32 %conv1.i.4.6.15, -1571 *)
mul v_mul_i_4_6_15 v_conv1_i_4_6_15 (-1571)@sint32;
(*   %call.i.4.6.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.6.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_6_15, v_call_i_4_6_15);
(*   %arrayidx11.4.6.15 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %1277 = load i16, i16* %arrayidx11.4.6.15, align 2, !tbaa !3 *)
mov v1277 mem0_492;
(*   %sub.4.6.15 = sub i16 %1277, %call.i.4.6.15 *)
sub v_sub_4_6_15 v1277 v_call_i_4_6_15;
(*   store i16 %sub.4.6.15, i16* %arrayidx9.4.6.15, align 2, !tbaa !3 *)
mov mem0_508 v_sub_4_6_15;
(*   %add21.4.6.15 = add i16 %1277, %call.i.4.6.15 *)
add v_add21_4_6_15 v1277 v_call_i_4_6_15;
(*   store i16 %add21.4.6.15, i16* %arrayidx11.4.6.15, align 2, !tbaa !3 *)
mov mem0_492 v_add21_4_6_15;
(*   %arrayidx9.4.7.15 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %1278 = load i16, i16* %arrayidx9.4.7.15, align 2, !tbaa !3 *)
mov v1278 mem0_510;
(*   %conv1.i.4.7.15 = sext i16 %1278 to i32 *)
cast v_conv1_i_4_7_15@sint32 v1278@sint16;
(*   %mul.i.4.7.15 = mul nsw i32 %conv1.i.4.7.15, -1571 *)
mul v_mul_i_4_7_15 v_conv1_i_4_7_15 (-1571)@sint32;
(*   %call.i.4.7.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.4.7.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_4_7_15, v_call_i_4_7_15);
(*   %arrayidx11.4.7.15 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %1279 = load i16, i16* %arrayidx11.4.7.15, align 2, !tbaa !3 *)
mov v1279 mem0_494;
(*   %sub.4.7.15 = sub i16 %1279, %call.i.4.7.15 *)
sub v_sub_4_7_15 v1279 v_call_i_4_7_15;
(*   store i16 %sub.4.7.15, i16* %arrayidx9.4.7.15, align 2, !tbaa !3 *)
mov mem0_510 v_sub_4_7_15;
(*   %add21.4.7.15 = add i16 %1279, %call.i.4.7.15 *)
add v_add21_4_7_15 v1279 v_call_i_4_7_15;
(*   store i16 %add21.4.7.15, i16* %arrayidx11.4.7.15, align 2, !tbaa !3 *)
mov mem0_494 v_add21_4_7_15;

(* NOTE: summary of 5 iterations *)

(*
assert eqmod 
(2285 * (a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255))
(mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3) + 
 mem0_8*(x**4) + mem0_10*(x**5) + mem0_12*(x**6) + mem0_14*(x**7))
[3329, x**8 - 296] && true;
*)

(*    Z_q[x]/(x**256 - zeta**128)
          ~= Z_q[x]/( x**8 -   zeta**4) * Z_q[x]/(x** 8 - zeta**132) *
             Z_q[x]/( x**8 -  zeta**68) * Z_q[x]/(x** 8 - zeta**196) *
             Z_q[x]/( x**8 -  zeta**36) * Z_q[x]/(x** 8 - zeta**164) *
             Z_q[x]/( x**8 - zeta**100) * Z_q[x]/(x** 8 - zeta**228) *
             Z_q[x]/( x**8 -  zeta**20) * Z_q[x]/(x** 8 - zeta**148) *
             Z_q[x]/( x**8 -  zeta**84) * Z_q[x]/(x** 8 - zeta**212) *
             Z_q[x]/( x**8 -  zeta**52) * Z_q[x]/(x** 8 - zeta**180) *
             Z_q[x]/( x**8 - zeta**116) * Z_q[x]/(x** 8 - zeta**244) *
             Z_q[x]/( x**8 -  zeta**12) * Z_q[x]/(x** 8 - zeta**140) *
             Z_q[x]/( x**8 -  zeta**76) * Z_q[x]/(x** 8 - zeta**204) *
             Z_q[x]/( x**8 -  zeta**44) * Z_q[x]/(x** 8 - zeta**172) *
             Z_q[x]/( x**8 - zeta**108) * Z_q[x]/(x** 8 - zeta**236) *
             Z_q[x]/( x**8 -  zeta**28) * Z_q[x]/(x** 8 - zeta**156) *
             Z_q[x]/( x**8 -  zeta**92) * Z_q[x]/(x** 8 - zeta**220) *
             Z_q[x]/( x**8 -  zeta**60) * Z_q[x]/(x** 8 - zeta**188) *
             Z_q[x]/( x**8 - zeta**124) * Z_q[x]/(x** 8 - zeta**252)
*)

cut and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3) + 
mem0_8*(x**4) + mem0_10*(x**5) + mem0_12*(x**6) + mem0_14*(x**7)
)
[3329, x**8 - 296],
eqmod 
input_polynomial * input_polynomial
(
mem0_16*(x**0) + mem0_18*(x**1) + mem0_20*(x**2) + mem0_22*(x**3) + 
mem0_24*(x**4) + mem0_26*(x**5) + mem0_28*(x**6) + mem0_30*(x**7)
)
[3329, x**8 - 3033],
eqmod 
input_polynomial * input_polynomial
(
mem0_32*(x**0) + mem0_34*(x**1) + mem0_36*(x**2) + mem0_38*(x**3) + 
mem0_40*(x**4) + mem0_42*(x**5) + mem0_44*(x**6) + mem0_46*(x**7)
)
[3329, x**8 - 2447],
eqmod 
input_polynomial * input_polynomial
(
mem0_48*(x**0) + mem0_50*(x**1) + mem0_52*(x**2) + mem0_54*(x**3) + 
mem0_56*(x**4) + mem0_58*(x**5) + mem0_60*(x**6) + mem0_62*(x**7)
)
[3329, x**8 - 882],
eqmod 
input_polynomial * input_polynomial
(
mem0_64*(x**0) + mem0_66*(x**1) + mem0_68*(x**2) + mem0_70*(x**3) + 
mem0_72*(x**4) + mem0_74*(x**5) + mem0_76*(x**6) + mem0_78*(x**7)
)
[3329, x**8 - 1339],
eqmod 
input_polynomial * input_polynomial
(
mem0_80*(x**0) + mem0_82*(x**1) + mem0_84*(x**2) + mem0_86*(x**3) + 
mem0_88*(x**4) + mem0_90*(x**5) + mem0_92*(x**6) + mem0_94*(x**7)
)
[3329, x**8 - 1990],
eqmod 
input_polynomial * input_polynomial
(
mem0_96*(x**0) + mem0_98*(x**1) + mem0_100*(x**2) + mem0_102*(x**3) + 
mem0_104*(x**4) + mem0_106*(x**5) + mem0_108*(x**6) + mem0_110*(x**7)
)
[3329, x**8 - 1476],
eqmod 
input_polynomial * input_polynomial
(
mem0_112*(x**0) + mem0_114*(x**1) + mem0_116*(x**2) + mem0_118*(x**3) + 
mem0_120*(x**4) + mem0_122*(x**5) + mem0_124*(x**6) + mem0_126*(x**7)
)
[3329, x**8 - 1853],
eqmod 
input_polynomial * input_polynomial
(
mem0_128*(x**0) + mem0_130*(x**1) + mem0_132*(x**2) + mem0_134*(x**3) + 
mem0_136*(x**4) + mem0_138*(x**5) + mem0_140*(x**6) + mem0_142*(x**7)
)
[3329, x**8 - 3046],
eqmod 
input_polynomial * input_polynomial
(
mem0_144*(x**0) + mem0_146*(x**1) + mem0_148*(x**2) + mem0_150*(x**3) + 
mem0_152*(x**4) + mem0_154*(x**5) + mem0_156*(x**6) + mem0_158*(x**7)
)
[3329, x**8 - 283],
eqmod 
input_polynomial * input_polynomial
(
mem0_160*(x**0) + mem0_162*(x**1) + mem0_164*(x**2) + mem0_166*(x**3) + 
mem0_168*(x**4) + mem0_170*(x**5) + mem0_172*(x**6) + mem0_174*(x**7)
)
[3329, x**8 - 56],
eqmod 
input_polynomial * input_polynomial
(
mem0_176*(x**0) + mem0_178*(x**1) + mem0_180*(x**2) + mem0_182*(x**3) + 
mem0_184*(x**4) + mem0_186*(x**5) + mem0_188*(x**6) + mem0_190*(x**7)
)
[3329, x**8 - 3273],
eqmod 
input_polynomial * input_polynomial
(
mem0_192*(x**0) + mem0_194*(x**1) + mem0_196*(x**2) + mem0_198*(x**3) + 
mem0_200*(x**4) + mem0_202*(x**5) + mem0_204*(x**6) + mem0_206*(x**7)
)
[3329, x**8 - 2240],
eqmod 
input_polynomial * input_polynomial
(
mem0_208*(x**0) + mem0_210*(x**1) + mem0_212*(x**2) + mem0_214*(x**3) + 
mem0_216*(x**4) + mem0_218*(x**5) + mem0_220*(x**6) + mem0_222*(x**7)
)
[3329, x**8 - 1089],
eqmod 
input_polynomial * input_polynomial
(
mem0_224*(x**0) + mem0_226*(x**1) + mem0_228*(x**2) + mem0_230*(x**3) + 
mem0_232*(x**4) + mem0_234*(x**5) + mem0_236*(x**6) + mem0_238*(x**7)
)
[3329, x**8 - 1333],
eqmod 
input_polynomial * input_polynomial
(
mem0_240*(x**0) + mem0_242*(x**1) + mem0_244*(x**2) + mem0_246*(x**3) + 
mem0_248*(x**4) + mem0_250*(x**5) + mem0_252*(x**6) + mem0_254*(x**7)
)
[3329, x**8 - 1996],
eqmod 
input_polynomial * input_polynomial
(
mem0_256*(x**0) + mem0_258*(x**1) + mem0_260*(x**2) + mem0_262*(x**3) + 
mem0_264*(x**4) + mem0_266*(x**5) + mem0_268*(x**6) + mem0_270*(x**7)
)
[3329, x**8 - 1426],
eqmod 
input_polynomial * input_polynomial
(
mem0_272*(x**0) + mem0_274*(x**1) + mem0_276*(x**2) + mem0_278*(x**3) + 
mem0_280*(x**4) + mem0_282*(x**5) + mem0_284*(x**6) + mem0_286*(x**7)
)
[3329, x**8 - 1903],
eqmod 
input_polynomial * input_polynomial
(
mem0_288*(x**0) + mem0_290*(x**1) + mem0_292*(x**2) + mem0_294*(x**3) + 
mem0_296*(x**4) + mem0_298*(x**5) + mem0_300*(x**6) + mem0_302*(x**7)
)
[3329, x**8 - 2094],
eqmod 
input_polynomial * input_polynomial
(
mem0_304*(x**0) + mem0_306*(x**1) + mem0_308*(x**2) + mem0_310*(x**3) + 
mem0_312*(x**4) + mem0_314*(x**5) + mem0_316*(x**6) + mem0_318*(x**7)
)
[3329, x**8 - 1235],
eqmod 
input_polynomial * input_polynomial
(
mem0_320*(x**0) + mem0_322*(x**1) + mem0_324*(x**2) + mem0_326*(x**3) + 
mem0_328*(x**4) + mem0_330*(x**5) + mem0_332*(x**6) + mem0_334*(x**7)
)
[3329, x**8 - 535],
eqmod 
input_polynomial * input_polynomial
(
mem0_336*(x**0) + mem0_338*(x**1) + mem0_340*(x**2) + mem0_342*(x**3) + 
mem0_344*(x**4) + mem0_346*(x**5) + mem0_348*(x**6) + mem0_350*(x**7)
)
[3329, x**8 - 2794],
eqmod 
input_polynomial * input_polynomial
(
mem0_352*(x**0) + mem0_354*(x**1) + mem0_356*(x**2) + mem0_358*(x**3) + 
mem0_360*(x**4) + mem0_362*(x**5) + mem0_364*(x**6) + mem0_366*(x**7)
)
[3329, x**8 - 2882],
eqmod 
input_polynomial * input_polynomial
(
mem0_368*(x**0) + mem0_370*(x**1) + mem0_372*(x**2) + mem0_374*(x**3) + 
mem0_376*(x**4) + mem0_378*(x**5) + mem0_380*(x**6) + mem0_382*(x**7)
)
[3329, x**8 - 447],
eqmod 
input_polynomial * input_polynomial
(
mem0_384*(x**0) + mem0_386*(x**1) + mem0_388*(x**2) + mem0_390*(x**3) + 
mem0_392*(x**4) + mem0_394*(x**5) + mem0_396*(x**6) + mem0_398*(x**7)
)
[3329, x**8 - 2393],
eqmod 
input_polynomial * input_polynomial
(
mem0_400*(x**0) + mem0_402*(x**1) + mem0_404*(x**2) + mem0_406*(x**3) + 
mem0_408*(x**4) + mem0_410*(x**5) + mem0_412*(x**6) + mem0_414*(x**7)
)
[3329, x**8 - 936],
eqmod 
input_polynomial * input_polynomial
(
mem0_416*(x**0) + mem0_418*(x**1) + mem0_420*(x**2) + mem0_422*(x**3) + 
mem0_424*(x**4) + mem0_426*(x**5) + mem0_428*(x**6) + mem0_430*(x**7)
)
[3329, x**8 - 2879],
eqmod 
input_polynomial * input_polynomial
(
mem0_432*(x**0) + mem0_434*(x**1) + mem0_436*(x**2) + mem0_438*(x**3) + 
mem0_440*(x**4) + mem0_442*(x**5) + mem0_444*(x**6) + mem0_446*(x**7)
)
[3329, x**8 - 450],
eqmod 
input_polynomial * input_polynomial
(
mem0_448*(x**0) + mem0_450*(x**1) + mem0_452*(x**2) + mem0_454*(x**3) + 
mem0_456*(x**4) + mem0_458*(x**5) + mem0_460*(x**6) + mem0_462*(x**7)
)
[3329, x**8 - 1974],
eqmod 
input_polynomial * input_polynomial
(
mem0_464*(x**0) + mem0_466*(x**1) + mem0_468*(x**2) + mem0_470*(x**3) + 
mem0_472*(x**4) + mem0_474*(x**5) + mem0_476*(x**6) + mem0_478*(x**7)
)
[3329, x**8 - 1355],
eqmod 
input_polynomial * input_polynomial
(
mem0_480*(x**0) + mem0_482*(x**1) + mem0_484*(x**2) + mem0_486*(x**3) + 
mem0_488*(x**4) + mem0_490*(x**5) + mem0_492*(x**6) + mem0_494*(x**7)
)
[3329, x**8 - 821],
eqmod 
input_polynomial * input_polynomial
(
mem0_496*(x**0) + mem0_498*(x**1) + mem0_500*(x**2) + mem0_502*(x**3) + 
mem0_504*(x**4) + mem0_506*(x**5) + mem0_508*(x**6) + mem0_510*(x**7)
)
[3329, x**8 - 2508]
] && and [
   (-7)@16 * 3329@16 <s mem0_0, mem0_0 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_2, mem0_2 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_4, mem0_4 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_6, mem0_6 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_8, mem0_8 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_10, mem0_10 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_12, mem0_12 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_14, mem0_14 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_16, mem0_16 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_18, mem0_18 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_20, mem0_20 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_22, mem0_22 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_24, mem0_24 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_26, mem0_26 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_28, mem0_28 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_30, mem0_30 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_32, mem0_32 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_34, mem0_34 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_36, mem0_36 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_38, mem0_38 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_40, mem0_40 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_42, mem0_42 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_44, mem0_44 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_46, mem0_46 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_48, mem0_48 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_50, mem0_50 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_52, mem0_52 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_54, mem0_54 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_56, mem0_56 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_58, mem0_58 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_60, mem0_60 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_62, mem0_62 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_64, mem0_64 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_66, mem0_66 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_68, mem0_68 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_70, mem0_70 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_72, mem0_72 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_74, mem0_74 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_76, mem0_76 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_78, mem0_78 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_80, mem0_80 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_82, mem0_82 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_84, mem0_84 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_86, mem0_86 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_88, mem0_88 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_90, mem0_90 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_92, mem0_92 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_94, mem0_94 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_96, mem0_96 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_98, mem0_98 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_100, mem0_100 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_102, mem0_102 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_104, mem0_104 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_106, mem0_106 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_108, mem0_108 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_110, mem0_110 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_112, mem0_112 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_114, mem0_114 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_116, mem0_116 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_118, mem0_118 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_120, mem0_120 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_122, mem0_122 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_124, mem0_124 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_126, mem0_126 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_128, mem0_128 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_130, mem0_130 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_132, mem0_132 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_134, mem0_134 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_136, mem0_136 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_138, mem0_138 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_140, mem0_140 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_142, mem0_142 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_144, mem0_144 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_146, mem0_146 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_148, mem0_148 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_150, mem0_150 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_152, mem0_152 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_154, mem0_154 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_156, mem0_156 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_158, mem0_158 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_160, mem0_160 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_162, mem0_162 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_164, mem0_164 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_166, mem0_166 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_168, mem0_168 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_170, mem0_170 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_172, mem0_172 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_174, mem0_174 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_176, mem0_176 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_178, mem0_178 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_180, mem0_180 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_182, mem0_182 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_184, mem0_184 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_186, mem0_186 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_188, mem0_188 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_190, mem0_190 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_192, mem0_192 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_194, mem0_194 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_196, mem0_196 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_198, mem0_198 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_200, mem0_200 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_202, mem0_202 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_204, mem0_204 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_206, mem0_206 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_208, mem0_208 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_210, mem0_210 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_212, mem0_212 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_214, mem0_214 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_216, mem0_216 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_218, mem0_218 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_220, mem0_220 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_222, mem0_222 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_224, mem0_224 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_226, mem0_226 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_228, mem0_228 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_230, mem0_230 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_232, mem0_232 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_234, mem0_234 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_236, mem0_236 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_238, mem0_238 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_240, mem0_240 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_242, mem0_242 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_244, mem0_244 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_246, mem0_246 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_248, mem0_248 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_250, mem0_250 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_252, mem0_252 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_254, mem0_254 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_256, mem0_256 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_258, mem0_258 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_260, mem0_260 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_262, mem0_262 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_264, mem0_264 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_266, mem0_266 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_268, mem0_268 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_270, mem0_270 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_272, mem0_272 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_274, mem0_274 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_276, mem0_276 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_278, mem0_278 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_280, mem0_280 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_282, mem0_282 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_284, mem0_284 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_286, mem0_286 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_288, mem0_288 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_290, mem0_290 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_292, mem0_292 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_294, mem0_294 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_296, mem0_296 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_298, mem0_298 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_300, mem0_300 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_302, mem0_302 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_304, mem0_304 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_306, mem0_306 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_308, mem0_308 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_310, mem0_310 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_312, mem0_312 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_314, mem0_314 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_316, mem0_316 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_318, mem0_318 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_320, mem0_320 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_322, mem0_322 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_324, mem0_324 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_326, mem0_326 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_328, mem0_328 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_330, mem0_330 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_332, mem0_332 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_334, mem0_334 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_336, mem0_336 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_338, mem0_338 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_340, mem0_340 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_342, mem0_342 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_344, mem0_344 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_346, mem0_346 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_348, mem0_348 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_350, mem0_350 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_352, mem0_352 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_354, mem0_354 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_356, mem0_356 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_358, mem0_358 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_360, mem0_360 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_362, mem0_362 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_364, mem0_364 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_366, mem0_366 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_368, mem0_368 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_370, mem0_370 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_372, mem0_372 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_374, mem0_374 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_376, mem0_376 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_378, mem0_378 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_380, mem0_380 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_382, mem0_382 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_384, mem0_384 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_386, mem0_386 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_388, mem0_388 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_390, mem0_390 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_392, mem0_392 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_394, mem0_394 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_396, mem0_396 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_398, mem0_398 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_400, mem0_400 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_402, mem0_402 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_404, mem0_404 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_406, mem0_406 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_408, mem0_408 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_410, mem0_410 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_412, mem0_412 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_414, mem0_414 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_416, mem0_416 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_418, mem0_418 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_420, mem0_420 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_422, mem0_422 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_424, mem0_424 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_426, mem0_426 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_428, mem0_428 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_430, mem0_430 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_432, mem0_432 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_434, mem0_434 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_436, mem0_436 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_438, mem0_438 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_440, mem0_440 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_442, mem0_442 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_444, mem0_444 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_446, mem0_446 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_448, mem0_448 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_450, mem0_450 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_452, mem0_452 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_454, mem0_454 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_456, mem0_456 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_458, mem0_458 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_460, mem0_460 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_462, mem0_462 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_464, mem0_464 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_466, mem0_466 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_468, mem0_468 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_470, mem0_470 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_472, mem0_472 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_474, mem0_474 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_476, mem0_476 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_478, mem0_478 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_480, mem0_480 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_482, mem0_482 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_484, mem0_484 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_486, mem0_486 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_488, mem0_488 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_490, mem0_490 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_492, mem0_492 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_494, mem0_494 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_496, mem0_496 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_498, mem0_498 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_500, mem0_500 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_502, mem0_502 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_504, mem0_504 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_506, mem0_506 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_508, mem0_508 <s 7@16 * 3329@16,
   (-7)@16 * 3329@16 <s mem0_510, mem0_510 <s 7@16 * 3329@16
];


(* NOTE: k = 32 *)

(*   %arrayidx9.5 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %1280 = load i16, i16* %arrayidx9.5, align 2, !tbaa !3 *)
mov v1280 mem0_8;
(*   %conv1.i.5 = sext i16 %1280 to i32 *)
cast v_conv1_i_5@sint32 v1280@sint16;
(*   %mul.i.5 = mul nsw i32 %conv1.i.5, 1223 *)
mul v_mul_i_5 v_conv1_i_5 (1223)@sint32;
(*   %call.i.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5, v_call_i_5);
(*   %1281 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v1281 mem0_0;
(*   %sub.5 = sub i16 %1281, %call.i.5 *)
sub v_sub_5 v1281 v_call_i_5;
(*   store i16 %sub.5, i16* %arrayidx9.5, align 2, !tbaa !3 *)
mov mem0_8 v_sub_5;
(*   %add21.5 = add i16 %1281, %call.i.5 *)
add v_add21_5 v1281 v_call_i_5;
(*   store i16 %add21.5, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21_5;
(*   %arrayidx9.5.1 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %1282 = load i16, i16* %arrayidx9.5.1, align 2, !tbaa !3 *)
mov v1282 mem0_10;
(*   %conv1.i.5.1 = sext i16 %1282 to i32 *)
cast v_conv1_i_5_1@sint32 v1282@sint16;
(*   %mul.i.5.1 = mul nsw i32 %conv1.i.5.1, 1223 *)
mul v_mul_i_5_1 v_conv1_i_5_1 (1223)@sint32;
(*   %call.i.5.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1, v_call_i_5_1);
(*   %arrayidx11.5.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %1283 = load i16, i16* %arrayidx11.5.1, align 2, !tbaa !3 *)
mov v1283 mem0_2;
(*   %sub.5.1 = sub i16 %1283, %call.i.5.1 *)
sub v_sub_5_1 v1283 v_call_i_5_1;
(*   store i16 %sub.5.1, i16* %arrayidx9.5.1, align 2, !tbaa !3 *)
mov mem0_10 v_sub_5_1;
(*   %add21.5.1 = add i16 %1283, %call.i.5.1 *)
add v_add21_5_1 v1283 v_call_i_5_1;
(*   store i16 %add21.5.1, i16* %arrayidx11.5.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_5_1;
(*   %arrayidx9.5.2 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %1284 = load i16, i16* %arrayidx9.5.2, align 2, !tbaa !3 *)
mov v1284 mem0_12;
(*   %conv1.i.5.2 = sext i16 %1284 to i32 *)
cast v_conv1_i_5_2@sint32 v1284@sint16;
(*   %mul.i.5.2 = mul nsw i32 %conv1.i.5.2, 1223 *)
mul v_mul_i_5_2 v_conv1_i_5_2 (1223)@sint32;
(*   %call.i.5.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2, v_call_i_5_2);
(*   %arrayidx11.5.2 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %1285 = load i16, i16* %arrayidx11.5.2, align 2, !tbaa !3 *)
mov v1285 mem0_4;
(*   %sub.5.2 = sub i16 %1285, %call.i.5.2 *)
sub v_sub_5_2 v1285 v_call_i_5_2;
(*   store i16 %sub.5.2, i16* %arrayidx9.5.2, align 2, !tbaa !3 *)
mov mem0_12 v_sub_5_2;
(*   %add21.5.2 = add i16 %1285, %call.i.5.2 *)
add v_add21_5_2 v1285 v_call_i_5_2;
(*   store i16 %add21.5.2, i16* %arrayidx11.5.2, align 2, !tbaa !3 *)
mov mem0_4 v_add21_5_2;
(*   %arrayidx9.5.3 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %1286 = load i16, i16* %arrayidx9.5.3, align 2, !tbaa !3 *)
mov v1286 mem0_14;
(*   %conv1.i.5.3 = sext i16 %1286 to i32 *)
cast v_conv1_i_5_3@sint32 v1286@sint16;
(*   %mul.i.5.3 = mul nsw i32 %conv1.i.5.3, 1223 *)
mul v_mul_i_5_3 v_conv1_i_5_3 (1223)@sint32;
(*   %call.i.5.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3, v_call_i_5_3);
(*   %arrayidx11.5.3 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %1287 = load i16, i16* %arrayidx11.5.3, align 2, !tbaa !3 *)
mov v1287 mem0_6;
(*   %sub.5.3 = sub i16 %1287, %call.i.5.3 *)
sub v_sub_5_3 v1287 v_call_i_5_3;
(*   store i16 %sub.5.3, i16* %arrayidx9.5.3, align 2, !tbaa !3 *)
mov mem0_14 v_sub_5_3;
(*   %add21.5.3 = add i16 %1287, %call.i.5.3 *)
add v_add21_5_3 v1287 v_call_i_5_3;
(*   store i16 %add21.5.3, i16* %arrayidx11.5.3, align 2, !tbaa !3 *)
mov mem0_6 v_add21_5_3;

(* NOTE: k = 33 *)

(*   %arrayidx9.5.178 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %1288 = load i16, i16* %arrayidx9.5.178, align 2, !tbaa !3 *)
mov v1288 mem0_24;
(*   %conv1.i.5.179 = sext i16 %1288 to i32 *)
cast v_conv1_i_5_179@sint32 v1288@sint16;
(*   %mul.i.5.180 = mul nsw i32 %conv1.i.5.179, 652 *)
mul v_mul_i_5_180 v_conv1_i_5_179 (652)@sint32;
(*   %call.i.5.181 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.180) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_180, v_call_i_5_181);
(*   %arrayidx11.5.182 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %1289 = load i16, i16* %arrayidx11.5.182, align 2, !tbaa !3 *)
mov v1289 mem0_16;
(*   %sub.5.183 = sub i16 %1289, %call.i.5.181 *)
sub v_sub_5_183 v1289 v_call_i_5_181;
(*   store i16 %sub.5.183, i16* %arrayidx9.5.178, align 2, !tbaa !3 *)
mov mem0_24 v_sub_5_183;
(*   %add21.5.184 = add i16 %1289, %call.i.5.181 *)
add v_add21_5_184 v1289 v_call_i_5_181;
(*   store i16 %add21.5.184, i16* %arrayidx11.5.182, align 2, !tbaa !3 *)
mov mem0_16 v_add21_5_184;
(*   %arrayidx9.5.1.1 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %1290 = load i16, i16* %arrayidx9.5.1.1, align 2, !tbaa !3 *)
mov v1290 mem0_26;
(*   %conv1.i.5.1.1 = sext i16 %1290 to i32 *)
cast v_conv1_i_5_1_1@sint32 v1290@sint16;
(*   %mul.i.5.1.1 = mul nsw i32 %conv1.i.5.1.1, 652 *)
mul v_mul_i_5_1_1 v_conv1_i_5_1_1 (652)@sint32;
(*   %call.i.5.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_1, v_call_i_5_1_1);
(*   %arrayidx11.5.1.1 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %1291 = load i16, i16* %arrayidx11.5.1.1, align 2, !tbaa !3 *)
mov v1291 mem0_18;
(*   %sub.5.1.1 = sub i16 %1291, %call.i.5.1.1 *)
sub v_sub_5_1_1 v1291 v_call_i_5_1_1;
(*   store i16 %sub.5.1.1, i16* %arrayidx9.5.1.1, align 2, !tbaa !3 *)
mov mem0_26 v_sub_5_1_1;
(*   %add21.5.1.1 = add i16 %1291, %call.i.5.1.1 *)
add v_add21_5_1_1 v1291 v_call_i_5_1_1;
(*   store i16 %add21.5.1.1, i16* %arrayidx11.5.1.1, align 2, !tbaa !3 *)
mov mem0_18 v_add21_5_1_1;
(*   %arrayidx9.5.2.1 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %1292 = load i16, i16* %arrayidx9.5.2.1, align 2, !tbaa !3 *)
mov v1292 mem0_28;
(*   %conv1.i.5.2.1 = sext i16 %1292 to i32 *)
cast v_conv1_i_5_2_1@sint32 v1292@sint16;
(*   %mul.i.5.2.1 = mul nsw i32 %conv1.i.5.2.1, 652 *)
mul v_mul_i_5_2_1 v_conv1_i_5_2_1 (652)@sint32;
(*   %call.i.5.2.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_1, v_call_i_5_2_1);
(*   %arrayidx11.5.2.1 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %1293 = load i16, i16* %arrayidx11.5.2.1, align 2, !tbaa !3 *)
mov v1293 mem0_20;
(*   %sub.5.2.1 = sub i16 %1293, %call.i.5.2.1 *)
sub v_sub_5_2_1 v1293 v_call_i_5_2_1;
(*   store i16 %sub.5.2.1, i16* %arrayidx9.5.2.1, align 2, !tbaa !3 *)
mov mem0_28 v_sub_5_2_1;
(*   %add21.5.2.1 = add i16 %1293, %call.i.5.2.1 *)
add v_add21_5_2_1 v1293 v_call_i_5_2_1;
(*   store i16 %add21.5.2.1, i16* %arrayidx11.5.2.1, align 2, !tbaa !3 *)
mov mem0_20 v_add21_5_2_1;
(*   %arrayidx9.5.3.1 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %1294 = load i16, i16* %arrayidx9.5.3.1, align 2, !tbaa !3 *)
mov v1294 mem0_30;
(*   %conv1.i.5.3.1 = sext i16 %1294 to i32 *)
cast v_conv1_i_5_3_1@sint32 v1294@sint16;
(*   %mul.i.5.3.1 = mul nsw i32 %conv1.i.5.3.1, 652 *)
mul v_mul_i_5_3_1 v_conv1_i_5_3_1 (652)@sint32;
(*   %call.i.5.3.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_1, v_call_i_5_3_1);
(*   %arrayidx11.5.3.1 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %1295 = load i16, i16* %arrayidx11.5.3.1, align 2, !tbaa !3 *)
mov v1295 mem0_22;
(*   %sub.5.3.1 = sub i16 %1295, %call.i.5.3.1 *)
sub v_sub_5_3_1 v1295 v_call_i_5_3_1;
(*   store i16 %sub.5.3.1, i16* %arrayidx9.5.3.1, align 2, !tbaa !3 *)
mov mem0_30 v_sub_5_3_1;
(*   %add21.5.3.1 = add i16 %1295, %call.i.5.3.1 *)
add v_add21_5_3_1 v1295 v_call_i_5_3_1;
(*   store i16 %add21.5.3.1, i16* %arrayidx11.5.3.1, align 2, !tbaa !3 *)
mov mem0_22 v_add21_5_3_1;

(* NOTE: k = 34 *)

(*   %arrayidx9.5.288 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %1296 = load i16, i16* %arrayidx9.5.288, align 2, !tbaa !3 *)
mov v1296 mem0_40;
(*   %conv1.i.5.289 = sext i16 %1296 to i32 *)
cast v_conv1_i_5_289@sint32 v1296@sint16;
(*   %mul.i.5.290 = mul nsw i32 %conv1.i.5.289, -552 *)
mul v_mul_i_5_290 v_conv1_i_5_289 (-552)@sint32;
(*   %call.i.5.291 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.290) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_290, v_call_i_5_291);
(*   %arrayidx11.5.292 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %1297 = load i16, i16* %arrayidx11.5.292, align 2, !tbaa !3 *)
mov v1297 mem0_32;
(*   %sub.5.293 = sub i16 %1297, %call.i.5.291 *)
sub v_sub_5_293 v1297 v_call_i_5_291;
(*   store i16 %sub.5.293, i16* %arrayidx9.5.288, align 2, !tbaa !3 *)
mov mem0_40 v_sub_5_293;
(*   %add21.5.294 = add i16 %1297, %call.i.5.291 *)
add v_add21_5_294 v1297 v_call_i_5_291;
(*   store i16 %add21.5.294, i16* %arrayidx11.5.292, align 2, !tbaa !3 *)
mov mem0_32 v_add21_5_294;
(*   %arrayidx9.5.1.2 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %1298 = load i16, i16* %arrayidx9.5.1.2, align 2, !tbaa !3 *)
mov v1298 mem0_42;
(*   %conv1.i.5.1.2 = sext i16 %1298 to i32 *)
cast v_conv1_i_5_1_2@sint32 v1298@sint16;
(*   %mul.i.5.1.2 = mul nsw i32 %conv1.i.5.1.2, -552 *)
mul v_mul_i_5_1_2 v_conv1_i_5_1_2 (-552)@sint32;
(*   %call.i.5.1.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_2, v_call_i_5_1_2);
(*   %arrayidx11.5.1.2 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %1299 = load i16, i16* %arrayidx11.5.1.2, align 2, !tbaa !3 *)
mov v1299 mem0_34;
(*   %sub.5.1.2 = sub i16 %1299, %call.i.5.1.2 *)
sub v_sub_5_1_2 v1299 v_call_i_5_1_2;
(*   store i16 %sub.5.1.2, i16* %arrayidx9.5.1.2, align 2, !tbaa !3 *)
mov mem0_42 v_sub_5_1_2;
(*   %add21.5.1.2 = add i16 %1299, %call.i.5.1.2 *)
add v_add21_5_1_2 v1299 v_call_i_5_1_2;
(*   store i16 %add21.5.1.2, i16* %arrayidx11.5.1.2, align 2, !tbaa !3 *)
mov mem0_34 v_add21_5_1_2;
(*   %arrayidx9.5.2.2 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %1300 = load i16, i16* %arrayidx9.5.2.2, align 2, !tbaa !3 *)
mov v1300 mem0_44;
(*   %conv1.i.5.2.2 = sext i16 %1300 to i32 *)
cast v_conv1_i_5_2_2@sint32 v1300@sint16;
(*   %mul.i.5.2.2 = mul nsw i32 %conv1.i.5.2.2, -552 *)
mul v_mul_i_5_2_2 v_conv1_i_5_2_2 (-552)@sint32;
(*   %call.i.5.2.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_2, v_call_i_5_2_2);
(*   %arrayidx11.5.2.2 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %1301 = load i16, i16* %arrayidx11.5.2.2, align 2, !tbaa !3 *)
mov v1301 mem0_36;
(*   %sub.5.2.2 = sub i16 %1301, %call.i.5.2.2 *)
sub v_sub_5_2_2 v1301 v_call_i_5_2_2;
(*   store i16 %sub.5.2.2, i16* %arrayidx9.5.2.2, align 2, !tbaa !3 *)
mov mem0_44 v_sub_5_2_2;
(*   %add21.5.2.2 = add i16 %1301, %call.i.5.2.2 *)
add v_add21_5_2_2 v1301 v_call_i_5_2_2;
(*   store i16 %add21.5.2.2, i16* %arrayidx11.5.2.2, align 2, !tbaa !3 *)
mov mem0_36 v_add21_5_2_2;
(*   %arrayidx9.5.3.2 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %1302 = load i16, i16* %arrayidx9.5.3.2, align 2, !tbaa !3 *)
mov v1302 mem0_46;
(*   %conv1.i.5.3.2 = sext i16 %1302 to i32 *)
cast v_conv1_i_5_3_2@sint32 v1302@sint16;
(*   %mul.i.5.3.2 = mul nsw i32 %conv1.i.5.3.2, -552 *)
mul v_mul_i_5_3_2 v_conv1_i_5_3_2 (-552)@sint32;
(*   %call.i.5.3.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_2, v_call_i_5_3_2);
(*   %arrayidx11.5.3.2 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %1303 = load i16, i16* %arrayidx11.5.3.2, align 2, !tbaa !3 *)
mov v1303 mem0_38;
(*   %sub.5.3.2 = sub i16 %1303, %call.i.5.3.2 *)
sub v_sub_5_3_2 v1303 v_call_i_5_3_2;
(*   store i16 %sub.5.3.2, i16* %arrayidx9.5.3.2, align 2, !tbaa !3 *)
mov mem0_46 v_sub_5_3_2;
(*   %add21.5.3.2 = add i16 %1303, %call.i.5.3.2 *)
add v_add21_5_3_2 v1303 v_call_i_5_3_2;
(*   store i16 %add21.5.3.2, i16* %arrayidx11.5.3.2, align 2, !tbaa !3 *)
mov mem0_38 v_add21_5_3_2;

(* NOTE: k = 35 *)

(*   %arrayidx9.5.398 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %1304 = load i16, i16* %arrayidx9.5.398, align 2, !tbaa !3 *)
mov v1304 mem0_56;
(*   %conv1.i.5.399 = sext i16 %1304 to i32 *)
cast v_conv1_i_5_399@sint32 v1304@sint16;
(*   %mul.i.5.3100 = mul nsw i32 %conv1.i.5.399, 1015 *)
mul v_mul_i_5_3100 v_conv1_i_5_399 (1015)@sint32;
(*   %call.i.5.3101 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3100) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3100, v_call_i_5_3101);
(*   %arrayidx11.5.3102 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %1305 = load i16, i16* %arrayidx11.5.3102, align 2, !tbaa !3 *)
mov v1305 mem0_48;
(*   %sub.5.3103 = sub i16 %1305, %call.i.5.3101 *)
sub v_sub_5_3103 v1305 v_call_i_5_3101;
(*   store i16 %sub.5.3103, i16* %arrayidx9.5.398, align 2, !tbaa !3 *)
mov mem0_56 v_sub_5_3103;
(*   %add21.5.3104 = add i16 %1305, %call.i.5.3101 *)
add v_add21_5_3104 v1305 v_call_i_5_3101;
(*   store i16 %add21.5.3104, i16* %arrayidx11.5.3102, align 2, !tbaa !3 *)
mov mem0_48 v_add21_5_3104;
(*   %arrayidx9.5.1.3 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %1306 = load i16, i16* %arrayidx9.5.1.3, align 2, !tbaa !3 *)
mov v1306 mem0_58;
(*   %conv1.i.5.1.3 = sext i16 %1306 to i32 *)
cast v_conv1_i_5_1_3@sint32 v1306@sint16;
(*   %mul.i.5.1.3 = mul nsw i32 %conv1.i.5.1.3, 1015 *)
mul v_mul_i_5_1_3 v_conv1_i_5_1_3 (1015)@sint32;
(*   %call.i.5.1.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_3, v_call_i_5_1_3);
(*   %arrayidx11.5.1.3 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %1307 = load i16, i16* %arrayidx11.5.1.3, align 2, !tbaa !3 *)
mov v1307 mem0_50;
(*   %sub.5.1.3 = sub i16 %1307, %call.i.5.1.3 *)
sub v_sub_5_1_3 v1307 v_call_i_5_1_3;
(*   store i16 %sub.5.1.3, i16* %arrayidx9.5.1.3, align 2, !tbaa !3 *)
mov mem0_58 v_sub_5_1_3;
(*   %add21.5.1.3 = add i16 %1307, %call.i.5.1.3 *)
add v_add21_5_1_3 v1307 v_call_i_5_1_3;
(*   store i16 %add21.5.1.3, i16* %arrayidx11.5.1.3, align 2, !tbaa !3 *)
mov mem0_50 v_add21_5_1_3;
(*   %arrayidx9.5.2.3 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %1308 = load i16, i16* %arrayidx9.5.2.3, align 2, !tbaa !3 *)
mov v1308 mem0_60;
(*   %conv1.i.5.2.3 = sext i16 %1308 to i32 *)
cast v_conv1_i_5_2_3@sint32 v1308@sint16;
(*   %mul.i.5.2.3 = mul nsw i32 %conv1.i.5.2.3, 1015 *)
mul v_mul_i_5_2_3 v_conv1_i_5_2_3 (1015)@sint32;
(*   %call.i.5.2.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_3, v_call_i_5_2_3);
(*   %arrayidx11.5.2.3 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %1309 = load i16, i16* %arrayidx11.5.2.3, align 2, !tbaa !3 *)
mov v1309 mem0_52;
(*   %sub.5.2.3 = sub i16 %1309, %call.i.5.2.3 *)
sub v_sub_5_2_3 v1309 v_call_i_5_2_3;
(*   store i16 %sub.5.2.3, i16* %arrayidx9.5.2.3, align 2, !tbaa !3 *)
mov mem0_60 v_sub_5_2_3;
(*   %add21.5.2.3 = add i16 %1309, %call.i.5.2.3 *)
add v_add21_5_2_3 v1309 v_call_i_5_2_3;
(*   store i16 %add21.5.2.3, i16* %arrayidx11.5.2.3, align 2, !tbaa !3 *)
mov mem0_52 v_add21_5_2_3;
(*   %arrayidx9.5.3.3 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %1310 = load i16, i16* %arrayidx9.5.3.3, align 2, !tbaa !3 *)
mov v1310 mem0_62;
(*   %conv1.i.5.3.3 = sext i16 %1310 to i32 *)
cast v_conv1_i_5_3_3@sint32 v1310@sint16;
(*   %mul.i.5.3.3 = mul nsw i32 %conv1.i.5.3.3, 1015 *)
mul v_mul_i_5_3_3 v_conv1_i_5_3_3 (1015)@sint32;
(*   %call.i.5.3.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_3, v_call_i_5_3_3);
(*   %arrayidx11.5.3.3 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %1311 = load i16, i16* %arrayidx11.5.3.3, align 2, !tbaa !3 *)
mov v1311 mem0_54;
(*   %sub.5.3.3 = sub i16 %1311, %call.i.5.3.3 *)
sub v_sub_5_3_3 v1311 v_call_i_5_3_3;
(*   store i16 %sub.5.3.3, i16* %arrayidx9.5.3.3, align 2, !tbaa !3 *)
mov mem0_62 v_sub_5_3_3;
(*   %add21.5.3.3 = add i16 %1311, %call.i.5.3.3 *)
add v_add21_5_3_3 v1311 v_call_i_5_3_3;
(*   store i16 %add21.5.3.3, i16* %arrayidx11.5.3.3, align 2, !tbaa !3 *)
mov mem0_54 v_add21_5_3_3;

(* NOTE: k = 36 *)

(*   %arrayidx9.5.4 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %1312 = load i16, i16* %arrayidx9.5.4, align 2, !tbaa !3 *)
mov v1312 mem0_72;
(*   %conv1.i.5.4 = sext i16 %1312 to i32 *)
cast v_conv1_i_5_4@sint32 v1312@sint16;
(*   %mul.i.5.4 = mul nsw i32 %conv1.i.5.4, -1293 *)
mul v_mul_i_5_4 v_conv1_i_5_4 (-1293)@sint32;
(*   %call.i.5.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_4, v_call_i_5_4);
(*   %arrayidx11.5.4 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %1313 = load i16, i16* %arrayidx11.5.4, align 2, !tbaa !3 *)
mov v1313 mem0_64;
(*   %sub.5.4 = sub i16 %1313, %call.i.5.4 *)
sub v_sub_5_4 v1313 v_call_i_5_4;
(*   store i16 %sub.5.4, i16* %arrayidx9.5.4, align 2, !tbaa !3 *)
mov mem0_72 v_sub_5_4;
(*   %add21.5.4 = add i16 %1313, %call.i.5.4 *)
add v_add21_5_4 v1313 v_call_i_5_4;
(*   store i16 %add21.5.4, i16* %arrayidx11.5.4, align 2, !tbaa !3 *)
mov mem0_64 v_add21_5_4;
(*   %arrayidx9.5.1.4 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %1314 = load i16, i16* %arrayidx9.5.1.4, align 2, !tbaa !3 *)
mov v1314 mem0_74;
(*   %conv1.i.5.1.4 = sext i16 %1314 to i32 *)
cast v_conv1_i_5_1_4@sint32 v1314@sint16;
(*   %mul.i.5.1.4 = mul nsw i32 %conv1.i.5.1.4, -1293 *)
mul v_mul_i_5_1_4 v_conv1_i_5_1_4 (-1293)@sint32;
(*   %call.i.5.1.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_4, v_call_i_5_1_4);
(*   %arrayidx11.5.1.4 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %1315 = load i16, i16* %arrayidx11.5.1.4, align 2, !tbaa !3 *)
mov v1315 mem0_66;
(*   %sub.5.1.4 = sub i16 %1315, %call.i.5.1.4 *)
sub v_sub_5_1_4 v1315 v_call_i_5_1_4;
(*   store i16 %sub.5.1.4, i16* %arrayidx9.5.1.4, align 2, !tbaa !3 *)
mov mem0_74 v_sub_5_1_4;
(*   %add21.5.1.4 = add i16 %1315, %call.i.5.1.4 *)
add v_add21_5_1_4 v1315 v_call_i_5_1_4;
(*   store i16 %add21.5.1.4, i16* %arrayidx11.5.1.4, align 2, !tbaa !3 *)
mov mem0_66 v_add21_5_1_4;
(*   %arrayidx9.5.2.4 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %1316 = load i16, i16* %arrayidx9.5.2.4, align 2, !tbaa !3 *)
mov v1316 mem0_76;
(*   %conv1.i.5.2.4 = sext i16 %1316 to i32 *)
cast v_conv1_i_5_2_4@sint32 v1316@sint16;
(*   %mul.i.5.2.4 = mul nsw i32 %conv1.i.5.2.4, -1293 *)
mul v_mul_i_5_2_4 v_conv1_i_5_2_4 (-1293)@sint32;
(*   %call.i.5.2.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_4, v_call_i_5_2_4);
(*   %arrayidx11.5.2.4 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %1317 = load i16, i16* %arrayidx11.5.2.4, align 2, !tbaa !3 *)
mov v1317 mem0_68;
(*   %sub.5.2.4 = sub i16 %1317, %call.i.5.2.4 *)
sub v_sub_5_2_4 v1317 v_call_i_5_2_4;
(*   store i16 %sub.5.2.4, i16* %arrayidx9.5.2.4, align 2, !tbaa !3 *)
mov mem0_76 v_sub_5_2_4;
(*   %add21.5.2.4 = add i16 %1317, %call.i.5.2.4 *)
add v_add21_5_2_4 v1317 v_call_i_5_2_4;
(*   store i16 %add21.5.2.4, i16* %arrayidx11.5.2.4, align 2, !tbaa !3 *)
mov mem0_68 v_add21_5_2_4;
(*   %arrayidx9.5.3.4 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %1318 = load i16, i16* %arrayidx9.5.3.4, align 2, !tbaa !3 *)
mov v1318 mem0_78;
(*   %conv1.i.5.3.4 = sext i16 %1318 to i32 *)
cast v_conv1_i_5_3_4@sint32 v1318@sint16;
(*   %mul.i.5.3.4 = mul nsw i32 %conv1.i.5.3.4, -1293 *)
mul v_mul_i_5_3_4 v_conv1_i_5_3_4 (-1293)@sint32;
(*   %call.i.5.3.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_4, v_call_i_5_3_4);
(*   %arrayidx11.5.3.4 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %1319 = load i16, i16* %arrayidx11.5.3.4, align 2, !tbaa !3 *)
mov v1319 mem0_70;
(*   %sub.5.3.4 = sub i16 %1319, %call.i.5.3.4 *)
sub v_sub_5_3_4 v1319 v_call_i_5_3_4;
(*   store i16 %sub.5.3.4, i16* %arrayidx9.5.3.4, align 2, !tbaa !3 *)
mov mem0_78 v_sub_5_3_4;
(*   %add21.5.3.4 = add i16 %1319, %call.i.5.3.4 *)
add v_add21_5_3_4 v1319 v_call_i_5_3_4;
(*   store i16 %add21.5.3.4, i16* %arrayidx11.5.3.4, align 2, !tbaa !3 *)
mov mem0_70 v_add21_5_3_4;

(* NOTE: k = 37 *)

(*   %arrayidx9.5.5 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %1320 = load i16, i16* %arrayidx9.5.5, align 2, !tbaa !3 *)
mov v1320 mem0_88;
(*   %conv1.i.5.5 = sext i16 %1320 to i32 *)
cast v_conv1_i_5_5@sint32 v1320@sint16;
(*   %mul.i.5.5 = mul nsw i32 %conv1.i.5.5, 1491 *)
mul v_mul_i_5_5 v_conv1_i_5_5 (1491)@sint32;
(*   %call.i.5.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_5, v_call_i_5_5);
(*   %arrayidx11.5.5 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %1321 = load i16, i16* %arrayidx11.5.5, align 2, !tbaa !3 *)
mov v1321 mem0_80;
(*   %sub.5.5 = sub i16 %1321, %call.i.5.5 *)
sub v_sub_5_5 v1321 v_call_i_5_5;
(*   store i16 %sub.5.5, i16* %arrayidx9.5.5, align 2, !tbaa !3 *)
mov mem0_88 v_sub_5_5;
(*   %add21.5.5 = add i16 %1321, %call.i.5.5 *)
add v_add21_5_5 v1321 v_call_i_5_5;
(*   store i16 %add21.5.5, i16* %arrayidx11.5.5, align 2, !tbaa !3 *)
mov mem0_80 v_add21_5_5;
(*   %arrayidx9.5.1.5 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %1322 = load i16, i16* %arrayidx9.5.1.5, align 2, !tbaa !3 *)
mov v1322 mem0_90;
(*   %conv1.i.5.1.5 = sext i16 %1322 to i32 *)
cast v_conv1_i_5_1_5@sint32 v1322@sint16;
(*   %mul.i.5.1.5 = mul nsw i32 %conv1.i.5.1.5, 1491 *)
mul v_mul_i_5_1_5 v_conv1_i_5_1_5 (1491)@sint32;
(*   %call.i.5.1.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_5, v_call_i_5_1_5);
(*   %arrayidx11.5.1.5 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %1323 = load i16, i16* %arrayidx11.5.1.5, align 2, !tbaa !3 *)
mov v1323 mem0_82;
(*   %sub.5.1.5 = sub i16 %1323, %call.i.5.1.5 *)
sub v_sub_5_1_5 v1323 v_call_i_5_1_5;
(*   store i16 %sub.5.1.5, i16* %arrayidx9.5.1.5, align 2, !tbaa !3 *)
mov mem0_90 v_sub_5_1_5;
(*   %add21.5.1.5 = add i16 %1323, %call.i.5.1.5 *)
add v_add21_5_1_5 v1323 v_call_i_5_1_5;
(*   store i16 %add21.5.1.5, i16* %arrayidx11.5.1.5, align 2, !tbaa !3 *)
mov mem0_82 v_add21_5_1_5;
(*   %arrayidx9.5.2.5 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %1324 = load i16, i16* %arrayidx9.5.2.5, align 2, !tbaa !3 *)
mov v1324 mem0_92;
(*   %conv1.i.5.2.5 = sext i16 %1324 to i32 *)
cast v_conv1_i_5_2_5@sint32 v1324@sint16;
(*   %mul.i.5.2.5 = mul nsw i32 %conv1.i.5.2.5, 1491 *)
mul v_mul_i_5_2_5 v_conv1_i_5_2_5 (1491)@sint32;
(*   %call.i.5.2.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_5, v_call_i_5_2_5);
(*   %arrayidx11.5.2.5 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %1325 = load i16, i16* %arrayidx11.5.2.5, align 2, !tbaa !3 *)
mov v1325 mem0_84;
(*   %sub.5.2.5 = sub i16 %1325, %call.i.5.2.5 *)
sub v_sub_5_2_5 v1325 v_call_i_5_2_5;
(*   store i16 %sub.5.2.5, i16* %arrayidx9.5.2.5, align 2, !tbaa !3 *)
mov mem0_92 v_sub_5_2_5;
(*   %add21.5.2.5 = add i16 %1325, %call.i.5.2.5 *)
add v_add21_5_2_5 v1325 v_call_i_5_2_5;
(*   store i16 %add21.5.2.5, i16* %arrayidx11.5.2.5, align 2, !tbaa !3 *)
mov mem0_84 v_add21_5_2_5;
(*   %arrayidx9.5.3.5 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %1326 = load i16, i16* %arrayidx9.5.3.5, align 2, !tbaa !3 *)
mov v1326 mem0_94;
(*   %conv1.i.5.3.5 = sext i16 %1326 to i32 *)
cast v_conv1_i_5_3_5@sint32 v1326@sint16;
(*   %mul.i.5.3.5 = mul nsw i32 %conv1.i.5.3.5, 1491 *)
mul v_mul_i_5_3_5 v_conv1_i_5_3_5 (1491)@sint32;
(*   %call.i.5.3.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_5, v_call_i_5_3_5);
(*   %arrayidx11.5.3.5 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %1327 = load i16, i16* %arrayidx11.5.3.5, align 2, !tbaa !3 *)
mov v1327 mem0_86;
(*   %sub.5.3.5 = sub i16 %1327, %call.i.5.3.5 *)
sub v_sub_5_3_5 v1327 v_call_i_5_3_5;
(*   store i16 %sub.5.3.5, i16* %arrayidx9.5.3.5, align 2, !tbaa !3 *)
mov mem0_94 v_sub_5_3_5;
(*   %add21.5.3.5 = add i16 %1327, %call.i.5.3.5 *)
add v_add21_5_3_5 v1327 v_call_i_5_3_5;
(*   store i16 %add21.5.3.5, i16* %arrayidx11.5.3.5, align 2, !tbaa !3 *)
mov mem0_86 v_add21_5_3_5;

(* NOTE: k = 38 *)

(*   %arrayidx9.5.6 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %1328 = load i16, i16* %arrayidx9.5.6, align 2, !tbaa !3 *)
mov v1328 mem0_104;
(*   %conv1.i.5.6 = sext i16 %1328 to i32 *)
cast v_conv1_i_5_6@sint32 v1328@sint16;
(*   %mul.i.5.6 = mul nsw i32 %conv1.i.5.6, -282 *)
mul v_mul_i_5_6 v_conv1_i_5_6 (-282)@sint32;
(*   %call.i.5.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_6, v_call_i_5_6);
(*   %arrayidx11.5.6 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %1329 = load i16, i16* %arrayidx11.5.6, align 2, !tbaa !3 *)
mov v1329 mem0_96;
(*   %sub.5.6 = sub i16 %1329, %call.i.5.6 *)
sub v_sub_5_6 v1329 v_call_i_5_6;
(*   store i16 %sub.5.6, i16* %arrayidx9.5.6, align 2, !tbaa !3 *)
mov mem0_104 v_sub_5_6;
(*   %add21.5.6 = add i16 %1329, %call.i.5.6 *)
add v_add21_5_6 v1329 v_call_i_5_6;
(*   store i16 %add21.5.6, i16* %arrayidx11.5.6, align 2, !tbaa !3 *)
mov mem0_96 v_add21_5_6;
(*   %arrayidx9.5.1.6 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %1330 = load i16, i16* %arrayidx9.5.1.6, align 2, !tbaa !3 *)
mov v1330 mem0_106;
(*   %conv1.i.5.1.6 = sext i16 %1330 to i32 *)
cast v_conv1_i_5_1_6@sint32 v1330@sint16;
(*   %mul.i.5.1.6 = mul nsw i32 %conv1.i.5.1.6, -282 *)
mul v_mul_i_5_1_6 v_conv1_i_5_1_6 (-282)@sint32;
(*   %call.i.5.1.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_6, v_call_i_5_1_6);
(*   %arrayidx11.5.1.6 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %1331 = load i16, i16* %arrayidx11.5.1.6, align 2, !tbaa !3 *)
mov v1331 mem0_98;
(*   %sub.5.1.6 = sub i16 %1331, %call.i.5.1.6 *)
sub v_sub_5_1_6 v1331 v_call_i_5_1_6;
(*   store i16 %sub.5.1.6, i16* %arrayidx9.5.1.6, align 2, !tbaa !3 *)
mov mem0_106 v_sub_5_1_6;
(*   %add21.5.1.6 = add i16 %1331, %call.i.5.1.6 *)
add v_add21_5_1_6 v1331 v_call_i_5_1_6;
(*   store i16 %add21.5.1.6, i16* %arrayidx11.5.1.6, align 2, !tbaa !3 *)
mov mem0_98 v_add21_5_1_6;
(*   %arrayidx9.5.2.6 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %1332 = load i16, i16* %arrayidx9.5.2.6, align 2, !tbaa !3 *)
mov v1332 mem0_108;
(*   %conv1.i.5.2.6 = sext i16 %1332 to i32 *)
cast v_conv1_i_5_2_6@sint32 v1332@sint16;
(*   %mul.i.5.2.6 = mul nsw i32 %conv1.i.5.2.6, -282 *)
mul v_mul_i_5_2_6 v_conv1_i_5_2_6 (-282)@sint32;
(*   %call.i.5.2.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_6, v_call_i_5_2_6);
(*   %arrayidx11.5.2.6 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %1333 = load i16, i16* %arrayidx11.5.2.6, align 2, !tbaa !3 *)
mov v1333 mem0_100;
(*   %sub.5.2.6 = sub i16 %1333, %call.i.5.2.6 *)
sub v_sub_5_2_6 v1333 v_call_i_5_2_6;
(*   store i16 %sub.5.2.6, i16* %arrayidx9.5.2.6, align 2, !tbaa !3 *)
mov mem0_108 v_sub_5_2_6;
(*   %add21.5.2.6 = add i16 %1333, %call.i.5.2.6 *)
add v_add21_5_2_6 v1333 v_call_i_5_2_6;
(*   store i16 %add21.5.2.6, i16* %arrayidx11.5.2.6, align 2, !tbaa !3 *)
mov mem0_100 v_add21_5_2_6;
(*   %arrayidx9.5.3.6 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %1334 = load i16, i16* %arrayidx9.5.3.6, align 2, !tbaa !3 *)
mov v1334 mem0_110;
(*   %conv1.i.5.3.6 = sext i16 %1334 to i32 *)
cast v_conv1_i_5_3_6@sint32 v1334@sint16;
(*   %mul.i.5.3.6 = mul nsw i32 %conv1.i.5.3.6, -282 *)
mul v_mul_i_5_3_6 v_conv1_i_5_3_6 (-282)@sint32;
(*   %call.i.5.3.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_6, v_call_i_5_3_6);
(*   %arrayidx11.5.3.6 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %1335 = load i16, i16* %arrayidx11.5.3.6, align 2, !tbaa !3 *)
mov v1335 mem0_102;
(*   %sub.5.3.6 = sub i16 %1335, %call.i.5.3.6 *)
sub v_sub_5_3_6 v1335 v_call_i_5_3_6;
(*   store i16 %sub.5.3.6, i16* %arrayidx9.5.3.6, align 2, !tbaa !3 *)
mov mem0_110 v_sub_5_3_6;
(*   %add21.5.3.6 = add i16 %1335, %call.i.5.3.6 *)
add v_add21_5_3_6 v1335 v_call_i_5_3_6;
(*   store i16 %add21.5.3.6, i16* %arrayidx11.5.3.6, align 2, !tbaa !3 *)
mov mem0_102 v_add21_5_3_6;

(* NOTE: k = 39 *)

(*   %arrayidx9.5.7 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %1336 = load i16, i16* %arrayidx9.5.7, align 2, !tbaa !3 *)
mov v1336 mem0_120;
(*   %conv1.i.5.7 = sext i16 %1336 to i32 *)
cast v_conv1_i_5_7@sint32 v1336@sint16;
(*   %mul.i.5.7 = mul nsw i32 %conv1.i.5.7, -1544 *)
mul v_mul_i_5_7 v_conv1_i_5_7 (-1544)@sint32;
(*   %call.i.5.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_7, v_call_i_5_7);
(*   %arrayidx11.5.7 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %1337 = load i16, i16* %arrayidx11.5.7, align 2, !tbaa !3 *)
mov v1337 mem0_112;
(*   %sub.5.7 = sub i16 %1337, %call.i.5.7 *)
sub v_sub_5_7 v1337 v_call_i_5_7;
(*   store i16 %sub.5.7, i16* %arrayidx9.5.7, align 2, !tbaa !3 *)
mov mem0_120 v_sub_5_7;
(*   %add21.5.7 = add i16 %1337, %call.i.5.7 *)
add v_add21_5_7 v1337 v_call_i_5_7;
(*   store i16 %add21.5.7, i16* %arrayidx11.5.7, align 2, !tbaa !3 *)
mov mem0_112 v_add21_5_7;
(*   %arrayidx9.5.1.7 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %1338 = load i16, i16* %arrayidx9.5.1.7, align 2, !tbaa !3 *)
mov v1338 mem0_122;
(*   %conv1.i.5.1.7 = sext i16 %1338 to i32 *)
cast v_conv1_i_5_1_7@sint32 v1338@sint16;
(*   %mul.i.5.1.7 = mul nsw i32 %conv1.i.5.1.7, -1544 *)
mul v_mul_i_5_1_7 v_conv1_i_5_1_7 (-1544)@sint32;
(*   %call.i.5.1.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_7, v_call_i_5_1_7);
(*   %arrayidx11.5.1.7 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %1339 = load i16, i16* %arrayidx11.5.1.7, align 2, !tbaa !3 *)
mov v1339 mem0_114;
(*   %sub.5.1.7 = sub i16 %1339, %call.i.5.1.7 *)
sub v_sub_5_1_7 v1339 v_call_i_5_1_7;
(*   store i16 %sub.5.1.7, i16* %arrayidx9.5.1.7, align 2, !tbaa !3 *)
mov mem0_122 v_sub_5_1_7;
(*   %add21.5.1.7 = add i16 %1339, %call.i.5.1.7 *)
add v_add21_5_1_7 v1339 v_call_i_5_1_7;
(*   store i16 %add21.5.1.7, i16* %arrayidx11.5.1.7, align 2, !tbaa !3 *)
mov mem0_114 v_add21_5_1_7;
(*   %arrayidx9.5.2.7 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %1340 = load i16, i16* %arrayidx9.5.2.7, align 2, !tbaa !3 *)
mov v1340 mem0_124;
(*   %conv1.i.5.2.7 = sext i16 %1340 to i32 *)
cast v_conv1_i_5_2_7@sint32 v1340@sint16;
(*   %mul.i.5.2.7 = mul nsw i32 %conv1.i.5.2.7, -1544 *)
mul v_mul_i_5_2_7 v_conv1_i_5_2_7 (-1544)@sint32;
(*   %call.i.5.2.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_7, v_call_i_5_2_7);
(*   %arrayidx11.5.2.7 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %1341 = load i16, i16* %arrayidx11.5.2.7, align 2, !tbaa !3 *)
mov v1341 mem0_116;
(*   %sub.5.2.7 = sub i16 %1341, %call.i.5.2.7 *)
sub v_sub_5_2_7 v1341 v_call_i_5_2_7;
(*   store i16 %sub.5.2.7, i16* %arrayidx9.5.2.7, align 2, !tbaa !3 *)
mov mem0_124 v_sub_5_2_7;
(*   %add21.5.2.7 = add i16 %1341, %call.i.5.2.7 *)
add v_add21_5_2_7 v1341 v_call_i_5_2_7;
(*   store i16 %add21.5.2.7, i16* %arrayidx11.5.2.7, align 2, !tbaa !3 *)
mov mem0_116 v_add21_5_2_7;
(*   %arrayidx9.5.3.7 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %1342 = load i16, i16* %arrayidx9.5.3.7, align 2, !tbaa !3 *)
mov v1342 mem0_126;
(*   %conv1.i.5.3.7 = sext i16 %1342 to i32 *)
cast v_conv1_i_5_3_7@sint32 v1342@sint16;
(*   %mul.i.5.3.7 = mul nsw i32 %conv1.i.5.3.7, -1544 *)
mul v_mul_i_5_3_7 v_conv1_i_5_3_7 (-1544)@sint32;
(*   %call.i.5.3.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_7, v_call_i_5_3_7);
(*   %arrayidx11.5.3.7 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %1343 = load i16, i16* %arrayidx11.5.3.7, align 2, !tbaa !3 *)
mov v1343 mem0_118;
(*   %sub.5.3.7 = sub i16 %1343, %call.i.5.3.7 *)
sub v_sub_5_3_7 v1343 v_call_i_5_3_7;
(*   store i16 %sub.5.3.7, i16* %arrayidx9.5.3.7, align 2, !tbaa !3 *)
mov mem0_126 v_sub_5_3_7;
(*   %add21.5.3.7 = add i16 %1343, %call.i.5.3.7 *)
add v_add21_5_3_7 v1343 v_call_i_5_3_7;
(*   store i16 %add21.5.3.7, i16* %arrayidx11.5.3.7, align 2, !tbaa !3 *)
mov mem0_118 v_add21_5_3_7;

(* NOTE: k = 40 *)

(*   %arrayidx9.5.8 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %1344 = load i16, i16* %arrayidx9.5.8, align 2, !tbaa !3 *)
mov v1344 mem0_136;
(*   %conv1.i.5.8 = sext i16 %1344 to i32 *)
cast v_conv1_i_5_8@sint32 v1344@sint16;
(*   %mul.i.5.8 = mul nsw i32 %conv1.i.5.8, 516 *)
mul v_mul_i_5_8 v_conv1_i_5_8 (516)@sint32;
(*   %call.i.5.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_8, v_call_i_5_8);
(*   %arrayidx11.5.8 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %1345 = load i16, i16* %arrayidx11.5.8, align 2, !tbaa !3 *)
mov v1345 mem0_128;
(*   %sub.5.8 = sub i16 %1345, %call.i.5.8 *)
sub v_sub_5_8 v1345 v_call_i_5_8;
(*   store i16 %sub.5.8, i16* %arrayidx9.5.8, align 2, !tbaa !3 *)
mov mem0_136 v_sub_5_8;
(*   %add21.5.8 = add i16 %1345, %call.i.5.8 *)
add v_add21_5_8 v1345 v_call_i_5_8;
(*   store i16 %add21.5.8, i16* %arrayidx11.5.8, align 2, !tbaa !3 *)
mov mem0_128 v_add21_5_8;
(*   %arrayidx9.5.1.8 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %1346 = load i16, i16* %arrayidx9.5.1.8, align 2, !tbaa !3 *)
mov v1346 mem0_138;
(*   %conv1.i.5.1.8 = sext i16 %1346 to i32 *)
cast v_conv1_i_5_1_8@sint32 v1346@sint16;
(*   %mul.i.5.1.8 = mul nsw i32 %conv1.i.5.1.8, 516 *)
mul v_mul_i_5_1_8 v_conv1_i_5_1_8 (516)@sint32;
(*   %call.i.5.1.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_8, v_call_i_5_1_8);
(*   %arrayidx11.5.1.8 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %1347 = load i16, i16* %arrayidx11.5.1.8, align 2, !tbaa !3 *)
mov v1347 mem0_130;
(*   %sub.5.1.8 = sub i16 %1347, %call.i.5.1.8 *)
sub v_sub_5_1_8 v1347 v_call_i_5_1_8;
(*   store i16 %sub.5.1.8, i16* %arrayidx9.5.1.8, align 2, !tbaa !3 *)
mov mem0_138 v_sub_5_1_8;
(*   %add21.5.1.8 = add i16 %1347, %call.i.5.1.8 *)
add v_add21_5_1_8 v1347 v_call_i_5_1_8;
(*   store i16 %add21.5.1.8, i16* %arrayidx11.5.1.8, align 2, !tbaa !3 *)
mov mem0_130 v_add21_5_1_8;
(*   %arrayidx9.5.2.8 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %1348 = load i16, i16* %arrayidx9.5.2.8, align 2, !tbaa !3 *)
mov v1348 mem0_140;
(*   %conv1.i.5.2.8 = sext i16 %1348 to i32 *)
cast v_conv1_i_5_2_8@sint32 v1348@sint16;
(*   %mul.i.5.2.8 = mul nsw i32 %conv1.i.5.2.8, 516 *)
mul v_mul_i_5_2_8 v_conv1_i_5_2_8 (516)@sint32;
(*   %call.i.5.2.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_8, v_call_i_5_2_8);
(*   %arrayidx11.5.2.8 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %1349 = load i16, i16* %arrayidx11.5.2.8, align 2, !tbaa !3 *)
mov v1349 mem0_132;
(*   %sub.5.2.8 = sub i16 %1349, %call.i.5.2.8 *)
sub v_sub_5_2_8 v1349 v_call_i_5_2_8;
(*   store i16 %sub.5.2.8, i16* %arrayidx9.5.2.8, align 2, !tbaa !3 *)
mov mem0_140 v_sub_5_2_8;
(*   %add21.5.2.8 = add i16 %1349, %call.i.5.2.8 *)
add v_add21_5_2_8 v1349 v_call_i_5_2_8;
(*   store i16 %add21.5.2.8, i16* %arrayidx11.5.2.8, align 2, !tbaa !3 *)
mov mem0_132 v_add21_5_2_8;
(*   %arrayidx9.5.3.8 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %1350 = load i16, i16* %arrayidx9.5.3.8, align 2, !tbaa !3 *)
mov v1350 mem0_142;
(*   %conv1.i.5.3.8 = sext i16 %1350 to i32 *)
cast v_conv1_i_5_3_8@sint32 v1350@sint16;
(*   %mul.i.5.3.8 = mul nsw i32 %conv1.i.5.3.8, 516 *)
mul v_mul_i_5_3_8 v_conv1_i_5_3_8 (516)@sint32;
(*   %call.i.5.3.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_8, v_call_i_5_3_8);
(*   %arrayidx11.5.3.8 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %1351 = load i16, i16* %arrayidx11.5.3.8, align 2, !tbaa !3 *)
mov v1351 mem0_134;
(*   %sub.5.3.8 = sub i16 %1351, %call.i.5.3.8 *)
sub v_sub_5_3_8 v1351 v_call_i_5_3_8;
(*   store i16 %sub.5.3.8, i16* %arrayidx9.5.3.8, align 2, !tbaa !3 *)
mov mem0_142 v_sub_5_3_8;
(*   %add21.5.3.8 = add i16 %1351, %call.i.5.3.8 *)
add v_add21_5_3_8 v1351 v_call_i_5_3_8;
(*   store i16 %add21.5.3.8, i16* %arrayidx11.5.3.8, align 2, !tbaa !3 *)
mov mem0_134 v_add21_5_3_8;

(* NOTE: k = 41 *)

(*   %arrayidx9.5.9 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %1352 = load i16, i16* %arrayidx9.5.9, align 2, !tbaa !3 *)
mov v1352 mem0_152;
(*   %conv1.i.5.9 = sext i16 %1352 to i32 *)
cast v_conv1_i_5_9@sint32 v1352@sint16;
(*   %mul.i.5.9 = mul nsw i32 %conv1.i.5.9, -8 *)
mul v_mul_i_5_9 v_conv1_i_5_9 (-8)@sint32;
(*   %call.i.5.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_9, v_call_i_5_9);
(*   %arrayidx11.5.9 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %1353 = load i16, i16* %arrayidx11.5.9, align 2, !tbaa !3 *)
mov v1353 mem0_144;
(*   %sub.5.9 = sub i16 %1353, %call.i.5.9 *)
sub v_sub_5_9 v1353 v_call_i_5_9;
(*   store i16 %sub.5.9, i16* %arrayidx9.5.9, align 2, !tbaa !3 *)
mov mem0_152 v_sub_5_9;
(*   %add21.5.9 = add i16 %1353, %call.i.5.9 *)
add v_add21_5_9 v1353 v_call_i_5_9;
(*   store i16 %add21.5.9, i16* %arrayidx11.5.9, align 2, !tbaa !3 *)
mov mem0_144 v_add21_5_9;
(*   %arrayidx9.5.1.9 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %1354 = load i16, i16* %arrayidx9.5.1.9, align 2, !tbaa !3 *)
mov v1354 mem0_154;
(*   %conv1.i.5.1.9 = sext i16 %1354 to i32 *)
cast v_conv1_i_5_1_9@sint32 v1354@sint16;
(*   %mul.i.5.1.9 = mul nsw i32 %conv1.i.5.1.9, -8 *)
mul v_mul_i_5_1_9 v_conv1_i_5_1_9 (-8)@sint32;
(*   %call.i.5.1.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_9, v_call_i_5_1_9);
(*   %arrayidx11.5.1.9 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %1355 = load i16, i16* %arrayidx11.5.1.9, align 2, !tbaa !3 *)
mov v1355 mem0_146;
(*   %sub.5.1.9 = sub i16 %1355, %call.i.5.1.9 *)
sub v_sub_5_1_9 v1355 v_call_i_5_1_9;
(*   store i16 %sub.5.1.9, i16* %arrayidx9.5.1.9, align 2, !tbaa !3 *)
mov mem0_154 v_sub_5_1_9;
(*   %add21.5.1.9 = add i16 %1355, %call.i.5.1.9 *)
add v_add21_5_1_9 v1355 v_call_i_5_1_9;
(*   store i16 %add21.5.1.9, i16* %arrayidx11.5.1.9, align 2, !tbaa !3 *)
mov mem0_146 v_add21_5_1_9;
(*   %arrayidx9.5.2.9 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %1356 = load i16, i16* %arrayidx9.5.2.9, align 2, !tbaa !3 *)
mov v1356 mem0_156;
(*   %conv1.i.5.2.9 = sext i16 %1356 to i32 *)
cast v_conv1_i_5_2_9@sint32 v1356@sint16;
(*   %mul.i.5.2.9 = mul nsw i32 %conv1.i.5.2.9, -8 *)
mul v_mul_i_5_2_9 v_conv1_i_5_2_9 (-8)@sint32;
(*   %call.i.5.2.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_9, v_call_i_5_2_9);
(*   %arrayidx11.5.2.9 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %1357 = load i16, i16* %arrayidx11.5.2.9, align 2, !tbaa !3 *)
mov v1357 mem0_148;
(*   %sub.5.2.9 = sub i16 %1357, %call.i.5.2.9 *)
sub v_sub_5_2_9 v1357 v_call_i_5_2_9;
(*   store i16 %sub.5.2.9, i16* %arrayidx9.5.2.9, align 2, !tbaa !3 *)
mov mem0_156 v_sub_5_2_9;
(*   %add21.5.2.9 = add i16 %1357, %call.i.5.2.9 *)
add v_add21_5_2_9 v1357 v_call_i_5_2_9;
(*   store i16 %add21.5.2.9, i16* %arrayidx11.5.2.9, align 2, !tbaa !3 *)
mov mem0_148 v_add21_5_2_9;
(*   %arrayidx9.5.3.9 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %1358 = load i16, i16* %arrayidx9.5.3.9, align 2, !tbaa !3 *)
mov v1358 mem0_158;
(*   %conv1.i.5.3.9 = sext i16 %1358 to i32 *)
cast v_conv1_i_5_3_9@sint32 v1358@sint16;
(*   %mul.i.5.3.9 = mul nsw i32 %conv1.i.5.3.9, -8 *)
mul v_mul_i_5_3_9 v_conv1_i_5_3_9 (-8)@sint32;
(*   %call.i.5.3.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_9, v_call_i_5_3_9);
(*   %arrayidx11.5.3.9 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %1359 = load i16, i16* %arrayidx11.5.3.9, align 2, !tbaa !3 *)
mov v1359 mem0_150;
(*   %sub.5.3.9 = sub i16 %1359, %call.i.5.3.9 *)
sub v_sub_5_3_9 v1359 v_call_i_5_3_9;
(*   store i16 %sub.5.3.9, i16* %arrayidx9.5.3.9, align 2, !tbaa !3 *)
mov mem0_158 v_sub_5_3_9;
(*   %add21.5.3.9 = add i16 %1359, %call.i.5.3.9 *)
add v_add21_5_3_9 v1359 v_call_i_5_3_9;
(*   store i16 %add21.5.3.9, i16* %arrayidx11.5.3.9, align 2, !tbaa !3 *)
mov mem0_150 v_add21_5_3_9;

(* NOTE: k = 42 *)

(*   %arrayidx9.5.10 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %1360 = load i16, i16* %arrayidx9.5.10, align 2, !tbaa !3 *)
mov v1360 mem0_168;
(*   %conv1.i.5.10 = sext i16 %1360 to i32 *)
cast v_conv1_i_5_10@sint32 v1360@sint16;
(*   %mul.i.5.10 = mul nsw i32 %conv1.i.5.10, -320 *)
mul v_mul_i_5_10 v_conv1_i_5_10 (-320)@sint32;
(*   %call.i.5.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_10, v_call_i_5_10);
(*   %arrayidx11.5.10 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %1361 = load i16, i16* %arrayidx11.5.10, align 2, !tbaa !3 *)
mov v1361 mem0_160;
(*   %sub.5.10 = sub i16 %1361, %call.i.5.10 *)
sub v_sub_5_10 v1361 v_call_i_5_10;
(*   store i16 %sub.5.10, i16* %arrayidx9.5.10, align 2, !tbaa !3 *)
mov mem0_168 v_sub_5_10;
(*   %add21.5.10 = add i16 %1361, %call.i.5.10 *)
add v_add21_5_10 v1361 v_call_i_5_10;
(*   store i16 %add21.5.10, i16* %arrayidx11.5.10, align 2, !tbaa !3 *)
mov mem0_160 v_add21_5_10;
(*   %arrayidx9.5.1.10 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %1362 = load i16, i16* %arrayidx9.5.1.10, align 2, !tbaa !3 *)
mov v1362 mem0_170;
(*   %conv1.i.5.1.10 = sext i16 %1362 to i32 *)
cast v_conv1_i_5_1_10@sint32 v1362@sint16;
(*   %mul.i.5.1.10 = mul nsw i32 %conv1.i.5.1.10, -320 *)
mul v_mul_i_5_1_10 v_conv1_i_5_1_10 (-320)@sint32;
(*   %call.i.5.1.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_10, v_call_i_5_1_10);
(*   %arrayidx11.5.1.10 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %1363 = load i16, i16* %arrayidx11.5.1.10, align 2, !tbaa !3 *)
mov v1363 mem0_162;
(*   %sub.5.1.10 = sub i16 %1363, %call.i.5.1.10 *)
sub v_sub_5_1_10 v1363 v_call_i_5_1_10;
(*   store i16 %sub.5.1.10, i16* %arrayidx9.5.1.10, align 2, !tbaa !3 *)
mov mem0_170 v_sub_5_1_10;
(*   %add21.5.1.10 = add i16 %1363, %call.i.5.1.10 *)
add v_add21_5_1_10 v1363 v_call_i_5_1_10;
(*   store i16 %add21.5.1.10, i16* %arrayidx11.5.1.10, align 2, !tbaa !3 *)
mov mem0_162 v_add21_5_1_10;
(*   %arrayidx9.5.2.10 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %1364 = load i16, i16* %arrayidx9.5.2.10, align 2, !tbaa !3 *)
mov v1364 mem0_172;
(*   %conv1.i.5.2.10 = sext i16 %1364 to i32 *)
cast v_conv1_i_5_2_10@sint32 v1364@sint16;
(*   %mul.i.5.2.10 = mul nsw i32 %conv1.i.5.2.10, -320 *)
mul v_mul_i_5_2_10 v_conv1_i_5_2_10 (-320)@sint32;
(*   %call.i.5.2.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_10, v_call_i_5_2_10);
(*   %arrayidx11.5.2.10 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %1365 = load i16, i16* %arrayidx11.5.2.10, align 2, !tbaa !3 *)
mov v1365 mem0_164;
(*   %sub.5.2.10 = sub i16 %1365, %call.i.5.2.10 *)
sub v_sub_5_2_10 v1365 v_call_i_5_2_10;
(*   store i16 %sub.5.2.10, i16* %arrayidx9.5.2.10, align 2, !tbaa !3 *)
mov mem0_172 v_sub_5_2_10;
(*   %add21.5.2.10 = add i16 %1365, %call.i.5.2.10 *)
add v_add21_5_2_10 v1365 v_call_i_5_2_10;
(*   store i16 %add21.5.2.10, i16* %arrayidx11.5.2.10, align 2, !tbaa !3 *)
mov mem0_164 v_add21_5_2_10;
(*   %arrayidx9.5.3.10 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %1366 = load i16, i16* %arrayidx9.5.3.10, align 2, !tbaa !3 *)
mov v1366 mem0_174;
(*   %conv1.i.5.3.10 = sext i16 %1366 to i32 *)
cast v_conv1_i_5_3_10@sint32 v1366@sint16;
(*   %mul.i.5.3.10 = mul nsw i32 %conv1.i.5.3.10, -320 *)
mul v_mul_i_5_3_10 v_conv1_i_5_3_10 (-320)@sint32;
(*   %call.i.5.3.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_10, v_call_i_5_3_10);
(*   %arrayidx11.5.3.10 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %1367 = load i16, i16* %arrayidx11.5.3.10, align 2, !tbaa !3 *)
mov v1367 mem0_166;
(*   %sub.5.3.10 = sub i16 %1367, %call.i.5.3.10 *)
sub v_sub_5_3_10 v1367 v_call_i_5_3_10;
(*   store i16 %sub.5.3.10, i16* %arrayidx9.5.3.10, align 2, !tbaa !3 *)
mov mem0_174 v_sub_5_3_10;
(*   %add21.5.3.10 = add i16 %1367, %call.i.5.3.10 *)
add v_add21_5_3_10 v1367 v_call_i_5_3_10;
(*   store i16 %add21.5.3.10, i16* %arrayidx11.5.3.10, align 2, !tbaa !3 *)
mov mem0_166 v_add21_5_3_10;

(* NOTE: k = 43 *)

(*   %arrayidx9.5.11 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %1368 = load i16, i16* %arrayidx9.5.11, align 2, !tbaa !3 *)
mov v1368 mem0_184;
(*   %conv1.i.5.11 = sext i16 %1368 to i32 *)
cast v_conv1_i_5_11@sint32 v1368@sint16;
(*   %mul.i.5.11 = mul nsw i32 %conv1.i.5.11, -666 *)
mul v_mul_i_5_11 v_conv1_i_5_11 (-666)@sint32;
(*   %call.i.5.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_11, v_call_i_5_11);
(*   %arrayidx11.5.11 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %1369 = load i16, i16* %arrayidx11.5.11, align 2, !tbaa !3 *)
mov v1369 mem0_176;
(*   %sub.5.11 = sub i16 %1369, %call.i.5.11 *)
sub v_sub_5_11 v1369 v_call_i_5_11;
(*   store i16 %sub.5.11, i16* %arrayidx9.5.11, align 2, !tbaa !3 *)
mov mem0_184 v_sub_5_11;
(*   %add21.5.11 = add i16 %1369, %call.i.5.11 *)
add v_add21_5_11 v1369 v_call_i_5_11;
(*   store i16 %add21.5.11, i16* %arrayidx11.5.11, align 2, !tbaa !3 *)
mov mem0_176 v_add21_5_11;
(*   %arrayidx9.5.1.11 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %1370 = load i16, i16* %arrayidx9.5.1.11, align 2, !tbaa !3 *)
mov v1370 mem0_186;
(*   %conv1.i.5.1.11 = sext i16 %1370 to i32 *)
cast v_conv1_i_5_1_11@sint32 v1370@sint16;
(*   %mul.i.5.1.11 = mul nsw i32 %conv1.i.5.1.11, -666 *)
mul v_mul_i_5_1_11 v_conv1_i_5_1_11 (-666)@sint32;
(*   %call.i.5.1.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_11, v_call_i_5_1_11);
(*   %arrayidx11.5.1.11 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %1371 = load i16, i16* %arrayidx11.5.1.11, align 2, !tbaa !3 *)
mov v1371 mem0_178;
(*   %sub.5.1.11 = sub i16 %1371, %call.i.5.1.11 *)
sub v_sub_5_1_11 v1371 v_call_i_5_1_11;
(*   store i16 %sub.5.1.11, i16* %arrayidx9.5.1.11, align 2, !tbaa !3 *)
mov mem0_186 v_sub_5_1_11;
(*   %add21.5.1.11 = add i16 %1371, %call.i.5.1.11 *)
add v_add21_5_1_11 v1371 v_call_i_5_1_11;
(*   store i16 %add21.5.1.11, i16* %arrayidx11.5.1.11, align 2, !tbaa !3 *)
mov mem0_178 v_add21_5_1_11;
(*   %arrayidx9.5.2.11 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %1372 = load i16, i16* %arrayidx9.5.2.11, align 2, !tbaa !3 *)
mov v1372 mem0_188;
(*   %conv1.i.5.2.11 = sext i16 %1372 to i32 *)
cast v_conv1_i_5_2_11@sint32 v1372@sint16;
(*   %mul.i.5.2.11 = mul nsw i32 %conv1.i.5.2.11, -666 *)
mul v_mul_i_5_2_11 v_conv1_i_5_2_11 (-666)@sint32;
(*   %call.i.5.2.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_11, v_call_i_5_2_11);
(*   %arrayidx11.5.2.11 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %1373 = load i16, i16* %arrayidx11.5.2.11, align 2, !tbaa !3 *)
mov v1373 mem0_180;
(*   %sub.5.2.11 = sub i16 %1373, %call.i.5.2.11 *)
sub v_sub_5_2_11 v1373 v_call_i_5_2_11;
(*   store i16 %sub.5.2.11, i16* %arrayidx9.5.2.11, align 2, !tbaa !3 *)
mov mem0_188 v_sub_5_2_11;
(*   %add21.5.2.11 = add i16 %1373, %call.i.5.2.11 *)
add v_add21_5_2_11 v1373 v_call_i_5_2_11;
(*   store i16 %add21.5.2.11, i16* %arrayidx11.5.2.11, align 2, !tbaa !3 *)
mov mem0_180 v_add21_5_2_11;
(*   %arrayidx9.5.3.11 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %1374 = load i16, i16* %arrayidx9.5.3.11, align 2, !tbaa !3 *)
mov v1374 mem0_190;
(*   %conv1.i.5.3.11 = sext i16 %1374 to i32 *)
cast v_conv1_i_5_3_11@sint32 v1374@sint16;
(*   %mul.i.5.3.11 = mul nsw i32 %conv1.i.5.3.11, -666 *)
mul v_mul_i_5_3_11 v_conv1_i_5_3_11 (-666)@sint32;
(*   %call.i.5.3.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_11, v_call_i_5_3_11);
(*   %arrayidx11.5.3.11 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %1375 = load i16, i16* %arrayidx11.5.3.11, align 2, !tbaa !3 *)
mov v1375 mem0_182;
(*   %sub.5.3.11 = sub i16 %1375, %call.i.5.3.11 *)
sub v_sub_5_3_11 v1375 v_call_i_5_3_11;
(*   store i16 %sub.5.3.11, i16* %arrayidx9.5.3.11, align 2, !tbaa !3 *)
mov mem0_190 v_sub_5_3_11;
(*   %add21.5.3.11 = add i16 %1375, %call.i.5.3.11 *)
add v_add21_5_3_11 v1375 v_call_i_5_3_11;
(*   store i16 %add21.5.3.11, i16* %arrayidx11.5.3.11, align 2, !tbaa !3 *)
mov mem0_182 v_add21_5_3_11;

(* NOTE: k = 44 *)

(*   %arrayidx9.5.12 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %1376 = load i16, i16* %arrayidx9.5.12, align 2, !tbaa !3 *)
mov v1376 mem0_200;
(*   %conv1.i.5.12 = sext i16 %1376 to i32 *)
cast v_conv1_i_5_12@sint32 v1376@sint16;
(*   %mul.i.5.12 = mul nsw i32 %conv1.i.5.12, -1618 *)
mul v_mul_i_5_12 v_conv1_i_5_12 (-1618)@sint32;
(*   %call.i.5.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_12, v_call_i_5_12);
(*   %arrayidx11.5.12 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %1377 = load i16, i16* %arrayidx11.5.12, align 2, !tbaa !3 *)
mov v1377 mem0_192;
(*   %sub.5.12 = sub i16 %1377, %call.i.5.12 *)
sub v_sub_5_12 v1377 v_call_i_5_12;
(*   store i16 %sub.5.12, i16* %arrayidx9.5.12, align 2, !tbaa !3 *)
mov mem0_200 v_sub_5_12;
(*   %add21.5.12 = add i16 %1377, %call.i.5.12 *)
add v_add21_5_12 v1377 v_call_i_5_12;
(*   store i16 %add21.5.12, i16* %arrayidx11.5.12, align 2, !tbaa !3 *)
mov mem0_192 v_add21_5_12;
(*   %arrayidx9.5.1.12 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %1378 = load i16, i16* %arrayidx9.5.1.12, align 2, !tbaa !3 *)
mov v1378 mem0_202;
(*   %conv1.i.5.1.12 = sext i16 %1378 to i32 *)
cast v_conv1_i_5_1_12@sint32 v1378@sint16;
(*   %mul.i.5.1.12 = mul nsw i32 %conv1.i.5.1.12, -1618 *)
mul v_mul_i_5_1_12 v_conv1_i_5_1_12 (-1618)@sint32;
(*   %call.i.5.1.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_12, v_call_i_5_1_12);
(*   %arrayidx11.5.1.12 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %1379 = load i16, i16* %arrayidx11.5.1.12, align 2, !tbaa !3 *)
mov v1379 mem0_194;
(*   %sub.5.1.12 = sub i16 %1379, %call.i.5.1.12 *)
sub v_sub_5_1_12 v1379 v_call_i_5_1_12;
(*   store i16 %sub.5.1.12, i16* %arrayidx9.5.1.12, align 2, !tbaa !3 *)
mov mem0_202 v_sub_5_1_12;
(*   %add21.5.1.12 = add i16 %1379, %call.i.5.1.12 *)
add v_add21_5_1_12 v1379 v_call_i_5_1_12;
(*   store i16 %add21.5.1.12, i16* %arrayidx11.5.1.12, align 2, !tbaa !3 *)
mov mem0_194 v_add21_5_1_12;
(*   %arrayidx9.5.2.12 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %1380 = load i16, i16* %arrayidx9.5.2.12, align 2, !tbaa !3 *)
mov v1380 mem0_204;
(*   %conv1.i.5.2.12 = sext i16 %1380 to i32 *)
cast v_conv1_i_5_2_12@sint32 v1380@sint16;
(*   %mul.i.5.2.12 = mul nsw i32 %conv1.i.5.2.12, -1618 *)
mul v_mul_i_5_2_12 v_conv1_i_5_2_12 (-1618)@sint32;
(*   %call.i.5.2.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_12, v_call_i_5_2_12);
(*   %arrayidx11.5.2.12 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %1381 = load i16, i16* %arrayidx11.5.2.12, align 2, !tbaa !3 *)
mov v1381 mem0_196;
(*   %sub.5.2.12 = sub i16 %1381, %call.i.5.2.12 *)
sub v_sub_5_2_12 v1381 v_call_i_5_2_12;
(*   store i16 %sub.5.2.12, i16* %arrayidx9.5.2.12, align 2, !tbaa !3 *)
mov mem0_204 v_sub_5_2_12;
(*   %add21.5.2.12 = add i16 %1381, %call.i.5.2.12 *)
add v_add21_5_2_12 v1381 v_call_i_5_2_12;
(*   store i16 %add21.5.2.12, i16* %arrayidx11.5.2.12, align 2, !tbaa !3 *)
mov mem0_196 v_add21_5_2_12;
(*   %arrayidx9.5.3.12 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %1382 = load i16, i16* %arrayidx9.5.3.12, align 2, !tbaa !3 *)
mov v1382 mem0_206;
(*   %conv1.i.5.3.12 = sext i16 %1382 to i32 *)
cast v_conv1_i_5_3_12@sint32 v1382@sint16;
(*   %mul.i.5.3.12 = mul nsw i32 %conv1.i.5.3.12, -1618 *)
mul v_mul_i_5_3_12 v_conv1_i_5_3_12 (-1618)@sint32;
(*   %call.i.5.3.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_12, v_call_i_5_3_12);
(*   %arrayidx11.5.3.12 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %1383 = load i16, i16* %arrayidx11.5.3.12, align 2, !tbaa !3 *)
mov v1383 mem0_198;
(*   %sub.5.3.12 = sub i16 %1383, %call.i.5.3.12 *)
sub v_sub_5_3_12 v1383 v_call_i_5_3_12;
(*   store i16 %sub.5.3.12, i16* %arrayidx9.5.3.12, align 2, !tbaa !3 *)
mov mem0_206 v_sub_5_3_12;
(*   %add21.5.3.12 = add i16 %1383, %call.i.5.3.12 *)
add v_add21_5_3_12 v1383 v_call_i_5_3_12;
(*   store i16 %add21.5.3.12, i16* %arrayidx11.5.3.12, align 2, !tbaa !3 *)
mov mem0_198 v_add21_5_3_12;

(* NOTE: k = 45 *)

(*   %arrayidx9.5.13 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %1384 = load i16, i16* %arrayidx9.5.13, align 2, !tbaa !3 *)
mov v1384 mem0_216;
(*   %conv1.i.5.13 = sext i16 %1384 to i32 *)
cast v_conv1_i_5_13@sint32 v1384@sint16;
(*   %mul.i.5.13 = mul nsw i32 %conv1.i.5.13, -1162 *)
mul v_mul_i_5_13 v_conv1_i_5_13 (-1162)@sint32;
(*   %call.i.5.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_13, v_call_i_5_13);
(*   %arrayidx11.5.13 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %1385 = load i16, i16* %arrayidx11.5.13, align 2, !tbaa !3 *)
mov v1385 mem0_208;
(*   %sub.5.13 = sub i16 %1385, %call.i.5.13 *)
sub v_sub_5_13 v1385 v_call_i_5_13;
(*   store i16 %sub.5.13, i16* %arrayidx9.5.13, align 2, !tbaa !3 *)
mov mem0_216 v_sub_5_13;
(*   %add21.5.13 = add i16 %1385, %call.i.5.13 *)
add v_add21_5_13 v1385 v_call_i_5_13;
(*   store i16 %add21.5.13, i16* %arrayidx11.5.13, align 2, !tbaa !3 *)
mov mem0_208 v_add21_5_13;
(*   %arrayidx9.5.1.13 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %1386 = load i16, i16* %arrayidx9.5.1.13, align 2, !tbaa !3 *)
mov v1386 mem0_218;
(*   %conv1.i.5.1.13 = sext i16 %1386 to i32 *)
cast v_conv1_i_5_1_13@sint32 v1386@sint16;
(*   %mul.i.5.1.13 = mul nsw i32 %conv1.i.5.1.13, -1162 *)
mul v_mul_i_5_1_13 v_conv1_i_5_1_13 (-1162)@sint32;
(*   %call.i.5.1.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_13, v_call_i_5_1_13);
(*   %arrayidx11.5.1.13 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %1387 = load i16, i16* %arrayidx11.5.1.13, align 2, !tbaa !3 *)
mov v1387 mem0_210;
(*   %sub.5.1.13 = sub i16 %1387, %call.i.5.1.13 *)
sub v_sub_5_1_13 v1387 v_call_i_5_1_13;
(*   store i16 %sub.5.1.13, i16* %arrayidx9.5.1.13, align 2, !tbaa !3 *)
mov mem0_218 v_sub_5_1_13;
(*   %add21.5.1.13 = add i16 %1387, %call.i.5.1.13 *)
add v_add21_5_1_13 v1387 v_call_i_5_1_13;
(*   store i16 %add21.5.1.13, i16* %arrayidx11.5.1.13, align 2, !tbaa !3 *)
mov mem0_210 v_add21_5_1_13;
(*   %arrayidx9.5.2.13 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %1388 = load i16, i16* %arrayidx9.5.2.13, align 2, !tbaa !3 *)
mov v1388 mem0_220;
(*   %conv1.i.5.2.13 = sext i16 %1388 to i32 *)
cast v_conv1_i_5_2_13@sint32 v1388@sint16;
(*   %mul.i.5.2.13 = mul nsw i32 %conv1.i.5.2.13, -1162 *)
mul v_mul_i_5_2_13 v_conv1_i_5_2_13 (-1162)@sint32;
(*   %call.i.5.2.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_13, v_call_i_5_2_13);
(*   %arrayidx11.5.2.13 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %1389 = load i16, i16* %arrayidx11.5.2.13, align 2, !tbaa !3 *)
mov v1389 mem0_212;
(*   %sub.5.2.13 = sub i16 %1389, %call.i.5.2.13 *)
sub v_sub_5_2_13 v1389 v_call_i_5_2_13;
(*   store i16 %sub.5.2.13, i16* %arrayidx9.5.2.13, align 2, !tbaa !3 *)
mov mem0_220 v_sub_5_2_13;
(*   %add21.5.2.13 = add i16 %1389, %call.i.5.2.13 *)
add v_add21_5_2_13 v1389 v_call_i_5_2_13;
(*   store i16 %add21.5.2.13, i16* %arrayidx11.5.2.13, align 2, !tbaa !3 *)
mov mem0_212 v_add21_5_2_13;
(*   %arrayidx9.5.3.13 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %1390 = load i16, i16* %arrayidx9.5.3.13, align 2, !tbaa !3 *)
mov v1390 mem0_222;
(*   %conv1.i.5.3.13 = sext i16 %1390 to i32 *)
cast v_conv1_i_5_3_13@sint32 v1390@sint16;
(*   %mul.i.5.3.13 = mul nsw i32 %conv1.i.5.3.13, -1162 *)
mul v_mul_i_5_3_13 v_conv1_i_5_3_13 (-1162)@sint32;
(*   %call.i.5.3.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_13, v_call_i_5_3_13);
(*   %arrayidx11.5.3.13 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %1391 = load i16, i16* %arrayidx11.5.3.13, align 2, !tbaa !3 *)
mov v1391 mem0_214;
(*   %sub.5.3.13 = sub i16 %1391, %call.i.5.3.13 *)
sub v_sub_5_3_13 v1391 v_call_i_5_3_13;
(*   store i16 %sub.5.3.13, i16* %arrayidx9.5.3.13, align 2, !tbaa !3 *)
mov mem0_222 v_sub_5_3_13;
(*   %add21.5.3.13 = add i16 %1391, %call.i.5.3.13 *)
add v_add21_5_3_13 v1391 v_call_i_5_3_13;
(*   store i16 %add21.5.3.13, i16* %arrayidx11.5.3.13, align 2, !tbaa !3 *)
mov mem0_214 v_add21_5_3_13;

(* NOTE: k = 46 *)

(*   %arrayidx9.5.14 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %1392 = load i16, i16* %arrayidx9.5.14, align 2, !tbaa !3 *)
mov v1392 mem0_232;
(*   %conv1.i.5.14 = sext i16 %1392 to i32 *)
cast v_conv1_i_5_14@sint32 v1392@sint16;
(*   %mul.i.5.14 = mul nsw i32 %conv1.i.5.14, 126 *)
mul v_mul_i_5_14 v_conv1_i_5_14 (126)@sint32;
(*   %call.i.5.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_14, v_call_i_5_14);
(*   %arrayidx11.5.14 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %1393 = load i16, i16* %arrayidx11.5.14, align 2, !tbaa !3 *)
mov v1393 mem0_224;
(*   %sub.5.14 = sub i16 %1393, %call.i.5.14 *)
sub v_sub_5_14 v1393 v_call_i_5_14;
(*   store i16 %sub.5.14, i16* %arrayidx9.5.14, align 2, !tbaa !3 *)
mov mem0_232 v_sub_5_14;
(*   %add21.5.14 = add i16 %1393, %call.i.5.14 *)
add v_add21_5_14 v1393 v_call_i_5_14;
(*   store i16 %add21.5.14, i16* %arrayidx11.5.14, align 2, !tbaa !3 *)
mov mem0_224 v_add21_5_14;
(*   %arrayidx9.5.1.14 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %1394 = load i16, i16* %arrayidx9.5.1.14, align 2, !tbaa !3 *)
mov v1394 mem0_234;
(*   %conv1.i.5.1.14 = sext i16 %1394 to i32 *)
cast v_conv1_i_5_1_14@sint32 v1394@sint16;
(*   %mul.i.5.1.14 = mul nsw i32 %conv1.i.5.1.14, 126 *)
mul v_mul_i_5_1_14 v_conv1_i_5_1_14 (126)@sint32;
(*   %call.i.5.1.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_14, v_call_i_5_1_14);
(*   %arrayidx11.5.1.14 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %1395 = load i16, i16* %arrayidx11.5.1.14, align 2, !tbaa !3 *)
mov v1395 mem0_226;
(*   %sub.5.1.14 = sub i16 %1395, %call.i.5.1.14 *)
sub v_sub_5_1_14 v1395 v_call_i_5_1_14;
(*   store i16 %sub.5.1.14, i16* %arrayidx9.5.1.14, align 2, !tbaa !3 *)
mov mem0_234 v_sub_5_1_14;
(*   %add21.5.1.14 = add i16 %1395, %call.i.5.1.14 *)
add v_add21_5_1_14 v1395 v_call_i_5_1_14;
(*   store i16 %add21.5.1.14, i16* %arrayidx11.5.1.14, align 2, !tbaa !3 *)
mov mem0_226 v_add21_5_1_14;
(*   %arrayidx9.5.2.14 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %1396 = load i16, i16* %arrayidx9.5.2.14, align 2, !tbaa !3 *)
mov v1396 mem0_236;
(*   %conv1.i.5.2.14 = sext i16 %1396 to i32 *)
cast v_conv1_i_5_2_14@sint32 v1396@sint16;
(*   %mul.i.5.2.14 = mul nsw i32 %conv1.i.5.2.14, 126 *)
mul v_mul_i_5_2_14 v_conv1_i_5_2_14 (126)@sint32;
(*   %call.i.5.2.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_14, v_call_i_5_2_14);
(*   %arrayidx11.5.2.14 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %1397 = load i16, i16* %arrayidx11.5.2.14, align 2, !tbaa !3 *)
mov v1397 mem0_228;
(*   %sub.5.2.14 = sub i16 %1397, %call.i.5.2.14 *)
sub v_sub_5_2_14 v1397 v_call_i_5_2_14;
(*   store i16 %sub.5.2.14, i16* %arrayidx9.5.2.14, align 2, !tbaa !3 *)
mov mem0_236 v_sub_5_2_14;
(*   %add21.5.2.14 = add i16 %1397, %call.i.5.2.14 *)
add v_add21_5_2_14 v1397 v_call_i_5_2_14;
(*   store i16 %add21.5.2.14, i16* %arrayidx11.5.2.14, align 2, !tbaa !3 *)
mov mem0_228 v_add21_5_2_14;
(*   %arrayidx9.5.3.14 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %1398 = load i16, i16* %arrayidx9.5.3.14, align 2, !tbaa !3 *)
mov v1398 mem0_238;
(*   %conv1.i.5.3.14 = sext i16 %1398 to i32 *)
cast v_conv1_i_5_3_14@sint32 v1398@sint16;
(*   %mul.i.5.3.14 = mul nsw i32 %conv1.i.5.3.14, 126 *)
mul v_mul_i_5_3_14 v_conv1_i_5_3_14 (126)@sint32;
(*   %call.i.5.3.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_14, v_call_i_5_3_14);
(*   %arrayidx11.5.3.14 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %1399 = load i16, i16* %arrayidx11.5.3.14, align 2, !tbaa !3 *)
mov v1399 mem0_230;
(*   %sub.5.3.14 = sub i16 %1399, %call.i.5.3.14 *)
sub v_sub_5_3_14 v1399 v_call_i_5_3_14;
(*   store i16 %sub.5.3.14, i16* %arrayidx9.5.3.14, align 2, !tbaa !3 *)
mov mem0_238 v_sub_5_3_14;
(*   %add21.5.3.14 = add i16 %1399, %call.i.5.3.14 *)
add v_add21_5_3_14 v1399 v_call_i_5_3_14;
(*   store i16 %add21.5.3.14, i16* %arrayidx11.5.3.14, align 2, !tbaa !3 *)
mov mem0_230 v_add21_5_3_14;

(* NOTE: k = 47 *)

(*   %arrayidx9.5.15 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %1400 = load i16, i16* %arrayidx9.5.15, align 2, !tbaa !3 *)
mov v1400 mem0_248;
(*   %conv1.i.5.15 = sext i16 %1400 to i32 *)
cast v_conv1_i_5_15@sint32 v1400@sint16;
(*   %mul.i.5.15 = mul nsw i32 %conv1.i.5.15, 1469 *)
mul v_mul_i_5_15 v_conv1_i_5_15 (1469)@sint32;
(*   %call.i.5.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_15, v_call_i_5_15);
(*   %arrayidx11.5.15 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %1401 = load i16, i16* %arrayidx11.5.15, align 2, !tbaa !3 *)
mov v1401 mem0_240;
(*   %sub.5.15 = sub i16 %1401, %call.i.5.15 *)
sub v_sub_5_15 v1401 v_call_i_5_15;
(*   store i16 %sub.5.15, i16* %arrayidx9.5.15, align 2, !tbaa !3 *)
mov mem0_248 v_sub_5_15;
(*   %add21.5.15 = add i16 %1401, %call.i.5.15 *)
add v_add21_5_15 v1401 v_call_i_5_15;
(*   store i16 %add21.5.15, i16* %arrayidx11.5.15, align 2, !tbaa !3 *)
mov mem0_240 v_add21_5_15;
(*   %arrayidx9.5.1.15 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %1402 = load i16, i16* %arrayidx9.5.1.15, align 2, !tbaa !3 *)
mov v1402 mem0_250;
(*   %conv1.i.5.1.15 = sext i16 %1402 to i32 *)
cast v_conv1_i_5_1_15@sint32 v1402@sint16;
(*   %mul.i.5.1.15 = mul nsw i32 %conv1.i.5.1.15, 1469 *)
mul v_mul_i_5_1_15 v_conv1_i_5_1_15 (1469)@sint32;
(*   %call.i.5.1.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_15, v_call_i_5_1_15);
(*   %arrayidx11.5.1.15 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %1403 = load i16, i16* %arrayidx11.5.1.15, align 2, !tbaa !3 *)
mov v1403 mem0_242;
(*   %sub.5.1.15 = sub i16 %1403, %call.i.5.1.15 *)
sub v_sub_5_1_15 v1403 v_call_i_5_1_15;
(*   store i16 %sub.5.1.15, i16* %arrayidx9.5.1.15, align 2, !tbaa !3 *)
mov mem0_250 v_sub_5_1_15;
(*   %add21.5.1.15 = add i16 %1403, %call.i.5.1.15 *)
add v_add21_5_1_15 v1403 v_call_i_5_1_15;
(*   store i16 %add21.5.1.15, i16* %arrayidx11.5.1.15, align 2, !tbaa !3 *)
mov mem0_242 v_add21_5_1_15;
(*   %arrayidx9.5.2.15 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %1404 = load i16, i16* %arrayidx9.5.2.15, align 2, !tbaa !3 *)
mov v1404 mem0_252;
(*   %conv1.i.5.2.15 = sext i16 %1404 to i32 *)
cast v_conv1_i_5_2_15@sint32 v1404@sint16;
(*   %mul.i.5.2.15 = mul nsw i32 %conv1.i.5.2.15, 1469 *)
mul v_mul_i_5_2_15 v_conv1_i_5_2_15 (1469)@sint32;
(*   %call.i.5.2.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_15, v_call_i_5_2_15);
(*   %arrayidx11.5.2.15 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %1405 = load i16, i16* %arrayidx11.5.2.15, align 2, !tbaa !3 *)
mov v1405 mem0_244;
(*   %sub.5.2.15 = sub i16 %1405, %call.i.5.2.15 *)
sub v_sub_5_2_15 v1405 v_call_i_5_2_15;
(*   store i16 %sub.5.2.15, i16* %arrayidx9.5.2.15, align 2, !tbaa !3 *)
mov mem0_252 v_sub_5_2_15;
(*   %add21.5.2.15 = add i16 %1405, %call.i.5.2.15 *)
add v_add21_5_2_15 v1405 v_call_i_5_2_15;
(*   store i16 %add21.5.2.15, i16* %arrayidx11.5.2.15, align 2, !tbaa !3 *)
mov mem0_244 v_add21_5_2_15;
(*   %arrayidx9.5.3.15 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %1406 = load i16, i16* %arrayidx9.5.3.15, align 2, !tbaa !3 *)
mov v1406 mem0_254;
(*   %conv1.i.5.3.15 = sext i16 %1406 to i32 *)
cast v_conv1_i_5_3_15@sint32 v1406@sint16;
(*   %mul.i.5.3.15 = mul nsw i32 %conv1.i.5.3.15, 1469 *)
mul v_mul_i_5_3_15 v_conv1_i_5_3_15 (1469)@sint32;
(*   %call.i.5.3.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_15, v_call_i_5_3_15);
(*   %arrayidx11.5.3.15 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %1407 = load i16, i16* %arrayidx11.5.3.15, align 2, !tbaa !3 *)
mov v1407 mem0_246;
(*   %sub.5.3.15 = sub i16 %1407, %call.i.5.3.15 *)
sub v_sub_5_3_15 v1407 v_call_i_5_3_15;
(*   store i16 %sub.5.3.15, i16* %arrayidx9.5.3.15, align 2, !tbaa !3 *)
mov mem0_254 v_sub_5_3_15;
(*   %add21.5.3.15 = add i16 %1407, %call.i.5.3.15 *)
add v_add21_5_3_15 v1407 v_call_i_5_3_15;
(*   store i16 %add21.5.3.15, i16* %arrayidx11.5.3.15, align 2, !tbaa !3 *)
mov mem0_246 v_add21_5_3_15;

(* NOTE: k = 48 *)

(*   %arrayidx9.5.16 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %1408 = load i16, i16* %arrayidx9.5.16, align 2, !tbaa !3 *)
mov v1408 mem0_264;
(*   %conv1.i.5.16 = sext i16 %1408 to i32 *)
cast v_conv1_i_5_16@sint32 v1408@sint16;
(*   %mul.i.5.16 = mul nsw i32 %conv1.i.5.16, -853 *)
mul v_mul_i_5_16 v_conv1_i_5_16 (-853)@sint32;
(*   %call.i.5.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_16, v_call_i_5_16);
(*   %arrayidx11.5.16 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %1409 = load i16, i16* %arrayidx11.5.16, align 2, !tbaa !3 *)
mov v1409 mem0_256;
(*   %sub.5.16 = sub i16 %1409, %call.i.5.16 *)
sub v_sub_5_16 v1409 v_call_i_5_16;
(*   store i16 %sub.5.16, i16* %arrayidx9.5.16, align 2, !tbaa !3 *)
mov mem0_264 v_sub_5_16;
(*   %add21.5.16 = add i16 %1409, %call.i.5.16 *)
add v_add21_5_16 v1409 v_call_i_5_16;
(*   store i16 %add21.5.16, i16* %arrayidx11.5.16, align 2, !tbaa !3 *)
mov mem0_256 v_add21_5_16;
(*   %arrayidx9.5.1.16 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %1410 = load i16, i16* %arrayidx9.5.1.16, align 2, !tbaa !3 *)
mov v1410 mem0_266;
(*   %conv1.i.5.1.16 = sext i16 %1410 to i32 *)
cast v_conv1_i_5_1_16@sint32 v1410@sint16;
(*   %mul.i.5.1.16 = mul nsw i32 %conv1.i.5.1.16, -853 *)
mul v_mul_i_5_1_16 v_conv1_i_5_1_16 (-853)@sint32;
(*   %call.i.5.1.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_16, v_call_i_5_1_16);
(*   %arrayidx11.5.1.16 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %1411 = load i16, i16* %arrayidx11.5.1.16, align 2, !tbaa !3 *)
mov v1411 mem0_258;
(*   %sub.5.1.16 = sub i16 %1411, %call.i.5.1.16 *)
sub v_sub_5_1_16 v1411 v_call_i_5_1_16;
(*   store i16 %sub.5.1.16, i16* %arrayidx9.5.1.16, align 2, !tbaa !3 *)
mov mem0_266 v_sub_5_1_16;
(*   %add21.5.1.16 = add i16 %1411, %call.i.5.1.16 *)
add v_add21_5_1_16 v1411 v_call_i_5_1_16;
(*   store i16 %add21.5.1.16, i16* %arrayidx11.5.1.16, align 2, !tbaa !3 *)
mov mem0_258 v_add21_5_1_16;
(*   %arrayidx9.5.2.16 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %1412 = load i16, i16* %arrayidx9.5.2.16, align 2, !tbaa !3 *)
mov v1412 mem0_268;
(*   %conv1.i.5.2.16 = sext i16 %1412 to i32 *)
cast v_conv1_i_5_2_16@sint32 v1412@sint16;
(*   %mul.i.5.2.16 = mul nsw i32 %conv1.i.5.2.16, -853 *)
mul v_mul_i_5_2_16 v_conv1_i_5_2_16 (-853)@sint32;
(*   %call.i.5.2.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_16, v_call_i_5_2_16);
(*   %arrayidx11.5.2.16 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %1413 = load i16, i16* %arrayidx11.5.2.16, align 2, !tbaa !3 *)
mov v1413 mem0_260;
(*   %sub.5.2.16 = sub i16 %1413, %call.i.5.2.16 *)
sub v_sub_5_2_16 v1413 v_call_i_5_2_16;
(*   store i16 %sub.5.2.16, i16* %arrayidx9.5.2.16, align 2, !tbaa !3 *)
mov mem0_268 v_sub_5_2_16;
(*   %add21.5.2.16 = add i16 %1413, %call.i.5.2.16 *)
add v_add21_5_2_16 v1413 v_call_i_5_2_16;
(*   store i16 %add21.5.2.16, i16* %arrayidx11.5.2.16, align 2, !tbaa !3 *)
mov mem0_260 v_add21_5_2_16;
(*   %arrayidx9.5.3.16 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %1414 = load i16, i16* %arrayidx9.5.3.16, align 2, !tbaa !3 *)
mov v1414 mem0_270;
(*   %conv1.i.5.3.16 = sext i16 %1414 to i32 *)
cast v_conv1_i_5_3_16@sint32 v1414@sint16;
(*   %mul.i.5.3.16 = mul nsw i32 %conv1.i.5.3.16, -853 *)
mul v_mul_i_5_3_16 v_conv1_i_5_3_16 (-853)@sint32;
(*   %call.i.5.3.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_16, v_call_i_5_3_16);
(*   %arrayidx11.5.3.16 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %1415 = load i16, i16* %arrayidx11.5.3.16, align 2, !tbaa !3 *)
mov v1415 mem0_262;
(*   %sub.5.3.16 = sub i16 %1415, %call.i.5.3.16 *)
sub v_sub_5_3_16 v1415 v_call_i_5_3_16;
(*   store i16 %sub.5.3.16, i16* %arrayidx9.5.3.16, align 2, !tbaa !3 *)
mov mem0_270 v_sub_5_3_16;
(*   %add21.5.3.16 = add i16 %1415, %call.i.5.3.16 *)
add v_add21_5_3_16 v1415 v_call_i_5_3_16;
(*   store i16 %add21.5.3.16, i16* %arrayidx11.5.3.16, align 2, !tbaa !3 *)
mov mem0_262 v_add21_5_3_16;

(* NOTE: k = 49 *)

(*   %arrayidx9.5.17 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %1416 = load i16, i16* %arrayidx9.5.17, align 2, !tbaa !3 *)
mov v1416 mem0_280;
(*   %conv1.i.5.17 = sext i16 %1416 to i32 *)
cast v_conv1_i_5_17@sint32 v1416@sint16;
(*   %mul.i.5.17 = mul nsw i32 %conv1.i.5.17, -90 *)
mul v_mul_i_5_17 v_conv1_i_5_17 (-90)@sint32;
(*   %call.i.5.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_17, v_call_i_5_17);
(*   %arrayidx11.5.17 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %1417 = load i16, i16* %arrayidx11.5.17, align 2, !tbaa !3 *)
mov v1417 mem0_272;
(*   %sub.5.17 = sub i16 %1417, %call.i.5.17 *)
sub v_sub_5_17 v1417 v_call_i_5_17;
(*   store i16 %sub.5.17, i16* %arrayidx9.5.17, align 2, !tbaa !3 *)
mov mem0_280 v_sub_5_17;
(*   %add21.5.17 = add i16 %1417, %call.i.5.17 *)
add v_add21_5_17 v1417 v_call_i_5_17;
(*   store i16 %add21.5.17, i16* %arrayidx11.5.17, align 2, !tbaa !3 *)
mov mem0_272 v_add21_5_17;
(*   %arrayidx9.5.1.17 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %1418 = load i16, i16* %arrayidx9.5.1.17, align 2, !tbaa !3 *)
mov v1418 mem0_282;
(*   %conv1.i.5.1.17 = sext i16 %1418 to i32 *)
cast v_conv1_i_5_1_17@sint32 v1418@sint16;
(*   %mul.i.5.1.17 = mul nsw i32 %conv1.i.5.1.17, -90 *)
mul v_mul_i_5_1_17 v_conv1_i_5_1_17 (-90)@sint32;
(*   %call.i.5.1.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_17, v_call_i_5_1_17);
(*   %arrayidx11.5.1.17 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %1419 = load i16, i16* %arrayidx11.5.1.17, align 2, !tbaa !3 *)
mov v1419 mem0_274;
(*   %sub.5.1.17 = sub i16 %1419, %call.i.5.1.17 *)
sub v_sub_5_1_17 v1419 v_call_i_5_1_17;
(*   store i16 %sub.5.1.17, i16* %arrayidx9.5.1.17, align 2, !tbaa !3 *)
mov mem0_282 v_sub_5_1_17;
(*   %add21.5.1.17 = add i16 %1419, %call.i.5.1.17 *)
add v_add21_5_1_17 v1419 v_call_i_5_1_17;
(*   store i16 %add21.5.1.17, i16* %arrayidx11.5.1.17, align 2, !tbaa !3 *)
mov mem0_274 v_add21_5_1_17;
(*   %arrayidx9.5.2.17 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %1420 = load i16, i16* %arrayidx9.5.2.17, align 2, !tbaa !3 *)
mov v1420 mem0_284;
(*   %conv1.i.5.2.17 = sext i16 %1420 to i32 *)
cast v_conv1_i_5_2_17@sint32 v1420@sint16;
(*   %mul.i.5.2.17 = mul nsw i32 %conv1.i.5.2.17, -90 *)
mul v_mul_i_5_2_17 v_conv1_i_5_2_17 (-90)@sint32;
(*   %call.i.5.2.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_17, v_call_i_5_2_17);
(*   %arrayidx11.5.2.17 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %1421 = load i16, i16* %arrayidx11.5.2.17, align 2, !tbaa !3 *)
mov v1421 mem0_276;
(*   %sub.5.2.17 = sub i16 %1421, %call.i.5.2.17 *)
sub v_sub_5_2_17 v1421 v_call_i_5_2_17;
(*   store i16 %sub.5.2.17, i16* %arrayidx9.5.2.17, align 2, !tbaa !3 *)
mov mem0_284 v_sub_5_2_17;
(*   %add21.5.2.17 = add i16 %1421, %call.i.5.2.17 *)
add v_add21_5_2_17 v1421 v_call_i_5_2_17;
(*   store i16 %add21.5.2.17, i16* %arrayidx11.5.2.17, align 2, !tbaa !3 *)
mov mem0_276 v_add21_5_2_17;
(*   %arrayidx9.5.3.17 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %1422 = load i16, i16* %arrayidx9.5.3.17, align 2, !tbaa !3 *)
mov v1422 mem0_286;
(*   %conv1.i.5.3.17 = sext i16 %1422 to i32 *)
cast v_conv1_i_5_3_17@sint32 v1422@sint16;
(*   %mul.i.5.3.17 = mul nsw i32 %conv1.i.5.3.17, -90 *)
mul v_mul_i_5_3_17 v_conv1_i_5_3_17 (-90)@sint32;
(*   %call.i.5.3.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_17, v_call_i_5_3_17);
(*   %arrayidx11.5.3.17 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %1423 = load i16, i16* %arrayidx11.5.3.17, align 2, !tbaa !3 *)
mov v1423 mem0_278;
(*   %sub.5.3.17 = sub i16 %1423, %call.i.5.3.17 *)
sub v_sub_5_3_17 v1423 v_call_i_5_3_17;
(*   store i16 %sub.5.3.17, i16* %arrayidx9.5.3.17, align 2, !tbaa !3 *)
mov mem0_286 v_sub_5_3_17;
(*   %add21.5.3.17 = add i16 %1423, %call.i.5.3.17 *)
add v_add21_5_3_17 v1423 v_call_i_5_3_17;
(*   store i16 %add21.5.3.17, i16* %arrayidx11.5.3.17, align 2, !tbaa !3 *)
mov mem0_278 v_add21_5_3_17;

(* NOTE: k = 50 *)

(*   %arrayidx9.5.18 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %1424 = load i16, i16* %arrayidx9.5.18, align 2, !tbaa !3 *)
mov v1424 mem0_296;
(*   %conv1.i.5.18 = sext i16 %1424 to i32 *)
cast v_conv1_i_5_18@sint32 v1424@sint16;
(*   %mul.i.5.18 = mul nsw i32 %conv1.i.5.18, -271 *)
mul v_mul_i_5_18 v_conv1_i_5_18 (-271)@sint32;
(*   %call.i.5.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_18, v_call_i_5_18);
(*   %arrayidx11.5.18 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %1425 = load i16, i16* %arrayidx11.5.18, align 2, !tbaa !3 *)
mov v1425 mem0_288;
(*   %sub.5.18 = sub i16 %1425, %call.i.5.18 *)
sub v_sub_5_18 v1425 v_call_i_5_18;
(*   store i16 %sub.5.18, i16* %arrayidx9.5.18, align 2, !tbaa !3 *)
mov mem0_296 v_sub_5_18;
(*   %add21.5.18 = add i16 %1425, %call.i.5.18 *)
add v_add21_5_18 v1425 v_call_i_5_18;
(*   store i16 %add21.5.18, i16* %arrayidx11.5.18, align 2, !tbaa !3 *)
mov mem0_288 v_add21_5_18;
(*   %arrayidx9.5.1.18 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %1426 = load i16, i16* %arrayidx9.5.1.18, align 2, !tbaa !3 *)
mov v1426 mem0_298;
(*   %conv1.i.5.1.18 = sext i16 %1426 to i32 *)
cast v_conv1_i_5_1_18@sint32 v1426@sint16;
(*   %mul.i.5.1.18 = mul nsw i32 %conv1.i.5.1.18, -271 *)
mul v_mul_i_5_1_18 v_conv1_i_5_1_18 (-271)@sint32;
(*   %call.i.5.1.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_18, v_call_i_5_1_18);
(*   %arrayidx11.5.1.18 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %1427 = load i16, i16* %arrayidx11.5.1.18, align 2, !tbaa !3 *)
mov v1427 mem0_290;
(*   %sub.5.1.18 = sub i16 %1427, %call.i.5.1.18 *)
sub v_sub_5_1_18 v1427 v_call_i_5_1_18;
(*   store i16 %sub.5.1.18, i16* %arrayidx9.5.1.18, align 2, !tbaa !3 *)
mov mem0_298 v_sub_5_1_18;
(*   %add21.5.1.18 = add i16 %1427, %call.i.5.1.18 *)
add v_add21_5_1_18 v1427 v_call_i_5_1_18;
(*   store i16 %add21.5.1.18, i16* %arrayidx11.5.1.18, align 2, !tbaa !3 *)
mov mem0_290 v_add21_5_1_18;
(*   %arrayidx9.5.2.18 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %1428 = load i16, i16* %arrayidx9.5.2.18, align 2, !tbaa !3 *)
mov v1428 mem0_300;
(*   %conv1.i.5.2.18 = sext i16 %1428 to i32 *)
cast v_conv1_i_5_2_18@sint32 v1428@sint16;
(*   %mul.i.5.2.18 = mul nsw i32 %conv1.i.5.2.18, -271 *)
mul v_mul_i_5_2_18 v_conv1_i_5_2_18 (-271)@sint32;
(*   %call.i.5.2.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_18, v_call_i_5_2_18);
(*   %arrayidx11.5.2.18 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %1429 = load i16, i16* %arrayidx11.5.2.18, align 2, !tbaa !3 *)
mov v1429 mem0_292;
(*   %sub.5.2.18 = sub i16 %1429, %call.i.5.2.18 *)
sub v_sub_5_2_18 v1429 v_call_i_5_2_18;
(*   store i16 %sub.5.2.18, i16* %arrayidx9.5.2.18, align 2, !tbaa !3 *)
mov mem0_300 v_sub_5_2_18;
(*   %add21.5.2.18 = add i16 %1429, %call.i.5.2.18 *)
add v_add21_5_2_18 v1429 v_call_i_5_2_18;
(*   store i16 %add21.5.2.18, i16* %arrayidx11.5.2.18, align 2, !tbaa !3 *)
mov mem0_292 v_add21_5_2_18;
(*   %arrayidx9.5.3.18 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %1430 = load i16, i16* %arrayidx9.5.3.18, align 2, !tbaa !3 *)
mov v1430 mem0_302;
(*   %conv1.i.5.3.18 = sext i16 %1430 to i32 *)
cast v_conv1_i_5_3_18@sint32 v1430@sint16;
(*   %mul.i.5.3.18 = mul nsw i32 %conv1.i.5.3.18, -271 *)
mul v_mul_i_5_3_18 v_conv1_i_5_3_18 (-271)@sint32;
(*   %call.i.5.3.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_18, v_call_i_5_3_18);
(*   %arrayidx11.5.3.18 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %1431 = load i16, i16* %arrayidx11.5.3.18, align 2, !tbaa !3 *)
mov v1431 mem0_294;
(*   %sub.5.3.18 = sub i16 %1431, %call.i.5.3.18 *)
sub v_sub_5_3_18 v1431 v_call_i_5_3_18;
(*   store i16 %sub.5.3.18, i16* %arrayidx9.5.3.18, align 2, !tbaa !3 *)
mov mem0_302 v_sub_5_3_18;
(*   %add21.5.3.18 = add i16 %1431, %call.i.5.3.18 *)
add v_add21_5_3_18 v1431 v_call_i_5_3_18;
(*   store i16 %add21.5.3.18, i16* %arrayidx11.5.3.18, align 2, !tbaa !3 *)
mov mem0_294 v_add21_5_3_18;

(* NOTE: k = 51 *)

(*   %arrayidx9.5.19 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %1432 = load i16, i16* %arrayidx9.5.19, align 2, !tbaa !3 *)
mov v1432 mem0_312;
(*   %conv1.i.5.19 = sext i16 %1432 to i32 *)
cast v_conv1_i_5_19@sint32 v1432@sint16;
(*   %mul.i.5.19 = mul nsw i32 %conv1.i.5.19, 830 *)
mul v_mul_i_5_19 v_conv1_i_5_19 (830)@sint32;
(*   %call.i.5.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_19, v_call_i_5_19);
(*   %arrayidx11.5.19 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %1433 = load i16, i16* %arrayidx11.5.19, align 2, !tbaa !3 *)
mov v1433 mem0_304;
(*   %sub.5.19 = sub i16 %1433, %call.i.5.19 *)
sub v_sub_5_19 v1433 v_call_i_5_19;
(*   store i16 %sub.5.19, i16* %arrayidx9.5.19, align 2, !tbaa !3 *)
mov mem0_312 v_sub_5_19;
(*   %add21.5.19 = add i16 %1433, %call.i.5.19 *)
add v_add21_5_19 v1433 v_call_i_5_19;
(*   store i16 %add21.5.19, i16* %arrayidx11.5.19, align 2, !tbaa !3 *)
mov mem0_304 v_add21_5_19;
(*   %arrayidx9.5.1.19 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %1434 = load i16, i16* %arrayidx9.5.1.19, align 2, !tbaa !3 *)
mov v1434 mem0_314;
(*   %conv1.i.5.1.19 = sext i16 %1434 to i32 *)
cast v_conv1_i_5_1_19@sint32 v1434@sint16;
(*   %mul.i.5.1.19 = mul nsw i32 %conv1.i.5.1.19, 830 *)
mul v_mul_i_5_1_19 v_conv1_i_5_1_19 (830)@sint32;
(*   %call.i.5.1.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_19, v_call_i_5_1_19);
(*   %arrayidx11.5.1.19 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %1435 = load i16, i16* %arrayidx11.5.1.19, align 2, !tbaa !3 *)
mov v1435 mem0_306;
(*   %sub.5.1.19 = sub i16 %1435, %call.i.5.1.19 *)
sub v_sub_5_1_19 v1435 v_call_i_5_1_19;
(*   store i16 %sub.5.1.19, i16* %arrayidx9.5.1.19, align 2, !tbaa !3 *)
mov mem0_314 v_sub_5_1_19;
(*   %add21.5.1.19 = add i16 %1435, %call.i.5.1.19 *)
add v_add21_5_1_19 v1435 v_call_i_5_1_19;
(*   store i16 %add21.5.1.19, i16* %arrayidx11.5.1.19, align 2, !tbaa !3 *)
mov mem0_306 v_add21_5_1_19;
(*   %arrayidx9.5.2.19 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %1436 = load i16, i16* %arrayidx9.5.2.19, align 2, !tbaa !3 *)
mov v1436 mem0_316;
(*   %conv1.i.5.2.19 = sext i16 %1436 to i32 *)
cast v_conv1_i_5_2_19@sint32 v1436@sint16;
(*   %mul.i.5.2.19 = mul nsw i32 %conv1.i.5.2.19, 830 *)
mul v_mul_i_5_2_19 v_conv1_i_5_2_19 (830)@sint32;
(*   %call.i.5.2.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_19, v_call_i_5_2_19);
(*   %arrayidx11.5.2.19 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %1437 = load i16, i16* %arrayidx11.5.2.19, align 2, !tbaa !3 *)
mov v1437 mem0_308;
(*   %sub.5.2.19 = sub i16 %1437, %call.i.5.2.19 *)
sub v_sub_5_2_19 v1437 v_call_i_5_2_19;
(*   store i16 %sub.5.2.19, i16* %arrayidx9.5.2.19, align 2, !tbaa !3 *)
mov mem0_316 v_sub_5_2_19;
(*   %add21.5.2.19 = add i16 %1437, %call.i.5.2.19 *)
add v_add21_5_2_19 v1437 v_call_i_5_2_19;
(*   store i16 %add21.5.2.19, i16* %arrayidx11.5.2.19, align 2, !tbaa !3 *)
mov mem0_308 v_add21_5_2_19;
(*   %arrayidx9.5.3.19 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %1438 = load i16, i16* %arrayidx9.5.3.19, align 2, !tbaa !3 *)
mov v1438 mem0_318;
(*   %conv1.i.5.3.19 = sext i16 %1438 to i32 *)
cast v_conv1_i_5_3_19@sint32 v1438@sint16;
(*   %mul.i.5.3.19 = mul nsw i32 %conv1.i.5.3.19, 830 *)
mul v_mul_i_5_3_19 v_conv1_i_5_3_19 (830)@sint32;
(*   %call.i.5.3.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_19, v_call_i_5_3_19);
(*   %arrayidx11.5.3.19 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %1439 = load i16, i16* %arrayidx11.5.3.19, align 2, !tbaa !3 *)
mov v1439 mem0_310;
(*   %sub.5.3.19 = sub i16 %1439, %call.i.5.3.19 *)
sub v_sub_5_3_19 v1439 v_call_i_5_3_19;
(*   store i16 %sub.5.3.19, i16* %arrayidx9.5.3.19, align 2, !tbaa !3 *)
mov mem0_318 v_sub_5_3_19;
(*   %add21.5.3.19 = add i16 %1439, %call.i.5.3.19 *)
add v_add21_5_3_19 v1439 v_call_i_5_3_19;
(*   store i16 %add21.5.3.19, i16* %arrayidx11.5.3.19, align 2, !tbaa !3 *)
mov mem0_310 v_add21_5_3_19;

(* NOTE: k = 52 *)

(*   %arrayidx9.5.20 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %1440 = load i16, i16* %arrayidx9.5.20, align 2, !tbaa !3 *)
mov v1440 mem0_328;
(*   %conv1.i.5.20 = sext i16 %1440 to i32 *)
cast v_conv1_i_5_20@sint32 v1440@sint16;
(*   %mul.i.5.20 = mul nsw i32 %conv1.i.5.20, 107 *)
mul v_mul_i_5_20 v_conv1_i_5_20 (107)@sint32;
(*   %call.i.5.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_20, v_call_i_5_20);
(*   %arrayidx11.5.20 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %1441 = load i16, i16* %arrayidx11.5.20, align 2, !tbaa !3 *)
mov v1441 mem0_320;
(*   %sub.5.20 = sub i16 %1441, %call.i.5.20 *)
sub v_sub_5_20 v1441 v_call_i_5_20;
(*   store i16 %sub.5.20, i16* %arrayidx9.5.20, align 2, !tbaa !3 *)
mov mem0_328 v_sub_5_20;
(*   %add21.5.20 = add i16 %1441, %call.i.5.20 *)
add v_add21_5_20 v1441 v_call_i_5_20;
(*   store i16 %add21.5.20, i16* %arrayidx11.5.20, align 2, !tbaa !3 *)
mov mem0_320 v_add21_5_20;
(*   %arrayidx9.5.1.20 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %1442 = load i16, i16* %arrayidx9.5.1.20, align 2, !tbaa !3 *)
mov v1442 mem0_330;
(*   %conv1.i.5.1.20 = sext i16 %1442 to i32 *)
cast v_conv1_i_5_1_20@sint32 v1442@sint16;
(*   %mul.i.5.1.20 = mul nsw i32 %conv1.i.5.1.20, 107 *)
mul v_mul_i_5_1_20 v_conv1_i_5_1_20 (107)@sint32;
(*   %call.i.5.1.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_20, v_call_i_5_1_20);
(*   %arrayidx11.5.1.20 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %1443 = load i16, i16* %arrayidx11.5.1.20, align 2, !tbaa !3 *)
mov v1443 mem0_322;
(*   %sub.5.1.20 = sub i16 %1443, %call.i.5.1.20 *)
sub v_sub_5_1_20 v1443 v_call_i_5_1_20;
(*   store i16 %sub.5.1.20, i16* %arrayidx9.5.1.20, align 2, !tbaa !3 *)
mov mem0_330 v_sub_5_1_20;
(*   %add21.5.1.20 = add i16 %1443, %call.i.5.1.20 *)
add v_add21_5_1_20 v1443 v_call_i_5_1_20;
(*   store i16 %add21.5.1.20, i16* %arrayidx11.5.1.20, align 2, !tbaa !3 *)
mov mem0_322 v_add21_5_1_20;
(*   %arrayidx9.5.2.20 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %1444 = load i16, i16* %arrayidx9.5.2.20, align 2, !tbaa !3 *)
mov v1444 mem0_332;
(*   %conv1.i.5.2.20 = sext i16 %1444 to i32 *)
cast v_conv1_i_5_2_20@sint32 v1444@sint16;
(*   %mul.i.5.2.20 = mul nsw i32 %conv1.i.5.2.20, 107 *)
mul v_mul_i_5_2_20 v_conv1_i_5_2_20 (107)@sint32;
(*   %call.i.5.2.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_20, v_call_i_5_2_20);
(*   %arrayidx11.5.2.20 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %1445 = load i16, i16* %arrayidx11.5.2.20, align 2, !tbaa !3 *)
mov v1445 mem0_324;
(*   %sub.5.2.20 = sub i16 %1445, %call.i.5.2.20 *)
sub v_sub_5_2_20 v1445 v_call_i_5_2_20;
(*   store i16 %sub.5.2.20, i16* %arrayidx9.5.2.20, align 2, !tbaa !3 *)
mov mem0_332 v_sub_5_2_20;
(*   %add21.5.2.20 = add i16 %1445, %call.i.5.2.20 *)
add v_add21_5_2_20 v1445 v_call_i_5_2_20;
(*   store i16 %add21.5.2.20, i16* %arrayidx11.5.2.20, align 2, !tbaa !3 *)
mov mem0_324 v_add21_5_2_20;
(*   %arrayidx9.5.3.20 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %1446 = load i16, i16* %arrayidx9.5.3.20, align 2, !tbaa !3 *)
mov v1446 mem0_334;
(*   %conv1.i.5.3.20 = sext i16 %1446 to i32 *)
cast v_conv1_i_5_3_20@sint32 v1446@sint16;
(*   %mul.i.5.3.20 = mul nsw i32 %conv1.i.5.3.20, 107 *)
mul v_mul_i_5_3_20 v_conv1_i_5_3_20 (107)@sint32;
(*   %call.i.5.3.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_20, v_call_i_5_3_20);
(*   %arrayidx11.5.3.20 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %1447 = load i16, i16* %arrayidx11.5.3.20, align 2, !tbaa !3 *)
mov v1447 mem0_326;
(*   %sub.5.3.20 = sub i16 %1447, %call.i.5.3.20 *)
sub v_sub_5_3_20 v1447 v_call_i_5_3_20;
(*   store i16 %sub.5.3.20, i16* %arrayidx9.5.3.20, align 2, !tbaa !3 *)
mov mem0_334 v_sub_5_3_20;
(*   %add21.5.3.20 = add i16 %1447, %call.i.5.3.20 *)
add v_add21_5_3_20 v1447 v_call_i_5_3_20;
(*   store i16 %add21.5.3.20, i16* %arrayidx11.5.3.20, align 2, !tbaa !3 *)
mov mem0_326 v_add21_5_3_20;

(* NOTE: k = 53 *)

(*   %arrayidx9.5.21 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %1448 = load i16, i16* %arrayidx9.5.21, align 2, !tbaa !3 *)
mov v1448 mem0_344;
(*   %conv1.i.5.21 = sext i16 %1448 to i32 *)
cast v_conv1_i_5_21@sint32 v1448@sint16;
(*   %mul.i.5.21 = mul nsw i32 %conv1.i.5.21, -1421 *)
mul v_mul_i_5_21 v_conv1_i_5_21 (-1421)@sint32;
(*   %call.i.5.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_21, v_call_i_5_21);
(*   %arrayidx11.5.21 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %1449 = load i16, i16* %arrayidx11.5.21, align 2, !tbaa !3 *)
mov v1449 mem0_336;
(*   %sub.5.21 = sub i16 %1449, %call.i.5.21 *)
sub v_sub_5_21 v1449 v_call_i_5_21;
(*   store i16 %sub.5.21, i16* %arrayidx9.5.21, align 2, !tbaa !3 *)
mov mem0_344 v_sub_5_21;
(*   %add21.5.21 = add i16 %1449, %call.i.5.21 *)
add v_add21_5_21 v1449 v_call_i_5_21;
(*   store i16 %add21.5.21, i16* %arrayidx11.5.21, align 2, !tbaa !3 *)
mov mem0_336 v_add21_5_21;
(*   %arrayidx9.5.1.21 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %1450 = load i16, i16* %arrayidx9.5.1.21, align 2, !tbaa !3 *)
mov v1450 mem0_346;
(*   %conv1.i.5.1.21 = sext i16 %1450 to i32 *)
cast v_conv1_i_5_1_21@sint32 v1450@sint16;
(*   %mul.i.5.1.21 = mul nsw i32 %conv1.i.5.1.21, -1421 *)
mul v_mul_i_5_1_21 v_conv1_i_5_1_21 (-1421)@sint32;
(*   %call.i.5.1.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_21, v_call_i_5_1_21);
(*   %arrayidx11.5.1.21 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %1451 = load i16, i16* %arrayidx11.5.1.21, align 2, !tbaa !3 *)
mov v1451 mem0_338;
(*   %sub.5.1.21 = sub i16 %1451, %call.i.5.1.21 *)
sub v_sub_5_1_21 v1451 v_call_i_5_1_21;
(*   store i16 %sub.5.1.21, i16* %arrayidx9.5.1.21, align 2, !tbaa !3 *)
mov mem0_346 v_sub_5_1_21;
(*   %add21.5.1.21 = add i16 %1451, %call.i.5.1.21 *)
add v_add21_5_1_21 v1451 v_call_i_5_1_21;
(*   store i16 %add21.5.1.21, i16* %arrayidx11.5.1.21, align 2, !tbaa !3 *)
mov mem0_338 v_add21_5_1_21;
(*   %arrayidx9.5.2.21 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %1452 = load i16, i16* %arrayidx9.5.2.21, align 2, !tbaa !3 *)
mov v1452 mem0_348;
(*   %conv1.i.5.2.21 = sext i16 %1452 to i32 *)
cast v_conv1_i_5_2_21@sint32 v1452@sint16;
(*   %mul.i.5.2.21 = mul nsw i32 %conv1.i.5.2.21, -1421 *)
mul v_mul_i_5_2_21 v_conv1_i_5_2_21 (-1421)@sint32;
(*   %call.i.5.2.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_21, v_call_i_5_2_21);
(*   %arrayidx11.5.2.21 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %1453 = load i16, i16* %arrayidx11.5.2.21, align 2, !tbaa !3 *)
mov v1453 mem0_340;
(*   %sub.5.2.21 = sub i16 %1453, %call.i.5.2.21 *)
sub v_sub_5_2_21 v1453 v_call_i_5_2_21;
(*   store i16 %sub.5.2.21, i16* %arrayidx9.5.2.21, align 2, !tbaa !3 *)
mov mem0_348 v_sub_5_2_21;
(*   %add21.5.2.21 = add i16 %1453, %call.i.5.2.21 *)
add v_add21_5_2_21 v1453 v_call_i_5_2_21;
(*   store i16 %add21.5.2.21, i16* %arrayidx11.5.2.21, align 2, !tbaa !3 *)
mov mem0_340 v_add21_5_2_21;
(*   %arrayidx9.5.3.21 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %1454 = load i16, i16* %arrayidx9.5.3.21, align 2, !tbaa !3 *)
mov v1454 mem0_350;
(*   %conv1.i.5.3.21 = sext i16 %1454 to i32 *)
cast v_conv1_i_5_3_21@sint32 v1454@sint16;
(*   %mul.i.5.3.21 = mul nsw i32 %conv1.i.5.3.21, -1421 *)
mul v_mul_i_5_3_21 v_conv1_i_5_3_21 (-1421)@sint32;
(*   %call.i.5.3.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_21, v_call_i_5_3_21);
(*   %arrayidx11.5.3.21 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %1455 = load i16, i16* %arrayidx11.5.3.21, align 2, !tbaa !3 *)
mov v1455 mem0_342;
(*   %sub.5.3.21 = sub i16 %1455, %call.i.5.3.21 *)
sub v_sub_5_3_21 v1455 v_call_i_5_3_21;
(*   store i16 %sub.5.3.21, i16* %arrayidx9.5.3.21, align 2, !tbaa !3 *)
mov mem0_350 v_sub_5_3_21;
(*   %add21.5.3.21 = add i16 %1455, %call.i.5.3.21 *)
add v_add21_5_3_21 v1455 v_call_i_5_3_21;
(*   store i16 %add21.5.3.21, i16* %arrayidx11.5.3.21, align 2, !tbaa !3 *)
mov mem0_342 v_add21_5_3_21;

(* NOTE: k = 54 *)

(*   %arrayidx9.5.22 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %1456 = load i16, i16* %arrayidx9.5.22, align 2, !tbaa !3 *)
mov v1456 mem0_360;
(*   %conv1.i.5.22 = sext i16 %1456 to i32 *)
cast v_conv1_i_5_22@sint32 v1456@sint16;
(*   %mul.i.5.22 = mul nsw i32 %conv1.i.5.22, -247 *)
mul v_mul_i_5_22 v_conv1_i_5_22 (-247)@sint32;
(*   %call.i.5.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_22, v_call_i_5_22);
(*   %arrayidx11.5.22 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %1457 = load i16, i16* %arrayidx11.5.22, align 2, !tbaa !3 *)
mov v1457 mem0_352;
(*   %sub.5.22 = sub i16 %1457, %call.i.5.22 *)
sub v_sub_5_22 v1457 v_call_i_5_22;
(*   store i16 %sub.5.22, i16* %arrayidx9.5.22, align 2, !tbaa !3 *)
mov mem0_360 v_sub_5_22;
(*   %add21.5.22 = add i16 %1457, %call.i.5.22 *)
add v_add21_5_22 v1457 v_call_i_5_22;
(*   store i16 %add21.5.22, i16* %arrayidx11.5.22, align 2, !tbaa !3 *)
mov mem0_352 v_add21_5_22;
(*   %arrayidx9.5.1.22 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %1458 = load i16, i16* %arrayidx9.5.1.22, align 2, !tbaa !3 *)
mov v1458 mem0_362;
(*   %conv1.i.5.1.22 = sext i16 %1458 to i32 *)
cast v_conv1_i_5_1_22@sint32 v1458@sint16;
(*   %mul.i.5.1.22 = mul nsw i32 %conv1.i.5.1.22, -247 *)
mul v_mul_i_5_1_22 v_conv1_i_5_1_22 (-247)@sint32;
(*   %call.i.5.1.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_22, v_call_i_5_1_22);
(*   %arrayidx11.5.1.22 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %1459 = load i16, i16* %arrayidx11.5.1.22, align 2, !tbaa !3 *)
mov v1459 mem0_354;
(*   %sub.5.1.22 = sub i16 %1459, %call.i.5.1.22 *)
sub v_sub_5_1_22 v1459 v_call_i_5_1_22;
(*   store i16 %sub.5.1.22, i16* %arrayidx9.5.1.22, align 2, !tbaa !3 *)
mov mem0_362 v_sub_5_1_22;
(*   %add21.5.1.22 = add i16 %1459, %call.i.5.1.22 *)
add v_add21_5_1_22 v1459 v_call_i_5_1_22;
(*   store i16 %add21.5.1.22, i16* %arrayidx11.5.1.22, align 2, !tbaa !3 *)
mov mem0_354 v_add21_5_1_22;
(*   %arrayidx9.5.2.22 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %1460 = load i16, i16* %arrayidx9.5.2.22, align 2, !tbaa !3 *)
mov v1460 mem0_364;
(*   %conv1.i.5.2.22 = sext i16 %1460 to i32 *)
cast v_conv1_i_5_2_22@sint32 v1460@sint16;
(*   %mul.i.5.2.22 = mul nsw i32 %conv1.i.5.2.22, -247 *)
mul v_mul_i_5_2_22 v_conv1_i_5_2_22 (-247)@sint32;
(*   %call.i.5.2.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_22, v_call_i_5_2_22);
(*   %arrayidx11.5.2.22 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %1461 = load i16, i16* %arrayidx11.5.2.22, align 2, !tbaa !3 *)
mov v1461 mem0_356;
(*   %sub.5.2.22 = sub i16 %1461, %call.i.5.2.22 *)
sub v_sub_5_2_22 v1461 v_call_i_5_2_22;
(*   store i16 %sub.5.2.22, i16* %arrayidx9.5.2.22, align 2, !tbaa !3 *)
mov mem0_364 v_sub_5_2_22;
(*   %add21.5.2.22 = add i16 %1461, %call.i.5.2.22 *)
add v_add21_5_2_22 v1461 v_call_i_5_2_22;
(*   store i16 %add21.5.2.22, i16* %arrayidx11.5.2.22, align 2, !tbaa !3 *)
mov mem0_356 v_add21_5_2_22;
(*   %arrayidx9.5.3.22 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %1462 = load i16, i16* %arrayidx9.5.3.22, align 2, !tbaa !3 *)
mov v1462 mem0_366;
(*   %conv1.i.5.3.22 = sext i16 %1462 to i32 *)
cast v_conv1_i_5_3_22@sint32 v1462@sint16;
(*   %mul.i.5.3.22 = mul nsw i32 %conv1.i.5.3.22, -247 *)
mul v_mul_i_5_3_22 v_conv1_i_5_3_22 (-247)@sint32;
(*   %call.i.5.3.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_22, v_call_i_5_3_22);
(*   %arrayidx11.5.3.22 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %1463 = load i16, i16* %arrayidx11.5.3.22, align 2, !tbaa !3 *)
mov v1463 mem0_358;
(*   %sub.5.3.22 = sub i16 %1463, %call.i.5.3.22 *)
sub v_sub_5_3_22 v1463 v_call_i_5_3_22;
(*   store i16 %sub.5.3.22, i16* %arrayidx9.5.3.22, align 2, !tbaa !3 *)
mov mem0_366 v_sub_5_3_22;
(*   %add21.5.3.22 = add i16 %1463, %call.i.5.3.22 *)
add v_add21_5_3_22 v1463 v_call_i_5_3_22;
(*   store i16 %add21.5.3.22, i16* %arrayidx11.5.3.22, align 2, !tbaa !3 *)
mov mem0_358 v_add21_5_3_22;

(* NOTE: k = 55 *)

(*   %arrayidx9.5.23 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %1464 = load i16, i16* %arrayidx9.5.23, align 2, !tbaa !3 *)
mov v1464 mem0_376;
(*   %conv1.i.5.23 = sext i16 %1464 to i32 *)
cast v_conv1_i_5_23@sint32 v1464@sint16;
(*   %mul.i.5.23 = mul nsw i32 %conv1.i.5.23, -951 *)
mul v_mul_i_5_23 v_conv1_i_5_23 (-951)@sint32;
(*   %call.i.5.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_23, v_call_i_5_23);
(*   %arrayidx11.5.23 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %1465 = load i16, i16* %arrayidx11.5.23, align 2, !tbaa !3 *)
mov v1465 mem0_368;
(*   %sub.5.23 = sub i16 %1465, %call.i.5.23 *)
sub v_sub_5_23 v1465 v_call_i_5_23;
(*   store i16 %sub.5.23, i16* %arrayidx9.5.23, align 2, !tbaa !3 *)
mov mem0_376 v_sub_5_23;
(*   %add21.5.23 = add i16 %1465, %call.i.5.23 *)
add v_add21_5_23 v1465 v_call_i_5_23;
(*   store i16 %add21.5.23, i16* %arrayidx11.5.23, align 2, !tbaa !3 *)
mov mem0_368 v_add21_5_23;
(*   %arrayidx9.5.1.23 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %1466 = load i16, i16* %arrayidx9.5.1.23, align 2, !tbaa !3 *)
mov v1466 mem0_378;
(*   %conv1.i.5.1.23 = sext i16 %1466 to i32 *)
cast v_conv1_i_5_1_23@sint32 v1466@sint16;
(*   %mul.i.5.1.23 = mul nsw i32 %conv1.i.5.1.23, -951 *)
mul v_mul_i_5_1_23 v_conv1_i_5_1_23 (-951)@sint32;
(*   %call.i.5.1.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_23, v_call_i_5_1_23);
(*   %arrayidx11.5.1.23 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %1467 = load i16, i16* %arrayidx11.5.1.23, align 2, !tbaa !3 *)
mov v1467 mem0_370;
(*   %sub.5.1.23 = sub i16 %1467, %call.i.5.1.23 *)
sub v_sub_5_1_23 v1467 v_call_i_5_1_23;
(*   store i16 %sub.5.1.23, i16* %arrayidx9.5.1.23, align 2, !tbaa !3 *)
mov mem0_378 v_sub_5_1_23;
(*   %add21.5.1.23 = add i16 %1467, %call.i.5.1.23 *)
add v_add21_5_1_23 v1467 v_call_i_5_1_23;
(*   store i16 %add21.5.1.23, i16* %arrayidx11.5.1.23, align 2, !tbaa !3 *)
mov mem0_370 v_add21_5_1_23;
(*   %arrayidx9.5.2.23 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %1468 = load i16, i16* %arrayidx9.5.2.23, align 2, !tbaa !3 *)
mov v1468 mem0_380;
(*   %conv1.i.5.2.23 = sext i16 %1468 to i32 *)
cast v_conv1_i_5_2_23@sint32 v1468@sint16;
(*   %mul.i.5.2.23 = mul nsw i32 %conv1.i.5.2.23, -951 *)
mul v_mul_i_5_2_23 v_conv1_i_5_2_23 (-951)@sint32;
(*   %call.i.5.2.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_23, v_call_i_5_2_23);
(*   %arrayidx11.5.2.23 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %1469 = load i16, i16* %arrayidx11.5.2.23, align 2, !tbaa !3 *)
mov v1469 mem0_372;
(*   %sub.5.2.23 = sub i16 %1469, %call.i.5.2.23 *)
sub v_sub_5_2_23 v1469 v_call_i_5_2_23;
(*   store i16 %sub.5.2.23, i16* %arrayidx9.5.2.23, align 2, !tbaa !3 *)
mov mem0_380 v_sub_5_2_23;
(*   %add21.5.2.23 = add i16 %1469, %call.i.5.2.23 *)
add v_add21_5_2_23 v1469 v_call_i_5_2_23;
(*   store i16 %add21.5.2.23, i16* %arrayidx11.5.2.23, align 2, !tbaa !3 *)
mov mem0_372 v_add21_5_2_23;
(*   %arrayidx9.5.3.23 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %1470 = load i16, i16* %arrayidx9.5.3.23, align 2, !tbaa !3 *)
mov v1470 mem0_382;
(*   %conv1.i.5.3.23 = sext i16 %1470 to i32 *)
cast v_conv1_i_5_3_23@sint32 v1470@sint16;
(*   %mul.i.5.3.23 = mul nsw i32 %conv1.i.5.3.23, -951 *)
mul v_mul_i_5_3_23 v_conv1_i_5_3_23 (-951)@sint32;
(*   %call.i.5.3.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_23, v_call_i_5_3_23);
(*   %arrayidx11.5.3.23 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %1471 = load i16, i16* %arrayidx11.5.3.23, align 2, !tbaa !3 *)
mov v1471 mem0_374;
(*   %sub.5.3.23 = sub i16 %1471, %call.i.5.3.23 *)
sub v_sub_5_3_23 v1471 v_call_i_5_3_23;
(*   store i16 %sub.5.3.23, i16* %arrayidx9.5.3.23, align 2, !tbaa !3 *)
mov mem0_382 v_sub_5_3_23;
(*   %add21.5.3.23 = add i16 %1471, %call.i.5.3.23 *)
add v_add21_5_3_23 v1471 v_call_i_5_3_23;
(*   store i16 %add21.5.3.23, i16* %arrayidx11.5.3.23, align 2, !tbaa !3 *)
mov mem0_374 v_add21_5_3_23;

(* NOTE: k = 56 *)

(*   %arrayidx9.5.24 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %1472 = load i16, i16* %arrayidx9.5.24, align 2, !tbaa !3 *)
mov v1472 mem0_392;
(*   %conv1.i.5.24 = sext i16 %1472 to i32 *)
cast v_conv1_i_5_24@sint32 v1472@sint16;
(*   %mul.i.5.24 = mul nsw i32 %conv1.i.5.24, -398 *)
mul v_mul_i_5_24 v_conv1_i_5_24 (-398)@sint32;
(*   %call.i.5.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_24, v_call_i_5_24);
(*   %arrayidx11.5.24 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %1473 = load i16, i16* %arrayidx11.5.24, align 2, !tbaa !3 *)
mov v1473 mem0_384;
(*   %sub.5.24 = sub i16 %1473, %call.i.5.24 *)
sub v_sub_5_24 v1473 v_call_i_5_24;
(*   store i16 %sub.5.24, i16* %arrayidx9.5.24, align 2, !tbaa !3 *)
mov mem0_392 v_sub_5_24;
(*   %add21.5.24 = add i16 %1473, %call.i.5.24 *)
add v_add21_5_24 v1473 v_call_i_5_24;
(*   store i16 %add21.5.24, i16* %arrayidx11.5.24, align 2, !tbaa !3 *)
mov mem0_384 v_add21_5_24;
(*   %arrayidx9.5.1.24 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %1474 = load i16, i16* %arrayidx9.5.1.24, align 2, !tbaa !3 *)
mov v1474 mem0_394;
(*   %conv1.i.5.1.24 = sext i16 %1474 to i32 *)
cast v_conv1_i_5_1_24@sint32 v1474@sint16;
(*   %mul.i.5.1.24 = mul nsw i32 %conv1.i.5.1.24, -398 *)
mul v_mul_i_5_1_24 v_conv1_i_5_1_24 (-398)@sint32;
(*   %call.i.5.1.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_24, v_call_i_5_1_24);
(*   %arrayidx11.5.1.24 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %1475 = load i16, i16* %arrayidx11.5.1.24, align 2, !tbaa !3 *)
mov v1475 mem0_386;
(*   %sub.5.1.24 = sub i16 %1475, %call.i.5.1.24 *)
sub v_sub_5_1_24 v1475 v_call_i_5_1_24;
(*   store i16 %sub.5.1.24, i16* %arrayidx9.5.1.24, align 2, !tbaa !3 *)
mov mem0_394 v_sub_5_1_24;
(*   %add21.5.1.24 = add i16 %1475, %call.i.5.1.24 *)
add v_add21_5_1_24 v1475 v_call_i_5_1_24;
(*   store i16 %add21.5.1.24, i16* %arrayidx11.5.1.24, align 2, !tbaa !3 *)
mov mem0_386 v_add21_5_1_24;
(*   %arrayidx9.5.2.24 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %1476 = load i16, i16* %arrayidx9.5.2.24, align 2, !tbaa !3 *)
mov v1476 mem0_396;
(*   %conv1.i.5.2.24 = sext i16 %1476 to i32 *)
cast v_conv1_i_5_2_24@sint32 v1476@sint16;
(*   %mul.i.5.2.24 = mul nsw i32 %conv1.i.5.2.24, -398 *)
mul v_mul_i_5_2_24 v_conv1_i_5_2_24 (-398)@sint32;
(*   %call.i.5.2.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_24, v_call_i_5_2_24);
(*   %arrayidx11.5.2.24 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %1477 = load i16, i16* %arrayidx11.5.2.24, align 2, !tbaa !3 *)
mov v1477 mem0_388;
(*   %sub.5.2.24 = sub i16 %1477, %call.i.5.2.24 *)
sub v_sub_5_2_24 v1477 v_call_i_5_2_24;
(*   store i16 %sub.5.2.24, i16* %arrayidx9.5.2.24, align 2, !tbaa !3 *)
mov mem0_396 v_sub_5_2_24;
(*   %add21.5.2.24 = add i16 %1477, %call.i.5.2.24 *)
add v_add21_5_2_24 v1477 v_call_i_5_2_24;
(*   store i16 %add21.5.2.24, i16* %arrayidx11.5.2.24, align 2, !tbaa !3 *)
mov mem0_388 v_add21_5_2_24;
(*   %arrayidx9.5.3.24 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %1478 = load i16, i16* %arrayidx9.5.3.24, align 2, !tbaa !3 *)
mov v1478 mem0_398;
(*   %conv1.i.5.3.24 = sext i16 %1478 to i32 *)
cast v_conv1_i_5_3_24@sint32 v1478@sint16;
(*   %mul.i.5.3.24 = mul nsw i32 %conv1.i.5.3.24, -398 *)
mul v_mul_i_5_3_24 v_conv1_i_5_3_24 (-398)@sint32;
(*   %call.i.5.3.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_24, v_call_i_5_3_24);
(*   %arrayidx11.5.3.24 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %1479 = load i16, i16* %arrayidx11.5.3.24, align 2, !tbaa !3 *)
mov v1479 mem0_390;
(*   %sub.5.3.24 = sub i16 %1479, %call.i.5.3.24 *)
sub v_sub_5_3_24 v1479 v_call_i_5_3_24;
(*   store i16 %sub.5.3.24, i16* %arrayidx9.5.3.24, align 2, !tbaa !3 *)
mov mem0_398 v_sub_5_3_24;
(*   %add21.5.3.24 = add i16 %1479, %call.i.5.3.24 *)
add v_add21_5_3_24 v1479 v_call_i_5_3_24;
(*   store i16 %add21.5.3.24, i16* %arrayidx11.5.3.24, align 2, !tbaa !3 *)
mov mem0_390 v_add21_5_3_24;

(* NOTE: k = 57 *)

(*   %arrayidx9.5.25 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %1480 = load i16, i16* %arrayidx9.5.25, align 2, !tbaa !3 *)
mov v1480 mem0_408;
(*   %conv1.i.5.25 = sext i16 %1480 to i32 *)
cast v_conv1_i_5_25@sint32 v1480@sint16;
(*   %mul.i.5.25 = mul nsw i32 %conv1.i.5.25, 961 *)
mul v_mul_i_5_25 v_conv1_i_5_25 (961)@sint32;
(*   %call.i.5.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_25, v_call_i_5_25);
(*   %arrayidx11.5.25 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %1481 = load i16, i16* %arrayidx11.5.25, align 2, !tbaa !3 *)
mov v1481 mem0_400;
(*   %sub.5.25 = sub i16 %1481, %call.i.5.25 *)
sub v_sub_5_25 v1481 v_call_i_5_25;
(*   store i16 %sub.5.25, i16* %arrayidx9.5.25, align 2, !tbaa !3 *)
mov mem0_408 v_sub_5_25;
(*   %add21.5.25 = add i16 %1481, %call.i.5.25 *)
add v_add21_5_25 v1481 v_call_i_5_25;
(*   store i16 %add21.5.25, i16* %arrayidx11.5.25, align 2, !tbaa !3 *)
mov mem0_400 v_add21_5_25;
(*   %arrayidx9.5.1.25 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %1482 = load i16, i16* %arrayidx9.5.1.25, align 2, !tbaa !3 *)
mov v1482 mem0_410;
(*   %conv1.i.5.1.25 = sext i16 %1482 to i32 *)
cast v_conv1_i_5_1_25@sint32 v1482@sint16;
(*   %mul.i.5.1.25 = mul nsw i32 %conv1.i.5.1.25, 961 *)
mul v_mul_i_5_1_25 v_conv1_i_5_1_25 (961)@sint32;
(*   %call.i.5.1.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_25, v_call_i_5_1_25);
(*   %arrayidx11.5.1.25 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %1483 = load i16, i16* %arrayidx11.5.1.25, align 2, !tbaa !3 *)
mov v1483 mem0_402;
(*   %sub.5.1.25 = sub i16 %1483, %call.i.5.1.25 *)
sub v_sub_5_1_25 v1483 v_call_i_5_1_25;
(*   store i16 %sub.5.1.25, i16* %arrayidx9.5.1.25, align 2, !tbaa !3 *)
mov mem0_410 v_sub_5_1_25;
(*   %add21.5.1.25 = add i16 %1483, %call.i.5.1.25 *)
add v_add21_5_1_25 v1483 v_call_i_5_1_25;
(*   store i16 %add21.5.1.25, i16* %arrayidx11.5.1.25, align 2, !tbaa !3 *)
mov mem0_402 v_add21_5_1_25;
(*   %arrayidx9.5.2.25 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %1484 = load i16, i16* %arrayidx9.5.2.25, align 2, !tbaa !3 *)
mov v1484 mem0_412;
(*   %conv1.i.5.2.25 = sext i16 %1484 to i32 *)
cast v_conv1_i_5_2_25@sint32 v1484@sint16;
(*   %mul.i.5.2.25 = mul nsw i32 %conv1.i.5.2.25, 961 *)
mul v_mul_i_5_2_25 v_conv1_i_5_2_25 (961)@sint32;
(*   %call.i.5.2.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_25, v_call_i_5_2_25);
(*   %arrayidx11.5.2.25 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %1485 = load i16, i16* %arrayidx11.5.2.25, align 2, !tbaa !3 *)
mov v1485 mem0_404;
(*   %sub.5.2.25 = sub i16 %1485, %call.i.5.2.25 *)
sub v_sub_5_2_25 v1485 v_call_i_5_2_25;
(*   store i16 %sub.5.2.25, i16* %arrayidx9.5.2.25, align 2, !tbaa !3 *)
mov mem0_412 v_sub_5_2_25;
(*   %add21.5.2.25 = add i16 %1485, %call.i.5.2.25 *)
add v_add21_5_2_25 v1485 v_call_i_5_2_25;
(*   store i16 %add21.5.2.25, i16* %arrayidx11.5.2.25, align 2, !tbaa !3 *)
mov mem0_404 v_add21_5_2_25;
(*   %arrayidx9.5.3.25 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %1486 = load i16, i16* %arrayidx9.5.3.25, align 2, !tbaa !3 *)
mov v1486 mem0_414;
(*   %conv1.i.5.3.25 = sext i16 %1486 to i32 *)
cast v_conv1_i_5_3_25@sint32 v1486@sint16;
(*   %mul.i.5.3.25 = mul nsw i32 %conv1.i.5.3.25, 961 *)
mul v_mul_i_5_3_25 v_conv1_i_5_3_25 (961)@sint32;
(*   %call.i.5.3.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_25, v_call_i_5_3_25);
(*   %arrayidx11.5.3.25 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %1487 = load i16, i16* %arrayidx11.5.3.25, align 2, !tbaa !3 *)
mov v1487 mem0_406;
(*   %sub.5.3.25 = sub i16 %1487, %call.i.5.3.25 *)
sub v_sub_5_3_25 v1487 v_call_i_5_3_25;
(*   store i16 %sub.5.3.25, i16* %arrayidx9.5.3.25, align 2, !tbaa !3 *)
mov mem0_414 v_sub_5_3_25;
(*   %add21.5.3.25 = add i16 %1487, %call.i.5.3.25 *)
add v_add21_5_3_25 v1487 v_call_i_5_3_25;
(*   store i16 %add21.5.3.25, i16* %arrayidx11.5.3.25, align 2, !tbaa !3 *)
mov mem0_406 v_add21_5_3_25;

(* NOTE: k = 58 *)

(*   %arrayidx9.5.26 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %1488 = load i16, i16* %arrayidx9.5.26, align 2, !tbaa !3 *)
mov v1488 mem0_424;
(*   %conv1.i.5.26 = sext i16 %1488 to i32 *)
cast v_conv1_i_5_26@sint32 v1488@sint16;
(*   %mul.i.5.26 = mul nsw i32 %conv1.i.5.26, -1508 *)
mul v_mul_i_5_26 v_conv1_i_5_26 (-1508)@sint32;
(*   %call.i.5.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_26, v_call_i_5_26);
(*   %arrayidx11.5.26 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %1489 = load i16, i16* %arrayidx11.5.26, align 2, !tbaa !3 *)
mov v1489 mem0_416;
(*   %sub.5.26 = sub i16 %1489, %call.i.5.26 *)
sub v_sub_5_26 v1489 v_call_i_5_26;
(*   store i16 %sub.5.26, i16* %arrayidx9.5.26, align 2, !tbaa !3 *)
mov mem0_424 v_sub_5_26;
(*   %add21.5.26 = add i16 %1489, %call.i.5.26 *)
add v_add21_5_26 v1489 v_call_i_5_26;
(*   store i16 %add21.5.26, i16* %arrayidx11.5.26, align 2, !tbaa !3 *)
mov mem0_416 v_add21_5_26;
(*   %arrayidx9.5.1.26 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %1490 = load i16, i16* %arrayidx9.5.1.26, align 2, !tbaa !3 *)
mov v1490 mem0_426;
(*   %conv1.i.5.1.26 = sext i16 %1490 to i32 *)
cast v_conv1_i_5_1_26@sint32 v1490@sint16;
(*   %mul.i.5.1.26 = mul nsw i32 %conv1.i.5.1.26, -1508 *)
mul v_mul_i_5_1_26 v_conv1_i_5_1_26 (-1508)@sint32;
(*   %call.i.5.1.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_26, v_call_i_5_1_26);
(*   %arrayidx11.5.1.26 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %1491 = load i16, i16* %arrayidx11.5.1.26, align 2, !tbaa !3 *)
mov v1491 mem0_418;
(*   %sub.5.1.26 = sub i16 %1491, %call.i.5.1.26 *)
sub v_sub_5_1_26 v1491 v_call_i_5_1_26;
(*   store i16 %sub.5.1.26, i16* %arrayidx9.5.1.26, align 2, !tbaa !3 *)
mov mem0_426 v_sub_5_1_26;
(*   %add21.5.1.26 = add i16 %1491, %call.i.5.1.26 *)
add v_add21_5_1_26 v1491 v_call_i_5_1_26;
(*   store i16 %add21.5.1.26, i16* %arrayidx11.5.1.26, align 2, !tbaa !3 *)
mov mem0_418 v_add21_5_1_26;
(*   %arrayidx9.5.2.26 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %1492 = load i16, i16* %arrayidx9.5.2.26, align 2, !tbaa !3 *)
mov v1492 mem0_428;
(*   %conv1.i.5.2.26 = sext i16 %1492 to i32 *)
cast v_conv1_i_5_2_26@sint32 v1492@sint16;
(*   %mul.i.5.2.26 = mul nsw i32 %conv1.i.5.2.26, -1508 *)
mul v_mul_i_5_2_26 v_conv1_i_5_2_26 (-1508)@sint32;
(*   %call.i.5.2.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_26, v_call_i_5_2_26);
(*   %arrayidx11.5.2.26 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %1493 = load i16, i16* %arrayidx11.5.2.26, align 2, !tbaa !3 *)
mov v1493 mem0_420;
(*   %sub.5.2.26 = sub i16 %1493, %call.i.5.2.26 *)
sub v_sub_5_2_26 v1493 v_call_i_5_2_26;
(*   store i16 %sub.5.2.26, i16* %arrayidx9.5.2.26, align 2, !tbaa !3 *)
mov mem0_428 v_sub_5_2_26;
(*   %add21.5.2.26 = add i16 %1493, %call.i.5.2.26 *)
add v_add21_5_2_26 v1493 v_call_i_5_2_26;
(*   store i16 %add21.5.2.26, i16* %arrayidx11.5.2.26, align 2, !tbaa !3 *)
mov mem0_420 v_add21_5_2_26;
(*   %arrayidx9.5.3.26 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %1494 = load i16, i16* %arrayidx9.5.3.26, align 2, !tbaa !3 *)
mov v1494 mem0_430;
(*   %conv1.i.5.3.26 = sext i16 %1494 to i32 *)
cast v_conv1_i_5_3_26@sint32 v1494@sint16;
(*   %mul.i.5.3.26 = mul nsw i32 %conv1.i.5.3.26, -1508 *)
mul v_mul_i_5_3_26 v_conv1_i_5_3_26 (-1508)@sint32;
(*   %call.i.5.3.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_26, v_call_i_5_3_26);
(*   %arrayidx11.5.3.26 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %1495 = load i16, i16* %arrayidx11.5.3.26, align 2, !tbaa !3 *)
mov v1495 mem0_422;
(*   %sub.5.3.26 = sub i16 %1495, %call.i.5.3.26 *)
sub v_sub_5_3_26 v1495 v_call_i_5_3_26;
(*   store i16 %sub.5.3.26, i16* %arrayidx9.5.3.26, align 2, !tbaa !3 *)
mov mem0_430 v_sub_5_3_26;
(*   %add21.5.3.26 = add i16 %1495, %call.i.5.3.26 *)
add v_add21_5_3_26 v1495 v_call_i_5_3_26;
(*   store i16 %add21.5.3.26, i16* %arrayidx11.5.3.26, align 2, !tbaa !3 *)
mov mem0_422 v_add21_5_3_26;
(*   %arrayidx9.5.27 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %1496 = load i16, i16* %arrayidx9.5.27, align 2, !tbaa !3 *)
mov v1496 mem0_440;
(*   %conv1.i.5.27 = sext i16 %1496 to i32 *)
cast v_conv1_i_5_27@sint32 v1496@sint16;
(*   %mul.i.5.27 = mul nsw i32 %conv1.i.5.27, -725 *)
mul v_mul_i_5_27 v_conv1_i_5_27 (-725)@sint32;
(*   %call.i.5.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_27, v_call_i_5_27);
(*   %arrayidx11.5.27 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %1497 = load i16, i16* %arrayidx11.5.27, align 2, !tbaa !3 *)
mov v1497 mem0_432;
(*   %sub.5.27 = sub i16 %1497, %call.i.5.27 *)
sub v_sub_5_27 v1497 v_call_i_5_27;
(*   store i16 %sub.5.27, i16* %arrayidx9.5.27, align 2, !tbaa !3 *)
mov mem0_440 v_sub_5_27;
(*   %add21.5.27 = add i16 %1497, %call.i.5.27 *)
add v_add21_5_27 v1497 v_call_i_5_27;
(*   store i16 %add21.5.27, i16* %arrayidx11.5.27, align 2, !tbaa !3 *)
mov mem0_432 v_add21_5_27;

(* NOTE: k = 59 *)

(*   %arrayidx9.5.1.27 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %1498 = load i16, i16* %arrayidx9.5.1.27, align 2, !tbaa !3 *)
mov v1498 mem0_442;
(*   %conv1.i.5.1.27 = sext i16 %1498 to i32 *)
cast v_conv1_i_5_1_27@sint32 v1498@sint16;
(*   %mul.i.5.1.27 = mul nsw i32 %conv1.i.5.1.27, -725 *)
mul v_mul_i_5_1_27 v_conv1_i_5_1_27 (-725)@sint32;
(*   %call.i.5.1.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_27, v_call_i_5_1_27);
(*   %arrayidx11.5.1.27 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %1499 = load i16, i16* %arrayidx11.5.1.27, align 2, !tbaa !3 *)
mov v1499 mem0_434;
(*   %sub.5.1.27 = sub i16 %1499, %call.i.5.1.27 *)
sub v_sub_5_1_27 v1499 v_call_i_5_1_27;
(*   store i16 %sub.5.1.27, i16* %arrayidx9.5.1.27, align 2, !tbaa !3 *)
mov mem0_442 v_sub_5_1_27;
(*   %add21.5.1.27 = add i16 %1499, %call.i.5.1.27 *)
add v_add21_5_1_27 v1499 v_call_i_5_1_27;
(*   store i16 %add21.5.1.27, i16* %arrayidx11.5.1.27, align 2, !tbaa !3 *)
mov mem0_434 v_add21_5_1_27;
(*   %arrayidx9.5.2.27 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %1500 = load i16, i16* %arrayidx9.5.2.27, align 2, !tbaa !3 *)
mov v1500 mem0_444;
(*   %conv1.i.5.2.27 = sext i16 %1500 to i32 *)
cast v_conv1_i_5_2_27@sint32 v1500@sint16;
(*   %mul.i.5.2.27 = mul nsw i32 %conv1.i.5.2.27, -725 *)
mul v_mul_i_5_2_27 v_conv1_i_5_2_27 (-725)@sint32;
(*   %call.i.5.2.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_27, v_call_i_5_2_27);
(*   %arrayidx11.5.2.27 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %1501 = load i16, i16* %arrayidx11.5.2.27, align 2, !tbaa !3 *)
mov v1501 mem0_436;
(*   %sub.5.2.27 = sub i16 %1501, %call.i.5.2.27 *)
sub v_sub_5_2_27 v1501 v_call_i_5_2_27;
(*   store i16 %sub.5.2.27, i16* %arrayidx9.5.2.27, align 2, !tbaa !3 *)
mov mem0_444 v_sub_5_2_27;
(*   %add21.5.2.27 = add i16 %1501, %call.i.5.2.27 *)
add v_add21_5_2_27 v1501 v_call_i_5_2_27;
(*   store i16 %add21.5.2.27, i16* %arrayidx11.5.2.27, align 2, !tbaa !3 *)
mov mem0_436 v_add21_5_2_27;
(*   %arrayidx9.5.3.27 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %1502 = load i16, i16* %arrayidx9.5.3.27, align 2, !tbaa !3 *)
mov v1502 mem0_446;
(*   %conv1.i.5.3.27 = sext i16 %1502 to i32 *)
cast v_conv1_i_5_3_27@sint32 v1502@sint16;
(*   %mul.i.5.3.27 = mul nsw i32 %conv1.i.5.3.27, -725 *)
mul v_mul_i_5_3_27 v_conv1_i_5_3_27 (-725)@sint32;
(*   %call.i.5.3.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_27, v_call_i_5_3_27);
(*   %arrayidx11.5.3.27 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %1503 = load i16, i16* %arrayidx11.5.3.27, align 2, !tbaa !3 *)
mov v1503 mem0_438;
(*   %sub.5.3.27 = sub i16 %1503, %call.i.5.3.27 *)
sub v_sub_5_3_27 v1503 v_call_i_5_3_27;
(*   store i16 %sub.5.3.27, i16* %arrayidx9.5.3.27, align 2, !tbaa !3 *)
mov mem0_446 v_sub_5_3_27;
(*   %add21.5.3.27 = add i16 %1503, %call.i.5.3.27 *)
add v_add21_5_3_27 v1503 v_call_i_5_3_27;
(*   store i16 %add21.5.3.27, i16* %arrayidx11.5.3.27, align 2, !tbaa !3 *)
mov mem0_438 v_add21_5_3_27;

(* NOTE: k = 60 *)

(*   %arrayidx9.5.28 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %1504 = load i16, i16* %arrayidx9.5.28, align 2, !tbaa !3 *)
mov v1504 mem0_456;
(*   %conv1.i.5.28 = sext i16 %1504 to i32 *)
cast v_conv1_i_5_28@sint32 v1504@sint16;
(*   %mul.i.5.28 = mul nsw i32 %conv1.i.5.28, 448 *)
mul v_mul_i_5_28 v_conv1_i_5_28 (448)@sint32;
(*   %call.i.5.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_28, v_call_i_5_28);
(*   %arrayidx11.5.28 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %1505 = load i16, i16* %arrayidx11.5.28, align 2, !tbaa !3 *)
mov v1505 mem0_448;
(*   %sub.5.28 = sub i16 %1505, %call.i.5.28 *)
sub v_sub_5_28 v1505 v_call_i_5_28;
(*   store i16 %sub.5.28, i16* %arrayidx9.5.28, align 2, !tbaa !3 *)
mov mem0_456 v_sub_5_28;
(*   %add21.5.28 = add i16 %1505, %call.i.5.28 *)
add v_add21_5_28 v1505 v_call_i_5_28;
(*   store i16 %add21.5.28, i16* %arrayidx11.5.28, align 2, !tbaa !3 *)
mov mem0_448 v_add21_5_28;
(*   %arrayidx9.5.1.28 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %1506 = load i16, i16* %arrayidx9.5.1.28, align 2, !tbaa !3 *)
mov v1506 mem0_458;
(*   %conv1.i.5.1.28 = sext i16 %1506 to i32 *)
cast v_conv1_i_5_1_28@sint32 v1506@sint16;
(*   %mul.i.5.1.28 = mul nsw i32 %conv1.i.5.1.28, 448 *)
mul v_mul_i_5_1_28 v_conv1_i_5_1_28 (448)@sint32;
(*   %call.i.5.1.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_28, v_call_i_5_1_28);
(*   %arrayidx11.5.1.28 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %1507 = load i16, i16* %arrayidx11.5.1.28, align 2, !tbaa !3 *)
mov v1507 mem0_450;
(*   %sub.5.1.28 = sub i16 %1507, %call.i.5.1.28 *)
sub v_sub_5_1_28 v1507 v_call_i_5_1_28;
(*   store i16 %sub.5.1.28, i16* %arrayidx9.5.1.28, align 2, !tbaa !3 *)
mov mem0_458 v_sub_5_1_28;
(*   %add21.5.1.28 = add i16 %1507, %call.i.5.1.28 *)
add v_add21_5_1_28 v1507 v_call_i_5_1_28;
(*   store i16 %add21.5.1.28, i16* %arrayidx11.5.1.28, align 2, !tbaa !3 *)
mov mem0_450 v_add21_5_1_28;
(*   %arrayidx9.5.2.28 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %1508 = load i16, i16* %arrayidx9.5.2.28, align 2, !tbaa !3 *)
mov v1508 mem0_460;
(*   %conv1.i.5.2.28 = sext i16 %1508 to i32 *)
cast v_conv1_i_5_2_28@sint32 v1508@sint16;
(*   %mul.i.5.2.28 = mul nsw i32 %conv1.i.5.2.28, 448 *)
mul v_mul_i_5_2_28 v_conv1_i_5_2_28 (448)@sint32;
(*   %call.i.5.2.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_28, v_call_i_5_2_28);
(*   %arrayidx11.5.2.28 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %1509 = load i16, i16* %arrayidx11.5.2.28, align 2, !tbaa !3 *)
mov v1509 mem0_452;
(*   %sub.5.2.28 = sub i16 %1509, %call.i.5.2.28 *)
sub v_sub_5_2_28 v1509 v_call_i_5_2_28;
(*   store i16 %sub.5.2.28, i16* %arrayidx9.5.2.28, align 2, !tbaa !3 *)
mov mem0_460 v_sub_5_2_28;
(*   %add21.5.2.28 = add i16 %1509, %call.i.5.2.28 *)
add v_add21_5_2_28 v1509 v_call_i_5_2_28;
(*   store i16 %add21.5.2.28, i16* %arrayidx11.5.2.28, align 2, !tbaa !3 *)
mov mem0_452 v_add21_5_2_28;
(*   %arrayidx9.5.3.28 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %1510 = load i16, i16* %arrayidx9.5.3.28, align 2, !tbaa !3 *)
mov v1510 mem0_462;
(*   %conv1.i.5.3.28 = sext i16 %1510 to i32 *)
cast v_conv1_i_5_3_28@sint32 v1510@sint16;
(*   %mul.i.5.3.28 = mul nsw i32 %conv1.i.5.3.28, 448 *)
mul v_mul_i_5_3_28 v_conv1_i_5_3_28 (448)@sint32;
(*   %call.i.5.3.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_28, v_call_i_5_3_28);
(*   %arrayidx11.5.3.28 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %1511 = load i16, i16* %arrayidx11.5.3.28, align 2, !tbaa !3 *)
mov v1511 mem0_454;
(*   %sub.5.3.28 = sub i16 %1511, %call.i.5.3.28 *)
sub v_sub_5_3_28 v1511 v_call_i_5_3_28;
(*   store i16 %sub.5.3.28, i16* %arrayidx9.5.3.28, align 2, !tbaa !3 *)
mov mem0_462 v_sub_5_3_28;
(*   %add21.5.3.28 = add i16 %1511, %call.i.5.3.28 *)
add v_add21_5_3_28 v1511 v_call_i_5_3_28;
(*   store i16 %add21.5.3.28, i16* %arrayidx11.5.3.28, align 2, !tbaa !3 *)
mov mem0_454 v_add21_5_3_28;

(* NOTE: k = 61 *)

(*   %arrayidx9.5.29 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %1512 = load i16, i16* %arrayidx9.5.29, align 2, !tbaa !3 *)
mov v1512 mem0_472;
(*   %conv1.i.5.29 = sext i16 %1512 to i32 *)
cast v_conv1_i_5_29@sint32 v1512@sint16;
(*   %mul.i.5.29 = mul nsw i32 %conv1.i.5.29, -1065 *)
mul v_mul_i_5_29 v_conv1_i_5_29 (-1065)@sint32;
(*   %call.i.5.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_29, v_call_i_5_29);
(*   %arrayidx11.5.29 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %1513 = load i16, i16* %arrayidx11.5.29, align 2, !tbaa !3 *)
mov v1513 mem0_464;
(*   %sub.5.29 = sub i16 %1513, %call.i.5.29 *)
sub v_sub_5_29 v1513 v_call_i_5_29;
(*   store i16 %sub.5.29, i16* %arrayidx9.5.29, align 2, !tbaa !3 *)
mov mem0_472 v_sub_5_29;
(*   %add21.5.29 = add i16 %1513, %call.i.5.29 *)
add v_add21_5_29 v1513 v_call_i_5_29;
(*   store i16 %add21.5.29, i16* %arrayidx11.5.29, align 2, !tbaa !3 *)
mov mem0_464 v_add21_5_29;
(*   %arrayidx9.5.1.29 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %1514 = load i16, i16* %arrayidx9.5.1.29, align 2, !tbaa !3 *)
mov v1514 mem0_474;
(*   %conv1.i.5.1.29 = sext i16 %1514 to i32 *)
cast v_conv1_i_5_1_29@sint32 v1514@sint16;
(*   %mul.i.5.1.29 = mul nsw i32 %conv1.i.5.1.29, -1065 *)
mul v_mul_i_5_1_29 v_conv1_i_5_1_29 (-1065)@sint32;
(*   %call.i.5.1.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_29, v_call_i_5_1_29);
(*   %arrayidx11.5.1.29 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %1515 = load i16, i16* %arrayidx11.5.1.29, align 2, !tbaa !3 *)
mov v1515 mem0_466;
(*   %sub.5.1.29 = sub i16 %1515, %call.i.5.1.29 *)
sub v_sub_5_1_29 v1515 v_call_i_5_1_29;
(*   store i16 %sub.5.1.29, i16* %arrayidx9.5.1.29, align 2, !tbaa !3 *)
mov mem0_474 v_sub_5_1_29;
(*   %add21.5.1.29 = add i16 %1515, %call.i.5.1.29 *)
add v_add21_5_1_29 v1515 v_call_i_5_1_29;
(*   store i16 %add21.5.1.29, i16* %arrayidx11.5.1.29, align 2, !tbaa !3 *)
mov mem0_466 v_add21_5_1_29;
(*   %arrayidx9.5.2.29 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %1516 = load i16, i16* %arrayidx9.5.2.29, align 2, !tbaa !3 *)
mov v1516 mem0_476;
(*   %conv1.i.5.2.29 = sext i16 %1516 to i32 *)
cast v_conv1_i_5_2_29@sint32 v1516@sint16;
(*   %mul.i.5.2.29 = mul nsw i32 %conv1.i.5.2.29, -1065 *)
mul v_mul_i_5_2_29 v_conv1_i_5_2_29 (-1065)@sint32;
(*   %call.i.5.2.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_29, v_call_i_5_2_29);
(*   %arrayidx11.5.2.29 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %1517 = load i16, i16* %arrayidx11.5.2.29, align 2, !tbaa !3 *)
mov v1517 mem0_468;
(*   %sub.5.2.29 = sub i16 %1517, %call.i.5.2.29 *)
sub v_sub_5_2_29 v1517 v_call_i_5_2_29;
(*   store i16 %sub.5.2.29, i16* %arrayidx9.5.2.29, align 2, !tbaa !3 *)
mov mem0_476 v_sub_5_2_29;
(*   %add21.5.2.29 = add i16 %1517, %call.i.5.2.29 *)
add v_add21_5_2_29 v1517 v_call_i_5_2_29;
(*   store i16 %add21.5.2.29, i16* %arrayidx11.5.2.29, align 2, !tbaa !3 *)
mov mem0_468 v_add21_5_2_29;
(*   %arrayidx9.5.3.29 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %1518 = load i16, i16* %arrayidx9.5.3.29, align 2, !tbaa !3 *)
mov v1518 mem0_478;
(*   %conv1.i.5.3.29 = sext i16 %1518 to i32 *)
cast v_conv1_i_5_3_29@sint32 v1518@sint16;
(*   %mul.i.5.3.29 = mul nsw i32 %conv1.i.5.3.29, -1065 *)
mul v_mul_i_5_3_29 v_conv1_i_5_3_29 (-1065)@sint32;
(*   %call.i.5.3.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_29, v_call_i_5_3_29);
(*   %arrayidx11.5.3.29 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %1519 = load i16, i16* %arrayidx11.5.3.29, align 2, !tbaa !3 *)
mov v1519 mem0_470;
(*   %sub.5.3.29 = sub i16 %1519, %call.i.5.3.29 *)
sub v_sub_5_3_29 v1519 v_call_i_5_3_29;
(*   store i16 %sub.5.3.29, i16* %arrayidx9.5.3.29, align 2, !tbaa !3 *)
mov mem0_478 v_sub_5_3_29;
(*   %add21.5.3.29 = add i16 %1519, %call.i.5.3.29 *)
add v_add21_5_3_29 v1519 v_call_i_5_3_29;
(*   store i16 %add21.5.3.29, i16* %arrayidx11.5.3.29, align 2, !tbaa !3 *)
mov mem0_470 v_add21_5_3_29;

(* NOTE: k = 62 *)

(*   %arrayidx9.5.30 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %1520 = load i16, i16* %arrayidx9.5.30, align 2, !tbaa !3 *)
mov v1520 mem0_488;
(*   %conv1.i.5.30 = sext i16 %1520 to i32 *)
cast v_conv1_i_5_30@sint32 v1520@sint16;
(*   %mul.i.5.30 = mul nsw i32 %conv1.i.5.30, 677 *)
mul v_mul_i_5_30 v_conv1_i_5_30 (677)@sint32;
(*   %call.i.5.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_30, v_call_i_5_30);
(*   %arrayidx11.5.30 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %1521 = load i16, i16* %arrayidx11.5.30, align 2, !tbaa !3 *)
mov v1521 mem0_480;
(*   %sub.5.30 = sub i16 %1521, %call.i.5.30 *)
sub v_sub_5_30 v1521 v_call_i_5_30;
(*   store i16 %sub.5.30, i16* %arrayidx9.5.30, align 2, !tbaa !3 *)
mov mem0_488 v_sub_5_30;
(*   %add21.5.30 = add i16 %1521, %call.i.5.30 *)
add v_add21_5_30 v1521 v_call_i_5_30;
(*   store i16 %add21.5.30, i16* %arrayidx11.5.30, align 2, !tbaa !3 *)
mov mem0_480 v_add21_5_30;
(*   %arrayidx9.5.1.30 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %1522 = load i16, i16* %arrayidx9.5.1.30, align 2, !tbaa !3 *)
mov v1522 mem0_490;
(*   %conv1.i.5.1.30 = sext i16 %1522 to i32 *)
cast v_conv1_i_5_1_30@sint32 v1522@sint16;
(*   %mul.i.5.1.30 = mul nsw i32 %conv1.i.5.1.30, 677 *)
mul v_mul_i_5_1_30 v_conv1_i_5_1_30 (677)@sint32;
(*   %call.i.5.1.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_30, v_call_i_5_1_30);
(*   %arrayidx11.5.1.30 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %1523 = load i16, i16* %arrayidx11.5.1.30, align 2, !tbaa !3 *)
mov v1523 mem0_482;
(*   %sub.5.1.30 = sub i16 %1523, %call.i.5.1.30 *)
sub v_sub_5_1_30 v1523 v_call_i_5_1_30;
(*   store i16 %sub.5.1.30, i16* %arrayidx9.5.1.30, align 2, !tbaa !3 *)
mov mem0_490 v_sub_5_1_30;
(*   %add21.5.1.30 = add i16 %1523, %call.i.5.1.30 *)
add v_add21_5_1_30 v1523 v_call_i_5_1_30;
(*   store i16 %add21.5.1.30, i16* %arrayidx11.5.1.30, align 2, !tbaa !3 *)
mov mem0_482 v_add21_5_1_30;
(*   %arrayidx9.5.2.30 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %1524 = load i16, i16* %arrayidx9.5.2.30, align 2, !tbaa !3 *)
mov v1524 mem0_492;
(*   %conv1.i.5.2.30 = sext i16 %1524 to i32 *)
cast v_conv1_i_5_2_30@sint32 v1524@sint16;
(*   %mul.i.5.2.30 = mul nsw i32 %conv1.i.5.2.30, 677 *)
mul v_mul_i_5_2_30 v_conv1_i_5_2_30 (677)@sint32;
(*   %call.i.5.2.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_30, v_call_i_5_2_30);
(*   %arrayidx11.5.2.30 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %1525 = load i16, i16* %arrayidx11.5.2.30, align 2, !tbaa !3 *)
mov v1525 mem0_484;
(*   %sub.5.2.30 = sub i16 %1525, %call.i.5.2.30 *)
sub v_sub_5_2_30 v1525 v_call_i_5_2_30;
(*   store i16 %sub.5.2.30, i16* %arrayidx9.5.2.30, align 2, !tbaa !3 *)
mov mem0_492 v_sub_5_2_30;
(*   %add21.5.2.30 = add i16 %1525, %call.i.5.2.30 *)
add v_add21_5_2_30 v1525 v_call_i_5_2_30;
(*   store i16 %add21.5.2.30, i16* %arrayidx11.5.2.30, align 2, !tbaa !3 *)
mov mem0_484 v_add21_5_2_30;
(*   %arrayidx9.5.3.30 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %1526 = load i16, i16* %arrayidx9.5.3.30, align 2, !tbaa !3 *)
mov v1526 mem0_494;
(*   %conv1.i.5.3.30 = sext i16 %1526 to i32 *)
cast v_conv1_i_5_3_30@sint32 v1526@sint16;
(*   %mul.i.5.3.30 = mul nsw i32 %conv1.i.5.3.30, 677 *)
mul v_mul_i_5_3_30 v_conv1_i_5_3_30 (677)@sint32;
(*   %call.i.5.3.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_30, v_call_i_5_3_30);
(*   %arrayidx11.5.3.30 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %1527 = load i16, i16* %arrayidx11.5.3.30, align 2, !tbaa !3 *)
mov v1527 mem0_486;
(*   %sub.5.3.30 = sub i16 %1527, %call.i.5.3.30 *)
sub v_sub_5_3_30 v1527 v_call_i_5_3_30;
(*   store i16 %sub.5.3.30, i16* %arrayidx9.5.3.30, align 2, !tbaa !3 *)
mov mem0_494 v_sub_5_3_30;
(*   %add21.5.3.30 = add i16 %1527, %call.i.5.3.30 *)
add v_add21_5_3_30 v1527 v_call_i_5_3_30;
(*   store i16 %add21.5.3.30, i16* %arrayidx11.5.3.30, align 2, !tbaa !3 *)
mov mem0_486 v_add21_5_3_30;

(* NOTE: k = 63 *)

(*   %arrayidx9.5.31 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %1528 = load i16, i16* %arrayidx9.5.31, align 2, !tbaa !3 *)
mov v1528 mem0_504;
(*   %conv1.i.5.31 = sext i16 %1528 to i32 *)
cast v_conv1_i_5_31@sint32 v1528@sint16;
(*   %mul.i.5.31 = mul nsw i32 %conv1.i.5.31, -1275 *)
mul v_mul_i_5_31 v_conv1_i_5_31 (-1275)@sint32;
(*   %call.i.5.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_31, v_call_i_5_31);
(*   %arrayidx11.5.31 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %1529 = load i16, i16* %arrayidx11.5.31, align 2, !tbaa !3 *)
mov v1529 mem0_496;
(*   %sub.5.31 = sub i16 %1529, %call.i.5.31 *)
sub v_sub_5_31 v1529 v_call_i_5_31;
(*   store i16 %sub.5.31, i16* %arrayidx9.5.31, align 2, !tbaa !3 *)
mov mem0_504 v_sub_5_31;
(*   %add21.5.31 = add i16 %1529, %call.i.5.31 *)
add v_add21_5_31 v1529 v_call_i_5_31;
(*   store i16 %add21.5.31, i16* %arrayidx11.5.31, align 2, !tbaa !3 *)
mov mem0_496 v_add21_5_31;
(*   %arrayidx9.5.1.31 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %1530 = load i16, i16* %arrayidx9.5.1.31, align 2, !tbaa !3 *)
mov v1530 mem0_506;
(*   %conv1.i.5.1.31 = sext i16 %1530 to i32 *)
cast v_conv1_i_5_1_31@sint32 v1530@sint16;
(*   %mul.i.5.1.31 = mul nsw i32 %conv1.i.5.1.31, -1275 *)
mul v_mul_i_5_1_31 v_conv1_i_5_1_31 (-1275)@sint32;
(*   %call.i.5.1.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.1.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_1_31, v_call_i_5_1_31);
(*   %arrayidx11.5.1.31 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %1531 = load i16, i16* %arrayidx11.5.1.31, align 2, !tbaa !3 *)
mov v1531 mem0_498;
(*   %sub.5.1.31 = sub i16 %1531, %call.i.5.1.31 *)
sub v_sub_5_1_31 v1531 v_call_i_5_1_31;
(*   store i16 %sub.5.1.31, i16* %arrayidx9.5.1.31, align 2, !tbaa !3 *)
mov mem0_506 v_sub_5_1_31;
(*   %add21.5.1.31 = add i16 %1531, %call.i.5.1.31 *)
add v_add21_5_1_31 v1531 v_call_i_5_1_31;
(*   store i16 %add21.5.1.31, i16* %arrayidx11.5.1.31, align 2, !tbaa !3 *)
mov mem0_498 v_add21_5_1_31;
(*   %arrayidx9.5.2.31 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %1532 = load i16, i16* %arrayidx9.5.2.31, align 2, !tbaa !3 *)
mov v1532 mem0_508;
(*   %conv1.i.5.2.31 = sext i16 %1532 to i32 *)
cast v_conv1_i_5_2_31@sint32 v1532@sint16;
(*   %mul.i.5.2.31 = mul nsw i32 %conv1.i.5.2.31, -1275 *)
mul v_mul_i_5_2_31 v_conv1_i_5_2_31 (-1275)@sint32;
(*   %call.i.5.2.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.2.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_2_31, v_call_i_5_2_31);
(*   %arrayidx11.5.2.31 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %1533 = load i16, i16* %arrayidx11.5.2.31, align 2, !tbaa !3 *)
mov v1533 mem0_500;
(*   %sub.5.2.31 = sub i16 %1533, %call.i.5.2.31 *)
sub v_sub_5_2_31 v1533 v_call_i_5_2_31;
(*   store i16 %sub.5.2.31, i16* %arrayidx9.5.2.31, align 2, !tbaa !3 *)
mov mem0_508 v_sub_5_2_31;
(*   %add21.5.2.31 = add i16 %1533, %call.i.5.2.31 *)
add v_add21_5_2_31 v1533 v_call_i_5_2_31;
(*   store i16 %add21.5.2.31, i16* %arrayidx11.5.2.31, align 2, !tbaa !3 *)
mov mem0_500 v_add21_5_2_31;
(*   %arrayidx9.5.3.31 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %1534 = load i16, i16* %arrayidx9.5.3.31, align 2, !tbaa !3 *)
mov v1534 mem0_510;
(*   %conv1.i.5.3.31 = sext i16 %1534 to i32 *)
cast v_conv1_i_5_3_31@sint32 v1534@sint16;
(*   %mul.i.5.3.31 = mul nsw i32 %conv1.i.5.3.31, -1275 *)
mul v_mul_i_5_3_31 v_conv1_i_5_3_31 (-1275)@sint32;
(*   %call.i.5.3.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.5.3.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_5_3_31, v_call_i_5_3_31);
(*   %arrayidx11.5.3.31 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %1535 = load i16, i16* %arrayidx11.5.3.31, align 2, !tbaa !3 *)
mov v1535 mem0_502;
(*   %sub.5.3.31 = sub i16 %1535, %call.i.5.3.31 *)
sub v_sub_5_3_31 v1535 v_call_i_5_3_31;
(*   store i16 %sub.5.3.31, i16* %arrayidx9.5.3.31, align 2, !tbaa !3 *)
mov mem0_510 v_sub_5_3_31;
(*   %add21.5.3.31 = add i16 %1535, %call.i.5.3.31 *)
add v_add21_5_3_31 v1535 v_call_i_5_3_31;
(*   store i16 %add21.5.3.31, i16* %arrayidx11.5.3.31, align 2, !tbaa !3 *)
mov mem0_502 v_add21_5_3_31;

cut and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
mem0_0*(x**0) + mem0_2*(x**1) + mem0_4*(x**2) + mem0_6*(x**3)
)
[3329, x**4 - 289],
eqmod 
input_polynomial * input_polynomial
(
mem0_8*(x**0) + mem0_10*(x**1) + mem0_12*(x**2) + mem0_14*(x**3)
)
[3329, x**4 - 3040],
eqmod 
input_polynomial * input_polynomial
(
mem0_16*(x**0) + mem0_18*(x**1) + mem0_20*(x**2) + mem0_22*(x**3)
)
[3329, x**4 - 331],
eqmod 
input_polynomial * input_polynomial
(
mem0_24*(x**0) + mem0_26*(x**1) + mem0_28*(x**2) + mem0_30*(x**3)
)
[3329, x**4 - 2998],
eqmod 
input_polynomial * input_polynomial
(
mem0_32*(x**0) + mem0_34*(x**1) + mem0_36*(x**2) + mem0_38*(x**3)
)
[3329, x**4 - 3253],
eqmod 
input_polynomial * input_polynomial
(
mem0_40*(x**0) + mem0_42*(x**1) + mem0_44*(x**2) + mem0_46*(x**3)
)
[3329, x**4 - 76],
eqmod 
input_polynomial * input_polynomial
(
mem0_48*(x**0) + mem0_50*(x**1) + mem0_52*(x**2) + mem0_54*(x**3)
)
[3329, x**4 - 1756],
eqmod 
input_polynomial * input_polynomial
(
mem0_56*(x**0) + mem0_58*(x**1) + mem0_60*(x**2) + mem0_62*(x**3)
)
[3329, x**4 - 1573],
eqmod 
input_polynomial * input_polynomial
(
mem0_64*(x**0) + mem0_66*(x**1) + mem0_68*(x**2) + mem0_70*(x**3)
)
[3329, x**4 - 1197],
eqmod 
input_polynomial * input_polynomial
(
mem0_72*(x**0) + mem0_74*(x**1) + mem0_76*(x**2) + mem0_78*(x**3)
)
[3329, x**4 - 2132],
eqmod 
input_polynomial * input_polynomial
(
mem0_80*(x**0) + mem0_82*(x**1) + mem0_84*(x**2) + mem0_86*(x**3)
)
[3329, x**4 - 2304],
eqmod 
input_polynomial * input_polynomial
(
mem0_88*(x**0) + mem0_90*(x**1) + mem0_92*(x**2) + mem0_94*(x**3)
)
[3329, x**4 - 1025],
eqmod 
input_polynomial * input_polynomial
(
mem0_96*(x**0) + mem0_98*(x**1) + mem0_100*(x**2) + mem0_102*(x**3)
)
[3329, x**4 - 2277],
eqmod 
input_polynomial * input_polynomial
(
mem0_104*(x**0) + mem0_106*(x**1) + mem0_108*(x**2) + mem0_110*(x**3)
)
[3329, x**4 - 1052],
eqmod 
input_polynomial * input_polynomial
(
mem0_112*(x**0) + mem0_114*(x**1) + mem0_116*(x**2) + mem0_118*(x**3)
)
[3329, x**4 - 2055],
eqmod 
input_polynomial * input_polynomial
(
mem0_120*(x**0) + mem0_122*(x**1) + mem0_124*(x**2) + mem0_126*(x**3)
)
[3329, x**4 - 1274],
eqmod 
input_polynomial * input_polynomial
(
mem0_128*(x**0) + mem0_130*(x**1) + mem0_132*(x**2) + mem0_134*(x**3)
)
[3329, x**4 - 650],
eqmod 
input_polynomial * input_polynomial
(
mem0_136*(x**0) + mem0_138*(x**1) + mem0_140*(x**2) + mem0_142*(x**3)
)
[3329, x**4 - 2679],
eqmod 
input_polynomial * input_polynomial
(
mem0_144*(x**0) + mem0_146*(x**1) + mem0_148*(x**2) + mem0_150*(x**3)
)
[3329, x**4 - 1977],
eqmod 
input_polynomial * input_polynomial
(
mem0_152*(x**0) + mem0_154*(x**1) + mem0_156*(x**2) + mem0_158*(x**3)
)
[3329, x**4 - 1352],
eqmod 
input_polynomial * input_polynomial
(
mem0_160*(x**0) + mem0_162*(x**1) + mem0_164*(x**2) + mem0_166*(x**3)
)
[3329, x**4 - 2513],
eqmod 
input_polynomial * input_polynomial
(
mem0_168*(x**0) + mem0_170*(x**1) + mem0_172*(x**2) + mem0_174*(x**3)
)
[3329, x**4 - 816],
eqmod 
input_polynomial * input_polynomial
(
mem0_176*(x**0) + mem0_178*(x**1) + mem0_180*(x**2) + mem0_182*(x**3)
)
[3329, x**4 - 632],
eqmod 
input_polynomial * input_polynomial
(
mem0_184*(x**0) + mem0_186*(x**1) + mem0_188*(x**2) + mem0_190*(x**3)
)
[3329, x**4 - 2697],
eqmod 
input_polynomial * input_polynomial
(
mem0_192*(x**0) + mem0_194*(x**1) + mem0_196*(x**2) + mem0_198*(x**3)
)
[3329, x**4 - 2865],
eqmod 
input_polynomial * input_polynomial
(
mem0_200*(x**0) + mem0_202*(x**1) + mem0_204*(x**2) + mem0_206*(x**3)
)
[3329, x**4 - 464],
eqmod 
input_polynomial * input_polynomial
(
mem0_208*(x**0) + mem0_210*(x**1) + mem0_212*(x**2) + mem0_214*(x**3)
)
[3329, x**4 - 33],
eqmod 
input_polynomial * input_polynomial
(
mem0_216*(x**0) + mem0_218*(x**1) + mem0_220*(x**2) + mem0_222*(x**3)
)
[3329, x**4 - 3296],
eqmod 
input_polynomial * input_polynomial
(
mem0_224*(x**0) + mem0_226*(x**1) + mem0_228*(x**2) + mem0_230*(x**3)
)
[3329, x**4 - 1320],
eqmod 
input_polynomial * input_polynomial
(
mem0_232*(x**0) + mem0_234*(x**1) + mem0_236*(x**2) + mem0_238*(x**3)
)
[3329, x**4 - 2009],
eqmod 
input_polynomial * input_polynomial
(
mem0_240*(x**0) + mem0_242*(x**1) + mem0_244*(x**2) + mem0_246*(x**3)
)
[3329, x**4 - 1915],
eqmod 
input_polynomial * input_polynomial
(
mem0_248*(x**0) + mem0_250*(x**1) + mem0_252*(x**2) + mem0_254*(x**3)
)
[3329, x**4 - 1414],
eqmod 
input_polynomial * input_polynomial
(
mem0_256*(x**0) + mem0_258*(x**1) + mem0_260*(x**2) + mem0_262*(x**3)
)
[3329, x**4 - 2319],
eqmod 
input_polynomial * input_polynomial
(
mem0_264*(x**0) + mem0_266*(x**1) + mem0_268*(x**2) + mem0_270*(x**3)
)
[3329, x**4 - 1010],
eqmod 
input_polynomial * input_polynomial
(
mem0_272*(x**0) + mem0_274*(x**1) + mem0_276*(x**2) + mem0_278*(x**3)
)
[3329, x**4 - 1435],
eqmod 
input_polynomial * input_polynomial
(
mem0_280*(x**0) + mem0_282*(x**1) + mem0_284*(x**2) + mem0_286*(x**3)
)
[3329, x**4 - 1894],
eqmod 
input_polynomial * input_polynomial
(
mem0_288*(x**0) + mem0_290*(x**1) + mem0_292*(x**2) + mem0_294*(x**3)
)
[3329, x**4 - 807],
eqmod 
input_polynomial * input_polynomial
(
mem0_296*(x**0) + mem0_298*(x**1) + mem0_300*(x**2) + mem0_302*(x**3)
)
[3329, x**4 - 2522],
eqmod 
input_polynomial * input_polynomial
(
mem0_304*(x**0) + mem0_306*(x**1) + mem0_308*(x**2) + mem0_310*(x**3)
)
[3329, x**4 - 452],
eqmod 
input_polynomial * input_polynomial
(
mem0_312*(x**0) + mem0_314*(x**1) + mem0_316*(x**2) + mem0_318*(x**3)
)
[3329, x**4 - 2877],
eqmod 
input_polynomial * input_polynomial
(
mem0_320*(x**0) + mem0_322*(x**1) + mem0_324*(x**2) + mem0_326*(x**3)
)
[3329, x**4 - 1438],
eqmod 
input_polynomial * input_polynomial
(
mem0_328*(x**0) + mem0_330*(x**1) + mem0_332*(x**2) + mem0_334*(x**3)
)
[3329, x**4 - 1891],
eqmod 
input_polynomial * input_polynomial
(
mem0_336*(x**0) + mem0_338*(x**1) + mem0_340*(x**2) + mem0_342*(x**3)
)
[3329, x**4 - 2868],
eqmod 
input_polynomial * input_polynomial
(
mem0_344*(x**0) + mem0_346*(x**1) + mem0_348*(x**2) + mem0_350*(x**3)
)
[3329, x**4 - 461],
eqmod 
input_polynomial * input_polynomial
(
mem0_352*(x**0) + mem0_354*(x**1) + mem0_356*(x**2) + mem0_358*(x**3)
)
[3329, x**4 - 1534],
eqmod 
input_polynomial * input_polynomial
(
mem0_360*(x**0) + mem0_362*(x**1) + mem0_364*(x**2) + mem0_366*(x**3)
)
[3329, x**4 - 1795],
eqmod 
input_polynomial * input_polynomial
(
mem0_368*(x**0) + mem0_370*(x**1) + mem0_372*(x**2) + mem0_374*(x**3)
)
[3329, x**4 - 2402],
eqmod 
input_polynomial * input_polynomial
(
mem0_376*(x**0) + mem0_378*(x**1) + mem0_380*(x**2) + mem0_382*(x**3)
)
[3329, x**4 - 927],
eqmod 
input_polynomial * input_polynomial
(
mem0_384*(x**0) + mem0_386*(x**1) + mem0_388*(x**2) + mem0_390*(x**3)
)
[3329, x**4 - 2647],
eqmod 
input_polynomial * input_polynomial
(
mem0_392*(x**0) + mem0_394*(x**1) + mem0_396*(x**2) + mem0_398*(x**3)
)
[3329, x**4 - 682],
eqmod 
input_polynomial * input_polynomial
(
mem0_400*(x**0) + mem0_402*(x**1) + mem0_404*(x**2) + mem0_406*(x**3)
)
[3329, x**4 - 2617],
eqmod 
input_polynomial * input_polynomial
(
mem0_408*(x**0) + mem0_410*(x**1) + mem0_412*(x**2) + mem0_414*(x**3)
)
[3329, x**4 - 712],
eqmod 
input_polynomial * input_polynomial
(
mem0_416*(x**0) + mem0_418*(x**1) + mem0_420*(x**2) + mem0_422*(x**3)
)
[3329, x**4 - 1481],
eqmod 
input_polynomial * input_polynomial
(
mem0_424*(x**0) + mem0_426*(x**1) + mem0_428*(x**2) + mem0_430*(x**3)
)
[3329, x**4 - 1848],
eqmod 
input_polynomial * input_polynomial
(
mem0_432*(x**0) + mem0_434*(x**1) + mem0_436*(x**2) + mem0_438*(x**3)
)
[3329, x**4 - 648],
eqmod 
input_polynomial * input_polynomial
(
mem0_440*(x**0) + mem0_442*(x**1) + mem0_444*(x**2) + mem0_446*(x**3)
)
[3329, x**4 - 2681],
eqmod 
input_polynomial * input_polynomial
(
mem0_448*(x**0) + mem0_450*(x**1) + mem0_452*(x**2) + mem0_454*(x**3)
)
[3329, x**4 - 2474],
eqmod 
input_polynomial * input_polynomial
(
mem0_456*(x**0) + mem0_458*(x**1) + mem0_460*(x**2) + mem0_462*(x**3)
)
[3329, x**4 - 855],
eqmod 
input_polynomial * input_polynomial
(
mem0_464*(x**0) + mem0_466*(x**1) + mem0_468*(x**2) + mem0_470*(x**3)
)
[3329, x**4 - 3110],
eqmod 
input_polynomial * input_polynomial
(
mem0_472*(x**0) + mem0_474*(x**1) + mem0_476*(x**2) + mem0_478*(x**3)
)
[3329, x**4 - 219],
eqmod 
input_polynomial * input_polynomial
(
mem0_480*(x**0) + mem0_482*(x**1) + mem0_484*(x**2) + mem0_486*(x**3)
)
[3329, x**4 - 1227],
eqmod 
input_polynomial * input_polynomial
(
mem0_488*(x**0) + mem0_490*(x**1) + mem0_492*(x**2) + mem0_494*(x**3)
)
[3329, x**4 - 2102],
eqmod 
input_polynomial * input_polynomial
(
mem0_496*(x**0) + mem0_498*(x**1) + mem0_500*(x**2) + mem0_502*(x**3)
)
[3329, x**4 - 910],
eqmod 
input_polynomial * input_polynomial
(
mem0_504*(x**0) + mem0_506*(x**1) + mem0_508*(x**2) + mem0_510*(x**3)
)
[3329, x**4 - 2419]
] && and [
   (-8)@16 * 3329@16 <s mem0_0, mem0_0 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_2, mem0_2 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_4, mem0_4 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_6, mem0_6 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_8, mem0_8 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_10, mem0_10 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_12, mem0_12 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_14, mem0_14 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_16, mem0_16 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_18, mem0_18 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_20, mem0_20 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_22, mem0_22 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_24, mem0_24 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_26, mem0_26 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_28, mem0_28 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_30, mem0_30 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_32, mem0_32 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_34, mem0_34 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_36, mem0_36 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_38, mem0_38 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_40, mem0_40 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_42, mem0_42 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_44, mem0_44 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_46, mem0_46 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_48, mem0_48 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_50, mem0_50 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_52, mem0_52 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_54, mem0_54 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_56, mem0_56 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_58, mem0_58 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_60, mem0_60 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_62, mem0_62 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_64, mem0_64 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_66, mem0_66 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_68, mem0_68 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_70, mem0_70 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_72, mem0_72 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_74, mem0_74 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_76, mem0_76 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_78, mem0_78 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_80, mem0_80 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_82, mem0_82 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_84, mem0_84 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_86, mem0_86 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_88, mem0_88 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_90, mem0_90 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_92, mem0_92 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_94, mem0_94 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_96, mem0_96 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_98, mem0_98 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_100, mem0_100 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_102, mem0_102 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_104, mem0_104 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_106, mem0_106 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_108, mem0_108 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_110, mem0_110 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_112, mem0_112 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_114, mem0_114 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_116, mem0_116 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_118, mem0_118 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_120, mem0_120 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_122, mem0_122 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_124, mem0_124 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_126, mem0_126 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_128, mem0_128 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_130, mem0_130 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_132, mem0_132 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_134, mem0_134 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_136, mem0_136 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_138, mem0_138 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_140, mem0_140 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_142, mem0_142 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_144, mem0_144 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_146, mem0_146 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_148, mem0_148 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_150, mem0_150 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_152, mem0_152 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_154, mem0_154 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_156, mem0_156 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_158, mem0_158 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_160, mem0_160 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_162, mem0_162 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_164, mem0_164 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_166, mem0_166 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_168, mem0_168 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_170, mem0_170 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_172, mem0_172 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_174, mem0_174 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_176, mem0_176 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_178, mem0_178 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_180, mem0_180 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_182, mem0_182 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_184, mem0_184 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_186, mem0_186 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_188, mem0_188 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_190, mem0_190 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_192, mem0_192 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_194, mem0_194 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_196, mem0_196 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_198, mem0_198 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_200, mem0_200 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_202, mem0_202 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_204, mem0_204 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_206, mem0_206 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_208, mem0_208 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_210, mem0_210 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_212, mem0_212 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_214, mem0_214 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_216, mem0_216 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_218, mem0_218 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_220, mem0_220 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_222, mem0_222 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_224, mem0_224 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_226, mem0_226 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_228, mem0_228 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_230, mem0_230 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_232, mem0_232 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_234, mem0_234 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_236, mem0_236 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_238, mem0_238 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_240, mem0_240 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_242, mem0_242 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_244, mem0_244 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_246, mem0_246 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_248, mem0_248 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_250, mem0_250 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_252, mem0_252 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_254, mem0_254 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_256, mem0_256 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_258, mem0_258 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_260, mem0_260 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_262, mem0_262 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_264, mem0_264 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_266, mem0_266 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_268, mem0_268 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_270, mem0_270 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_272, mem0_272 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_274, mem0_274 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_276, mem0_276 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_278, mem0_278 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_280, mem0_280 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_282, mem0_282 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_284, mem0_284 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_286, mem0_286 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_288, mem0_288 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_290, mem0_290 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_292, mem0_292 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_294, mem0_294 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_296, mem0_296 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_298, mem0_298 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_300, mem0_300 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_302, mem0_302 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_304, mem0_304 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_306, mem0_306 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_308, mem0_308 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_310, mem0_310 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_312, mem0_312 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_314, mem0_314 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_316, mem0_316 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_318, mem0_318 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_320, mem0_320 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_322, mem0_322 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_324, mem0_324 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_326, mem0_326 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_328, mem0_328 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_330, mem0_330 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_332, mem0_332 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_334, mem0_334 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_336, mem0_336 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_338, mem0_338 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_340, mem0_340 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_342, mem0_342 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_344, mem0_344 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_346, mem0_346 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_348, mem0_348 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_350, mem0_350 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_352, mem0_352 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_354, mem0_354 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_356, mem0_356 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_358, mem0_358 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_360, mem0_360 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_362, mem0_362 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_364, mem0_364 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_366, mem0_366 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_368, mem0_368 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_370, mem0_370 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_372, mem0_372 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_374, mem0_374 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_376, mem0_376 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_378, mem0_378 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_380, mem0_380 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_382, mem0_382 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_384, mem0_384 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_386, mem0_386 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_388, mem0_388 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_390, mem0_390 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_392, mem0_392 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_394, mem0_394 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_396, mem0_396 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_398, mem0_398 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_400, mem0_400 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_402, mem0_402 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_404, mem0_404 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_406, mem0_406 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_408, mem0_408 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_410, mem0_410 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_412, mem0_412 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_414, mem0_414 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_416, mem0_416 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_418, mem0_418 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_420, mem0_420 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_422, mem0_422 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_424, mem0_424 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_426, mem0_426 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_428, mem0_428 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_430, mem0_430 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_432, mem0_432 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_434, mem0_434 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_436, mem0_436 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_438, mem0_438 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_440, mem0_440 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_442, mem0_442 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_444, mem0_444 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_446, mem0_446 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_448, mem0_448 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_450, mem0_450 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_452, mem0_452 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_454, mem0_454 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_456, mem0_456 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_458, mem0_458 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_460, mem0_460 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_462, mem0_462 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_464, mem0_464 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_466, mem0_466 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_468, mem0_468 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_470, mem0_470 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_472, mem0_472 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_474, mem0_474 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_476, mem0_476 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_478, mem0_478 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_480, mem0_480 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_482, mem0_482 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_484, mem0_484 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_486, mem0_486 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_488, mem0_488 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_490, mem0_490 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_492, mem0_492 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_494, mem0_494 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_496, mem0_496 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_498, mem0_498 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_500, mem0_500 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_502, mem0_502 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_504, mem0_504 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_506, mem0_506 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_508, mem0_508 <s 8@16 * 3329@16,
   (-8)@16 * 3329@16 <s mem0_510, mem0_510 <s 8@16 * 3329@16
];

(* NOTE: k = 64 *)

(*   %arrayidx9.6 = getelementptr inbounds i16, i16* %r, i64 2 *)
(*   %1536 = load i16, i16* %arrayidx9.6, align 2, !tbaa !3 *)
mov v1536 mem0_4;
(*   %conv1.i.6 = sext i16 %1536 to i32 *)
cast v_conv1_i_6@sint32 v1536@sint16;
(*   %mul.i.6 = mul nsw i32 %conv1.i.6, -1103 *)
mul v_mul_i_6 v_conv1_i_6 (-1103)@sint32;
(*   %call.i.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6, v_call_i_6);
(*   %1537 = load i16, i16* %r, align 2, !tbaa !3 *)
mov v1537 mem0_0;
(*   %sub.6 = sub i16 %1537, %call.i.6 *)
sub v_sub_6 v1537 v_call_i_6;
(*   store i16 %sub.6, i16* %arrayidx9.6, align 2, !tbaa !3 *)
mov mem0_4 v_sub_6;
(*   %add21.6 = add i16 %1537, %call.i.6 *)
add v_add21_6 v1537 v_call_i_6;
(*   store i16 %add21.6, i16* %r, align 2, !tbaa !3 *)
mov mem0_0 v_add21_6;
(*   %arrayidx9.6.1 = getelementptr inbounds i16, i16* %r, i64 3 *)
(*   %1538 = load i16, i16* %arrayidx9.6.1, align 2, !tbaa !3 *)
mov v1538 mem0_6;
(*   %conv1.i.6.1 = sext i16 %1538 to i32 *)
cast v_conv1_i_6_1@sint32 v1538@sint16;
(*   %mul.i.6.1 = mul nsw i32 %conv1.i.6.1, -1103 *)
mul v_mul_i_6_1 v_conv1_i_6_1 (-1103)@sint32;
(*   %call.i.6.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1, v_call_i_6_1);
(*   %arrayidx11.6.1 = getelementptr inbounds i16, i16* %r, i64 1 *)
(*   %1539 = load i16, i16* %arrayidx11.6.1, align 2, !tbaa !3 *)
mov v1539 mem0_2;
(*   %sub.6.1 = sub i16 %1539, %call.i.6.1 *)
sub v_sub_6_1 v1539 v_call_i_6_1;
(*   store i16 %sub.6.1, i16* %arrayidx9.6.1, align 2, !tbaa !3 *)
mov mem0_6 v_sub_6_1;
(*   %add21.6.1 = add i16 %1539, %call.i.6.1 *)
add v_add21_6_1 v1539 v_call_i_6_1;
(*   store i16 %add21.6.1, i16* %arrayidx11.6.1, align 2, !tbaa !3 *)
mov mem0_2 v_add21_6_1;

(* NOTE: k = 65 *)

(*   %arrayidx9.6.168 = getelementptr inbounds i16, i16* %r, i64 6 *)
(*   %1540 = load i16, i16* %arrayidx9.6.168, align 2, !tbaa !3 *)
mov v1540 mem0_12;
(*   %conv1.i.6.169 = sext i16 %1540 to i32 *)
cast v_conv1_i_6_169@sint32 v1540@sint16;
(*   %mul.i.6.170 = mul nsw i32 %conv1.i.6.169, 430 *)
mul v_mul_i_6_170 v_conv1_i_6_169 (430)@sint32;
(*   %call.i.6.171 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.170) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_170, v_call_i_6_171);
(*   %arrayidx11.6.172 = getelementptr inbounds i16, i16* %r, i64 4 *)
(*   %1541 = load i16, i16* %arrayidx11.6.172, align 2, !tbaa !3 *)
mov v1541 mem0_8;
(*   %sub.6.173 = sub i16 %1541, %call.i.6.171 *)
sub v_sub_6_173 v1541 v_call_i_6_171;
(*   store i16 %sub.6.173, i16* %arrayidx9.6.168, align 2, !tbaa !3 *)
mov mem0_12 v_sub_6_173;
(*   %add21.6.174 = add i16 %1541, %call.i.6.171 *)
add v_add21_6_174 v1541 v_call_i_6_171;
(*   store i16 %add21.6.174, i16* %arrayidx11.6.172, align 2, !tbaa !3 *)
mov mem0_8 v_add21_6_174;
(*   %arrayidx9.6.1.1 = getelementptr inbounds i16, i16* %r, i64 7 *)
(*   %1542 = load i16, i16* %arrayidx9.6.1.1, align 2, !tbaa !3 *)
mov v1542 mem0_14;
(*   %conv1.i.6.1.1 = sext i16 %1542 to i32 *)
cast v_conv1_i_6_1_1@sint32 v1542@sint16;
(*   %mul.i.6.1.1 = mul nsw i32 %conv1.i.6.1.1, 430 *)
mul v_mul_i_6_1_1 v_conv1_i_6_1_1 (430)@sint32;
(*   %call.i.6.1.1 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.1) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_1, v_call_i_6_1_1);
(*   %arrayidx11.6.1.1 = getelementptr inbounds i16, i16* %r, i64 5 *)
(*   %1543 = load i16, i16* %arrayidx11.6.1.1, align 2, !tbaa !3 *)
mov v1543 mem0_10;
(*   %sub.6.1.1 = sub i16 %1543, %call.i.6.1.1 *)
sub v_sub_6_1_1 v1543 v_call_i_6_1_1;
(*   store i16 %sub.6.1.1, i16* %arrayidx9.6.1.1, align 2, !tbaa !3 *)
mov mem0_14 v_sub_6_1_1;
(*   %add21.6.1.1 = add i16 %1543, %call.i.6.1.1 *)
add v_add21_6_1_1 v1543 v_call_i_6_1_1;
(*   store i16 %add21.6.1.1, i16* %arrayidx11.6.1.1, align 2, !tbaa !3 *)
mov mem0_10 v_add21_6_1_1;

(* NOTE: k = 66 *)

(*   %arrayidx9.6.2 = getelementptr inbounds i16, i16* %r, i64 10 *)
(*   %1544 = load i16, i16* %arrayidx9.6.2, align 2, !tbaa !3 *)
mov v1544 mem0_20;
(*   %conv1.i.6.2 = sext i16 %1544 to i32 *)
cast v_conv1_i_6_2@sint32 v1544@sint16;
(*   %mul.i.6.2 = mul nsw i32 %conv1.i.6.2, 555 *)
mul v_mul_i_6_2 v_conv1_i_6_2 (555)@sint32;
(*   %call.i.6.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_2, v_call_i_6_2);
(*   %arrayidx11.6.2 = getelementptr inbounds i16, i16* %r, i64 8 *)
(*   %1545 = load i16, i16* %arrayidx11.6.2, align 2, !tbaa !3 *)
mov v1545 mem0_16;
(*   %sub.6.2 = sub i16 %1545, %call.i.6.2 *)
sub v_sub_6_2 v1545 v_call_i_6_2;
(*   store i16 %sub.6.2, i16* %arrayidx9.6.2, align 2, !tbaa !3 *)
mov mem0_20 v_sub_6_2;
(*   %add21.6.2 = add i16 %1545, %call.i.6.2 *)
add v_add21_6_2 v1545 v_call_i_6_2;
(*   store i16 %add21.6.2, i16* %arrayidx11.6.2, align 2, !tbaa !3 *)
mov mem0_16 v_add21_6_2;
(*   %arrayidx9.6.1.2 = getelementptr inbounds i16, i16* %r, i64 11 *)
(*   %1546 = load i16, i16* %arrayidx9.6.1.2, align 2, !tbaa !3 *)
mov v1546 mem0_22;
(*   %conv1.i.6.1.2 = sext i16 %1546 to i32 *)
cast v_conv1_i_6_1_2@sint32 v1546@sint16;
(*   %mul.i.6.1.2 = mul nsw i32 %conv1.i.6.1.2, 555 *)
mul v_mul_i_6_1_2 v_conv1_i_6_1_2 (555)@sint32;
(*   %call.i.6.1.2 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.2) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_2, v_call_i_6_1_2);
(*   %arrayidx11.6.1.2 = getelementptr inbounds i16, i16* %r, i64 9 *)
(*   %1547 = load i16, i16* %arrayidx11.6.1.2, align 2, !tbaa !3 *)
mov v1547 mem0_18;
(*   %sub.6.1.2 = sub i16 %1547, %call.i.6.1.2 *)
sub v_sub_6_1_2 v1547 v_call_i_6_1_2;
(*   store i16 %sub.6.1.2, i16* %arrayidx9.6.1.2, align 2, !tbaa !3 *)
mov mem0_22 v_sub_6_1_2;
(*   %add21.6.1.2 = add i16 %1547, %call.i.6.1.2 *)
add v_add21_6_1_2 v1547 v_call_i_6_1_2;
(*   store i16 %add21.6.1.2, i16* %arrayidx11.6.1.2, align 2, !tbaa !3 *)
mov mem0_18 v_add21_6_1_2;

(* NOTE: k = 67 *)

(*   %arrayidx9.6.3 = getelementptr inbounds i16, i16* %r, i64 14 *)
(*   %1548 = load i16, i16* %arrayidx9.6.3, align 2, !tbaa !3 *)
mov v1548 mem0_28;
(*   %conv1.i.6.3 = sext i16 %1548 to i32 *)
cast v_conv1_i_6_3@sint32 v1548@sint16;
(*   %mul.i.6.3 = mul nsw i32 %conv1.i.6.3, 843 *)
mul v_mul_i_6_3 v_conv1_i_6_3 (843)@sint32;
(*   %call.i.6.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_3, v_call_i_6_3);
(*   %arrayidx11.6.3 = getelementptr inbounds i16, i16* %r, i64 12 *)
(*   %1549 = load i16, i16* %arrayidx11.6.3, align 2, !tbaa !3 *)
mov v1549 mem0_24;
(*   %sub.6.3 = sub i16 %1549, %call.i.6.3 *)
sub v_sub_6_3 v1549 v_call_i_6_3;
(*   store i16 %sub.6.3, i16* %arrayidx9.6.3, align 2, !tbaa !3 *)
mov mem0_28 v_sub_6_3;
(*   %add21.6.3 = add i16 %1549, %call.i.6.3 *)
add v_add21_6_3 v1549 v_call_i_6_3;
(*   store i16 %add21.6.3, i16* %arrayidx11.6.3, align 2, !tbaa !3 *)
mov mem0_24 v_add21_6_3;
(*   %arrayidx9.6.1.3 = getelementptr inbounds i16, i16* %r, i64 15 *)
(*   %1550 = load i16, i16* %arrayidx9.6.1.3, align 2, !tbaa !3 *)
mov v1550 mem0_30;
(*   %conv1.i.6.1.3 = sext i16 %1550 to i32 *)
cast v_conv1_i_6_1_3@sint32 v1550@sint16;
(*   %mul.i.6.1.3 = mul nsw i32 %conv1.i.6.1.3, 843 *)
mul v_mul_i_6_1_3 v_conv1_i_6_1_3 (843)@sint32;
(*   %call.i.6.1.3 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.3) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_3, v_call_i_6_1_3);
(*   %arrayidx11.6.1.3 = getelementptr inbounds i16, i16* %r, i64 13 *)
(*   %1551 = load i16, i16* %arrayidx11.6.1.3, align 2, !tbaa !3 *)
mov v1551 mem0_26;
(*   %sub.6.1.3 = sub i16 %1551, %call.i.6.1.3 *)
sub v_sub_6_1_3 v1551 v_call_i_6_1_3;
(*   store i16 %sub.6.1.3, i16* %arrayidx9.6.1.3, align 2, !tbaa !3 *)
mov mem0_30 v_sub_6_1_3;
(*   %add21.6.1.3 = add i16 %1551, %call.i.6.1.3 *)
add v_add21_6_1_3 v1551 v_call_i_6_1_3;
(*   store i16 %add21.6.1.3, i16* %arrayidx11.6.1.3, align 2, !tbaa !3 *)
mov mem0_26 v_add21_6_1_3;

(* NOTE: k = 68 *)

(*   %arrayidx9.6.4 = getelementptr inbounds i16, i16* %r, i64 18 *)
(*   %1552 = load i16, i16* %arrayidx9.6.4, align 2, !tbaa !3 *)
mov v1552 mem0_36;
(*   %conv1.i.6.4 = sext i16 %1552 to i32 *)
cast v_conv1_i_6_4@sint32 v1552@sint16;
(*   %mul.i.6.4 = mul nsw i32 %conv1.i.6.4, -1251 *)
mul v_mul_i_6_4 v_conv1_i_6_4 (-1251)@sint32;
(*   %call.i.6.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_4, v_call_i_6_4);
(*   %arrayidx11.6.4 = getelementptr inbounds i16, i16* %r, i64 16 *)
(*   %1553 = load i16, i16* %arrayidx11.6.4, align 2, !tbaa !3 *)
mov v1553 mem0_32;
(*   %sub.6.4 = sub i16 %1553, %call.i.6.4 *)
sub v_sub_6_4 v1553 v_call_i_6_4;
(*   store i16 %sub.6.4, i16* %arrayidx9.6.4, align 2, !tbaa !3 *)
mov mem0_36 v_sub_6_4;
(*   %add21.6.4 = add i16 %1553, %call.i.6.4 *)
add v_add21_6_4 v1553 v_call_i_6_4;
(*   store i16 %add21.6.4, i16* %arrayidx11.6.4, align 2, !tbaa !3 *)
mov mem0_32 v_add21_6_4;
(*   %arrayidx9.6.1.4 = getelementptr inbounds i16, i16* %r, i64 19 *)
(*   %1554 = load i16, i16* %arrayidx9.6.1.4, align 2, !tbaa !3 *)
mov v1554 mem0_38;
(*   %conv1.i.6.1.4 = sext i16 %1554 to i32 *)
cast v_conv1_i_6_1_4@sint32 v1554@sint16;
(*   %mul.i.6.1.4 = mul nsw i32 %conv1.i.6.1.4, -1251 *)
mul v_mul_i_6_1_4 v_conv1_i_6_1_4 (-1251)@sint32;
(*   %call.i.6.1.4 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.4) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_4, v_call_i_6_1_4);
(*   %arrayidx11.6.1.4 = getelementptr inbounds i16, i16* %r, i64 17 *)
(*   %1555 = load i16, i16* %arrayidx11.6.1.4, align 2, !tbaa !3 *)
mov v1555 mem0_34;
(*   %sub.6.1.4 = sub i16 %1555, %call.i.6.1.4 *)
sub v_sub_6_1_4 v1555 v_call_i_6_1_4;
(*   store i16 %sub.6.1.4, i16* %arrayidx9.6.1.4, align 2, !tbaa !3 *)
mov mem0_38 v_sub_6_1_4;
(*   %add21.6.1.4 = add i16 %1555, %call.i.6.1.4 *)
add v_add21_6_1_4 v1555 v_call_i_6_1_4;
(*   store i16 %add21.6.1.4, i16* %arrayidx11.6.1.4, align 2, !tbaa !3 *)
mov mem0_34 v_add21_6_1_4;

(* NOTE: k = 69 *)

(*   %arrayidx9.6.5 = getelementptr inbounds i16, i16* %r, i64 22 *)
(*   %1556 = load i16, i16* %arrayidx9.6.5, align 2, !tbaa !3 *)
mov v1556 mem0_44;
(*   %conv1.i.6.5 = sext i16 %1556 to i32 *)
cast v_conv1_i_6_5@sint32 v1556@sint16;
(*   %mul.i.6.5 = mul nsw i32 %conv1.i.6.5, 871 *)
mul v_mul_i_6_5 v_conv1_i_6_5 (871)@sint32;
(*   %call.i.6.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_5, v_call_i_6_5);
(*   %arrayidx11.6.5 = getelementptr inbounds i16, i16* %r, i64 20 *)
(*   %1557 = load i16, i16* %arrayidx11.6.5, align 2, !tbaa !3 *)
mov v1557 mem0_40;
(*   %sub.6.5 = sub i16 %1557, %call.i.6.5 *)
sub v_sub_6_5 v1557 v_call_i_6_5;
(*   store i16 %sub.6.5, i16* %arrayidx9.6.5, align 2, !tbaa !3 *)
mov mem0_44 v_sub_6_5;
(*   %add21.6.5 = add i16 %1557, %call.i.6.5 *)
add v_add21_6_5 v1557 v_call_i_6_5;
(*   store i16 %add21.6.5, i16* %arrayidx11.6.5, align 2, !tbaa !3 *)
mov mem0_40 v_add21_6_5;
(*   %arrayidx9.6.1.5 = getelementptr inbounds i16, i16* %r, i64 23 *)
(*   %1558 = load i16, i16* %arrayidx9.6.1.5, align 2, !tbaa !3 *)
mov v1558 mem0_46;
(*   %conv1.i.6.1.5 = sext i16 %1558 to i32 *)
cast v_conv1_i_6_1_5@sint32 v1558@sint16;
(*   %mul.i.6.1.5 = mul nsw i32 %conv1.i.6.1.5, 871 *)
mul v_mul_i_6_1_5 v_conv1_i_6_1_5 (871)@sint32;
(*   %call.i.6.1.5 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.5) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_5, v_call_i_6_1_5);
(*   %arrayidx11.6.1.5 = getelementptr inbounds i16, i16* %r, i64 21 *)
(*   %1559 = load i16, i16* %arrayidx11.6.1.5, align 2, !tbaa !3 *)
mov v1559 mem0_42;
(*   %sub.6.1.5 = sub i16 %1559, %call.i.6.1.5 *)
sub v_sub_6_1_5 v1559 v_call_i_6_1_5;
(*   store i16 %sub.6.1.5, i16* %arrayidx9.6.1.5, align 2, !tbaa !3 *)
mov mem0_46 v_sub_6_1_5;
(*   %add21.6.1.5 = add i16 %1559, %call.i.6.1.5 *)
add v_add21_6_1_5 v1559 v_call_i_6_1_5;
(*   store i16 %add21.6.1.5, i16* %arrayidx11.6.1.5, align 2, !tbaa !3 *)
mov mem0_42 v_add21_6_1_5;

(* NOTE: k = 70 *)

(*   %arrayidx9.6.6 = getelementptr inbounds i16, i16* %r, i64 26 *)
(*   %1560 = load i16, i16* %arrayidx9.6.6, align 2, !tbaa !3 *)
mov v1560 mem0_52;
(*   %conv1.i.6.6 = sext i16 %1560 to i32 *)
cast v_conv1_i_6_6@sint32 v1560@sint16;
(*   %mul.i.6.6 = mul nsw i32 %conv1.i.6.6, 1550 *)
mul v_mul_i_6_6 v_conv1_i_6_6 (1550)@sint32;
(*   %call.i.6.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_6, v_call_i_6_6);
(*   %arrayidx11.6.6 = getelementptr inbounds i16, i16* %r, i64 24 *)
(*   %1561 = load i16, i16* %arrayidx11.6.6, align 2, !tbaa !3 *)
mov v1561 mem0_48;
(*   %sub.6.6 = sub i16 %1561, %call.i.6.6 *)
sub v_sub_6_6 v1561 v_call_i_6_6;
(*   store i16 %sub.6.6, i16* %arrayidx9.6.6, align 2, !tbaa !3 *)
mov mem0_52 v_sub_6_6;
(*   %add21.6.6 = add i16 %1561, %call.i.6.6 *)
add v_add21_6_6 v1561 v_call_i_6_6;
(*   store i16 %add21.6.6, i16* %arrayidx11.6.6, align 2, !tbaa !3 *)
mov mem0_48 v_add21_6_6;
(*   %arrayidx9.6.1.6 = getelementptr inbounds i16, i16* %r, i64 27 *)
(*   %1562 = load i16, i16* %arrayidx9.6.1.6, align 2, !tbaa !3 *)
mov v1562 mem0_54;
(*   %conv1.i.6.1.6 = sext i16 %1562 to i32 *)
cast v_conv1_i_6_1_6@sint32 v1562@sint16;
(*   %mul.i.6.1.6 = mul nsw i32 %conv1.i.6.1.6, 1550 *)
mul v_mul_i_6_1_6 v_conv1_i_6_1_6 (1550)@sint32;
(*   %call.i.6.1.6 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.6) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_6, v_call_i_6_1_6);
(*   %arrayidx11.6.1.6 = getelementptr inbounds i16, i16* %r, i64 25 *)
(*   %1563 = load i16, i16* %arrayidx11.6.1.6, align 2, !tbaa !3 *)
mov v1563 mem0_50;
(*   %sub.6.1.6 = sub i16 %1563, %call.i.6.1.6 *)
sub v_sub_6_1_6 v1563 v_call_i_6_1_6;
(*   store i16 %sub.6.1.6, i16* %arrayidx9.6.1.6, align 2, !tbaa !3 *)
mov mem0_54 v_sub_6_1_6;
(*   %add21.6.1.6 = add i16 %1563, %call.i.6.1.6 *)
add v_add21_6_1_6 v1563 v_call_i_6_1_6;
(*   store i16 %add21.6.1.6, i16* %arrayidx11.6.1.6, align 2, !tbaa !3 *)
mov mem0_50 v_add21_6_1_6;

(* NOTE: k = 71 *)

(*   %arrayidx9.6.7 = getelementptr inbounds i16, i16* %r, i64 30 *)
(*   %1564 = load i16, i16* %arrayidx9.6.7, align 2, !tbaa !3 *)
mov v1564 mem0_60;
(*   %conv1.i.6.7 = sext i16 %1564 to i32 *)
cast v_conv1_i_6_7@sint32 v1564@sint16;
(*   %mul.i.6.7 = mul nsw i32 %conv1.i.6.7, 105 *)
mul v_mul_i_6_7 v_conv1_i_6_7 (105)@sint32;
(*   %call.i.6.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_7, v_call_i_6_7);
(*   %arrayidx11.6.7 = getelementptr inbounds i16, i16* %r, i64 28 *)
(*   %1565 = load i16, i16* %arrayidx11.6.7, align 2, !tbaa !3 *)
mov v1565 mem0_56;
(*   %sub.6.7 = sub i16 %1565, %call.i.6.7 *)
sub v_sub_6_7 v1565 v_call_i_6_7;
(*   store i16 %sub.6.7, i16* %arrayidx9.6.7, align 2, !tbaa !3 *)
mov mem0_60 v_sub_6_7;
(*   %add21.6.7 = add i16 %1565, %call.i.6.7 *)
add v_add21_6_7 v1565 v_call_i_6_7;
(*   store i16 %add21.6.7, i16* %arrayidx11.6.7, align 2, !tbaa !3 *)
mov mem0_56 v_add21_6_7;
(*   %arrayidx9.6.1.7 = getelementptr inbounds i16, i16* %r, i64 31 *)
(*   %1566 = load i16, i16* %arrayidx9.6.1.7, align 2, !tbaa !3 *)
mov v1566 mem0_62;
(*   %conv1.i.6.1.7 = sext i16 %1566 to i32 *)
cast v_conv1_i_6_1_7@sint32 v1566@sint16;
(*   %mul.i.6.1.7 = mul nsw i32 %conv1.i.6.1.7, 105 *)
mul v_mul_i_6_1_7 v_conv1_i_6_1_7 (105)@sint32;
(*   %call.i.6.1.7 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.7) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_7, v_call_i_6_1_7);
(*   %arrayidx11.6.1.7 = getelementptr inbounds i16, i16* %r, i64 29 *)
(*   %1567 = load i16, i16* %arrayidx11.6.1.7, align 2, !tbaa !3 *)
mov v1567 mem0_58;
(*   %sub.6.1.7 = sub i16 %1567, %call.i.6.1.7 *)
sub v_sub_6_1_7 v1567 v_call_i_6_1_7;
(*   store i16 %sub.6.1.7, i16* %arrayidx9.6.1.7, align 2, !tbaa !3 *)
mov mem0_62 v_sub_6_1_7;
(*   %add21.6.1.7 = add i16 %1567, %call.i.6.1.7 *)
add v_add21_6_1_7 v1567 v_call_i_6_1_7;
(*   store i16 %add21.6.1.7, i16* %arrayidx11.6.1.7, align 2, !tbaa !3 *)
mov mem0_58 v_add21_6_1_7;

(* NOTE: k = 72 *)

(*   %arrayidx9.6.8 = getelementptr inbounds i16, i16* %r, i64 34 *)
(*   %1568 = load i16, i16* %arrayidx9.6.8, align 2, !tbaa !3 *)
mov v1568 mem0_68;
(*   %conv1.i.6.8 = sext i16 %1568 to i32 *)
cast v_conv1_i_6_8@sint32 v1568@sint16;
(*   %mul.i.6.8 = mul nsw i32 %conv1.i.6.8, 422 *)
mul v_mul_i_6_8 v_conv1_i_6_8 (422)@sint32;
(*   %call.i.6.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_8, v_call_i_6_8);
(*   %arrayidx11.6.8 = getelementptr inbounds i16, i16* %r, i64 32 *)
(*   %1569 = load i16, i16* %arrayidx11.6.8, align 2, !tbaa !3 *)
mov v1569 mem0_64;
(*   %sub.6.8 = sub i16 %1569, %call.i.6.8 *)
sub v_sub_6_8 v1569 v_call_i_6_8;
(*   store i16 %sub.6.8, i16* %arrayidx9.6.8, align 2, !tbaa !3 *)
mov mem0_68 v_sub_6_8;
(*   %add21.6.8 = add i16 %1569, %call.i.6.8 *)
add v_add21_6_8 v1569 v_call_i_6_8;
(*   store i16 %add21.6.8, i16* %arrayidx11.6.8, align 2, !tbaa !3 *)
mov mem0_64 v_add21_6_8;
(*   %arrayidx9.6.1.8 = getelementptr inbounds i16, i16* %r, i64 35 *)
(*   %1570 = load i16, i16* %arrayidx9.6.1.8, align 2, !tbaa !3 *)
mov v1570 mem0_70;
(*   %conv1.i.6.1.8 = sext i16 %1570 to i32 *)
cast v_conv1_i_6_1_8@sint32 v1570@sint16;
(*   %mul.i.6.1.8 = mul nsw i32 %conv1.i.6.1.8, 422 *)
mul v_mul_i_6_1_8 v_conv1_i_6_1_8 (422)@sint32;
(*   %call.i.6.1.8 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.8) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_8, v_call_i_6_1_8);
(*   %arrayidx11.6.1.8 = getelementptr inbounds i16, i16* %r, i64 33 *)
(*   %1571 = load i16, i16* %arrayidx11.6.1.8, align 2, !tbaa !3 *)
mov v1571 mem0_66;
(*   %sub.6.1.8 = sub i16 %1571, %call.i.6.1.8 *)
sub v_sub_6_1_8 v1571 v_call_i_6_1_8;
(*   store i16 %sub.6.1.8, i16* %arrayidx9.6.1.8, align 2, !tbaa !3 *)
mov mem0_70 v_sub_6_1_8;
(*   %add21.6.1.8 = add i16 %1571, %call.i.6.1.8 *)
add v_add21_6_1_8 v1571 v_call_i_6_1_8;
(*   store i16 %add21.6.1.8, i16* %arrayidx11.6.1.8, align 2, !tbaa !3 *)
mov mem0_66 v_add21_6_1_8;

(* NOTE: k = 73 *)

(*   %arrayidx9.6.9 = getelementptr inbounds i16, i16* %r, i64 38 *)
(*   %1572 = load i16, i16* %arrayidx9.6.9, align 2, !tbaa !3 *)
mov v1572 mem0_76;
(*   %conv1.i.6.9 = sext i16 %1572 to i32 *)
cast v_conv1_i_6_9@sint32 v1572@sint16;
(*   %mul.i.6.9 = mul nsw i32 %conv1.i.6.9, 587 *)
mul v_mul_i_6_9 v_conv1_i_6_9 (587)@sint32;
(*   %call.i.6.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_9, v_call_i_6_9);
(*   %arrayidx11.6.9 = getelementptr inbounds i16, i16* %r, i64 36 *)
(*   %1573 = load i16, i16* %arrayidx11.6.9, align 2, !tbaa !3 *)
mov v1573 mem0_72;
(*   %sub.6.9 = sub i16 %1573, %call.i.6.9 *)
sub v_sub_6_9 v1573 v_call_i_6_9;
(*   store i16 %sub.6.9, i16* %arrayidx9.6.9, align 2, !tbaa !3 *)
mov mem0_76 v_sub_6_9;
(*   %add21.6.9 = add i16 %1573, %call.i.6.9 *)
add v_add21_6_9 v1573 v_call_i_6_9;
(*   store i16 %add21.6.9, i16* %arrayidx11.6.9, align 2, !tbaa !3 *)
mov mem0_72 v_add21_6_9;
(*   %arrayidx9.6.1.9 = getelementptr inbounds i16, i16* %r, i64 39 *)
(*   %1574 = load i16, i16* %arrayidx9.6.1.9, align 2, !tbaa !3 *)
mov v1574 mem0_78;
(*   %conv1.i.6.1.9 = sext i16 %1574 to i32 *)
cast v_conv1_i_6_1_9@sint32 v1574@sint16;
(*   %mul.i.6.1.9 = mul nsw i32 %conv1.i.6.1.9, 587 *)
mul v_mul_i_6_1_9 v_conv1_i_6_1_9 (587)@sint32;
(*   %call.i.6.1.9 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.9) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_9, v_call_i_6_1_9);
(*   %arrayidx11.6.1.9 = getelementptr inbounds i16, i16* %r, i64 37 *)
(*   %1575 = load i16, i16* %arrayidx11.6.1.9, align 2, !tbaa !3 *)
mov v1575 mem0_74;
(*   %sub.6.1.9 = sub i16 %1575, %call.i.6.1.9 *)
sub v_sub_6_1_9 v1575 v_call_i_6_1_9;
(*   store i16 %sub.6.1.9, i16* %arrayidx9.6.1.9, align 2, !tbaa !3 *)
mov mem0_78 v_sub_6_1_9;
(*   %add21.6.1.9 = add i16 %1575, %call.i.6.1.9 *)
add v_add21_6_1_9 v1575 v_call_i_6_1_9;
(*   store i16 %add21.6.1.9, i16* %arrayidx11.6.1.9, align 2, !tbaa !3 *)
mov mem0_74 v_add21_6_1_9;

(* NOTE: k = 74 *)

(*   %arrayidx9.6.10 = getelementptr inbounds i16, i16* %r, i64 42 *)
(*   %1576 = load i16, i16* %arrayidx9.6.10, align 2, !tbaa !3 *)
mov v1576 mem0_84;
(*   %conv1.i.6.10 = sext i16 %1576 to i32 *)
cast v_conv1_i_6_10@sint32 v1576@sint16;
(*   %mul.i.6.10 = mul nsw i32 %conv1.i.6.10, 177 *)
mul v_mul_i_6_10 v_conv1_i_6_10 (177)@sint32;
(*   %call.i.6.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_10, v_call_i_6_10);
(*   %arrayidx11.6.10 = getelementptr inbounds i16, i16* %r, i64 40 *)
(*   %1577 = load i16, i16* %arrayidx11.6.10, align 2, !tbaa !3 *)
mov v1577 mem0_80;
(*   %sub.6.10 = sub i16 %1577, %call.i.6.10 *)
sub v_sub_6_10 v1577 v_call_i_6_10;
(*   store i16 %sub.6.10, i16* %arrayidx9.6.10, align 2, !tbaa !3 *)
mov mem0_84 v_sub_6_10;
(*   %add21.6.10 = add i16 %1577, %call.i.6.10 *)
add v_add21_6_10 v1577 v_call_i_6_10;
(*   store i16 %add21.6.10, i16* %arrayidx11.6.10, align 2, !tbaa !3 *)
mov mem0_80 v_add21_6_10;
(*   %arrayidx9.6.1.10 = getelementptr inbounds i16, i16* %r, i64 43 *)
(*   %1578 = load i16, i16* %arrayidx9.6.1.10, align 2, !tbaa !3 *)
mov v1578 mem0_86;
(*   %conv1.i.6.1.10 = sext i16 %1578 to i32 *)
cast v_conv1_i_6_1_10@sint32 v1578@sint16;
(*   %mul.i.6.1.10 = mul nsw i32 %conv1.i.6.1.10, 177 *)
mul v_mul_i_6_1_10 v_conv1_i_6_1_10 (177)@sint32;
(*   %call.i.6.1.10 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.10) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_10, v_call_i_6_1_10);
(*   %arrayidx11.6.1.10 = getelementptr inbounds i16, i16* %r, i64 41 *)
(*   %1579 = load i16, i16* %arrayidx11.6.1.10, align 2, !tbaa !3 *)
mov v1579 mem0_82;
(*   %sub.6.1.10 = sub i16 %1579, %call.i.6.1.10 *)
sub v_sub_6_1_10 v1579 v_call_i_6_1_10;
(*   store i16 %sub.6.1.10, i16* %arrayidx9.6.1.10, align 2, !tbaa !3 *)
mov mem0_86 v_sub_6_1_10;
(*   %add21.6.1.10 = add i16 %1579, %call.i.6.1.10 *)
add v_add21_6_1_10 v1579 v_call_i_6_1_10;
(*   store i16 %add21.6.1.10, i16* %arrayidx11.6.1.10, align 2, !tbaa !3 *)
mov mem0_82 v_add21_6_1_10;

(* NOTE: k = 75 *)

(*   %arrayidx9.6.11 = getelementptr inbounds i16, i16* %r, i64 46 *)
(*   %1580 = load i16, i16* %arrayidx9.6.11, align 2, !tbaa !3 *)
mov v1580 mem0_92;
(*   %conv1.i.6.11 = sext i16 %1580 to i32 *)
cast v_conv1_i_6_11@sint32 v1580@sint16;
(*   %mul.i.6.11 = mul nsw i32 %conv1.i.6.11, -235 *)
mul v_mul_i_6_11 v_conv1_i_6_11 (-235)@sint32;
(*   %call.i.6.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_11, v_call_i_6_11);
(*   %arrayidx11.6.11 = getelementptr inbounds i16, i16* %r, i64 44 *)
(*   %1581 = load i16, i16* %arrayidx11.6.11, align 2, !tbaa !3 *)
mov v1581 mem0_88;
(*   %sub.6.11 = sub i16 %1581, %call.i.6.11 *)
sub v_sub_6_11 v1581 v_call_i_6_11;
(*   store i16 %sub.6.11, i16* %arrayidx9.6.11, align 2, !tbaa !3 *)
mov mem0_92 v_sub_6_11;
(*   %add21.6.11 = add i16 %1581, %call.i.6.11 *)
add v_add21_6_11 v1581 v_call_i_6_11;
(*   store i16 %add21.6.11, i16* %arrayidx11.6.11, align 2, !tbaa !3 *)
mov mem0_88 v_add21_6_11;
(*   %arrayidx9.6.1.11 = getelementptr inbounds i16, i16* %r, i64 47 *)
(*   %1582 = load i16, i16* %arrayidx9.6.1.11, align 2, !tbaa !3 *)
mov v1582 mem0_94;
(*   %conv1.i.6.1.11 = sext i16 %1582 to i32 *)
cast v_conv1_i_6_1_11@sint32 v1582@sint16;
(*   %mul.i.6.1.11 = mul nsw i32 %conv1.i.6.1.11, -235 *)
mul v_mul_i_6_1_11 v_conv1_i_6_1_11 (-235)@sint32;
(*   %call.i.6.1.11 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.11) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_11, v_call_i_6_1_11);
(*   %arrayidx11.6.1.11 = getelementptr inbounds i16, i16* %r, i64 45 *)
(*   %1583 = load i16, i16* %arrayidx11.6.1.11, align 2, !tbaa !3 *)
mov v1583 mem0_90;
(*   %sub.6.1.11 = sub i16 %1583, %call.i.6.1.11 *)
sub v_sub_6_1_11 v1583 v_call_i_6_1_11;
(*   store i16 %sub.6.1.11, i16* %arrayidx9.6.1.11, align 2, !tbaa !3 *)
mov mem0_94 v_sub_6_1_11;
(*   %add21.6.1.11 = add i16 %1583, %call.i.6.1.11 *)
add v_add21_6_1_11 v1583 v_call_i_6_1_11;
(*   store i16 %add21.6.1.11, i16* %arrayidx11.6.1.11, align 2, !tbaa !3 *)
mov mem0_90 v_add21_6_1_11;

(* NOTE: k = 76 *)

(*   %arrayidx9.6.12 = getelementptr inbounds i16, i16* %r, i64 50 *)
(*   %1584 = load i16, i16* %arrayidx9.6.12, align 2, !tbaa !3 *)
mov v1584 mem0_100;
(*   %conv1.i.6.12 = sext i16 %1584 to i32 *)
cast v_conv1_i_6_12@sint32 v1584@sint16;
(*   %mul.i.6.12 = mul nsw i32 %conv1.i.6.12, -291 *)
mul v_mul_i_6_12 v_conv1_i_6_12 (-291)@sint32;
(*   %call.i.6.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_12, v_call_i_6_12);
(*   %arrayidx11.6.12 = getelementptr inbounds i16, i16* %r, i64 48 *)
(*   %1585 = load i16, i16* %arrayidx11.6.12, align 2, !tbaa !3 *)
mov v1585 mem0_96;
(*   %sub.6.12 = sub i16 %1585, %call.i.6.12 *)
sub v_sub_6_12 v1585 v_call_i_6_12;
(*   store i16 %sub.6.12, i16* %arrayidx9.6.12, align 2, !tbaa !3 *)
mov mem0_100 v_sub_6_12;
(*   %add21.6.12 = add i16 %1585, %call.i.6.12 *)
add v_add21_6_12 v1585 v_call_i_6_12;
(*   store i16 %add21.6.12, i16* %arrayidx11.6.12, align 2, !tbaa !3 *)
mov mem0_96 v_add21_6_12;
(*   %arrayidx9.6.1.12 = getelementptr inbounds i16, i16* %r, i64 51 *)
(*   %1586 = load i16, i16* %arrayidx9.6.1.12, align 2, !tbaa !3 *)
mov v1586 mem0_102;
(*   %conv1.i.6.1.12 = sext i16 %1586 to i32 *)
cast v_conv1_i_6_1_12@sint32 v1586@sint16;
(*   %mul.i.6.1.12 = mul nsw i32 %conv1.i.6.1.12, -291 *)
mul v_mul_i_6_1_12 v_conv1_i_6_1_12 (-291)@sint32;
(*   %call.i.6.1.12 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.12) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_12, v_call_i_6_1_12);
(*   %arrayidx11.6.1.12 = getelementptr inbounds i16, i16* %r, i64 49 *)
(*   %1587 = load i16, i16* %arrayidx11.6.1.12, align 2, !tbaa !3 *)
mov v1587 mem0_98;
(*   %sub.6.1.12 = sub i16 %1587, %call.i.6.1.12 *)
sub v_sub_6_1_12 v1587 v_call_i_6_1_12;
(*   store i16 %sub.6.1.12, i16* %arrayidx9.6.1.12, align 2, !tbaa !3 *)
mov mem0_102 v_sub_6_1_12;
(*   %add21.6.1.12 = add i16 %1587, %call.i.6.1.12 *)
add v_add21_6_1_12 v1587 v_call_i_6_1_12;
(*   store i16 %add21.6.1.12, i16* %arrayidx11.6.1.12, align 2, !tbaa !3 *)
mov mem0_98 v_add21_6_1_12;

(* NOTE: k = 77 *)

(*   %arrayidx9.6.13 = getelementptr inbounds i16, i16* %r, i64 54 *)
(*   %1588 = load i16, i16* %arrayidx9.6.13, align 2, !tbaa !3 *)
mov v1588 mem0_108;
(*   %conv1.i.6.13 = sext i16 %1588 to i32 *)
cast v_conv1_i_6_13@sint32 v1588@sint16;
(*   %mul.i.6.13 = mul nsw i32 %conv1.i.6.13, -460 *)
mul v_mul_i_6_13 v_conv1_i_6_13 (-460)@sint32;
(*   %call.i.6.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_13, v_call_i_6_13);
(*   %arrayidx11.6.13 = getelementptr inbounds i16, i16* %r, i64 52 *)
(*   %1589 = load i16, i16* %arrayidx11.6.13, align 2, !tbaa !3 *)
mov v1589 mem0_104;
(*   %sub.6.13 = sub i16 %1589, %call.i.6.13 *)
sub v_sub_6_13 v1589 v_call_i_6_13;
(*   store i16 %sub.6.13, i16* %arrayidx9.6.13, align 2, !tbaa !3 *)
mov mem0_108 v_sub_6_13;
(*   %add21.6.13 = add i16 %1589, %call.i.6.13 *)
add v_add21_6_13 v1589 v_call_i_6_13;
(*   store i16 %add21.6.13, i16* %arrayidx11.6.13, align 2, !tbaa !3 *)
mov mem0_104 v_add21_6_13;
(*   %arrayidx9.6.1.13 = getelementptr inbounds i16, i16* %r, i64 55 *)
(*   %1590 = load i16, i16* %arrayidx9.6.1.13, align 2, !tbaa !3 *)
mov v1590 mem0_110;
(*   %conv1.i.6.1.13 = sext i16 %1590 to i32 *)
cast v_conv1_i_6_1_13@sint32 v1590@sint16;
(*   %mul.i.6.1.13 = mul nsw i32 %conv1.i.6.1.13, -460 *)
mul v_mul_i_6_1_13 v_conv1_i_6_1_13 (-460)@sint32;
(*   %call.i.6.1.13 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.13) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_13, v_call_i_6_1_13);
(*   %arrayidx11.6.1.13 = getelementptr inbounds i16, i16* %r, i64 53 *)
(*   %1591 = load i16, i16* %arrayidx11.6.1.13, align 2, !tbaa !3 *)
mov v1591 mem0_106;
(*   %sub.6.1.13 = sub i16 %1591, %call.i.6.1.13 *)
sub v_sub_6_1_13 v1591 v_call_i_6_1_13;
(*   store i16 %sub.6.1.13, i16* %arrayidx9.6.1.13, align 2, !tbaa !3 *)
mov mem0_110 v_sub_6_1_13;
(*   %add21.6.1.13 = add i16 %1591, %call.i.6.1.13 *)
add v_add21_6_1_13 v1591 v_call_i_6_1_13;
(*   store i16 %add21.6.1.13, i16* %arrayidx11.6.1.13, align 2, !tbaa !3 *)
mov mem0_106 v_add21_6_1_13;

(* NOTE: k = 78 *)

(*   %arrayidx9.6.14 = getelementptr inbounds i16, i16* %r, i64 58 *)
(*   %1592 = load i16, i16* %arrayidx9.6.14, align 2, !tbaa !3 *)
mov v1592 mem0_116;
(*   %conv1.i.6.14 = sext i16 %1592 to i32 *)
cast v_conv1_i_6_14@sint32 v1592@sint16;
(*   %mul.i.6.14 = mul nsw i32 %conv1.i.6.14, 1574 *)
mul v_mul_i_6_14 v_conv1_i_6_14 (1574)@sint32;
(*   %call.i.6.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_14, v_call_i_6_14);
(*   %arrayidx11.6.14 = getelementptr inbounds i16, i16* %r, i64 56 *)
(*   %1593 = load i16, i16* %arrayidx11.6.14, align 2, !tbaa !3 *)
mov v1593 mem0_112;
(*   %sub.6.14 = sub i16 %1593, %call.i.6.14 *)
sub v_sub_6_14 v1593 v_call_i_6_14;
(*   store i16 %sub.6.14, i16* %arrayidx9.6.14, align 2, !tbaa !3 *)
mov mem0_116 v_sub_6_14;
(*   %add21.6.14 = add i16 %1593, %call.i.6.14 *)
add v_add21_6_14 v1593 v_call_i_6_14;
(*   store i16 %add21.6.14, i16* %arrayidx11.6.14, align 2, !tbaa !3 *)
mov mem0_112 v_add21_6_14;
(*   %arrayidx9.6.1.14 = getelementptr inbounds i16, i16* %r, i64 59 *)
(*   %1594 = load i16, i16* %arrayidx9.6.1.14, align 2, !tbaa !3 *)
mov v1594 mem0_118;
(*   %conv1.i.6.1.14 = sext i16 %1594 to i32 *)
cast v_conv1_i_6_1_14@sint32 v1594@sint16;
(*   %mul.i.6.1.14 = mul nsw i32 %conv1.i.6.1.14, 1574 *)
mul v_mul_i_6_1_14 v_conv1_i_6_1_14 (1574)@sint32;
(*   %call.i.6.1.14 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.14) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_14, v_call_i_6_1_14);
(*   %arrayidx11.6.1.14 = getelementptr inbounds i16, i16* %r, i64 57 *)
(*   %1595 = load i16, i16* %arrayidx11.6.1.14, align 2, !tbaa !3 *)
mov v1595 mem0_114;
(*   %sub.6.1.14 = sub i16 %1595, %call.i.6.1.14 *)
sub v_sub_6_1_14 v1595 v_call_i_6_1_14;
(*   store i16 %sub.6.1.14, i16* %arrayidx9.6.1.14, align 2, !tbaa !3 *)
mov mem0_118 v_sub_6_1_14;
(*   %add21.6.1.14 = add i16 %1595, %call.i.6.1.14 *)
add v_add21_6_1_14 v1595 v_call_i_6_1_14;
(*   store i16 %add21.6.1.14, i16* %arrayidx11.6.1.14, align 2, !tbaa !3 *)
mov mem0_114 v_add21_6_1_14;

(* NOTE: k = 79 *)

(*   %arrayidx9.6.15 = getelementptr inbounds i16, i16* %r, i64 62 *)
(*   %1596 = load i16, i16* %arrayidx9.6.15, align 2, !tbaa !3 *)
mov v1596 mem0_124;
(*   %conv1.i.6.15 = sext i16 %1596 to i32 *)
cast v_conv1_i_6_15@sint32 v1596@sint16;
(*   %mul.i.6.15 = mul nsw i32 %conv1.i.6.15, 1653 *)
mul v_mul_i_6_15 v_conv1_i_6_15 (1653)@sint32;
(*   %call.i.6.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_15, v_call_i_6_15);
(*   %arrayidx11.6.15 = getelementptr inbounds i16, i16* %r, i64 60 *)
(*   %1597 = load i16, i16* %arrayidx11.6.15, align 2, !tbaa !3 *)
mov v1597 mem0_120;
(*   %sub.6.15 = sub i16 %1597, %call.i.6.15 *)
sub v_sub_6_15 v1597 v_call_i_6_15;
(*   store i16 %sub.6.15, i16* %arrayidx9.6.15, align 2, !tbaa !3 *)
mov mem0_124 v_sub_6_15;
(*   %add21.6.15 = add i16 %1597, %call.i.6.15 *)
add v_add21_6_15 v1597 v_call_i_6_15;
(*   store i16 %add21.6.15, i16* %arrayidx11.6.15, align 2, !tbaa !3 *)
mov mem0_120 v_add21_6_15;
(*   %arrayidx9.6.1.15 = getelementptr inbounds i16, i16* %r, i64 63 *)
(*   %1598 = load i16, i16* %arrayidx9.6.1.15, align 2, !tbaa !3 *)
mov v1598 mem0_126;
(*   %conv1.i.6.1.15 = sext i16 %1598 to i32 *)
cast v_conv1_i_6_1_15@sint32 v1598@sint16;
(*   %mul.i.6.1.15 = mul nsw i32 %conv1.i.6.1.15, 1653 *)
mul v_mul_i_6_1_15 v_conv1_i_6_1_15 (1653)@sint32;
(*   %call.i.6.1.15 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.15) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_15, v_call_i_6_1_15);
(*   %arrayidx11.6.1.15 = getelementptr inbounds i16, i16* %r, i64 61 *)
(*   %1599 = load i16, i16* %arrayidx11.6.1.15, align 2, !tbaa !3 *)
mov v1599 mem0_122;
(*   %sub.6.1.15 = sub i16 %1599, %call.i.6.1.15 *)
sub v_sub_6_1_15 v1599 v_call_i_6_1_15;
(*   store i16 %sub.6.1.15, i16* %arrayidx9.6.1.15, align 2, !tbaa !3 *)
mov mem0_126 v_sub_6_1_15;
(*   %add21.6.1.15 = add i16 %1599, %call.i.6.1.15 *)
add v_add21_6_1_15 v1599 v_call_i_6_1_15;
(*   store i16 %add21.6.1.15, i16* %arrayidx11.6.1.15, align 2, !tbaa !3 *)
mov mem0_122 v_add21_6_1_15;

(* NOTE: k = 80 *)

(*   %arrayidx9.6.16 = getelementptr inbounds i16, i16* %r, i64 66 *)
(*   %1600 = load i16, i16* %arrayidx9.6.16, align 2, !tbaa !3 *)
mov v1600 mem0_132;
(*   %conv1.i.6.16 = sext i16 %1600 to i32 *)
cast v_conv1_i_6_16@sint32 v1600@sint16;
(*   %mul.i.6.16 = mul nsw i32 %conv1.i.6.16, -246 *)
mul v_mul_i_6_16 v_conv1_i_6_16 (-246)@sint32;
(*   %call.i.6.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_16, v_call_i_6_16);
(*   %arrayidx11.6.16 = getelementptr inbounds i16, i16* %r, i64 64 *)
(*   %1601 = load i16, i16* %arrayidx11.6.16, align 2, !tbaa !3 *)
mov v1601 mem0_128;
(*   %sub.6.16 = sub i16 %1601, %call.i.6.16 *)
sub v_sub_6_16 v1601 v_call_i_6_16;
(*   store i16 %sub.6.16, i16* %arrayidx9.6.16, align 2, !tbaa !3 *)
mov mem0_132 v_sub_6_16;
(*   %add21.6.16 = add i16 %1601, %call.i.6.16 *)
add v_add21_6_16 v1601 v_call_i_6_16;
(*   store i16 %add21.6.16, i16* %arrayidx11.6.16, align 2, !tbaa !3 *)
mov mem0_128 v_add21_6_16;
(*   %arrayidx9.6.1.16 = getelementptr inbounds i16, i16* %r, i64 67 *)
(*   %1602 = load i16, i16* %arrayidx9.6.1.16, align 2, !tbaa !3 *)
mov v1602 mem0_134;
(*   %conv1.i.6.1.16 = sext i16 %1602 to i32 *)
cast v_conv1_i_6_1_16@sint32 v1602@sint16;
(*   %mul.i.6.1.16 = mul nsw i32 %conv1.i.6.1.16, -246 *)
mul v_mul_i_6_1_16 v_conv1_i_6_1_16 (-246)@sint32;
(*   %call.i.6.1.16 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.16) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_16, v_call_i_6_1_16);
(*   %arrayidx11.6.1.16 = getelementptr inbounds i16, i16* %r, i64 65 *)
(*   %1603 = load i16, i16* %arrayidx11.6.1.16, align 2, !tbaa !3 *)
mov v1603 mem0_130;
(*   %sub.6.1.16 = sub i16 %1603, %call.i.6.1.16 *)
sub v_sub_6_1_16 v1603 v_call_i_6_1_16;
(*   store i16 %sub.6.1.16, i16* %arrayidx9.6.1.16, align 2, !tbaa !3 *)
mov mem0_134 v_sub_6_1_16;
(*   %add21.6.1.16 = add i16 %1603, %call.i.6.1.16 *)
add v_add21_6_1_16 v1603 v_call_i_6_1_16;
(*   store i16 %add21.6.1.16, i16* %arrayidx11.6.1.16, align 2, !tbaa !3 *)
mov mem0_130 v_add21_6_1_16;

(* NOTE: k = 81 *)

(*   %arrayidx9.6.17 = getelementptr inbounds i16, i16* %r, i64 70 *)
(*   %1604 = load i16, i16* %arrayidx9.6.17, align 2, !tbaa !3 *)
mov v1604 mem0_140;
(*   %conv1.i.6.17 = sext i16 %1604 to i32 *)
cast v_conv1_i_6_17@sint32 v1604@sint16;
(*   %mul.i.6.17 = mul nsw i32 %conv1.i.6.17, 778 *)
mul v_mul_i_6_17 v_conv1_i_6_17 (778)@sint32;
(*   %call.i.6.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_17, v_call_i_6_17);
(*   %arrayidx11.6.17 = getelementptr inbounds i16, i16* %r, i64 68 *)
(*   %1605 = load i16, i16* %arrayidx11.6.17, align 2, !tbaa !3 *)
mov v1605 mem0_136;
(*   %sub.6.17 = sub i16 %1605, %call.i.6.17 *)
sub v_sub_6_17 v1605 v_call_i_6_17;
(*   store i16 %sub.6.17, i16* %arrayidx9.6.17, align 2, !tbaa !3 *)
mov mem0_140 v_sub_6_17;
(*   %add21.6.17 = add i16 %1605, %call.i.6.17 *)
add v_add21_6_17 v1605 v_call_i_6_17;
(*   store i16 %add21.6.17, i16* %arrayidx11.6.17, align 2, !tbaa !3 *)
mov mem0_136 v_add21_6_17;
(*   %arrayidx9.6.1.17 = getelementptr inbounds i16, i16* %r, i64 71 *)
(*   %1606 = load i16, i16* %arrayidx9.6.1.17, align 2, !tbaa !3 *)
mov v1606 mem0_142;
(*   %conv1.i.6.1.17 = sext i16 %1606 to i32 *)
cast v_conv1_i_6_1_17@sint32 v1606@sint16;
(*   %mul.i.6.1.17 = mul nsw i32 %conv1.i.6.1.17, 778 *)
mul v_mul_i_6_1_17 v_conv1_i_6_1_17 (778)@sint32;
(*   %call.i.6.1.17 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.17) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_17, v_call_i_6_1_17);
(*   %arrayidx11.6.1.17 = getelementptr inbounds i16, i16* %r, i64 69 *)
(*   %1607 = load i16, i16* %arrayidx11.6.1.17, align 2, !tbaa !3 *)
mov v1607 mem0_138;
(*   %sub.6.1.17 = sub i16 %1607, %call.i.6.1.17 *)
sub v_sub_6_1_17 v1607 v_call_i_6_1_17;
(*   store i16 %sub.6.1.17, i16* %arrayidx9.6.1.17, align 2, !tbaa !3 *)
mov mem0_142 v_sub_6_1_17;
(*   %add21.6.1.17 = add i16 %1607, %call.i.6.1.17 *)
add v_add21_6_1_17 v1607 v_call_i_6_1_17;
(*   store i16 %add21.6.1.17, i16* %arrayidx11.6.1.17, align 2, !tbaa !3 *)
mov mem0_138 v_add21_6_1_17;

(* NOTE: k = 82 *)

(*   %arrayidx9.6.18 = getelementptr inbounds i16, i16* %r, i64 74 *)
(*   %1608 = load i16, i16* %arrayidx9.6.18, align 2, !tbaa !3 *)
mov v1608 mem0_148;
(*   %conv1.i.6.18 = sext i16 %1608 to i32 *)
cast v_conv1_i_6_18@sint32 v1608@sint16;
(*   %mul.i.6.18 = mul nsw i32 %conv1.i.6.18, 1159 *)
mul v_mul_i_6_18 v_conv1_i_6_18 (1159)@sint32;
(*   %call.i.6.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_18, v_call_i_6_18);
(*   %arrayidx11.6.18 = getelementptr inbounds i16, i16* %r, i64 72 *)
(*   %1609 = load i16, i16* %arrayidx11.6.18, align 2, !tbaa !3 *)
mov v1609 mem0_144;
(*   %sub.6.18 = sub i16 %1609, %call.i.6.18 *)
sub v_sub_6_18 v1609 v_call_i_6_18;
(*   store i16 %sub.6.18, i16* %arrayidx9.6.18, align 2, !tbaa !3 *)
mov mem0_148 v_sub_6_18;
(*   %add21.6.18 = add i16 %1609, %call.i.6.18 *)
add v_add21_6_18 v1609 v_call_i_6_18;
(*   store i16 %add21.6.18, i16* %arrayidx11.6.18, align 2, !tbaa !3 *)
mov mem0_144 v_add21_6_18;
(*   %arrayidx9.6.1.18 = getelementptr inbounds i16, i16* %r, i64 75 *)
(*   %1610 = load i16, i16* %arrayidx9.6.1.18, align 2, !tbaa !3 *)
mov v1610 mem0_150;
(*   %conv1.i.6.1.18 = sext i16 %1610 to i32 *)
cast v_conv1_i_6_1_18@sint32 v1610@sint16;
(*   %mul.i.6.1.18 = mul nsw i32 %conv1.i.6.1.18, 1159 *)
mul v_mul_i_6_1_18 v_conv1_i_6_1_18 (1159)@sint32;
(*   %call.i.6.1.18 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.18) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_18, v_call_i_6_1_18);
(*   %arrayidx11.6.1.18 = getelementptr inbounds i16, i16* %r, i64 73 *)
(*   %1611 = load i16, i16* %arrayidx11.6.1.18, align 2, !tbaa !3 *)
mov v1611 mem0_146;
(*   %sub.6.1.18 = sub i16 %1611, %call.i.6.1.18 *)
sub v_sub_6_1_18 v1611 v_call_i_6_1_18;
(*   store i16 %sub.6.1.18, i16* %arrayidx9.6.1.18, align 2, !tbaa !3 *)
mov mem0_150 v_sub_6_1_18;
(*   %add21.6.1.18 = add i16 %1611, %call.i.6.1.18 *)
add v_add21_6_1_18 v1611 v_call_i_6_1_18;
(*   store i16 %add21.6.1.18, i16* %arrayidx11.6.1.18, align 2, !tbaa !3 *)
mov mem0_146 v_add21_6_1_18;

(* NOTE: k = 83 *)

(*   %arrayidx9.6.19 = getelementptr inbounds i16, i16* %r, i64 78 *)
(*   %1612 = load i16, i16* %arrayidx9.6.19, align 2, !tbaa !3 *)
mov v1612 mem0_156;
(*   %conv1.i.6.19 = sext i16 %1612 to i32 *)
cast v_conv1_i_6_19@sint32 v1612@sint16;
(*   %mul.i.6.19 = mul nsw i32 %conv1.i.6.19, -147 *)
mul v_mul_i_6_19 v_conv1_i_6_19 (-147)@sint32;
(*   %call.i.6.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_19, v_call_i_6_19);
(*   %arrayidx11.6.19 = getelementptr inbounds i16, i16* %r, i64 76 *)
(*   %1613 = load i16, i16* %arrayidx11.6.19, align 2, !tbaa !3 *)
mov v1613 mem0_152;
(*   %sub.6.19 = sub i16 %1613, %call.i.6.19 *)
sub v_sub_6_19 v1613 v_call_i_6_19;
(*   store i16 %sub.6.19, i16* %arrayidx9.6.19, align 2, !tbaa !3 *)
mov mem0_156 v_sub_6_19;
(*   %add21.6.19 = add i16 %1613, %call.i.6.19 *)
add v_add21_6_19 v1613 v_call_i_6_19;
(*   store i16 %add21.6.19, i16* %arrayidx11.6.19, align 2, !tbaa !3 *)
mov mem0_152 v_add21_6_19;
(*   %arrayidx9.6.1.19 = getelementptr inbounds i16, i16* %r, i64 79 *)
(*   %1614 = load i16, i16* %arrayidx9.6.1.19, align 2, !tbaa !3 *)
mov v1614 mem0_158;
(*   %conv1.i.6.1.19 = sext i16 %1614 to i32 *)
cast v_conv1_i_6_1_19@sint32 v1614@sint16;
(*   %mul.i.6.1.19 = mul nsw i32 %conv1.i.6.1.19, -147 *)
mul v_mul_i_6_1_19 v_conv1_i_6_1_19 (-147)@sint32;
(*   %call.i.6.1.19 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.19) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_19, v_call_i_6_1_19);
(*   %arrayidx11.6.1.19 = getelementptr inbounds i16, i16* %r, i64 77 *)
(*   %1615 = load i16, i16* %arrayidx11.6.1.19, align 2, !tbaa !3 *)
mov v1615 mem0_154;
(*   %sub.6.1.19 = sub i16 %1615, %call.i.6.1.19 *)
sub v_sub_6_1_19 v1615 v_call_i_6_1_19;
(*   store i16 %sub.6.1.19, i16* %arrayidx9.6.1.19, align 2, !tbaa !3 *)
mov mem0_158 v_sub_6_1_19;
(*   %add21.6.1.19 = add i16 %1615, %call.i.6.1.19 *)
add v_add21_6_1_19 v1615 v_call_i_6_1_19;
(*   store i16 %add21.6.1.19, i16* %arrayidx11.6.1.19, align 2, !tbaa !3 *)
mov mem0_154 v_add21_6_1_19;

(* NOTE: k = 84 *)

(*   %arrayidx9.6.20 = getelementptr inbounds i16, i16* %r, i64 82 *)
(*   %1616 = load i16, i16* %arrayidx9.6.20, align 2, !tbaa !3 *)
mov v1616 mem0_164;
(*   %conv1.i.6.20 = sext i16 %1616 to i32 *)
cast v_conv1_i_6_20@sint32 v1616@sint16;
(*   %mul.i.6.20 = mul nsw i32 %conv1.i.6.20, -777 *)
mul v_mul_i_6_20 v_conv1_i_6_20 (-777)@sint32;
(*   %call.i.6.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_20, v_call_i_6_20);
(*   %arrayidx11.6.20 = getelementptr inbounds i16, i16* %r, i64 80 *)
(*   %1617 = load i16, i16* %arrayidx11.6.20, align 2, !tbaa !3 *)
mov v1617 mem0_160;
(*   %sub.6.20 = sub i16 %1617, %call.i.6.20 *)
sub v_sub_6_20 v1617 v_call_i_6_20;
(*   store i16 %sub.6.20, i16* %arrayidx9.6.20, align 2, !tbaa !3 *)
mov mem0_164 v_sub_6_20;
(*   %add21.6.20 = add i16 %1617, %call.i.6.20 *)
add v_add21_6_20 v1617 v_call_i_6_20;
(*   store i16 %add21.6.20, i16* %arrayidx11.6.20, align 2, !tbaa !3 *)
mov mem0_160 v_add21_6_20;
(*   %arrayidx9.6.1.20 = getelementptr inbounds i16, i16* %r, i64 83 *)
(*   %1618 = load i16, i16* %arrayidx9.6.1.20, align 2, !tbaa !3 *)
mov v1618 mem0_166;
(*   %conv1.i.6.1.20 = sext i16 %1618 to i32 *)
cast v_conv1_i_6_1_20@sint32 v1618@sint16;
(*   %mul.i.6.1.20 = mul nsw i32 %conv1.i.6.1.20, -777 *)
mul v_mul_i_6_1_20 v_conv1_i_6_1_20 (-777)@sint32;
(*   %call.i.6.1.20 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.20) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_20, v_call_i_6_1_20);
(*   %arrayidx11.6.1.20 = getelementptr inbounds i16, i16* %r, i64 81 *)
(*   %1619 = load i16, i16* %arrayidx11.6.1.20, align 2, !tbaa !3 *)
mov v1619 mem0_162;
(*   %sub.6.1.20 = sub i16 %1619, %call.i.6.1.20 *)
sub v_sub_6_1_20 v1619 v_call_i_6_1_20;
(*   store i16 %sub.6.1.20, i16* %arrayidx9.6.1.20, align 2, !tbaa !3 *)
mov mem0_166 v_sub_6_1_20;
(*   %add21.6.1.20 = add i16 %1619, %call.i.6.1.20 *)
add v_add21_6_1_20 v1619 v_call_i_6_1_20;
(*   store i16 %add21.6.1.20, i16* %arrayidx11.6.1.20, align 2, !tbaa !3 *)
mov mem0_162 v_add21_6_1_20;

(* NOTE: k = 85 *)

(*   %arrayidx9.6.21 = getelementptr inbounds i16, i16* %r, i64 86 *)
(*   %1620 = load i16, i16* %arrayidx9.6.21, align 2, !tbaa !3 *)
mov v1620 mem0_172;
(*   %conv1.i.6.21 = sext i16 %1620 to i32 *)
cast v_conv1_i_6_21@sint32 v1620@sint16;
(*   %mul.i.6.21 = mul nsw i32 %conv1.i.6.21, 1483 *)
mul v_mul_i_6_21 v_conv1_i_6_21 (1483)@sint32;
(*   %call.i.6.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_21, v_call_i_6_21);
(*   %arrayidx11.6.21 = getelementptr inbounds i16, i16* %r, i64 84 *)
(*   %1621 = load i16, i16* %arrayidx11.6.21, align 2, !tbaa !3 *)
mov v1621 mem0_168;
(*   %sub.6.21 = sub i16 %1621, %call.i.6.21 *)
sub v_sub_6_21 v1621 v_call_i_6_21;
(*   store i16 %sub.6.21, i16* %arrayidx9.6.21, align 2, !tbaa !3 *)
mov mem0_172 v_sub_6_21;
(*   %add21.6.21 = add i16 %1621, %call.i.6.21 *)
add v_add21_6_21 v1621 v_call_i_6_21;
(*   store i16 %add21.6.21, i16* %arrayidx11.6.21, align 2, !tbaa !3 *)
mov mem0_168 v_add21_6_21;
(*   %arrayidx9.6.1.21 = getelementptr inbounds i16, i16* %r, i64 87 *)
(*   %1622 = load i16, i16* %arrayidx9.6.1.21, align 2, !tbaa !3 *)
mov v1622 mem0_174;
(*   %conv1.i.6.1.21 = sext i16 %1622 to i32 *)
cast v_conv1_i_6_1_21@sint32 v1622@sint16;
(*   %mul.i.6.1.21 = mul nsw i32 %conv1.i.6.1.21, 1483 *)
mul v_mul_i_6_1_21 v_conv1_i_6_1_21 (1483)@sint32;
(*   %call.i.6.1.21 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.21) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_21, v_call_i_6_1_21);
(*   %arrayidx11.6.1.21 = getelementptr inbounds i16, i16* %r, i64 85 *)
(*   %1623 = load i16, i16* %arrayidx11.6.1.21, align 2, !tbaa !3 *)
mov v1623 mem0_170;
(*   %sub.6.1.21 = sub i16 %1623, %call.i.6.1.21 *)
sub v_sub_6_1_21 v1623 v_call_i_6_1_21;
(*   store i16 %sub.6.1.21, i16* %arrayidx9.6.1.21, align 2, !tbaa !3 *)
mov mem0_174 v_sub_6_1_21;
(*   %add21.6.1.21 = add i16 %1623, %call.i.6.1.21 *)
add v_add21_6_1_21 v1623 v_call_i_6_1_21;
(*   store i16 %add21.6.1.21, i16* %arrayidx11.6.1.21, align 2, !tbaa !3 *)
mov mem0_170 v_add21_6_1_21;

(* NOTE: k = 86 *)

(*   %arrayidx9.6.22 = getelementptr inbounds i16, i16* %r, i64 90 *)
(*   %1624 = load i16, i16* %arrayidx9.6.22, align 2, !tbaa !3 *)
mov v1624 mem0_180;
(*   %conv1.i.6.22 = sext i16 %1624 to i32 *)
cast v_conv1_i_6_22@sint32 v1624@sint16;
(*   %mul.i.6.22 = mul nsw i32 %conv1.i.6.22, -602 *)
mul v_mul_i_6_22 v_conv1_i_6_22 (-602)@sint32;
(*   %call.i.6.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_22, v_call_i_6_22);
(*   %arrayidx11.6.22 = getelementptr inbounds i16, i16* %r, i64 88 *)
(*   %1625 = load i16, i16* %arrayidx11.6.22, align 2, !tbaa !3 *)
mov v1625 mem0_176;
(*   %sub.6.22 = sub i16 %1625, %call.i.6.22 *)
sub v_sub_6_22 v1625 v_call_i_6_22;
(*   store i16 %sub.6.22, i16* %arrayidx9.6.22, align 2, !tbaa !3 *)
mov mem0_180 v_sub_6_22;
(*   %add21.6.22 = add i16 %1625, %call.i.6.22 *)
add v_add21_6_22 v1625 v_call_i_6_22;
(*   store i16 %add21.6.22, i16* %arrayidx11.6.22, align 2, !tbaa !3 *)
mov mem0_176 v_add21_6_22;
(*   %arrayidx9.6.1.22 = getelementptr inbounds i16, i16* %r, i64 91 *)
(*   %1626 = load i16, i16* %arrayidx9.6.1.22, align 2, !tbaa !3 *)
mov v1626 mem0_182;
(*   %conv1.i.6.1.22 = sext i16 %1626 to i32 *)
cast v_conv1_i_6_1_22@sint32 v1626@sint16;
(*   %mul.i.6.1.22 = mul nsw i32 %conv1.i.6.1.22, -602 *)
mul v_mul_i_6_1_22 v_conv1_i_6_1_22 (-602)@sint32;
(*   %call.i.6.1.22 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.22) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_22, v_call_i_6_1_22);
(*   %arrayidx11.6.1.22 = getelementptr inbounds i16, i16* %r, i64 89 *)
(*   %1627 = load i16, i16* %arrayidx11.6.1.22, align 2, !tbaa !3 *)
mov v1627 mem0_178;
(*   %sub.6.1.22 = sub i16 %1627, %call.i.6.1.22 *)
sub v_sub_6_1_22 v1627 v_call_i_6_1_22;
(*   store i16 %sub.6.1.22, i16* %arrayidx9.6.1.22, align 2, !tbaa !3 *)
mov mem0_182 v_sub_6_1_22;
(*   %add21.6.1.22 = add i16 %1627, %call.i.6.1.22 *)
add v_add21_6_1_22 v1627 v_call_i_6_1_22;
(*   store i16 %add21.6.1.22, i16* %arrayidx11.6.1.22, align 2, !tbaa !3 *)
mov mem0_178 v_add21_6_1_22;

(* NOTE: k = 87 *)

(*   %arrayidx9.6.23 = getelementptr inbounds i16, i16* %r, i64 94 *)
(*   %1628 = load i16, i16* %arrayidx9.6.23, align 2, !tbaa !3 *)
mov v1628 mem0_188;
(*   %conv1.i.6.23 = sext i16 %1628 to i32 *)
cast v_conv1_i_6_23@sint32 v1628@sint16;
(*   %mul.i.6.23 = mul nsw i32 %conv1.i.6.23, 1119 *)
mul v_mul_i_6_23 v_conv1_i_6_23 (1119)@sint32;
(*   %call.i.6.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_23, v_call_i_6_23);
(*   %arrayidx11.6.23 = getelementptr inbounds i16, i16* %r, i64 92 *)
(*   %1629 = load i16, i16* %arrayidx11.6.23, align 2, !tbaa !3 *)
mov v1629 mem0_184;
(*   %sub.6.23 = sub i16 %1629, %call.i.6.23 *)
sub v_sub_6_23 v1629 v_call_i_6_23;
(*   store i16 %sub.6.23, i16* %arrayidx9.6.23, align 2, !tbaa !3 *)
mov mem0_188 v_sub_6_23;
(*   %add21.6.23 = add i16 %1629, %call.i.6.23 *)
add v_add21_6_23 v1629 v_call_i_6_23;
(*   store i16 %add21.6.23, i16* %arrayidx11.6.23, align 2, !tbaa !3 *)
mov mem0_184 v_add21_6_23;
(*   %arrayidx9.6.1.23 = getelementptr inbounds i16, i16* %r, i64 95 *)
(*   %1630 = load i16, i16* %arrayidx9.6.1.23, align 2, !tbaa !3 *)
mov v1630 mem0_190;
(*   %conv1.i.6.1.23 = sext i16 %1630 to i32 *)
cast v_conv1_i_6_1_23@sint32 v1630@sint16;
(*   %mul.i.6.1.23 = mul nsw i32 %conv1.i.6.1.23, 1119 *)
mul v_mul_i_6_1_23 v_conv1_i_6_1_23 (1119)@sint32;
(*   %call.i.6.1.23 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.23) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_23, v_call_i_6_1_23);
(*   %arrayidx11.6.1.23 = getelementptr inbounds i16, i16* %r, i64 93 *)
(*   %1631 = load i16, i16* %arrayidx11.6.1.23, align 2, !tbaa !3 *)
mov v1631 mem0_186;
(*   %sub.6.1.23 = sub i16 %1631, %call.i.6.1.23 *)
sub v_sub_6_1_23 v1631 v_call_i_6_1_23;
(*   store i16 %sub.6.1.23, i16* %arrayidx9.6.1.23, align 2, !tbaa !3 *)
mov mem0_190 v_sub_6_1_23;
(*   %add21.6.1.23 = add i16 %1631, %call.i.6.1.23 *)
add v_add21_6_1_23 v1631 v_call_i_6_1_23;
(*   store i16 %add21.6.1.23, i16* %arrayidx11.6.1.23, align 2, !tbaa !3 *)
mov mem0_186 v_add21_6_1_23;

(* NOTE: k = 88 *)

(*   %arrayidx9.6.24 = getelementptr inbounds i16, i16* %r, i64 98 *)
(*   %1632 = load i16, i16* %arrayidx9.6.24, align 2, !tbaa !3 *)
mov v1632 mem0_196;
(*   %conv1.i.6.24 = sext i16 %1632 to i32 *)
cast v_conv1_i_6_24@sint32 v1632@sint16;
(*   %mul.i.6.24 = mul nsw i32 %conv1.i.6.24, -1590 *)
mul v_mul_i_6_24 v_conv1_i_6_24 (-1590)@sint32;
(*   %call.i.6.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_24, v_call_i_6_24);
(*   %arrayidx11.6.24 = getelementptr inbounds i16, i16* %r, i64 96 *)
(*   %1633 = load i16, i16* %arrayidx11.6.24, align 2, !tbaa !3 *)
mov v1633 mem0_192;
(*   %sub.6.24 = sub i16 %1633, %call.i.6.24 *)
sub v_sub_6_24 v1633 v_call_i_6_24;
(*   store i16 %sub.6.24, i16* %arrayidx9.6.24, align 2, !tbaa !3 *)
mov mem0_196 v_sub_6_24;
(*   %add21.6.24 = add i16 %1633, %call.i.6.24 *)
add v_add21_6_24 v1633 v_call_i_6_24;
(*   store i16 %add21.6.24, i16* %arrayidx11.6.24, align 2, !tbaa !3 *)
mov mem0_192 v_add21_6_24;
(*   %arrayidx9.6.1.24 = getelementptr inbounds i16, i16* %r, i64 99 *)
(*   %1634 = load i16, i16* %arrayidx9.6.1.24, align 2, !tbaa !3 *)
mov v1634 mem0_198;
(*   %conv1.i.6.1.24 = sext i16 %1634 to i32 *)
cast v_conv1_i_6_1_24@sint32 v1634@sint16;
(*   %mul.i.6.1.24 = mul nsw i32 %conv1.i.6.1.24, -1590 *)
mul v_mul_i_6_1_24 v_conv1_i_6_1_24 (-1590)@sint32;
(*   %call.i.6.1.24 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.24) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_24, v_call_i_6_1_24);
(*   %arrayidx11.6.1.24 = getelementptr inbounds i16, i16* %r, i64 97 *)
(*   %1635 = load i16, i16* %arrayidx11.6.1.24, align 2, !tbaa !3 *)
mov v1635 mem0_194;
(*   %sub.6.1.24 = sub i16 %1635, %call.i.6.1.24 *)
sub v_sub_6_1_24 v1635 v_call_i_6_1_24;
(*   store i16 %sub.6.1.24, i16* %arrayidx9.6.1.24, align 2, !tbaa !3 *)
mov mem0_198 v_sub_6_1_24;
(*   %add21.6.1.24 = add i16 %1635, %call.i.6.1.24 *)
add v_add21_6_1_24 v1635 v_call_i_6_1_24;
(*   store i16 %add21.6.1.24, i16* %arrayidx11.6.1.24, align 2, !tbaa !3 *)
mov mem0_194 v_add21_6_1_24;

(* NOTE: k = 89 *)

(*   %arrayidx9.6.25 = getelementptr inbounds i16, i16* %r, i64 102 *)
(*   %1636 = load i16, i16* %arrayidx9.6.25, align 2, !tbaa !3 *)
mov v1636 mem0_204;
(*   %conv1.i.6.25 = sext i16 %1636 to i32 *)
cast v_conv1_i_6_25@sint32 v1636@sint16;
(*   %mul.i.6.25 = mul nsw i32 %conv1.i.6.25, 644 *)
mul v_mul_i_6_25 v_conv1_i_6_25 (644)@sint32;
(*   %call.i.6.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_25, v_call_i_6_25);
(*   %arrayidx11.6.25 = getelementptr inbounds i16, i16* %r, i64 100 *)
(*   %1637 = load i16, i16* %arrayidx11.6.25, align 2, !tbaa !3 *)
mov v1637 mem0_200;
(*   %sub.6.25 = sub i16 %1637, %call.i.6.25 *)
sub v_sub_6_25 v1637 v_call_i_6_25;
(*   store i16 %sub.6.25, i16* %arrayidx9.6.25, align 2, !tbaa !3 *)
mov mem0_204 v_sub_6_25;
(*   %add21.6.25 = add i16 %1637, %call.i.6.25 *)
add v_add21_6_25 v1637 v_call_i_6_25;
(*   store i16 %add21.6.25, i16* %arrayidx11.6.25, align 2, !tbaa !3 *)
mov mem0_200 v_add21_6_25;
(*   %arrayidx9.6.1.25 = getelementptr inbounds i16, i16* %r, i64 103 *)
(*   %1638 = load i16, i16* %arrayidx9.6.1.25, align 2, !tbaa !3 *)
mov v1638 mem0_206;
(*   %conv1.i.6.1.25 = sext i16 %1638 to i32 *)
cast v_conv1_i_6_1_25@sint32 v1638@sint16;
(*   %mul.i.6.1.25 = mul nsw i32 %conv1.i.6.1.25, 644 *)
mul v_mul_i_6_1_25 v_conv1_i_6_1_25 (644)@sint32;
(*   %call.i.6.1.25 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.25) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_25, v_call_i_6_1_25);
(*   %arrayidx11.6.1.25 = getelementptr inbounds i16, i16* %r, i64 101 *)
(*   %1639 = load i16, i16* %arrayidx11.6.1.25, align 2, !tbaa !3 *)
mov v1639 mem0_202;
(*   %sub.6.1.25 = sub i16 %1639, %call.i.6.1.25 *)
sub v_sub_6_1_25 v1639 v_call_i_6_1_25;
(*   store i16 %sub.6.1.25, i16* %arrayidx9.6.1.25, align 2, !tbaa !3 *)
mov mem0_206 v_sub_6_1_25;
(*   %add21.6.1.25 = add i16 %1639, %call.i.6.1.25 *)
add v_add21_6_1_25 v1639 v_call_i_6_1_25;
(*   store i16 %add21.6.1.25, i16* %arrayidx11.6.1.25, align 2, !tbaa !3 *)
mov mem0_202 v_add21_6_1_25;

(* NOTE: k = 90 *)

(*   %arrayidx9.6.26 = getelementptr inbounds i16, i16* %r, i64 106 *)
(*   %1640 = load i16, i16* %arrayidx9.6.26, align 2, !tbaa !3 *)
mov v1640 mem0_212;
(*   %conv1.i.6.26 = sext i16 %1640 to i32 *)
cast v_conv1_i_6_26@sint32 v1640@sint16;
(*   %mul.i.6.26 = mul nsw i32 %conv1.i.6.26, -872 *)
mul v_mul_i_6_26 v_conv1_i_6_26 (-872)@sint32;
(*   %call.i.6.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_26, v_call_i_6_26);
(*   %arrayidx11.6.26 = getelementptr inbounds i16, i16* %r, i64 104 *)
(*   %1641 = load i16, i16* %arrayidx11.6.26, align 2, !tbaa !3 *)
mov v1641 mem0_208;
(*   %sub.6.26 = sub i16 %1641, %call.i.6.26 *)
sub v_sub_6_26 v1641 v_call_i_6_26;
(*   store i16 %sub.6.26, i16* %arrayidx9.6.26, align 2, !tbaa !3 *)
mov mem0_212 v_sub_6_26;
(*   %add21.6.26 = add i16 %1641, %call.i.6.26 *)
add v_add21_6_26 v1641 v_call_i_6_26;
(*   store i16 %add21.6.26, i16* %arrayidx11.6.26, align 2, !tbaa !3 *)
mov mem0_208 v_add21_6_26;
(*   %arrayidx9.6.1.26 = getelementptr inbounds i16, i16* %r, i64 107 *)
(*   %1642 = load i16, i16* %arrayidx9.6.1.26, align 2, !tbaa !3 *)
mov v1642 mem0_214;
(*   %conv1.i.6.1.26 = sext i16 %1642 to i32 *)
cast v_conv1_i_6_1_26@sint32 v1642@sint16;
(*   %mul.i.6.1.26 = mul nsw i32 %conv1.i.6.1.26, -872 *)
mul v_mul_i_6_1_26 v_conv1_i_6_1_26 (-872)@sint32;
(*   %call.i.6.1.26 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.26) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_26, v_call_i_6_1_26);
(*   %arrayidx11.6.1.26 = getelementptr inbounds i16, i16* %r, i64 105 *)
(*   %1643 = load i16, i16* %arrayidx11.6.1.26, align 2, !tbaa !3 *)
mov v1643 mem0_210;
(*   %sub.6.1.26 = sub i16 %1643, %call.i.6.1.26 *)
sub v_sub_6_1_26 v1643 v_call_i_6_1_26;
(*   store i16 %sub.6.1.26, i16* %arrayidx9.6.1.26, align 2, !tbaa !3 *)
mov mem0_214 v_sub_6_1_26;
(*   %add21.6.1.26 = add i16 %1643, %call.i.6.1.26 *)
add v_add21_6_1_26 v1643 v_call_i_6_1_26;
(*   store i16 %add21.6.1.26, i16* %arrayidx11.6.1.26, align 2, !tbaa !3 *)
mov mem0_210 v_add21_6_1_26;

(* NOTE: k = 91 *)

(*   %arrayidx9.6.27 = getelementptr inbounds i16, i16* %r, i64 110 *)
(*   %1644 = load i16, i16* %arrayidx9.6.27, align 2, !tbaa !3 *)
mov v1644 mem0_220;
(*   %conv1.i.6.27 = sext i16 %1644 to i32 *)
cast v_conv1_i_6_27@sint32 v1644@sint16;
(*   %mul.i.6.27 = mul nsw i32 %conv1.i.6.27, 349 *)
mul v_mul_i_6_27 v_conv1_i_6_27 (349)@sint32;
(*   %call.i.6.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_27, v_call_i_6_27);
(*   %arrayidx11.6.27 = getelementptr inbounds i16, i16* %r, i64 108 *)
(*   %1645 = load i16, i16* %arrayidx11.6.27, align 2, !tbaa !3 *)
mov v1645 mem0_216;
(*   %sub.6.27 = sub i16 %1645, %call.i.6.27 *)
sub v_sub_6_27 v1645 v_call_i_6_27;
(*   store i16 %sub.6.27, i16* %arrayidx9.6.27, align 2, !tbaa !3 *)
mov mem0_220 v_sub_6_27;
(*   %add21.6.27 = add i16 %1645, %call.i.6.27 *)
add v_add21_6_27 v1645 v_call_i_6_27;
(*   store i16 %add21.6.27, i16* %arrayidx11.6.27, align 2, !tbaa !3 *)
mov mem0_216 v_add21_6_27;
(*   %arrayidx9.6.1.27 = getelementptr inbounds i16, i16* %r, i64 111 *)
(*   %1646 = load i16, i16* %arrayidx9.6.1.27, align 2, !tbaa !3 *)
mov v1646 mem0_222;
(*   %conv1.i.6.1.27 = sext i16 %1646 to i32 *)
cast v_conv1_i_6_1_27@sint32 v1646@sint16;
(*   %mul.i.6.1.27 = mul nsw i32 %conv1.i.6.1.27, 349 *)
mul v_mul_i_6_1_27 v_conv1_i_6_1_27 (349)@sint32;
(*   %call.i.6.1.27 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.27) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_27, v_call_i_6_1_27);
(*   %arrayidx11.6.1.27 = getelementptr inbounds i16, i16* %r, i64 109 *)
(*   %1647 = load i16, i16* %arrayidx11.6.1.27, align 2, !tbaa !3 *)
mov v1647 mem0_218;
(*   %sub.6.1.27 = sub i16 %1647, %call.i.6.1.27 *)
sub v_sub_6_1_27 v1647 v_call_i_6_1_27;
(*   store i16 %sub.6.1.27, i16* %arrayidx9.6.1.27, align 2, !tbaa !3 *)
mov mem0_222 v_sub_6_1_27;
(*   %add21.6.1.27 = add i16 %1647, %call.i.6.1.27 *)
add v_add21_6_1_27 v1647 v_call_i_6_1_27;
(*   store i16 %add21.6.1.27, i16* %arrayidx11.6.1.27, align 2, !tbaa !3 *)
mov mem0_218 v_add21_6_1_27;

(* NOTE: k = 92 *)

(*   %arrayidx9.6.28 = getelementptr inbounds i16, i16* %r, i64 114 *)
(*   %1648 = load i16, i16* %arrayidx9.6.28, align 2, !tbaa !3 *)
mov v1648 mem0_228;
(*   %conv1.i.6.28 = sext i16 %1648 to i32 *)
cast v_conv1_i_6_28@sint32 v1648@sint16;
(*   %mul.i.6.28 = mul nsw i32 %conv1.i.6.28, 418 *)
mul v_mul_i_6_28 v_conv1_i_6_28 (418)@sint32;
(*   %call.i.6.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_28, v_call_i_6_28);
(*   %arrayidx11.6.28 = getelementptr inbounds i16, i16* %r, i64 112 *)
(*   %1649 = load i16, i16* %arrayidx11.6.28, align 2, !tbaa !3 *)
mov v1649 mem0_224;
(*   %sub.6.28 = sub i16 %1649, %call.i.6.28 *)
sub v_sub_6_28 v1649 v_call_i_6_28;
(*   store i16 %sub.6.28, i16* %arrayidx9.6.28, align 2, !tbaa !3 *)
mov mem0_228 v_sub_6_28;
(*   %add21.6.28 = add i16 %1649, %call.i.6.28 *)
add v_add21_6_28 v1649 v_call_i_6_28;
(*   store i16 %add21.6.28, i16* %arrayidx11.6.28, align 2, !tbaa !3 *)
mov mem0_224 v_add21_6_28;
(*   %arrayidx9.6.1.28 = getelementptr inbounds i16, i16* %r, i64 115 *)
(*   %1650 = load i16, i16* %arrayidx9.6.1.28, align 2, !tbaa !3 *)
mov v1650 mem0_230;
(*   %conv1.i.6.1.28 = sext i16 %1650 to i32 *)
cast v_conv1_i_6_1_28@sint32 v1650@sint16;
(*   %mul.i.6.1.28 = mul nsw i32 %conv1.i.6.1.28, 418 *)
mul v_mul_i_6_1_28 v_conv1_i_6_1_28 (418)@sint32;
(*   %call.i.6.1.28 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.28) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_28, v_call_i_6_1_28);
(*   %arrayidx11.6.1.28 = getelementptr inbounds i16, i16* %r, i64 113 *)
(*   %1651 = load i16, i16* %arrayidx11.6.1.28, align 2, !tbaa !3 *)
mov v1651 mem0_226;
(*   %sub.6.1.28 = sub i16 %1651, %call.i.6.1.28 *)
sub v_sub_6_1_28 v1651 v_call_i_6_1_28;
(*   store i16 %sub.6.1.28, i16* %arrayidx9.6.1.28, align 2, !tbaa !3 *)
mov mem0_230 v_sub_6_1_28;
(*   %add21.6.1.28 = add i16 %1651, %call.i.6.1.28 *)
add v_add21_6_1_28 v1651 v_call_i_6_1_28;
(*   store i16 %add21.6.1.28, i16* %arrayidx11.6.1.28, align 2, !tbaa !3 *)
mov mem0_226 v_add21_6_1_28;

(* NOTE: k = 93 *)

(*   %arrayidx9.6.29 = getelementptr inbounds i16, i16* %r, i64 118 *)
(*   %1652 = load i16, i16* %arrayidx9.6.29, align 2, !tbaa !3 *)
mov v1652 mem0_236;
(*   %conv1.i.6.29 = sext i16 %1652 to i32 *)
cast v_conv1_i_6_29@sint32 v1652@sint16;
(*   %mul.i.6.29 = mul nsw i32 %conv1.i.6.29, 329 *)
mul v_mul_i_6_29 v_conv1_i_6_29 (329)@sint32;
(*   %call.i.6.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_29, v_call_i_6_29);
(*   %arrayidx11.6.29 = getelementptr inbounds i16, i16* %r, i64 116 *)
(*   %1653 = load i16, i16* %arrayidx11.6.29, align 2, !tbaa !3 *)
mov v1653 mem0_232;
(*   %sub.6.29 = sub i16 %1653, %call.i.6.29 *)
sub v_sub_6_29 v1653 v_call_i_6_29;
(*   store i16 %sub.6.29, i16* %arrayidx9.6.29, align 2, !tbaa !3 *)
mov mem0_236 v_sub_6_29;
(*   %add21.6.29 = add i16 %1653, %call.i.6.29 *)
add v_add21_6_29 v1653 v_call_i_6_29;
(*   store i16 %add21.6.29, i16* %arrayidx11.6.29, align 2, !tbaa !3 *)
mov mem0_232 v_add21_6_29;
(*   %arrayidx9.6.1.29 = getelementptr inbounds i16, i16* %r, i64 119 *)
(*   %1654 = load i16, i16* %arrayidx9.6.1.29, align 2, !tbaa !3 *)
mov v1654 mem0_238;
(*   %conv1.i.6.1.29 = sext i16 %1654 to i32 *)
cast v_conv1_i_6_1_29@sint32 v1654@sint16;
(*   %mul.i.6.1.29 = mul nsw i32 %conv1.i.6.1.29, 329 *)
mul v_mul_i_6_1_29 v_conv1_i_6_1_29 (329)@sint32;
(*   %call.i.6.1.29 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.29) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_29, v_call_i_6_1_29);
(*   %arrayidx11.6.1.29 = getelementptr inbounds i16, i16* %r, i64 117 *)
(*   %1655 = load i16, i16* %arrayidx11.6.1.29, align 2, !tbaa !3 *)
mov v1655 mem0_234;
(*   %sub.6.1.29 = sub i16 %1655, %call.i.6.1.29 *)
sub v_sub_6_1_29 v1655 v_call_i_6_1_29;
(*   store i16 %sub.6.1.29, i16* %arrayidx9.6.1.29, align 2, !tbaa !3 *)
mov mem0_238 v_sub_6_1_29;
(*   %add21.6.1.29 = add i16 %1655, %call.i.6.1.29 *)
add v_add21_6_1_29 v1655 v_call_i_6_1_29;
(*   store i16 %add21.6.1.29, i16* %arrayidx11.6.1.29, align 2, !tbaa !3 *)
mov mem0_234 v_add21_6_1_29;

(* NOTE: k = 94 *)

(*   %arrayidx9.6.30 = getelementptr inbounds i16, i16* %r, i64 122 *)
(*   %1656 = load i16, i16* %arrayidx9.6.30, align 2, !tbaa !3 *)
mov v1656 mem0_244;
(*   %conv1.i.6.30 = sext i16 %1656 to i32 *)
cast v_conv1_i_6_30@sint32 v1656@sint16;
(*   %mul.i.6.30 = mul nsw i32 %conv1.i.6.30, -156 *)
mul v_mul_i_6_30 v_conv1_i_6_30 (-156)@sint32;
(*   %call.i.6.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_30, v_call_i_6_30);
(*   %arrayidx11.6.30 = getelementptr inbounds i16, i16* %r, i64 120 *)
(*   %1657 = load i16, i16* %arrayidx11.6.30, align 2, !tbaa !3 *)
mov v1657 mem0_240;
(*   %sub.6.30 = sub i16 %1657, %call.i.6.30 *)
sub v_sub_6_30 v1657 v_call_i_6_30;
(*   store i16 %sub.6.30, i16* %arrayidx9.6.30, align 2, !tbaa !3 *)
mov mem0_244 v_sub_6_30;
(*   %add21.6.30 = add i16 %1657, %call.i.6.30 *)
add v_add21_6_30 v1657 v_call_i_6_30;
(*   store i16 %add21.6.30, i16* %arrayidx11.6.30, align 2, !tbaa !3 *)
mov mem0_240 v_add21_6_30;
(*   %arrayidx9.6.1.30 = getelementptr inbounds i16, i16* %r, i64 123 *)
(*   %1658 = load i16, i16* %arrayidx9.6.1.30, align 2, !tbaa !3 *)
mov v1658 mem0_246;
(*   %conv1.i.6.1.30 = sext i16 %1658 to i32 *)
cast v_conv1_i_6_1_30@sint32 v1658@sint16;
(*   %mul.i.6.1.30 = mul nsw i32 %conv1.i.6.1.30, -156 *)
mul v_mul_i_6_1_30 v_conv1_i_6_1_30 (-156)@sint32;
(*   %call.i.6.1.30 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.30) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_30, v_call_i_6_1_30);
(*   %arrayidx11.6.1.30 = getelementptr inbounds i16, i16* %r, i64 121 *)
(*   %1659 = load i16, i16* %arrayidx11.6.1.30, align 2, !tbaa !3 *)
mov v1659 mem0_242;
(*   %sub.6.1.30 = sub i16 %1659, %call.i.6.1.30 *)
sub v_sub_6_1_30 v1659 v_call_i_6_1_30;
(*   store i16 %sub.6.1.30, i16* %arrayidx9.6.1.30, align 2, !tbaa !3 *)
mov mem0_246 v_sub_6_1_30;
(*   %add21.6.1.30 = add i16 %1659, %call.i.6.1.30 *)
add v_add21_6_1_30 v1659 v_call_i_6_1_30;
(*   store i16 %add21.6.1.30, i16* %arrayidx11.6.1.30, align 2, !tbaa !3 *)
mov mem0_242 v_add21_6_1_30;

(* NOTE: k = 95 *)

(*   %arrayidx9.6.31 = getelementptr inbounds i16, i16* %r, i64 126 *)
(*   %1660 = load i16, i16* %arrayidx9.6.31, align 2, !tbaa !3 *)
mov v1660 mem0_252;
(*   %conv1.i.6.31 = sext i16 %1660 to i32 *)
cast v_conv1_i_6_31@sint32 v1660@sint16;
(*   %mul.i.6.31 = mul nsw i32 %conv1.i.6.31, -75 *)
mul v_mul_i_6_31 v_conv1_i_6_31 (-75)@sint32;
(*   %call.i.6.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_31, v_call_i_6_31);
(*   %arrayidx11.6.31 = getelementptr inbounds i16, i16* %r, i64 124 *)
(*   %1661 = load i16, i16* %arrayidx11.6.31, align 2, !tbaa !3 *)
mov v1661 mem0_248;
(*   %sub.6.31 = sub i16 %1661, %call.i.6.31 *)
sub v_sub_6_31 v1661 v_call_i_6_31;
(*   store i16 %sub.6.31, i16* %arrayidx9.6.31, align 2, !tbaa !3 *)
mov mem0_252 v_sub_6_31;
(*   %add21.6.31 = add i16 %1661, %call.i.6.31 *)
add v_add21_6_31 v1661 v_call_i_6_31;
(*   store i16 %add21.6.31, i16* %arrayidx11.6.31, align 2, !tbaa !3 *)
mov mem0_248 v_add21_6_31;
(*   %arrayidx9.6.1.31 = getelementptr inbounds i16, i16* %r, i64 127 *)
(*   %1662 = load i16, i16* %arrayidx9.6.1.31, align 2, !tbaa !3 *)
mov v1662 mem0_254;
(*   %conv1.i.6.1.31 = sext i16 %1662 to i32 *)
cast v_conv1_i_6_1_31@sint32 v1662@sint16;
(*   %mul.i.6.1.31 = mul nsw i32 %conv1.i.6.1.31, -75 *)
mul v_mul_i_6_1_31 v_conv1_i_6_1_31 (-75)@sint32;
(*   %call.i.6.1.31 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.31) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_31, v_call_i_6_1_31);
(*   %arrayidx11.6.1.31 = getelementptr inbounds i16, i16* %r, i64 125 *)
(*   %1663 = load i16, i16* %arrayidx11.6.1.31, align 2, !tbaa !3 *)
mov v1663 mem0_250;
(*   %sub.6.1.31 = sub i16 %1663, %call.i.6.1.31 *)
sub v_sub_6_1_31 v1663 v_call_i_6_1_31;
(*   store i16 %sub.6.1.31, i16* %arrayidx9.6.1.31, align 2, !tbaa !3 *)
mov mem0_254 v_sub_6_1_31;
(*   %add21.6.1.31 = add i16 %1663, %call.i.6.1.31 *)
add v_add21_6_1_31 v1663 v_call_i_6_1_31;
(*   store i16 %add21.6.1.31, i16* %arrayidx11.6.1.31, align 2, !tbaa !3 *)
mov mem0_250 v_add21_6_1_31;

(* NOTE: k = 96 *)

(*   %arrayidx9.6.32 = getelementptr inbounds i16, i16* %r, i64 130 *)
(*   %1664 = load i16, i16* %arrayidx9.6.32, align 2, !tbaa !3 *)
mov v1664 mem0_260;
(*   %conv1.i.6.32 = sext i16 %1664 to i32 *)
cast v_conv1_i_6_32@sint32 v1664@sint16;
(*   %mul.i.6.32 = mul nsw i32 %conv1.i.6.32, 817 *)
mul v_mul_i_6_32 v_conv1_i_6_32 (817)@sint32;
(*   %call.i.6.32 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.32) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_32, v_call_i_6_32);
(*   %arrayidx11.6.32 = getelementptr inbounds i16, i16* %r, i64 128 *)
(*   %1665 = load i16, i16* %arrayidx11.6.32, align 2, !tbaa !3 *)
mov v1665 mem0_256;
(*   %sub.6.32 = sub i16 %1665, %call.i.6.32 *)
sub v_sub_6_32 v1665 v_call_i_6_32;
(*   store i16 %sub.6.32, i16* %arrayidx9.6.32, align 2, !tbaa !3 *)
mov mem0_260 v_sub_6_32;
(*   %add21.6.32 = add i16 %1665, %call.i.6.32 *)
add v_add21_6_32 v1665 v_call_i_6_32;
(*   store i16 %add21.6.32, i16* %arrayidx11.6.32, align 2, !tbaa !3 *)
mov mem0_256 v_add21_6_32;
(*   %arrayidx9.6.1.32 = getelementptr inbounds i16, i16* %r, i64 131 *)
(*   %1666 = load i16, i16* %arrayidx9.6.1.32, align 2, !tbaa !3 *)
mov v1666 mem0_262;
(*   %conv1.i.6.1.32 = sext i16 %1666 to i32 *)
cast v_conv1_i_6_1_32@sint32 v1666@sint16;
(*   %mul.i.6.1.32 = mul nsw i32 %conv1.i.6.1.32, 817 *)
mul v_mul_i_6_1_32 v_conv1_i_6_1_32 (817)@sint32;
(*   %call.i.6.1.32 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.32) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_32, v_call_i_6_1_32);
(*   %arrayidx11.6.1.32 = getelementptr inbounds i16, i16* %r, i64 129 *)
(*   %1667 = load i16, i16* %arrayidx11.6.1.32, align 2, !tbaa !3 *)
mov v1667 mem0_258;
(*   %sub.6.1.32 = sub i16 %1667, %call.i.6.1.32 *)
sub v_sub_6_1_32 v1667 v_call_i_6_1_32;
(*   store i16 %sub.6.1.32, i16* %arrayidx9.6.1.32, align 2, !tbaa !3 *)
mov mem0_262 v_sub_6_1_32;
(*   %add21.6.1.32 = add i16 %1667, %call.i.6.1.32 *)
add v_add21_6_1_32 v1667 v_call_i_6_1_32;
(*   store i16 %add21.6.1.32, i16* %arrayidx11.6.1.32, align 2, !tbaa !3 *)
mov mem0_258 v_add21_6_1_32;

(* NOTE: k = 97 *)

(*   %arrayidx9.6.33 = getelementptr inbounds i16, i16* %r, i64 134 *)
(*   %1668 = load i16, i16* %arrayidx9.6.33, align 2, !tbaa !3 *)
mov v1668 mem0_268;
(*   %conv1.i.6.33 = sext i16 %1668 to i32 *)
cast v_conv1_i_6_33@sint32 v1668@sint16;
(*   %mul.i.6.33 = mul nsw i32 %conv1.i.6.33, 1097 *)
mul v_mul_i_6_33 v_conv1_i_6_33 (1097)@sint32;
(*   %call.i.6.33 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.33) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_33, v_call_i_6_33);
(*   %arrayidx11.6.33 = getelementptr inbounds i16, i16* %r, i64 132 *)
(*   %1669 = load i16, i16* %arrayidx11.6.33, align 2, !tbaa !3 *)
mov v1669 mem0_264;
(*   %sub.6.33 = sub i16 %1669, %call.i.6.33 *)
sub v_sub_6_33 v1669 v_call_i_6_33;
(*   store i16 %sub.6.33, i16* %arrayidx9.6.33, align 2, !tbaa !3 *)
mov mem0_268 v_sub_6_33;
(*   %add21.6.33 = add i16 %1669, %call.i.6.33 *)
add v_add21_6_33 v1669 v_call_i_6_33;
(*   store i16 %add21.6.33, i16* %arrayidx11.6.33, align 2, !tbaa !3 *)
mov mem0_264 v_add21_6_33;
(*   %arrayidx9.6.1.33 = getelementptr inbounds i16, i16* %r, i64 135 *)
(*   %1670 = load i16, i16* %arrayidx9.6.1.33, align 2, !tbaa !3 *)
mov v1670 mem0_270;
(*   %conv1.i.6.1.33 = sext i16 %1670 to i32 *)
cast v_conv1_i_6_1_33@sint32 v1670@sint16;
(*   %mul.i.6.1.33 = mul nsw i32 %conv1.i.6.1.33, 1097 *)
mul v_mul_i_6_1_33 v_conv1_i_6_1_33 (1097)@sint32;
(*   %call.i.6.1.33 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.33) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_33, v_call_i_6_1_33);
(*   %arrayidx11.6.1.33 = getelementptr inbounds i16, i16* %r, i64 133 *)
(*   %1671 = load i16, i16* %arrayidx11.6.1.33, align 2, !tbaa !3 *)
mov v1671 mem0_266;
(*   %sub.6.1.33 = sub i16 %1671, %call.i.6.1.33 *)
sub v_sub_6_1_33 v1671 v_call_i_6_1_33;
(*   store i16 %sub.6.1.33, i16* %arrayidx9.6.1.33, align 2, !tbaa !3 *)
mov mem0_270 v_sub_6_1_33;
(*   %add21.6.1.33 = add i16 %1671, %call.i.6.1.33 *)
add v_add21_6_1_33 v1671 v_call_i_6_1_33;
(*   store i16 %add21.6.1.33, i16* %arrayidx11.6.1.33, align 2, !tbaa !3 *)
mov mem0_266 v_add21_6_1_33;

(* NOTE: k = 98 *)

(*   %arrayidx9.6.34 = getelementptr inbounds i16, i16* %r, i64 138 *)
(*   %1672 = load i16, i16* %arrayidx9.6.34, align 2, !tbaa !3 *)
mov v1672 mem0_276;
(*   %conv1.i.6.34 = sext i16 %1672 to i32 *)
cast v_conv1_i_6_34@sint32 v1672@sint16;
(*   %mul.i.6.34 = mul nsw i32 %conv1.i.6.34, 603 *)
mul v_mul_i_6_34 v_conv1_i_6_34 (603)@sint32;
(*   %call.i.6.34 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.34) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_34, v_call_i_6_34);
(*   %arrayidx11.6.34 = getelementptr inbounds i16, i16* %r, i64 136 *)
(*   %1673 = load i16, i16* %arrayidx11.6.34, align 2, !tbaa !3 *)
mov v1673 mem0_272;
(*   %sub.6.34 = sub i16 %1673, %call.i.6.34 *)
sub v_sub_6_34 v1673 v_call_i_6_34;
(*   store i16 %sub.6.34, i16* %arrayidx9.6.34, align 2, !tbaa !3 *)
mov mem0_276 v_sub_6_34;
(*   %add21.6.34 = add i16 %1673, %call.i.6.34 *)
add v_add21_6_34 v1673 v_call_i_6_34;
(*   store i16 %add21.6.34, i16* %arrayidx11.6.34, align 2, !tbaa !3 *)
mov mem0_272 v_add21_6_34;
(*   %arrayidx9.6.1.34 = getelementptr inbounds i16, i16* %r, i64 139 *)
(*   %1674 = load i16, i16* %arrayidx9.6.1.34, align 2, !tbaa !3 *)
mov v1674 mem0_278;
(*   %conv1.i.6.1.34 = sext i16 %1674 to i32 *)
cast v_conv1_i_6_1_34@sint32 v1674@sint16;
(*   %mul.i.6.1.34 = mul nsw i32 %conv1.i.6.1.34, 603 *)
mul v_mul_i_6_1_34 v_conv1_i_6_1_34 (603)@sint32;
(*   %call.i.6.1.34 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.34) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_34, v_call_i_6_1_34);
(*   %arrayidx11.6.1.34 = getelementptr inbounds i16, i16* %r, i64 137 *)
(*   %1675 = load i16, i16* %arrayidx11.6.1.34, align 2, !tbaa !3 *)
mov v1675 mem0_274;
(*   %sub.6.1.34 = sub i16 %1675, %call.i.6.1.34 *)
sub v_sub_6_1_34 v1675 v_call_i_6_1_34;
(*   store i16 %sub.6.1.34, i16* %arrayidx9.6.1.34, align 2, !tbaa !3 *)
mov mem0_278 v_sub_6_1_34;
(*   %add21.6.1.34 = add i16 %1675, %call.i.6.1.34 *)
add v_add21_6_1_34 v1675 v_call_i_6_1_34;
(*   store i16 %add21.6.1.34, i16* %arrayidx11.6.1.34, align 2, !tbaa !3 *)
mov mem0_274 v_add21_6_1_34;

(* NOTE: k = 99 *)

(*   %arrayidx9.6.35 = getelementptr inbounds i16, i16* %r, i64 142 *)
(*   %1676 = load i16, i16* %arrayidx9.6.35, align 2, !tbaa !3 *)
mov v1676 mem0_284;
(*   %conv1.i.6.35 = sext i16 %1676 to i32 *)
cast v_conv1_i_6_35@sint32 v1676@sint16;
(*   %mul.i.6.35 = mul nsw i32 %conv1.i.6.35, 610 *)
mul v_mul_i_6_35 v_conv1_i_6_35 (610)@sint32;
(*   %call.i.6.35 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.35) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_35, v_call_i_6_35);
(*   %arrayidx11.6.35 = getelementptr inbounds i16, i16* %r, i64 140 *)
(*   %1677 = load i16, i16* %arrayidx11.6.35, align 2, !tbaa !3 *)
mov v1677 mem0_280;
(*   %sub.6.35 = sub i16 %1677, %call.i.6.35 *)
sub v_sub_6_35 v1677 v_call_i_6_35;
(*   store i16 %sub.6.35, i16* %arrayidx9.6.35, align 2, !tbaa !3 *)
mov mem0_284 v_sub_6_35;
(*   %add21.6.35 = add i16 %1677, %call.i.6.35 *)
add v_add21_6_35 v1677 v_call_i_6_35;
(*   store i16 %add21.6.35, i16* %arrayidx11.6.35, align 2, !tbaa !3 *)
mov mem0_280 v_add21_6_35;
(*   %arrayidx9.6.1.35 = getelementptr inbounds i16, i16* %r, i64 143 *)
(*   %1678 = load i16, i16* %arrayidx9.6.1.35, align 2, !tbaa !3 *)
mov v1678 mem0_286;
(*   %conv1.i.6.1.35 = sext i16 %1678 to i32 *)
cast v_conv1_i_6_1_35@sint32 v1678@sint16;
(*   %mul.i.6.1.35 = mul nsw i32 %conv1.i.6.1.35, 610 *)
mul v_mul_i_6_1_35 v_conv1_i_6_1_35 (610)@sint32;
(*   %call.i.6.1.35 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.35) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_35, v_call_i_6_1_35);
(*   %arrayidx11.6.1.35 = getelementptr inbounds i16, i16* %r, i64 141 *)
(*   %1679 = load i16, i16* %arrayidx11.6.1.35, align 2, !tbaa !3 *)
mov v1679 mem0_282;
(*   %sub.6.1.35 = sub i16 %1679, %call.i.6.1.35 *)
sub v_sub_6_1_35 v1679 v_call_i_6_1_35;
(*   store i16 %sub.6.1.35, i16* %arrayidx9.6.1.35, align 2, !tbaa !3 *)
mov mem0_286 v_sub_6_1_35;
(*   %add21.6.1.35 = add i16 %1679, %call.i.6.1.35 *)
add v_add21_6_1_35 v1679 v_call_i_6_1_35;
(*   store i16 %add21.6.1.35, i16* %arrayidx11.6.1.35, align 2, !tbaa !3 *)
mov mem0_282 v_add21_6_1_35;

(* NOTE: k = 100 *)

(*   %arrayidx9.6.36 = getelementptr inbounds i16, i16* %r, i64 146 *)
(*   %1680 = load i16, i16* %arrayidx9.6.36, align 2, !tbaa !3 *)
mov v1680 mem0_292;
(*   %conv1.i.6.36 = sext i16 %1680 to i32 *)
cast v_conv1_i_6_36@sint32 v1680@sint16;
(*   %mul.i.6.36 = mul nsw i32 %conv1.i.6.36, 1322 *)
mul v_mul_i_6_36 v_conv1_i_6_36 (1322)@sint32;
(*   %call.i.6.36 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.36) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_36, v_call_i_6_36);
(*   %arrayidx11.6.36 = getelementptr inbounds i16, i16* %r, i64 144 *)
(*   %1681 = load i16, i16* %arrayidx11.6.36, align 2, !tbaa !3 *)
mov v1681 mem0_288;
(*   %sub.6.36 = sub i16 %1681, %call.i.6.36 *)
sub v_sub_6_36 v1681 v_call_i_6_36;
(*   store i16 %sub.6.36, i16* %arrayidx9.6.36, align 2, !tbaa !3 *)
mov mem0_292 v_sub_6_36;
(*   %add21.6.36 = add i16 %1681, %call.i.6.36 *)
add v_add21_6_36 v1681 v_call_i_6_36;
(*   store i16 %add21.6.36, i16* %arrayidx11.6.36, align 2, !tbaa !3 *)
mov mem0_288 v_add21_6_36;
(*   %arrayidx9.6.1.36 = getelementptr inbounds i16, i16* %r, i64 147 *)
(*   %1682 = load i16, i16* %arrayidx9.6.1.36, align 2, !tbaa !3 *)
mov v1682 mem0_294;
(*   %conv1.i.6.1.36 = sext i16 %1682 to i32 *)
cast v_conv1_i_6_1_36@sint32 v1682@sint16;
(*   %mul.i.6.1.36 = mul nsw i32 %conv1.i.6.1.36, 1322 *)
mul v_mul_i_6_1_36 v_conv1_i_6_1_36 (1322)@sint32;
(*   %call.i.6.1.36 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.36) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_36, v_call_i_6_1_36);
(*   %arrayidx11.6.1.36 = getelementptr inbounds i16, i16* %r, i64 145 *)
(*   %1683 = load i16, i16* %arrayidx11.6.1.36, align 2, !tbaa !3 *)
mov v1683 mem0_290;
(*   %sub.6.1.36 = sub i16 %1683, %call.i.6.1.36 *)
sub v_sub_6_1_36 v1683 v_call_i_6_1_36;
(*   store i16 %sub.6.1.36, i16* %arrayidx9.6.1.36, align 2, !tbaa !3 *)
mov mem0_294 v_sub_6_1_36;
(*   %add21.6.1.36 = add i16 %1683, %call.i.6.1.36 *)
add v_add21_6_1_36 v1683 v_call_i_6_1_36;
(*   store i16 %add21.6.1.36, i16* %arrayidx11.6.1.36, align 2, !tbaa !3 *)
mov mem0_290 v_add21_6_1_36;

(* NOTE: k = 101 *)

(*   %arrayidx9.6.37 = getelementptr inbounds i16, i16* %r, i64 150 *)
(*   %1684 = load i16, i16* %arrayidx9.6.37, align 2, !tbaa !3 *)
mov v1684 mem0_300;
(*   %conv1.i.6.37 = sext i16 %1684 to i32 *)
cast v_conv1_i_6_37@sint32 v1684@sint16;
(*   %mul.i.6.37 = mul nsw i32 %conv1.i.6.37, -1285 *)
mul v_mul_i_6_37 v_conv1_i_6_37 (-1285)@sint32;
(*   %call.i.6.37 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.37) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_37, v_call_i_6_37);
(*   %arrayidx11.6.37 = getelementptr inbounds i16, i16* %r, i64 148 *)
(*   %1685 = load i16, i16* %arrayidx11.6.37, align 2, !tbaa !3 *)
mov v1685 mem0_296;
(*   %sub.6.37 = sub i16 %1685, %call.i.6.37 *)
sub v_sub_6_37 v1685 v_call_i_6_37;
(*   store i16 %sub.6.37, i16* %arrayidx9.6.37, align 2, !tbaa !3 *)
mov mem0_300 v_sub_6_37;
(*   %add21.6.37 = add i16 %1685, %call.i.6.37 *)
add v_add21_6_37 v1685 v_call_i_6_37;
(*   store i16 %add21.6.37, i16* %arrayidx11.6.37, align 2, !tbaa !3 *)
mov mem0_296 v_add21_6_37;
(*   %arrayidx9.6.1.37 = getelementptr inbounds i16, i16* %r, i64 151 *)
(*   %1686 = load i16, i16* %arrayidx9.6.1.37, align 2, !tbaa !3 *)
mov v1686 mem0_302;
(*   %conv1.i.6.1.37 = sext i16 %1686 to i32 *)
cast v_conv1_i_6_1_37@sint32 v1686@sint16;
(*   %mul.i.6.1.37 = mul nsw i32 %conv1.i.6.1.37, -1285 *)
mul v_mul_i_6_1_37 v_conv1_i_6_1_37 (-1285)@sint32;
(*   %call.i.6.1.37 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.37) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_37, v_call_i_6_1_37);
(*   %arrayidx11.6.1.37 = getelementptr inbounds i16, i16* %r, i64 149 *)
(*   %1687 = load i16, i16* %arrayidx11.6.1.37, align 2, !tbaa !3 *)
mov v1687 mem0_298;
(*   %sub.6.1.37 = sub i16 %1687, %call.i.6.1.37 *)
sub v_sub_6_1_37 v1687 v_call_i_6_1_37;
(*   store i16 %sub.6.1.37, i16* %arrayidx9.6.1.37, align 2, !tbaa !3 *)
mov mem0_302 v_sub_6_1_37;
(*   %add21.6.1.37 = add i16 %1687, %call.i.6.1.37 *)
add v_add21_6_1_37 v1687 v_call_i_6_1_37;
(*   store i16 %add21.6.1.37, i16* %arrayidx11.6.1.37, align 2, !tbaa !3 *)
mov mem0_298 v_add21_6_1_37;

(* NOTE: k = 102 *)

(*   %arrayidx9.6.38 = getelementptr inbounds i16, i16* %r, i64 154 *)
(*   %1688 = load i16, i16* %arrayidx9.6.38, align 2, !tbaa !3 *)
mov v1688 mem0_308;
(*   %conv1.i.6.38 = sext i16 %1688 to i32 *)
cast v_conv1_i_6_38@sint32 v1688@sint16;
(*   %mul.i.6.38 = mul nsw i32 %conv1.i.6.38, -1465 *)
mul v_mul_i_6_38 v_conv1_i_6_38 (-1465)@sint32;
(*   %call.i.6.38 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.38) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_38, v_call_i_6_38);
(*   %arrayidx11.6.38 = getelementptr inbounds i16, i16* %r, i64 152 *)
(*   %1689 = load i16, i16* %arrayidx11.6.38, align 2, !tbaa !3 *)
mov v1689 mem0_304;
(*   %sub.6.38 = sub i16 %1689, %call.i.6.38 *)
sub v_sub_6_38 v1689 v_call_i_6_38;
(*   store i16 %sub.6.38, i16* %arrayidx9.6.38, align 2, !tbaa !3 *)
mov mem0_308 v_sub_6_38;
(*   %add21.6.38 = add i16 %1689, %call.i.6.38 *)
add v_add21_6_38 v1689 v_call_i_6_38;
(*   store i16 %add21.6.38, i16* %arrayidx11.6.38, align 2, !tbaa !3 *)
mov mem0_304 v_add21_6_38;
(*   %arrayidx9.6.1.38 = getelementptr inbounds i16, i16* %r, i64 155 *)
(*   %1690 = load i16, i16* %arrayidx9.6.1.38, align 2, !tbaa !3 *)
mov v1690 mem0_310;
(*   %conv1.i.6.1.38 = sext i16 %1690 to i32 *)
cast v_conv1_i_6_1_38@sint32 v1690@sint16;
(*   %mul.i.6.1.38 = mul nsw i32 %conv1.i.6.1.38, -1465 *)
mul v_mul_i_6_1_38 v_conv1_i_6_1_38 (-1465)@sint32;
(*   %call.i.6.1.38 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.38) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_38, v_call_i_6_1_38);
(*   %arrayidx11.6.1.38 = getelementptr inbounds i16, i16* %r, i64 153 *)
(*   %1691 = load i16, i16* %arrayidx11.6.1.38, align 2, !tbaa !3 *)
mov v1691 mem0_306;
(*   %sub.6.1.38 = sub i16 %1691, %call.i.6.1.38 *)
sub v_sub_6_1_38 v1691 v_call_i_6_1_38;
(*   store i16 %sub.6.1.38, i16* %arrayidx9.6.1.38, align 2, !tbaa !3 *)
mov mem0_310 v_sub_6_1_38;
(*   %add21.6.1.38 = add i16 %1691, %call.i.6.1.38 *)
add v_add21_6_1_38 v1691 v_call_i_6_1_38;
(*   store i16 %add21.6.1.38, i16* %arrayidx11.6.1.38, align 2, !tbaa !3 *)
mov mem0_306 v_add21_6_1_38;

(* NOTE: k = 103 *)

(*   %arrayidx9.6.39 = getelementptr inbounds i16, i16* %r, i64 158 *)
(*   %1692 = load i16, i16* %arrayidx9.6.39, align 2, !tbaa !3 *)
mov v1692 mem0_316;
(*   %conv1.i.6.39 = sext i16 %1692 to i32 *)
cast v_conv1_i_6_39@sint32 v1692@sint16;
(*   %mul.i.6.39 = mul nsw i32 %conv1.i.6.39, 384 *)
mul v_mul_i_6_39 v_conv1_i_6_39 (384)@sint32;
(*   %call.i.6.39 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.39) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_39, v_call_i_6_39);
(*   %arrayidx11.6.39 = getelementptr inbounds i16, i16* %r, i64 156 *)
(*   %1693 = load i16, i16* %arrayidx11.6.39, align 2, !tbaa !3 *)
mov v1693 mem0_312;
(*   %sub.6.39 = sub i16 %1693, %call.i.6.39 *)
sub v_sub_6_39 v1693 v_call_i_6_39;
(*   store i16 %sub.6.39, i16* %arrayidx9.6.39, align 2, !tbaa !3 *)
mov mem0_316 v_sub_6_39;
(*   %add21.6.39 = add i16 %1693, %call.i.6.39 *)
add v_add21_6_39 v1693 v_call_i_6_39;
(*   store i16 %add21.6.39, i16* %arrayidx11.6.39, align 2, !tbaa !3 *)
mov mem0_312 v_add21_6_39;
(*   %arrayidx9.6.1.39 = getelementptr inbounds i16, i16* %r, i64 159 *)
(*   %1694 = load i16, i16* %arrayidx9.6.1.39, align 2, !tbaa !3 *)
mov v1694 mem0_318;
(*   %conv1.i.6.1.39 = sext i16 %1694 to i32 *)
cast v_conv1_i_6_1_39@sint32 v1694@sint16;
(*   %mul.i.6.1.39 = mul nsw i32 %conv1.i.6.1.39, 384 *)
mul v_mul_i_6_1_39 v_conv1_i_6_1_39 (384)@sint32;
(*   %call.i.6.1.39 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.39) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_39, v_call_i_6_1_39);
(*   %arrayidx11.6.1.39 = getelementptr inbounds i16, i16* %r, i64 157 *)
(*   %1695 = load i16, i16* %arrayidx11.6.1.39, align 2, !tbaa !3 *)
mov v1695 mem0_314;
(*   %sub.6.1.39 = sub i16 %1695, %call.i.6.1.39 *)
sub v_sub_6_1_39 v1695 v_call_i_6_1_39;
(*   store i16 %sub.6.1.39, i16* %arrayidx9.6.1.39, align 2, !tbaa !3 *)
mov mem0_318 v_sub_6_1_39;
(*   %add21.6.1.39 = add i16 %1695, %call.i.6.1.39 *)
add v_add21_6_1_39 v1695 v_call_i_6_1_39;
(*   store i16 %add21.6.1.39, i16* %arrayidx11.6.1.39, align 2, !tbaa !3 *)
mov mem0_314 v_add21_6_1_39;

(* NOTE: k = 104 *)

(*   %arrayidx9.6.40 = getelementptr inbounds i16, i16* %r, i64 162 *)
(*   %1696 = load i16, i16* %arrayidx9.6.40, align 2, !tbaa !3 *)
mov v1696 mem0_324;
(*   %conv1.i.6.40 = sext i16 %1696 to i32 *)
cast v_conv1_i_6_40@sint32 v1696@sint16;
(*   %mul.i.6.40 = mul nsw i32 %conv1.i.6.40, -1215 *)
mul v_mul_i_6_40 v_conv1_i_6_40 (-1215)@sint32;
(*   %call.i.6.40 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.40) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_40, v_call_i_6_40);
(*   %arrayidx11.6.40 = getelementptr inbounds i16, i16* %r, i64 160 *)
(*   %1697 = load i16, i16* %arrayidx11.6.40, align 2, !tbaa !3 *)
mov v1697 mem0_320;
(*   %sub.6.40 = sub i16 %1697, %call.i.6.40 *)
sub v_sub_6_40 v1697 v_call_i_6_40;
(*   store i16 %sub.6.40, i16* %arrayidx9.6.40, align 2, !tbaa !3 *)
mov mem0_324 v_sub_6_40;
(*   %add21.6.40 = add i16 %1697, %call.i.6.40 *)
add v_add21_6_40 v1697 v_call_i_6_40;
(*   store i16 %add21.6.40, i16* %arrayidx11.6.40, align 2, !tbaa !3 *)
mov mem0_320 v_add21_6_40;
(*   %arrayidx9.6.1.40 = getelementptr inbounds i16, i16* %r, i64 163 *)
(*   %1698 = load i16, i16* %arrayidx9.6.1.40, align 2, !tbaa !3 *)
mov v1698 mem0_326;
(*   %conv1.i.6.1.40 = sext i16 %1698 to i32 *)
cast v_conv1_i_6_1_40@sint32 v1698@sint16;
(*   %mul.i.6.1.40 = mul nsw i32 %conv1.i.6.1.40, -1215 *)
mul v_mul_i_6_1_40 v_conv1_i_6_1_40 (-1215)@sint32;
(*   %call.i.6.1.40 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.40) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_40, v_call_i_6_1_40);
(*   %arrayidx11.6.1.40 = getelementptr inbounds i16, i16* %r, i64 161 *)
(*   %1699 = load i16, i16* %arrayidx11.6.1.40, align 2, !tbaa !3 *)
mov v1699 mem0_322;
(*   %sub.6.1.40 = sub i16 %1699, %call.i.6.1.40 *)
sub v_sub_6_1_40 v1699 v_call_i_6_1_40;
(*   store i16 %sub.6.1.40, i16* %arrayidx9.6.1.40, align 2, !tbaa !3 *)
mov mem0_326 v_sub_6_1_40;
(*   %add21.6.1.40 = add i16 %1699, %call.i.6.1.40 *)
add v_add21_6_1_40 v1699 v_call_i_6_1_40;
(*   store i16 %add21.6.1.40, i16* %arrayidx11.6.1.40, align 2, !tbaa !3 *)
mov mem0_322 v_add21_6_1_40;

(* NOTE: k = 105 *)

(*   %arrayidx9.6.41 = getelementptr inbounds i16, i16* %r, i64 166 *)
(*   %1700 = load i16, i16* %arrayidx9.6.41, align 2, !tbaa !3 *)
mov v1700 mem0_332;
(*   %conv1.i.6.41 = sext i16 %1700 to i32 *)
cast v_conv1_i_6_41@sint32 v1700@sint16;
(*   %mul.i.6.41 = mul nsw i32 %conv1.i.6.41, -136 *)
mul v_mul_i_6_41 v_conv1_i_6_41 (-136)@sint32;
(*   %call.i.6.41 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.41) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_41, v_call_i_6_41);
(*   %arrayidx11.6.41 = getelementptr inbounds i16, i16* %r, i64 164 *)
(*   %1701 = load i16, i16* %arrayidx11.6.41, align 2, !tbaa !3 *)
mov v1701 mem0_328;
(*   %sub.6.41 = sub i16 %1701, %call.i.6.41 *)
sub v_sub_6_41 v1701 v_call_i_6_41;
(*   store i16 %sub.6.41, i16* %arrayidx9.6.41, align 2, !tbaa !3 *)
mov mem0_332 v_sub_6_41;
(*   %add21.6.41 = add i16 %1701, %call.i.6.41 *)
add v_add21_6_41 v1701 v_call_i_6_41;
(*   store i16 %add21.6.41, i16* %arrayidx11.6.41, align 2, !tbaa !3 *)
mov mem0_328 v_add21_6_41;
(*   %arrayidx9.6.1.41 = getelementptr inbounds i16, i16* %r, i64 167 *)
(*   %1702 = load i16, i16* %arrayidx9.6.1.41, align 2, !tbaa !3 *)
mov v1702 mem0_334;
(*   %conv1.i.6.1.41 = sext i16 %1702 to i32 *)
cast v_conv1_i_6_1_41@sint32 v1702@sint16;
(*   %mul.i.6.1.41 = mul nsw i32 %conv1.i.6.1.41, -136 *)
mul v_mul_i_6_1_41 v_conv1_i_6_1_41 (-136)@sint32;
(*   %call.i.6.1.41 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.41) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_41, v_call_i_6_1_41);
(*   %arrayidx11.6.1.41 = getelementptr inbounds i16, i16* %r, i64 165 *)
(*   %1703 = load i16, i16* %arrayidx11.6.1.41, align 2, !tbaa !3 *)
mov v1703 mem0_330;
(*   %sub.6.1.41 = sub i16 %1703, %call.i.6.1.41 *)
sub v_sub_6_1_41 v1703 v_call_i_6_1_41;
(*   store i16 %sub.6.1.41, i16* %arrayidx9.6.1.41, align 2, !tbaa !3 *)
mov mem0_334 v_sub_6_1_41;
(*   %add21.6.1.41 = add i16 %1703, %call.i.6.1.41 *)
add v_add21_6_1_41 v1703 v_call_i_6_1_41;
(*   store i16 %add21.6.1.41, i16* %arrayidx11.6.1.41, align 2, !tbaa !3 *)
mov mem0_330 v_add21_6_1_41;

(* NOTE: k = 106 *)

(*   %arrayidx9.6.42 = getelementptr inbounds i16, i16* %r, i64 170 *)
(*   %1704 = load i16, i16* %arrayidx9.6.42, align 2, !tbaa !3 *)
mov v1704 mem0_340;
(*   %conv1.i.6.42 = sext i16 %1704 to i32 *)
cast v_conv1_i_6_42@sint32 v1704@sint16;
(*   %mul.i.6.42 = mul nsw i32 %conv1.i.6.42, 1218 *)
mul v_mul_i_6_42 v_conv1_i_6_42 (1218)@sint32;
(*   %call.i.6.42 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.42) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_42, v_call_i_6_42);
(*   %arrayidx11.6.42 = getelementptr inbounds i16, i16* %r, i64 168 *)
(*   %1705 = load i16, i16* %arrayidx11.6.42, align 2, !tbaa !3 *)
mov v1705 mem0_336;
(*   %sub.6.42 = sub i16 %1705, %call.i.6.42 *)
sub v_sub_6_42 v1705 v_call_i_6_42;
(*   store i16 %sub.6.42, i16* %arrayidx9.6.42, align 2, !tbaa !3 *)
mov mem0_340 v_sub_6_42;
(*   %add21.6.42 = add i16 %1705, %call.i.6.42 *)
add v_add21_6_42 v1705 v_call_i_6_42;
(*   store i16 %add21.6.42, i16* %arrayidx11.6.42, align 2, !tbaa !3 *)
mov mem0_336 v_add21_6_42;
(*   %arrayidx9.6.1.42 = getelementptr inbounds i16, i16* %r, i64 171 *)
(*   %1706 = load i16, i16* %arrayidx9.6.1.42, align 2, !tbaa !3 *)
mov v1706 mem0_342;
(*   %conv1.i.6.1.42 = sext i16 %1706 to i32 *)
cast v_conv1_i_6_1_42@sint32 v1706@sint16;
(*   %mul.i.6.1.42 = mul nsw i32 %conv1.i.6.1.42, 1218 *)
mul v_mul_i_6_1_42 v_conv1_i_6_1_42 (1218)@sint32;
(*   %call.i.6.1.42 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.42) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_42, v_call_i_6_1_42);
(*   %arrayidx11.6.1.42 = getelementptr inbounds i16, i16* %r, i64 169 *)
(*   %1707 = load i16, i16* %arrayidx11.6.1.42, align 2, !tbaa !3 *)
mov v1707 mem0_338;
(*   %sub.6.1.42 = sub i16 %1707, %call.i.6.1.42 *)
sub v_sub_6_1_42 v1707 v_call_i_6_1_42;
(*   store i16 %sub.6.1.42, i16* %arrayidx9.6.1.42, align 2, !tbaa !3 *)
mov mem0_342 v_sub_6_1_42;
(*   %add21.6.1.42 = add i16 %1707, %call.i.6.1.42 *)
add v_add21_6_1_42 v1707 v_call_i_6_1_42;
(*   store i16 %add21.6.1.42, i16* %arrayidx11.6.1.42, align 2, !tbaa !3 *)
mov mem0_338 v_add21_6_1_42;

(* NOTE: k = 107 *)

(*   %arrayidx9.6.43 = getelementptr inbounds i16, i16* %r, i64 174 *)
(*   %1708 = load i16, i16* %arrayidx9.6.43, align 2, !tbaa !3 *)
mov v1708 mem0_348;
(*   %conv1.i.6.43 = sext i16 %1708 to i32 *)
cast v_conv1_i_6_43@sint32 v1708@sint16;
(*   %mul.i.6.43 = mul nsw i32 %conv1.i.6.43, -1335 *)
mul v_mul_i_6_43 v_conv1_i_6_43 (-1335)@sint32;
(*   %call.i.6.43 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.43) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_43, v_call_i_6_43);
(*   %arrayidx11.6.43 = getelementptr inbounds i16, i16* %r, i64 172 *)
(*   %1709 = load i16, i16* %arrayidx11.6.43, align 2, !tbaa !3 *)
mov v1709 mem0_344;
(*   %sub.6.43 = sub i16 %1709, %call.i.6.43 *)
sub v_sub_6_43 v1709 v_call_i_6_43;
(*   store i16 %sub.6.43, i16* %arrayidx9.6.43, align 2, !tbaa !3 *)
mov mem0_348 v_sub_6_43;
(*   %add21.6.43 = add i16 %1709, %call.i.6.43 *)
add v_add21_6_43 v1709 v_call_i_6_43;
(*   store i16 %add21.6.43, i16* %arrayidx11.6.43, align 2, !tbaa !3 *)
mov mem0_344 v_add21_6_43;
(*   %arrayidx9.6.1.43 = getelementptr inbounds i16, i16* %r, i64 175 *)
(*   %1710 = load i16, i16* %arrayidx9.6.1.43, align 2, !tbaa !3 *)
mov v1710 mem0_350;
(*   %conv1.i.6.1.43 = sext i16 %1710 to i32 *)
cast v_conv1_i_6_1_43@sint32 v1710@sint16;
(*   %mul.i.6.1.43 = mul nsw i32 %conv1.i.6.1.43, -1335 *)
mul v_mul_i_6_1_43 v_conv1_i_6_1_43 (-1335)@sint32;
(*   %call.i.6.1.43 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.43) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_43, v_call_i_6_1_43);
(*   %arrayidx11.6.1.43 = getelementptr inbounds i16, i16* %r, i64 173 *)
(*   %1711 = load i16, i16* %arrayidx11.6.1.43, align 2, !tbaa !3 *)
mov v1711 mem0_346;
(*   %sub.6.1.43 = sub i16 %1711, %call.i.6.1.43 *)
sub v_sub_6_1_43 v1711 v_call_i_6_1_43;
(*   store i16 %sub.6.1.43, i16* %arrayidx9.6.1.43, align 2, !tbaa !3 *)
mov mem0_350 v_sub_6_1_43;
(*   %add21.6.1.43 = add i16 %1711, %call.i.6.1.43 *)
add v_add21_6_1_43 v1711 v_call_i_6_1_43;
(*   store i16 %add21.6.1.43, i16* %arrayidx11.6.1.43, align 2, !tbaa !3 *)
mov mem0_346 v_add21_6_1_43;

(* NOTE: k = 108 *)

(*   %arrayidx9.6.44 = getelementptr inbounds i16, i16* %r, i64 178 *)
(*   %1712 = load i16, i16* %arrayidx9.6.44, align 2, !tbaa !3 *)
mov v1712 mem0_356;
(*   %conv1.i.6.44 = sext i16 %1712 to i32 *)
cast v_conv1_i_6_44@sint32 v1712@sint16;
(*   %mul.i.6.44 = mul nsw i32 %conv1.i.6.44, -874 *)
mul v_mul_i_6_44 v_conv1_i_6_44 (-874)@sint32;
(*   %call.i.6.44 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.44) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_44, v_call_i_6_44);
(*   %arrayidx11.6.44 = getelementptr inbounds i16, i16* %r, i64 176 *)
(*   %1713 = load i16, i16* %arrayidx11.6.44, align 2, !tbaa !3 *)
mov v1713 mem0_352;
(*   %sub.6.44 = sub i16 %1713, %call.i.6.44 *)
sub v_sub_6_44 v1713 v_call_i_6_44;
(*   store i16 %sub.6.44, i16* %arrayidx9.6.44, align 2, !tbaa !3 *)
mov mem0_356 v_sub_6_44;
(*   %add21.6.44 = add i16 %1713, %call.i.6.44 *)
add v_add21_6_44 v1713 v_call_i_6_44;
(*   store i16 %add21.6.44, i16* %arrayidx11.6.44, align 2, !tbaa !3 *)
mov mem0_352 v_add21_6_44;
(*   %arrayidx9.6.1.44 = getelementptr inbounds i16, i16* %r, i64 179 *)
(*   %1714 = load i16, i16* %arrayidx9.6.1.44, align 2, !tbaa !3 *)
mov v1714 mem0_358;
(*   %conv1.i.6.1.44 = sext i16 %1714 to i32 *)
cast v_conv1_i_6_1_44@sint32 v1714@sint16;
(*   %mul.i.6.1.44 = mul nsw i32 %conv1.i.6.1.44, -874 *)
mul v_mul_i_6_1_44 v_conv1_i_6_1_44 (-874)@sint32;
(*   %call.i.6.1.44 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.44) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_44, v_call_i_6_1_44);
(*   %arrayidx11.6.1.44 = getelementptr inbounds i16, i16* %r, i64 177 *)
(*   %1715 = load i16, i16* %arrayidx11.6.1.44, align 2, !tbaa !3 *)
mov v1715 mem0_354;
(*   %sub.6.1.44 = sub i16 %1715, %call.i.6.1.44 *)
sub v_sub_6_1_44 v1715 v_call_i_6_1_44;
(*   store i16 %sub.6.1.44, i16* %arrayidx9.6.1.44, align 2, !tbaa !3 *)
mov mem0_358 v_sub_6_1_44;
(*   %add21.6.1.44 = add i16 %1715, %call.i.6.1.44 *)
add v_add21_6_1_44 v1715 v_call_i_6_1_44;
(*   store i16 %add21.6.1.44, i16* %arrayidx11.6.1.44, align 2, !tbaa !3 *)
mov mem0_354 v_add21_6_1_44;

(* NOTE: k = 109 *)

(*   %arrayidx9.6.45 = getelementptr inbounds i16, i16* %r, i64 182 *)
(*   %1716 = load i16, i16* %arrayidx9.6.45, align 2, !tbaa !3 *)
mov v1716 mem0_364;
(*   %conv1.i.6.45 = sext i16 %1716 to i32 *)
cast v_conv1_i_6_45@sint32 v1716@sint16;
(*   %mul.i.6.45 = mul nsw i32 %conv1.i.6.45, 220 *)
mul v_mul_i_6_45 v_conv1_i_6_45 (220)@sint32;
(*   %call.i.6.45 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.45) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_45, v_call_i_6_45);
(*   %arrayidx11.6.45 = getelementptr inbounds i16, i16* %r, i64 180 *)
(*   %1717 = load i16, i16* %arrayidx11.6.45, align 2, !tbaa !3 *)
mov v1717 mem0_360;
(*   %sub.6.45 = sub i16 %1717, %call.i.6.45 *)
sub v_sub_6_45 v1717 v_call_i_6_45;
(*   store i16 %sub.6.45, i16* %arrayidx9.6.45, align 2, !tbaa !3 *)
mov mem0_364 v_sub_6_45;
(*   %add21.6.45 = add i16 %1717, %call.i.6.45 *)
add v_add21_6_45 v1717 v_call_i_6_45;
(*   store i16 %add21.6.45, i16* %arrayidx11.6.45, align 2, !tbaa !3 *)
mov mem0_360 v_add21_6_45;
(*   %arrayidx9.6.1.45 = getelementptr inbounds i16, i16* %r, i64 183 *)
(*   %1718 = load i16, i16* %arrayidx9.6.1.45, align 2, !tbaa !3 *)
mov v1718 mem0_366;
(*   %conv1.i.6.1.45 = sext i16 %1718 to i32 *)
cast v_conv1_i_6_1_45@sint32 v1718@sint16;
(*   %mul.i.6.1.45 = mul nsw i32 %conv1.i.6.1.45, 220 *)
mul v_mul_i_6_1_45 v_conv1_i_6_1_45 (220)@sint32;
(*   %call.i.6.1.45 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.45) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_45, v_call_i_6_1_45);
(*   %arrayidx11.6.1.45 = getelementptr inbounds i16, i16* %r, i64 181 *)
(*   %1719 = load i16, i16* %arrayidx11.6.1.45, align 2, !tbaa !3 *)
mov v1719 mem0_362;
(*   %sub.6.1.45 = sub i16 %1719, %call.i.6.1.45 *)
sub v_sub_6_1_45 v1719 v_call_i_6_1_45;
(*   store i16 %sub.6.1.45, i16* %arrayidx9.6.1.45, align 2, !tbaa !3 *)
mov mem0_366 v_sub_6_1_45;
(*   %add21.6.1.45 = add i16 %1719, %call.i.6.1.45 *)
add v_add21_6_1_45 v1719 v_call_i_6_1_45;
(*   store i16 %add21.6.1.45, i16* %arrayidx11.6.1.45, align 2, !tbaa !3 *)
mov mem0_362 v_add21_6_1_45;

(* NOTE: k = 110 *)

(*   %arrayidx9.6.46 = getelementptr inbounds i16, i16* %r, i64 186 *)
(*   %1720 = load i16, i16* %arrayidx9.6.46, align 2, !tbaa !3 *)
mov v1720 mem0_372;
(*   %conv1.i.6.46 = sext i16 %1720 to i32 *)
cast v_conv1_i_6_46@sint32 v1720@sint16;
(*   %mul.i.6.46 = mul nsw i32 %conv1.i.6.46, -1187 *)
mul v_mul_i_6_46 v_conv1_i_6_46 (-1187)@sint32;
(*   %call.i.6.46 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.46) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_46, v_call_i_6_46);
(*   %arrayidx11.6.46 = getelementptr inbounds i16, i16* %r, i64 184 *)
(*   %1721 = load i16, i16* %arrayidx11.6.46, align 2, !tbaa !3 *)
mov v1721 mem0_368;
(*   %sub.6.46 = sub i16 %1721, %call.i.6.46 *)
sub v_sub_6_46 v1721 v_call_i_6_46;
(*   store i16 %sub.6.46, i16* %arrayidx9.6.46, align 2, !tbaa !3 *)
mov mem0_372 v_sub_6_46;
(*   %add21.6.46 = add i16 %1721, %call.i.6.46 *)
add v_add21_6_46 v1721 v_call_i_6_46;
(*   store i16 %add21.6.46, i16* %arrayidx11.6.46, align 2, !tbaa !3 *)
mov mem0_368 v_add21_6_46;
(*   %arrayidx9.6.1.46 = getelementptr inbounds i16, i16* %r, i64 187 *)
(*   %1722 = load i16, i16* %arrayidx9.6.1.46, align 2, !tbaa !3 *)
mov v1722 mem0_374;
(*   %conv1.i.6.1.46 = sext i16 %1722 to i32 *)
cast v_conv1_i_6_1_46@sint32 v1722@sint16;
(*   %mul.i.6.1.46 = mul nsw i32 %conv1.i.6.1.46, -1187 *)
mul v_mul_i_6_1_46 v_conv1_i_6_1_46 (-1187)@sint32;
(*   %call.i.6.1.46 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.46) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_46, v_call_i_6_1_46);
(*   %arrayidx11.6.1.46 = getelementptr inbounds i16, i16* %r, i64 185 *)
(*   %1723 = load i16, i16* %arrayidx11.6.1.46, align 2, !tbaa !3 *)
mov v1723 mem0_370;
(*   %sub.6.1.46 = sub i16 %1723, %call.i.6.1.46 *)
sub v_sub_6_1_46 v1723 v_call_i_6_1_46;
(*   store i16 %sub.6.1.46, i16* %arrayidx9.6.1.46, align 2, !tbaa !3 *)
mov mem0_374 v_sub_6_1_46;
(*   %add21.6.1.46 = add i16 %1723, %call.i.6.1.46 *)
add v_add21_6_1_46 v1723 v_call_i_6_1_46;
(*   store i16 %add21.6.1.46, i16* %arrayidx11.6.1.46, align 2, !tbaa !3 *)
mov mem0_370 v_add21_6_1_46;

(* NOTE: k = 111 *)

(*   %arrayidx9.6.47 = getelementptr inbounds i16, i16* %r, i64 190 *)
(*   %1724 = load i16, i16* %arrayidx9.6.47, align 2, !tbaa !3 *)
mov v1724 mem0_380;
(*   %conv1.i.6.47 = sext i16 %1724 to i32 *)
cast v_conv1_i_6_47@sint32 v1724@sint16;
(*   %mul.i.6.47 = mul nsw i32 %conv1.i.6.47, -1659 *)
mul v_mul_i_6_47 v_conv1_i_6_47 (-1659)@sint32;
(*   %call.i.6.47 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.47) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_47, v_call_i_6_47);
(*   %arrayidx11.6.47 = getelementptr inbounds i16, i16* %r, i64 188 *)
(*   %1725 = load i16, i16* %arrayidx11.6.47, align 2, !tbaa !3 *)
mov v1725 mem0_376;
(*   %sub.6.47 = sub i16 %1725, %call.i.6.47 *)
sub v_sub_6_47 v1725 v_call_i_6_47;
(*   store i16 %sub.6.47, i16* %arrayidx9.6.47, align 2, !tbaa !3 *)
mov mem0_380 v_sub_6_47;
(*   %add21.6.47 = add i16 %1725, %call.i.6.47 *)
add v_add21_6_47 v1725 v_call_i_6_47;
(*   store i16 %add21.6.47, i16* %arrayidx11.6.47, align 2, !tbaa !3 *)
mov mem0_376 v_add21_6_47;
(*   %arrayidx9.6.1.47 = getelementptr inbounds i16, i16* %r, i64 191 *)
(*   %1726 = load i16, i16* %arrayidx9.6.1.47, align 2, !tbaa !3 *)
mov v1726 mem0_382;
(*   %conv1.i.6.1.47 = sext i16 %1726 to i32 *)
cast v_conv1_i_6_1_47@sint32 v1726@sint16;
(*   %mul.i.6.1.47 = mul nsw i32 %conv1.i.6.1.47, -1659 *)
mul v_mul_i_6_1_47 v_conv1_i_6_1_47 (-1659)@sint32;
(*   %call.i.6.1.47 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.47) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_47, v_call_i_6_1_47);
(*   %arrayidx11.6.1.47 = getelementptr inbounds i16, i16* %r, i64 189 *)
(*   %1727 = load i16, i16* %arrayidx11.6.1.47, align 2, !tbaa !3 *)
mov v1727 mem0_378;
(*   %sub.6.1.47 = sub i16 %1727, %call.i.6.1.47 *)
sub v_sub_6_1_47 v1727 v_call_i_6_1_47;
(*   store i16 %sub.6.1.47, i16* %arrayidx9.6.1.47, align 2, !tbaa !3 *)
mov mem0_382 v_sub_6_1_47;
(*   %add21.6.1.47 = add i16 %1727, %call.i.6.1.47 *)
add v_add21_6_1_47 v1727 v_call_i_6_1_47;
(*   store i16 %add21.6.1.47, i16* %arrayidx11.6.1.47, align 2, !tbaa !3 *)
mov mem0_378 v_add21_6_1_47;

(* NOTE: k = 112 *)

(*   %arrayidx9.6.48 = getelementptr inbounds i16, i16* %r, i64 194 *)
(*   %1728 = load i16, i16* %arrayidx9.6.48, align 2, !tbaa !3 *)
mov v1728 mem0_388;
(*   %conv1.i.6.48 = sext i16 %1728 to i32 *)
cast v_conv1_i_6_48@sint32 v1728@sint16;
(*   %mul.i.6.48 = mul nsw i32 %conv1.i.6.48, -1185 *)
mul v_mul_i_6_48 v_conv1_i_6_48 (-1185)@sint32;
(*   %call.i.6.48 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.48) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_48, v_call_i_6_48);
(*   %arrayidx11.6.48 = getelementptr inbounds i16, i16* %r, i64 192 *)
(*   %1729 = load i16, i16* %arrayidx11.6.48, align 2, !tbaa !3 *)
mov v1729 mem0_384;
(*   %sub.6.48 = sub i16 %1729, %call.i.6.48 *)
sub v_sub_6_48 v1729 v_call_i_6_48;
(*   store i16 %sub.6.48, i16* %arrayidx9.6.48, align 2, !tbaa !3 *)
mov mem0_388 v_sub_6_48;
(*   %add21.6.48 = add i16 %1729, %call.i.6.48 *)
add v_add21_6_48 v1729 v_call_i_6_48;
(*   store i16 %add21.6.48, i16* %arrayidx11.6.48, align 2, !tbaa !3 *)
mov mem0_384 v_add21_6_48;
(*   %arrayidx9.6.1.48 = getelementptr inbounds i16, i16* %r, i64 195 *)
(*   %1730 = load i16, i16* %arrayidx9.6.1.48, align 2, !tbaa !3 *)
mov v1730 mem0_390;
(*   %conv1.i.6.1.48 = sext i16 %1730 to i32 *)
cast v_conv1_i_6_1_48@sint32 v1730@sint16;
(*   %mul.i.6.1.48 = mul nsw i32 %conv1.i.6.1.48, -1185 *)
mul v_mul_i_6_1_48 v_conv1_i_6_1_48 (-1185)@sint32;
(*   %call.i.6.1.48 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.48) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_48, v_call_i_6_1_48);
(*   %arrayidx11.6.1.48 = getelementptr inbounds i16, i16* %r, i64 193 *)
(*   %1731 = load i16, i16* %arrayidx11.6.1.48, align 2, !tbaa !3 *)
mov v1731 mem0_386;
(*   %sub.6.1.48 = sub i16 %1731, %call.i.6.1.48 *)
sub v_sub_6_1_48 v1731 v_call_i_6_1_48;
(*   store i16 %sub.6.1.48, i16* %arrayidx9.6.1.48, align 2, !tbaa !3 *)
mov mem0_390 v_sub_6_1_48;
(*   %add21.6.1.48 = add i16 %1731, %call.i.6.1.48 *)
add v_add21_6_1_48 v1731 v_call_i_6_1_48;
(*   store i16 %add21.6.1.48, i16* %arrayidx11.6.1.48, align 2, !tbaa !3 *)
mov mem0_386 v_add21_6_1_48;

(* NOTE: k = 113 *)

(*   %arrayidx9.6.49 = getelementptr inbounds i16, i16* %r, i64 198 *)
(*   %1732 = load i16, i16* %arrayidx9.6.49, align 2, !tbaa !3 *)
mov v1732 mem0_396;
(*   %conv1.i.6.49 = sext i16 %1732 to i32 *)
cast v_conv1_i_6_49@sint32 v1732@sint16;
(*   %mul.i.6.49 = mul nsw i32 %conv1.i.6.49, -1530 *)
mul v_mul_i_6_49 v_conv1_i_6_49 (-1530)@sint32;
(*   %call.i.6.49 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.49) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_49, v_call_i_6_49);
(*   %arrayidx11.6.49 = getelementptr inbounds i16, i16* %r, i64 196 *)
(*   %1733 = load i16, i16* %arrayidx11.6.49, align 2, !tbaa !3 *)
mov v1733 mem0_392;
(*   %sub.6.49 = sub i16 %1733, %call.i.6.49 *)
sub v_sub_6_49 v1733 v_call_i_6_49;
(*   store i16 %sub.6.49, i16* %arrayidx9.6.49, align 2, !tbaa !3 *)
mov mem0_396 v_sub_6_49;
(*   %add21.6.49 = add i16 %1733, %call.i.6.49 *)
add v_add21_6_49 v1733 v_call_i_6_49;
(*   store i16 %add21.6.49, i16* %arrayidx11.6.49, align 2, !tbaa !3 *)
mov mem0_392 v_add21_6_49;
(*   %arrayidx9.6.1.49 = getelementptr inbounds i16, i16* %r, i64 199 *)
(*   %1734 = load i16, i16* %arrayidx9.6.1.49, align 2, !tbaa !3 *)
mov v1734 mem0_398;
(*   %conv1.i.6.1.49 = sext i16 %1734 to i32 *)
cast v_conv1_i_6_1_49@sint32 v1734@sint16;
(*   %mul.i.6.1.49 = mul nsw i32 %conv1.i.6.1.49, -1530 *)
mul v_mul_i_6_1_49 v_conv1_i_6_1_49 (-1530)@sint32;
(*   %call.i.6.1.49 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.49) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_49, v_call_i_6_1_49);
(*   %arrayidx11.6.1.49 = getelementptr inbounds i16, i16* %r, i64 197 *)
(*   %1735 = load i16, i16* %arrayidx11.6.1.49, align 2, !tbaa !3 *)
mov v1735 mem0_394;
(*   %sub.6.1.49 = sub i16 %1735, %call.i.6.1.49 *)
sub v_sub_6_1_49 v1735 v_call_i_6_1_49;
(*   store i16 %sub.6.1.49, i16* %arrayidx9.6.1.49, align 2, !tbaa !3 *)
mov mem0_398 v_sub_6_1_49;
(*   %add21.6.1.49 = add i16 %1735, %call.i.6.1.49 *)
add v_add21_6_1_49 v1735 v_call_i_6_1_49;
(*   store i16 %add21.6.1.49, i16* %arrayidx11.6.1.49, align 2, !tbaa !3 *)
mov mem0_394 v_add21_6_1_49;

(* NOTE: k = 114 *)

(*   %arrayidx9.6.50 = getelementptr inbounds i16, i16* %r, i64 202 *)
(*   %1736 = load i16, i16* %arrayidx9.6.50, align 2, !tbaa !3 *)
mov v1736 mem0_404;
(*   %conv1.i.6.50 = sext i16 %1736 to i32 *)
cast v_conv1_i_6_50@sint32 v1736@sint16;
(*   %mul.i.6.50 = mul nsw i32 %conv1.i.6.50, -1278 *)
mul v_mul_i_6_50 v_conv1_i_6_50 (-1278)@sint32;
(*   %call.i.6.50 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.50) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_50, v_call_i_6_50);
(*   %arrayidx11.6.50 = getelementptr inbounds i16, i16* %r, i64 200 *)
(*   %1737 = load i16, i16* %arrayidx11.6.50, align 2, !tbaa !3 *)
mov v1737 mem0_400;
(*   %sub.6.50 = sub i16 %1737, %call.i.6.50 *)
sub v_sub_6_50 v1737 v_call_i_6_50;
(*   store i16 %sub.6.50, i16* %arrayidx9.6.50, align 2, !tbaa !3 *)
mov mem0_404 v_sub_6_50;
(*   %add21.6.50 = add i16 %1737, %call.i.6.50 *)
add v_add21_6_50 v1737 v_call_i_6_50;
(*   store i16 %add21.6.50, i16* %arrayidx11.6.50, align 2, !tbaa !3 *)
mov mem0_400 v_add21_6_50;
(*   %arrayidx9.6.1.50 = getelementptr inbounds i16, i16* %r, i64 203 *)
(*   %1738 = load i16, i16* %arrayidx9.6.1.50, align 2, !tbaa !3 *)
mov v1738 mem0_406;
(*   %conv1.i.6.1.50 = sext i16 %1738 to i32 *)
cast v_conv1_i_6_1_50@sint32 v1738@sint16;
(*   %mul.i.6.1.50 = mul nsw i32 %conv1.i.6.1.50, -1278 *)
mul v_mul_i_6_1_50 v_conv1_i_6_1_50 (-1278)@sint32;
(*   %call.i.6.1.50 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.50) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_50, v_call_i_6_1_50);
(*   %arrayidx11.6.1.50 = getelementptr inbounds i16, i16* %r, i64 201 *)
(*   %1739 = load i16, i16* %arrayidx11.6.1.50, align 2, !tbaa !3 *)
mov v1739 mem0_402;
(*   %sub.6.1.50 = sub i16 %1739, %call.i.6.1.50 *)
sub v_sub_6_1_50 v1739 v_call_i_6_1_50;
(*   store i16 %sub.6.1.50, i16* %arrayidx9.6.1.50, align 2, !tbaa !3 *)
mov mem0_406 v_sub_6_1_50;
(*   %add21.6.1.50 = add i16 %1739, %call.i.6.1.50 *)
add v_add21_6_1_50 v1739 v_call_i_6_1_50;
(*   store i16 %add21.6.1.50, i16* %arrayidx11.6.1.50, align 2, !tbaa !3 *)
mov mem0_402 v_add21_6_1_50;

(* NOTE: k = 115 *)

(*   %arrayidx9.6.51 = getelementptr inbounds i16, i16* %r, i64 206 *)
(*   %1740 = load i16, i16* %arrayidx9.6.51, align 2, !tbaa !3 *)
mov v1740 mem0_412;
(*   %conv1.i.6.51 = sext i16 %1740 to i32 *)
cast v_conv1_i_6_51@sint32 v1740@sint16;
(*   %mul.i.6.51 = mul nsw i32 %conv1.i.6.51, 794 *)
mul v_mul_i_6_51 v_conv1_i_6_51 (794)@sint32;
(*   %call.i.6.51 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.51) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_51, v_call_i_6_51);
(*   %arrayidx11.6.51 = getelementptr inbounds i16, i16* %r, i64 204 *)
(*   %1741 = load i16, i16* %arrayidx11.6.51, align 2, !tbaa !3 *)
mov v1741 mem0_408;
(*   %sub.6.51 = sub i16 %1741, %call.i.6.51 *)
sub v_sub_6_51 v1741 v_call_i_6_51;
(*   store i16 %sub.6.51, i16* %arrayidx9.6.51, align 2, !tbaa !3 *)
mov mem0_412 v_sub_6_51;
(*   %add21.6.51 = add i16 %1741, %call.i.6.51 *)
add v_add21_6_51 v1741 v_call_i_6_51;
(*   store i16 %add21.6.51, i16* %arrayidx11.6.51, align 2, !tbaa !3 *)
mov mem0_408 v_add21_6_51;
(*   %arrayidx9.6.1.51 = getelementptr inbounds i16, i16* %r, i64 207 *)
(*   %1742 = load i16, i16* %arrayidx9.6.1.51, align 2, !tbaa !3 *)
mov v1742 mem0_414;
(*   %conv1.i.6.1.51 = sext i16 %1742 to i32 *)
cast v_conv1_i_6_1_51@sint32 v1742@sint16;
(*   %mul.i.6.1.51 = mul nsw i32 %conv1.i.6.1.51, 794 *)
mul v_mul_i_6_1_51 v_conv1_i_6_1_51 (794)@sint32;
(*   %call.i.6.1.51 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.51) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_51, v_call_i_6_1_51);
(*   %arrayidx11.6.1.51 = getelementptr inbounds i16, i16* %r, i64 205 *)
(*   %1743 = load i16, i16* %arrayidx11.6.1.51, align 2, !tbaa !3 *)
mov v1743 mem0_410;
(*   %sub.6.1.51 = sub i16 %1743, %call.i.6.1.51 *)
sub v_sub_6_1_51 v1743 v_call_i_6_1_51;
(*   store i16 %sub.6.1.51, i16* %arrayidx9.6.1.51, align 2, !tbaa !3 *)
mov mem0_414 v_sub_6_1_51;
(*   %add21.6.1.51 = add i16 %1743, %call.i.6.1.51 *)
add v_add21_6_1_51 v1743 v_call_i_6_1_51;
(*   store i16 %add21.6.1.51, i16* %arrayidx11.6.1.51, align 2, !tbaa !3 *)
mov mem0_410 v_add21_6_1_51;

(* NOTE: k = 116 *)

(*   %arrayidx9.6.52 = getelementptr inbounds i16, i16* %r, i64 210 *)
(*   %1744 = load i16, i16* %arrayidx9.6.52, align 2, !tbaa !3 *)
mov v1744 mem0_420;
(*   %conv1.i.6.52 = sext i16 %1744 to i32 *)
cast v_conv1_i_6_52@sint32 v1744@sint16;
(*   %mul.i.6.52 = mul nsw i32 %conv1.i.6.52, -1510 *)
mul v_mul_i_6_52 v_conv1_i_6_52 (-1510)@sint32;
(*   %call.i.6.52 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.52) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_52, v_call_i_6_52);
(*   %arrayidx11.6.52 = getelementptr inbounds i16, i16* %r, i64 208 *)
(*   %1745 = load i16, i16* %arrayidx11.6.52, align 2, !tbaa !3 *)
mov v1745 mem0_416;
(*   %sub.6.52 = sub i16 %1745, %call.i.6.52 *)
sub v_sub_6_52 v1745 v_call_i_6_52;
(*   store i16 %sub.6.52, i16* %arrayidx9.6.52, align 2, !tbaa !3 *)
mov mem0_420 v_sub_6_52;
(*   %add21.6.52 = add i16 %1745, %call.i.6.52 *)
add v_add21_6_52 v1745 v_call_i_6_52;
(*   store i16 %add21.6.52, i16* %arrayidx11.6.52, align 2, !tbaa !3 *)
mov mem0_416 v_add21_6_52;
(*   %arrayidx9.6.1.52 = getelementptr inbounds i16, i16* %r, i64 211 *)
(*   %1746 = load i16, i16* %arrayidx9.6.1.52, align 2, !tbaa !3 *)
mov v1746 mem0_422;
(*   %conv1.i.6.1.52 = sext i16 %1746 to i32 *)
cast v_conv1_i_6_1_52@sint32 v1746@sint16;
(*   %mul.i.6.1.52 = mul nsw i32 %conv1.i.6.1.52, -1510 *)
mul v_mul_i_6_1_52 v_conv1_i_6_1_52 (-1510)@sint32;
(*   %call.i.6.1.52 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.52) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_52, v_call_i_6_1_52);
(*   %arrayidx11.6.1.52 = getelementptr inbounds i16, i16* %r, i64 209 *)
(*   %1747 = load i16, i16* %arrayidx11.6.1.52, align 2, !tbaa !3 *)
mov v1747 mem0_418;
(*   %sub.6.1.52 = sub i16 %1747, %call.i.6.1.52 *)
sub v_sub_6_1_52 v1747 v_call_i_6_1_52;
(*   store i16 %sub.6.1.52, i16* %arrayidx9.6.1.52, align 2, !tbaa !3 *)
mov mem0_422 v_sub_6_1_52;
(*   %add21.6.1.52 = add i16 %1747, %call.i.6.1.52 *)
add v_add21_6_1_52 v1747 v_call_i_6_1_52;
(*   store i16 %add21.6.1.52, i16* %arrayidx11.6.1.52, align 2, !tbaa !3 *)
mov mem0_418 v_add21_6_1_52;

(* NOTE: k = 117 *)

(*   %arrayidx9.6.53 = getelementptr inbounds i16, i16* %r, i64 214 *)
(*   %1748 = load i16, i16* %arrayidx9.6.53, align 2, !tbaa !3 *)
mov v1748 mem0_428;
(*   %conv1.i.6.53 = sext i16 %1748 to i32 *)
cast v_conv1_i_6_53@sint32 v1748@sint16;
(*   %mul.i.6.53 = mul nsw i32 %conv1.i.6.53, -854 *)
mul v_mul_i_6_53 v_conv1_i_6_53 (-854)@sint32;
(*   %call.i.6.53 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.53) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_53, v_call_i_6_53);
(*   %arrayidx11.6.53 = getelementptr inbounds i16, i16* %r, i64 212 *)
(*   %1749 = load i16, i16* %arrayidx11.6.53, align 2, !tbaa !3 *)
mov v1749 mem0_424;
(*   %sub.6.53 = sub i16 %1749, %call.i.6.53 *)
sub v_sub_6_53 v1749 v_call_i_6_53;
(*   store i16 %sub.6.53, i16* %arrayidx9.6.53, align 2, !tbaa !3 *)
mov mem0_428 v_sub_6_53;
(*   %add21.6.53 = add i16 %1749, %call.i.6.53 *)
add v_add21_6_53 v1749 v_call_i_6_53;
(*   store i16 %add21.6.53, i16* %arrayidx11.6.53, align 2, !tbaa !3 *)
mov mem0_424 v_add21_6_53;
(*   %arrayidx9.6.1.53 = getelementptr inbounds i16, i16* %r, i64 215 *)
(*   %1750 = load i16, i16* %arrayidx9.6.1.53, align 2, !tbaa !3 *)
mov v1750 mem0_430;
(*   %conv1.i.6.1.53 = sext i16 %1750 to i32 *)
cast v_conv1_i_6_1_53@sint32 v1750@sint16;
(*   %mul.i.6.1.53 = mul nsw i32 %conv1.i.6.1.53, -854 *)
mul v_mul_i_6_1_53 v_conv1_i_6_1_53 (-854)@sint32;
(*   %call.i.6.1.53 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.53) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_53, v_call_i_6_1_53);
(*   %arrayidx11.6.1.53 = getelementptr inbounds i16, i16* %r, i64 213 *)
(*   %1751 = load i16, i16* %arrayidx11.6.1.53, align 2, !tbaa !3 *)
mov v1751 mem0_426;
(*   %sub.6.1.53 = sub i16 %1751, %call.i.6.1.53 *)
sub v_sub_6_1_53 v1751 v_call_i_6_1_53;
(*   store i16 %sub.6.1.53, i16* %arrayidx9.6.1.53, align 2, !tbaa !3 *)
mov mem0_430 v_sub_6_1_53;
(*   %add21.6.1.53 = add i16 %1751, %call.i.6.1.53 *)
add v_add21_6_1_53 v1751 v_call_i_6_1_53;
(*   store i16 %add21.6.1.53, i16* %arrayidx11.6.1.53, align 2, !tbaa !3 *)
mov mem0_426 v_add21_6_1_53;

(* NOTE: k = 118 *)

(*   %arrayidx9.6.54 = getelementptr inbounds i16, i16* %r, i64 218 *)
(*   %1752 = load i16, i16* %arrayidx9.6.54, align 2, !tbaa !3 *)
mov v1752 mem0_436;
(*   %conv1.i.6.54 = sext i16 %1752 to i32 *)
cast v_conv1_i_6_54@sint32 v1752@sint16;
(*   %mul.i.6.54 = mul nsw i32 %conv1.i.6.54, -870 *)
mul v_mul_i_6_54 v_conv1_i_6_54 (-870)@sint32;
(*   %call.i.6.54 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.54) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_54, v_call_i_6_54);
(*   %arrayidx11.6.54 = getelementptr inbounds i16, i16* %r, i64 216 *)
(*   %1753 = load i16, i16* %arrayidx11.6.54, align 2, !tbaa !3 *)
mov v1753 mem0_432;
(*   %sub.6.54 = sub i16 %1753, %call.i.6.54 *)
sub v_sub_6_54 v1753 v_call_i_6_54;
(*   store i16 %sub.6.54, i16* %arrayidx9.6.54, align 2, !tbaa !3 *)
mov mem0_436 v_sub_6_54;
(*   %add21.6.54 = add i16 %1753, %call.i.6.54 *)
add v_add21_6_54 v1753 v_call_i_6_54;
(*   store i16 %add21.6.54, i16* %arrayidx11.6.54, align 2, !tbaa !3 *)
mov mem0_432 v_add21_6_54;
(*   %arrayidx9.6.1.54 = getelementptr inbounds i16, i16* %r, i64 219 *)
(*   %1754 = load i16, i16* %arrayidx9.6.1.54, align 2, !tbaa !3 *)
mov v1754 mem0_438;
(*   %conv1.i.6.1.54 = sext i16 %1754 to i32 *)
cast v_conv1_i_6_1_54@sint32 v1754@sint16;
(*   %mul.i.6.1.54 = mul nsw i32 %conv1.i.6.1.54, -870 *)
mul v_mul_i_6_1_54 v_conv1_i_6_1_54 (-870)@sint32;
(*   %call.i.6.1.54 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.54) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_54, v_call_i_6_1_54);
(*   %arrayidx11.6.1.54 = getelementptr inbounds i16, i16* %r, i64 217 *)
(*   %1755 = load i16, i16* %arrayidx11.6.1.54, align 2, !tbaa !3 *)
mov v1755 mem0_434;
(*   %sub.6.1.54 = sub i16 %1755, %call.i.6.1.54 *)
sub v_sub_6_1_54 v1755 v_call_i_6_1_54;
(*   store i16 %sub.6.1.54, i16* %arrayidx9.6.1.54, align 2, !tbaa !3 *)
mov mem0_438 v_sub_6_1_54;
(*   %add21.6.1.54 = add i16 %1755, %call.i.6.1.54 *)
add v_add21_6_1_54 v1755 v_call_i_6_1_54;
(*   store i16 %add21.6.1.54, i16* %arrayidx11.6.1.54, align 2, !tbaa !3 *)
mov mem0_434 v_add21_6_1_54;

(* NOTE: k = 119 *)

(*   %arrayidx9.6.55 = getelementptr inbounds i16, i16* %r, i64 222 *)
(*   %1756 = load i16, i16* %arrayidx9.6.55, align 2, !tbaa !3 *)
mov v1756 mem0_444;
(*   %conv1.i.6.55 = sext i16 %1756 to i32 *)
cast v_conv1_i_6_55@sint32 v1756@sint16;
(*   %mul.i.6.55 = mul nsw i32 %conv1.i.6.55, 478 *)
mul v_mul_i_6_55 v_conv1_i_6_55 (478)@sint32;
(*   %call.i.6.55 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.55) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_55, v_call_i_6_55);
(*   %arrayidx11.6.55 = getelementptr inbounds i16, i16* %r, i64 220 *)
(*   %1757 = load i16, i16* %arrayidx11.6.55, align 2, !tbaa !3 *)
mov v1757 mem0_440;
(*   %sub.6.55 = sub i16 %1757, %call.i.6.55 *)
sub v_sub_6_55 v1757 v_call_i_6_55;
(*   store i16 %sub.6.55, i16* %arrayidx9.6.55, align 2, !tbaa !3 *)
mov mem0_444 v_sub_6_55;
(*   %add21.6.55 = add i16 %1757, %call.i.6.55 *)
add v_add21_6_55 v1757 v_call_i_6_55;
(*   store i16 %add21.6.55, i16* %arrayidx11.6.55, align 2, !tbaa !3 *)
mov mem0_440 v_add21_6_55;
(*   %arrayidx9.6.1.55 = getelementptr inbounds i16, i16* %r, i64 223 *)
(*   %1758 = load i16, i16* %arrayidx9.6.1.55, align 2, !tbaa !3 *)
mov v1758 mem0_446;
(*   %conv1.i.6.1.55 = sext i16 %1758 to i32 *)
cast v_conv1_i_6_1_55@sint32 v1758@sint16;
(*   %mul.i.6.1.55 = mul nsw i32 %conv1.i.6.1.55, 478 *)
mul v_mul_i_6_1_55 v_conv1_i_6_1_55 (478)@sint32;
(*   %call.i.6.1.55 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.55) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_55, v_call_i_6_1_55);
(*   %arrayidx11.6.1.55 = getelementptr inbounds i16, i16* %r, i64 221 *)
(*   %1759 = load i16, i16* %arrayidx11.6.1.55, align 2, !tbaa !3 *)
mov v1759 mem0_442;
(*   %sub.6.1.55 = sub i16 %1759, %call.i.6.1.55 *)
sub v_sub_6_1_55 v1759 v_call_i_6_1_55;
(*   store i16 %sub.6.1.55, i16* %arrayidx9.6.1.55, align 2, !tbaa !3 *)
mov mem0_446 v_sub_6_1_55;
(*   %add21.6.1.55 = add i16 %1759, %call.i.6.1.55 *)
add v_add21_6_1_55 v1759 v_call_i_6_1_55;
(*   store i16 %add21.6.1.55, i16* %arrayidx11.6.1.55, align 2, !tbaa !3 *)
mov mem0_442 v_add21_6_1_55;

(* NOTE: k = 120 *)

(*   %arrayidx9.6.56 = getelementptr inbounds i16, i16* %r, i64 226 *)
(*   %1760 = load i16, i16* %arrayidx9.6.56, align 2, !tbaa !3 *)
mov v1760 mem0_452;
(*   %conv1.i.6.56 = sext i16 %1760 to i32 *)
cast v_conv1_i_6_56@sint32 v1760@sint16;
(*   %mul.i.6.56 = mul nsw i32 %conv1.i.6.56, -108 *)
mul v_mul_i_6_56 v_conv1_i_6_56 (-108)@sint32;
(*   %call.i.6.56 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.56) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_56, v_call_i_6_56);
(*   %arrayidx11.6.56 = getelementptr inbounds i16, i16* %r, i64 224 *)
(*   %1761 = load i16, i16* %arrayidx11.6.56, align 2, !tbaa !3 *)
mov v1761 mem0_448;
(*   %sub.6.56 = sub i16 %1761, %call.i.6.56 *)
sub v_sub_6_56 v1761 v_call_i_6_56;
(*   store i16 %sub.6.56, i16* %arrayidx9.6.56, align 2, !tbaa !3 *)
mov mem0_452 v_sub_6_56;
(*   %add21.6.56 = add i16 %1761, %call.i.6.56 *)
add v_add21_6_56 v1761 v_call_i_6_56;
(*   store i16 %add21.6.56, i16* %arrayidx11.6.56, align 2, !tbaa !3 *)
mov mem0_448 v_add21_6_56;
(*   %arrayidx9.6.1.56 = getelementptr inbounds i16, i16* %r, i64 227 *)
(*   %1762 = load i16, i16* %arrayidx9.6.1.56, align 2, !tbaa !3 *)
mov v1762 mem0_454;
(*   %conv1.i.6.1.56 = sext i16 %1762 to i32 *)
cast v_conv1_i_6_1_56@sint32 v1762@sint16;
(*   %mul.i.6.1.56 = mul nsw i32 %conv1.i.6.1.56, -108 *)
mul v_mul_i_6_1_56 v_conv1_i_6_1_56 (-108)@sint32;
(*   %call.i.6.1.56 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.56) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_56, v_call_i_6_1_56);
(*   %arrayidx11.6.1.56 = getelementptr inbounds i16, i16* %r, i64 225 *)
(*   %1763 = load i16, i16* %arrayidx11.6.1.56, align 2, !tbaa !3 *)
mov v1763 mem0_450;
(*   %sub.6.1.56 = sub i16 %1763, %call.i.6.1.56 *)
sub v_sub_6_1_56 v1763 v_call_i_6_1_56;
(*   store i16 %sub.6.1.56, i16* %arrayidx9.6.1.56, align 2, !tbaa !3 *)
mov mem0_454 v_sub_6_1_56;
(*   %add21.6.1.56 = add i16 %1763, %call.i.6.1.56 *)
add v_add21_6_1_56 v1763 v_call_i_6_1_56;
(*   store i16 %add21.6.1.56, i16* %arrayidx11.6.1.56, align 2, !tbaa !3 *)
mov mem0_450 v_add21_6_1_56;

(* NOTE: k = 121 *)

(*   %arrayidx9.6.57 = getelementptr inbounds i16, i16* %r, i64 230 *)
(*   %1764 = load i16, i16* %arrayidx9.6.57, align 2, !tbaa !3 *)
mov v1764 mem0_460;
(*   %conv1.i.6.57 = sext i16 %1764 to i32 *)
cast v_conv1_i_6_57@sint32 v1764@sint16;
(*   %mul.i.6.57 = mul nsw i32 %conv1.i.6.57, -308 *)
mul v_mul_i_6_57 v_conv1_i_6_57 (-308)@sint32;
(*   %call.i.6.57 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.57) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_57, v_call_i_6_57);
(*   %arrayidx11.6.57 = getelementptr inbounds i16, i16* %r, i64 228 *)
(*   %1765 = load i16, i16* %arrayidx11.6.57, align 2, !tbaa !3 *)
mov v1765 mem0_456;
(*   %sub.6.57 = sub i16 %1765, %call.i.6.57 *)
sub v_sub_6_57 v1765 v_call_i_6_57;
(*   store i16 %sub.6.57, i16* %arrayidx9.6.57, align 2, !tbaa !3 *)
mov mem0_460 v_sub_6_57;
(*   %add21.6.57 = add i16 %1765, %call.i.6.57 *)
add v_add21_6_57 v1765 v_call_i_6_57;
(*   store i16 %add21.6.57, i16* %arrayidx11.6.57, align 2, !tbaa !3 *)
mov mem0_456 v_add21_6_57;
(*   %arrayidx9.6.1.57 = getelementptr inbounds i16, i16* %r, i64 231 *)
(*   %1766 = load i16, i16* %arrayidx9.6.1.57, align 2, !tbaa !3 *)
mov v1766 mem0_462;
(*   %conv1.i.6.1.57 = sext i16 %1766 to i32 *)
cast v_conv1_i_6_1_57@sint32 v1766@sint16;
(*   %mul.i.6.1.57 = mul nsw i32 %conv1.i.6.1.57, -308 *)
mul v_mul_i_6_1_57 v_conv1_i_6_1_57 (-308)@sint32;
(*   %call.i.6.1.57 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.57) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_57, v_call_i_6_1_57);
(*   %arrayidx11.6.1.57 = getelementptr inbounds i16, i16* %r, i64 229 *)
(*   %1767 = load i16, i16* %arrayidx11.6.1.57, align 2, !tbaa !3 *)
mov v1767 mem0_458;
(*   %sub.6.1.57 = sub i16 %1767, %call.i.6.1.57 *)
sub v_sub_6_1_57 v1767 v_call_i_6_1_57;
(*   store i16 %sub.6.1.57, i16* %arrayidx9.6.1.57, align 2, !tbaa !3 *)
mov mem0_462 v_sub_6_1_57;
(*   %add21.6.1.57 = add i16 %1767, %call.i.6.1.57 *)
add v_add21_6_1_57 v1767 v_call_i_6_1_57;
(*   store i16 %add21.6.1.57, i16* %arrayidx11.6.1.57, align 2, !tbaa !3 *)
mov mem0_458 v_add21_6_1_57;

(* NOTE: k = 122 *)

(*   %arrayidx9.6.58 = getelementptr inbounds i16, i16* %r, i64 234 *)
(*   %1768 = load i16, i16* %arrayidx9.6.58, align 2, !tbaa !3 *)
mov v1768 mem0_468;
(*   %conv1.i.6.58 = sext i16 %1768 to i32 *)
cast v_conv1_i_6_58@sint32 v1768@sint16;
(*   %mul.i.6.58 = mul nsw i32 %conv1.i.6.58, 996 *)
mul v_mul_i_6_58 v_conv1_i_6_58 (996)@sint32;
(*   %call.i.6.58 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.58) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_58, v_call_i_6_58);
(*   %arrayidx11.6.58 = getelementptr inbounds i16, i16* %r, i64 232 *)
(*   %1769 = load i16, i16* %arrayidx11.6.58, align 2, !tbaa !3 *)
mov v1769 mem0_464;
(*   %sub.6.58 = sub i16 %1769, %call.i.6.58 *)
sub v_sub_6_58 v1769 v_call_i_6_58;
(*   store i16 %sub.6.58, i16* %arrayidx9.6.58, align 2, !tbaa !3 *)
mov mem0_468 v_sub_6_58;
(*   %add21.6.58 = add i16 %1769, %call.i.6.58 *)
add v_add21_6_58 v1769 v_call_i_6_58;
(*   store i16 %add21.6.58, i16* %arrayidx11.6.58, align 2, !tbaa !3 *)
mov mem0_464 v_add21_6_58;
(*   %arrayidx9.6.1.58 = getelementptr inbounds i16, i16* %r, i64 235 *)
(*   %1770 = load i16, i16* %arrayidx9.6.1.58, align 2, !tbaa !3 *)
mov v1770 mem0_470;
(*   %conv1.i.6.1.58 = sext i16 %1770 to i32 *)
cast v_conv1_i_6_1_58@sint32 v1770@sint16;
(*   %mul.i.6.1.58 = mul nsw i32 %conv1.i.6.1.58, 996 *)
mul v_mul_i_6_1_58 v_conv1_i_6_1_58 (996)@sint32;
(*   %call.i.6.1.58 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.58) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_58, v_call_i_6_1_58);
(*   %arrayidx11.6.1.58 = getelementptr inbounds i16, i16* %r, i64 233 *)
(*   %1771 = load i16, i16* %arrayidx11.6.1.58, align 2, !tbaa !3 *)
mov v1771 mem0_466;
(*   %sub.6.1.58 = sub i16 %1771, %call.i.6.1.58 *)
sub v_sub_6_1_58 v1771 v_call_i_6_1_58;
(*   store i16 %sub.6.1.58, i16* %arrayidx9.6.1.58, align 2, !tbaa !3 *)
mov mem0_470 v_sub_6_1_58;
(*   %add21.6.1.58 = add i16 %1771, %call.i.6.1.58 *)
add v_add21_6_1_58 v1771 v_call_i_6_1_58;
(*   store i16 %add21.6.1.58, i16* %arrayidx11.6.1.58, align 2, !tbaa !3 *)
mov mem0_466 v_add21_6_1_58;

(* NOTE: k = 123 *)

(*   %arrayidx9.6.59 = getelementptr inbounds i16, i16* %r, i64 238 *)
(*   %1772 = load i16, i16* %arrayidx9.6.59, align 2, !tbaa !3 *)
mov v1772 mem0_476;
(*   %conv1.i.6.59 = sext i16 %1772 to i32 *)
cast v_conv1_i_6_59@sint32 v1772@sint16;
(*   %mul.i.6.59 = mul nsw i32 %conv1.i.6.59, 991 *)
mul v_mul_i_6_59 v_conv1_i_6_59 (991)@sint32;
(*   %call.i.6.59 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.59) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_59, v_call_i_6_59);
(*   %arrayidx11.6.59 = getelementptr inbounds i16, i16* %r, i64 236 *)
(*   %1773 = load i16, i16* %arrayidx11.6.59, align 2, !tbaa !3 *)
mov v1773 mem0_472;
(*   %sub.6.59 = sub i16 %1773, %call.i.6.59 *)
sub v_sub_6_59 v1773 v_call_i_6_59;
(*   store i16 %sub.6.59, i16* %arrayidx9.6.59, align 2, !tbaa !3 *)
mov mem0_476 v_sub_6_59;
(*   %add21.6.59 = add i16 %1773, %call.i.6.59 *)
add v_add21_6_59 v1773 v_call_i_6_59;
(*   store i16 %add21.6.59, i16* %arrayidx11.6.59, align 2, !tbaa !3 *)
mov mem0_472 v_add21_6_59;
(*   %arrayidx9.6.1.59 = getelementptr inbounds i16, i16* %r, i64 239 *)
(*   %1774 = load i16, i16* %arrayidx9.6.1.59, align 2, !tbaa !3 *)
mov v1774 mem0_478;
(*   %conv1.i.6.1.59 = sext i16 %1774 to i32 *)
cast v_conv1_i_6_1_59@sint32 v1774@sint16;
(*   %mul.i.6.1.59 = mul nsw i32 %conv1.i.6.1.59, 991 *)
mul v_mul_i_6_1_59 v_conv1_i_6_1_59 (991)@sint32;
(*   %call.i.6.1.59 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.59) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_59, v_call_i_6_1_59);
(*   %arrayidx11.6.1.59 = getelementptr inbounds i16, i16* %r, i64 237 *)
(*   %1775 = load i16, i16* %arrayidx11.6.1.59, align 2, !tbaa !3 *)
mov v1775 mem0_474;
(*   %sub.6.1.59 = sub i16 %1775, %call.i.6.1.59 *)
sub v_sub_6_1_59 v1775 v_call_i_6_1_59;
(*   store i16 %sub.6.1.59, i16* %arrayidx9.6.1.59, align 2, !tbaa !3 *)
mov mem0_478 v_sub_6_1_59;
(*   %add21.6.1.59 = add i16 %1775, %call.i.6.1.59 *)
add v_add21_6_1_59 v1775 v_call_i_6_1_59;
(*   store i16 %add21.6.1.59, i16* %arrayidx11.6.1.59, align 2, !tbaa !3 *)
mov mem0_474 v_add21_6_1_59;

(* NOTE: k = 124 *)

(*   %arrayidx9.6.60 = getelementptr inbounds i16, i16* %r, i64 242 *)
(*   %1776 = load i16, i16* %arrayidx9.6.60, align 2, !tbaa !3 *)
mov v1776 mem0_484;
(*   %conv1.i.6.60 = sext i16 %1776 to i32 *)
cast v_conv1_i_6_60@sint32 v1776@sint16;
(*   %mul.i.6.60 = mul nsw i32 %conv1.i.6.60, 958 *)
mul v_mul_i_6_60 v_conv1_i_6_60 (958)@sint32;
(*   %call.i.6.60 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.60) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_60, v_call_i_6_60);
(*   %arrayidx11.6.60 = getelementptr inbounds i16, i16* %r, i64 240 *)
(*   %1777 = load i16, i16* %arrayidx11.6.60, align 2, !tbaa !3 *)
mov v1777 mem0_480;
(*   %sub.6.60 = sub i16 %1777, %call.i.6.60 *)
sub v_sub_6_60 v1777 v_call_i_6_60;
(*   store i16 %sub.6.60, i16* %arrayidx9.6.60, align 2, !tbaa !3 *)
mov mem0_484 v_sub_6_60;
(*   %add21.6.60 = add i16 %1777, %call.i.6.60 *)
add v_add21_6_60 v1777 v_call_i_6_60;
(*   store i16 %add21.6.60, i16* %arrayidx11.6.60, align 2, !tbaa !3 *)
mov mem0_480 v_add21_6_60;
(*   %arrayidx9.6.1.60 = getelementptr inbounds i16, i16* %r, i64 243 *)
(*   %1778 = load i16, i16* %arrayidx9.6.1.60, align 2, !tbaa !3 *)
mov v1778 mem0_486;
(*   %conv1.i.6.1.60 = sext i16 %1778 to i32 *)
cast v_conv1_i_6_1_60@sint32 v1778@sint16;
(*   %mul.i.6.1.60 = mul nsw i32 %conv1.i.6.1.60, 958 *)
mul v_mul_i_6_1_60 v_conv1_i_6_1_60 (958)@sint32;
(*   %call.i.6.1.60 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.60) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_60, v_call_i_6_1_60);
(*   %arrayidx11.6.1.60 = getelementptr inbounds i16, i16* %r, i64 241 *)
(*   %1779 = load i16, i16* %arrayidx11.6.1.60, align 2, !tbaa !3 *)
mov v1779 mem0_482;
(*   %sub.6.1.60 = sub i16 %1779, %call.i.6.1.60 *)
sub v_sub_6_1_60 v1779 v_call_i_6_1_60;
(*   store i16 %sub.6.1.60, i16* %arrayidx9.6.1.60, align 2, !tbaa !3 *)
mov mem0_486 v_sub_6_1_60;
(*   %add21.6.1.60 = add i16 %1779, %call.i.6.1.60 *)
add v_add21_6_1_60 v1779 v_call_i_6_1_60;
(*   store i16 %add21.6.1.60, i16* %arrayidx11.6.1.60, align 2, !tbaa !3 *)
mov mem0_482 v_add21_6_1_60;

(* NOTE: k = 125 *)

(*   %arrayidx9.6.61 = getelementptr inbounds i16, i16* %r, i64 246 *)
(*   %1780 = load i16, i16* %arrayidx9.6.61, align 2, !tbaa !3 *)
mov v1780 mem0_492;
(*   %conv1.i.6.61 = sext i16 %1780 to i32 *)
cast v_conv1_i_6_61@sint32 v1780@sint16;
(*   %mul.i.6.61 = mul nsw i32 %conv1.i.6.61, -1460 *)
mul v_mul_i_6_61 v_conv1_i_6_61 (-1460)@sint32;
(*   %call.i.6.61 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.61) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_61, v_call_i_6_61);
(*   %arrayidx11.6.61 = getelementptr inbounds i16, i16* %r, i64 244 *)
(*   %1781 = load i16, i16* %arrayidx11.6.61, align 2, !tbaa !3 *)
mov v1781 mem0_488;
(*   %sub.6.61 = sub i16 %1781, %call.i.6.61 *)
sub v_sub_6_61 v1781 v_call_i_6_61;
(*   store i16 %sub.6.61, i16* %arrayidx9.6.61, align 2, !tbaa !3 *)
mov mem0_492 v_sub_6_61;
(*   %add21.6.61 = add i16 %1781, %call.i.6.61 *)
add v_add21_6_61 v1781 v_call_i_6_61;
(*   store i16 %add21.6.61, i16* %arrayidx11.6.61, align 2, !tbaa !3 *)
mov mem0_488 v_add21_6_61;
(*   %arrayidx9.6.1.61 = getelementptr inbounds i16, i16* %r, i64 247 *)
(*   %1782 = load i16, i16* %arrayidx9.6.1.61, align 2, !tbaa !3 *)
mov v1782 mem0_494;
(*   %conv1.i.6.1.61 = sext i16 %1782 to i32 *)
cast v_conv1_i_6_1_61@sint32 v1782@sint16;
(*   %mul.i.6.1.61 = mul nsw i32 %conv1.i.6.1.61, -1460 *)
mul v_mul_i_6_1_61 v_conv1_i_6_1_61 (-1460)@sint32;
(*   %call.i.6.1.61 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.61) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_61, v_call_i_6_1_61);
(*   %arrayidx11.6.1.61 = getelementptr inbounds i16, i16* %r, i64 245 *)
(*   %1783 = load i16, i16* %arrayidx11.6.1.61, align 2, !tbaa !3 *)
mov v1783 mem0_490;
(*   %sub.6.1.61 = sub i16 %1783, %call.i.6.1.61 *)
sub v_sub_6_1_61 v1783 v_call_i_6_1_61;
(*   store i16 %sub.6.1.61, i16* %arrayidx9.6.1.61, align 2, !tbaa !3 *)
mov mem0_494 v_sub_6_1_61;
(*   %add21.6.1.61 = add i16 %1783, %call.i.6.1.61 *)
add v_add21_6_1_61 v1783 v_call_i_6_1_61;
(*   store i16 %add21.6.1.61, i16* %arrayidx11.6.1.61, align 2, !tbaa !3 *)
mov mem0_490 v_add21_6_1_61;

(* NOTE: k = 126 *)

(*   %arrayidx9.6.62 = getelementptr inbounds i16, i16* %r, i64 250 *)
(*   %1784 = load i16, i16* %arrayidx9.6.62, align 2, !tbaa !3 *)
mov v1784 mem0_500;
(*   %conv1.i.6.62 = sext i16 %1784 to i32 *)
cast v_conv1_i_6_62@sint32 v1784@sint16;
(*   %mul.i.6.62 = mul nsw i32 %conv1.i.6.62, 1522 *)
mul v_mul_i_6_62 v_conv1_i_6_62 (1522)@sint32;
(*   %call.i.6.62 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.62) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_62, v_call_i_6_62);
(*   %arrayidx11.6.62 = getelementptr inbounds i16, i16* %r, i64 248 *)
(*   %1785 = load i16, i16* %arrayidx11.6.62, align 2, !tbaa !3 *)
mov v1785 mem0_496;
(*   %sub.6.62 = sub i16 %1785, %call.i.6.62 *)
sub v_sub_6_62 v1785 v_call_i_6_62;
(*   store i16 %sub.6.62, i16* %arrayidx9.6.62, align 2, !tbaa !3 *)
mov mem0_500 v_sub_6_62;
(*   %add21.6.62 = add i16 %1785, %call.i.6.62 *)
add v_add21_6_62 v1785 v_call_i_6_62;
(*   store i16 %add21.6.62, i16* %arrayidx11.6.62, align 2, !tbaa !3 *)
mov mem0_496 v_add21_6_62;
(*   %arrayidx9.6.1.62 = getelementptr inbounds i16, i16* %r, i64 251 *)
(*   %1786 = load i16, i16* %arrayidx9.6.1.62, align 2, !tbaa !3 *)
mov v1786 mem0_502;
(*   %conv1.i.6.1.62 = sext i16 %1786 to i32 *)
cast v_conv1_i_6_1_62@sint32 v1786@sint16;
(*   %mul.i.6.1.62 = mul nsw i32 %conv1.i.6.1.62, 1522 *)
mul v_mul_i_6_1_62 v_conv1_i_6_1_62 (1522)@sint32;
(*   %call.i.6.1.62 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.62) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_62, v_call_i_6_1_62);
(*   %arrayidx11.6.1.62 = getelementptr inbounds i16, i16* %r, i64 249 *)
(*   %1787 = load i16, i16* %arrayidx11.6.1.62, align 2, !tbaa !3 *)
mov v1787 mem0_498;
(*   %sub.6.1.62 = sub i16 %1787, %call.i.6.1.62 *)
sub v_sub_6_1_62 v1787 v_call_i_6_1_62;
(*   store i16 %sub.6.1.62, i16* %arrayidx9.6.1.62, align 2, !tbaa !3 *)
mov mem0_502 v_sub_6_1_62;
(*   %add21.6.1.62 = add i16 %1787, %call.i.6.1.62 *)
add v_add21_6_1_62 v1787 v_call_i_6_1_62;
(*   store i16 %add21.6.1.62, i16* %arrayidx11.6.1.62, align 2, !tbaa !3 *)
mov mem0_498 v_add21_6_1_62;

(* NOTE: k = 127 *)

(*   %arrayidx9.6.63 = getelementptr inbounds i16, i16* %r, i64 254 *)
(*   %1788 = load i16, i16* %arrayidx9.6.63, align 2, !tbaa !3 *)
mov v1788 mem0_508;
(*   %conv1.i.6.63 = sext i16 %1788 to i32 *)
cast v_conv1_i_6_63@sint32 v1788@sint16;
(*   %mul.i.6.63 = mul nsw i32 %conv1.i.6.63, 1628 *)
mul v_mul_i_6_63 v_conv1_i_6_63 (1628)@sint32;
(*   %call.i.6.63 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.63) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_63, v_call_i_6_63);
(*   %arrayidx11.6.63 = getelementptr inbounds i16, i16* %r, i64 252 *)
(*   %1789 = load i16, i16* %arrayidx11.6.63, align 2, !tbaa !3 *)
mov v1789 mem0_504;
(*   %sub.6.63 = sub i16 %1789, %call.i.6.63 *)
sub v_sub_6_63 v1789 v_call_i_6_63;
(*   store i16 %sub.6.63, i16* %arrayidx9.6.63, align 2, !tbaa !3 *)
mov mem0_508 v_sub_6_63;
(*   %add21.6.63 = add i16 %1789, %call.i.6.63 *)
add v_add21_6_63 v1789 v_call_i_6_63;
(*   store i16 %add21.6.63, i16* %arrayidx11.6.63, align 2, !tbaa !3 *)
mov mem0_504 v_add21_6_63;
(*   %arrayidx9.6.1.63 = getelementptr inbounds i16, i16* %r, i64 255 *)
(*   %1790 = load i16, i16* %arrayidx9.6.1.63, align 2, !tbaa !3 *)
mov v1790 mem0_510;
(*   %conv1.i.6.1.63 = sext i16 %1790 to i32 *)
cast v_conv1_i_6_1_63@sint32 v1790@sint16;
(*   %mul.i.6.1.63 = mul nsw i32 %conv1.i.6.1.63, 1628 *)
mul v_mul_i_6_1_63 v_conv1_i_6_1_63 (1628)@sint32;
(*   %call.i.6.1.63 = tail call signext i16 @PQCLEAN_KYBER512_CLEAN_montgomery_reduce(i32 %mul.i.6.1.63) #2 *)
call PQCLEAN_KYBER512_CLEAN_montgomery_reduce(v_mul_i_6_1_63, v_call_i_6_1_63);
(*   %arrayidx11.6.1.63 = getelementptr inbounds i16, i16* %r, i64 253 *)
(*   %1791 = load i16, i16* %arrayidx11.6.1.63, align 2, !tbaa !3 *)
mov v1791 mem0_506;
(*   %sub.6.1.63 = sub i16 %1791, %call.i.6.1.63 *)
sub v_sub_6_1_63 v1791 v_call_i_6_1_63;
(*   store i16 %sub.6.1.63, i16* %arrayidx9.6.1.63, align 2, !tbaa !3 *)
mov mem0_510 v_sub_6_1_63;
(*   %add21.6.1.63 = add i16 %1791, %call.i.6.1.63 *)
add v_add21_6_1_63 v1791 v_call_i_6_1_63;
(*   store i16 %add21.6.1.63, i16* %arrayidx11.6.1.63, align 2, !tbaa !3 *)
mov mem0_506 v_add21_6_1_63;
(*   ret void *)


(* outputs *)



mov r_0 mem0_0@sint16; mov r_2 mem0_2@sint16;
mov r_4 mem0_4@sint16; mov r_6 mem0_6@sint16;
mov r_8 mem0_8@sint16; mov r_10 mem0_10@sint16;
mov r_12 mem0_12@sint16; mov r_14 mem0_14@sint16;
mov r_16 mem0_16@sint16; mov r_18 mem0_18@sint16;
mov r_20 mem0_20@sint16; mov r_22 mem0_22@sint16;
mov r_24 mem0_24@sint16; mov r_26 mem0_26@sint16;
mov r_28 mem0_28@sint16; mov r_30 mem0_30@sint16;
mov r_32 mem0_32@sint16; mov r_34 mem0_34@sint16;
mov r_36 mem0_36@sint16; mov r_38 mem0_38@sint16;
mov r_40 mem0_40@sint16; mov r_42 mem0_42@sint16;
mov r_44 mem0_44@sint16; mov r_46 mem0_46@sint16;
mov r_48 mem0_48@sint16; mov r_50 mem0_50@sint16;
mov r_52 mem0_52@sint16; mov r_54 mem0_54@sint16;
mov r_56 mem0_56@sint16; mov r_58 mem0_58@sint16;
mov r_60 mem0_60@sint16; mov r_62 mem0_62@sint16;
mov r_64 mem0_64@sint16; mov r_66 mem0_66@sint16;
mov r_68 mem0_68@sint16; mov r_70 mem0_70@sint16;
mov r_72 mem0_72@sint16; mov r_74 mem0_74@sint16;
mov r_76 mem0_76@sint16; mov r_78 mem0_78@sint16;
mov r_80 mem0_80@sint16; mov r_82 mem0_82@sint16;
mov r_84 mem0_84@sint16; mov r_86 mem0_86@sint16;
mov r_88 mem0_88@sint16; mov r_90 mem0_90@sint16;
mov r_92 mem0_92@sint16; mov r_94 mem0_94@sint16;
mov r_96 mem0_96@sint16; mov r_98 mem0_98@sint16;
mov r_100 mem0_100@sint16; mov r_102 mem0_102@sint16;
mov r_104 mem0_104@sint16; mov r_106 mem0_106@sint16;
mov r_108 mem0_108@sint16; mov r_110 mem0_110@sint16;
mov r_112 mem0_112@sint16; mov r_114 mem0_114@sint16;
mov r_116 mem0_116@sint16; mov r_118 mem0_118@sint16;
mov r_120 mem0_120@sint16; mov r_122 mem0_122@sint16;
mov r_124 mem0_124@sint16; mov r_126 mem0_126@sint16;
mov r_128 mem0_128@sint16; mov r_130 mem0_130@sint16;
mov r_132 mem0_132@sint16; mov r_134 mem0_134@sint16;
mov r_136 mem0_136@sint16; mov r_138 mem0_138@sint16;
mov r_140 mem0_140@sint16; mov r_142 mem0_142@sint16;
mov r_144 mem0_144@sint16; mov r_146 mem0_146@sint16;
mov r_148 mem0_148@sint16; mov r_150 mem0_150@sint16;
mov r_152 mem0_152@sint16; mov r_154 mem0_154@sint16;
mov r_156 mem0_156@sint16; mov r_158 mem0_158@sint16;
mov r_160 mem0_160@sint16; mov r_162 mem0_162@sint16;
mov r_164 mem0_164@sint16; mov r_166 mem0_166@sint16;
mov r_168 mem0_168@sint16; mov r_170 mem0_170@sint16;
mov r_172 mem0_172@sint16; mov r_174 mem0_174@sint16;
mov r_176 mem0_176@sint16; mov r_178 mem0_178@sint16;
mov r_180 mem0_180@sint16; mov r_182 mem0_182@sint16;
mov r_184 mem0_184@sint16; mov r_186 mem0_186@sint16;
mov r_188 mem0_188@sint16; mov r_190 mem0_190@sint16;
mov r_192 mem0_192@sint16; mov r_194 mem0_194@sint16;
mov r_196 mem0_196@sint16; mov r_198 mem0_198@sint16;
mov r_200 mem0_200@sint16; mov r_202 mem0_202@sint16;
mov r_204 mem0_204@sint16; mov r_206 mem0_206@sint16;
mov r_208 mem0_208@sint16; mov r_210 mem0_210@sint16;
mov r_212 mem0_212@sint16; mov r_214 mem0_214@sint16;
mov r_216 mem0_216@sint16; mov r_218 mem0_218@sint16;
mov r_220 mem0_220@sint16; mov r_222 mem0_222@sint16;
mov r_224 mem0_224@sint16; mov r_226 mem0_226@sint16;
mov r_228 mem0_228@sint16; mov r_230 mem0_230@sint16;
mov r_232 mem0_232@sint16; mov r_234 mem0_234@sint16;
mov r_236 mem0_236@sint16; mov r_238 mem0_238@sint16;
mov r_240 mem0_240@sint16; mov r_242 mem0_242@sint16;
mov r_244 mem0_244@sint16; mov r_246 mem0_246@sint16;
mov r_248 mem0_248@sint16; mov r_250 mem0_250@sint16;
mov r_252 mem0_252@sint16; mov r_254 mem0_254@sint16;
mov r_256 mem0_256@sint16; mov r_258 mem0_258@sint16;
mov r_260 mem0_260@sint16; mov r_262 mem0_262@sint16;
mov r_264 mem0_264@sint16; mov r_266 mem0_266@sint16;
mov r_268 mem0_268@sint16; mov r_270 mem0_270@sint16;
mov r_272 mem0_272@sint16; mov r_274 mem0_274@sint16;
mov r_276 mem0_276@sint16; mov r_278 mem0_278@sint16;
mov r_280 mem0_280@sint16; mov r_282 mem0_282@sint16;
mov r_284 mem0_284@sint16; mov r_286 mem0_286@sint16;
mov r_288 mem0_288@sint16; mov r_290 mem0_290@sint16;
mov r_292 mem0_292@sint16; mov r_294 mem0_294@sint16;
mov r_296 mem0_296@sint16; mov r_298 mem0_298@sint16;
mov r_300 mem0_300@sint16; mov r_302 mem0_302@sint16;
mov r_304 mem0_304@sint16; mov r_306 mem0_306@sint16;
mov r_308 mem0_308@sint16; mov r_310 mem0_310@sint16;
mov r_312 mem0_312@sint16; mov r_314 mem0_314@sint16;
mov r_316 mem0_316@sint16; mov r_318 mem0_318@sint16;
mov r_320 mem0_320@sint16; mov r_322 mem0_322@sint16;
mov r_324 mem0_324@sint16; mov r_326 mem0_326@sint16;
mov r_328 mem0_328@sint16; mov r_330 mem0_330@sint16;
mov r_332 mem0_332@sint16; mov r_334 mem0_334@sint16;
mov r_336 mem0_336@sint16; mov r_338 mem0_338@sint16;
mov r_340 mem0_340@sint16; mov r_342 mem0_342@sint16;
mov r_344 mem0_344@sint16; mov r_346 mem0_346@sint16;
mov r_348 mem0_348@sint16; mov r_350 mem0_350@sint16;
mov r_352 mem0_352@sint16; mov r_354 mem0_354@sint16;
mov r_356 mem0_356@sint16; mov r_358 mem0_358@sint16;
mov r_360 mem0_360@sint16; mov r_362 mem0_362@sint16;
mov r_364 mem0_364@sint16; mov r_366 mem0_366@sint16;
mov r_368 mem0_368@sint16; mov r_370 mem0_370@sint16;
mov r_372 mem0_372@sint16; mov r_374 mem0_374@sint16;
mov r_376 mem0_376@sint16; mov r_378 mem0_378@sint16;
mov r_380 mem0_380@sint16; mov r_382 mem0_382@sint16;
mov r_384 mem0_384@sint16; mov r_386 mem0_386@sint16;
mov r_388 mem0_388@sint16; mov r_390 mem0_390@sint16;
mov r_392 mem0_392@sint16; mov r_394 mem0_394@sint16;
mov r_396 mem0_396@sint16; mov r_398 mem0_398@sint16;
mov r_400 mem0_400@sint16; mov r_402 mem0_402@sint16;
mov r_404 mem0_404@sint16; mov r_406 mem0_406@sint16;
mov r_408 mem0_408@sint16; mov r_410 mem0_410@sint16;
mov r_412 mem0_412@sint16; mov r_414 mem0_414@sint16;
mov r_416 mem0_416@sint16; mov r_418 mem0_418@sint16;
mov r_420 mem0_420@sint16; mov r_422 mem0_422@sint16;
mov r_424 mem0_424@sint16; mov r_426 mem0_426@sint16;
mov r_428 mem0_428@sint16; mov r_430 mem0_430@sint16;
mov r_432 mem0_432@sint16; mov r_434 mem0_434@sint16;
mov r_436 mem0_436@sint16; mov r_438 mem0_438@sint16;
mov r_440 mem0_440@sint16; mov r_442 mem0_442@sint16;
mov r_444 mem0_444@sint16; mov r_446 mem0_446@sint16;
mov r_448 mem0_448@sint16; mov r_450 mem0_450@sint16;
mov r_452 mem0_452@sint16; mov r_454 mem0_454@sint16;
mov r_456 mem0_456@sint16; mov r_458 mem0_458@sint16;
mov r_460 mem0_460@sint16; mov r_462 mem0_462@sint16;
mov r_464 mem0_464@sint16; mov r_466 mem0_466@sint16;
mov r_468 mem0_468@sint16; mov r_470 mem0_470@sint16;
mov r_472 mem0_472@sint16; mov r_474 mem0_474@sint16;
mov r_476 mem0_476@sint16; mov r_478 mem0_478@sint16;
mov r_480 mem0_480@sint16; mov r_482 mem0_482@sint16;
mov r_484 mem0_484@sint16; mov r_486 mem0_486@sint16;
mov r_488 mem0_488@sint16; mov r_490 mem0_490@sint16;
mov r_492 mem0_492@sint16; mov r_494 mem0_494@sint16;
mov r_496 mem0_496@sint16; mov r_498 mem0_498@sint16;
mov r_500 mem0_500@sint16; mov r_502 mem0_502@sint16;
mov r_504 mem0_504@sint16; mov r_506 mem0_506@sint16;
mov r_508 mem0_508@sint16; mov r_510 mem0_510@sint16;

{
  and [ input_polynomial * input_polynomial = 
a_0 * x**0 + a_2 * x**1 + a_4 * x**2 + a_6 * x**3 + 
a_8 * x**4 + a_10 * x**5 + a_12 * x**6 + a_14 * x**7 + 
a_16 * x**8 + a_18 * x**9 + a_20 * x**10 + a_22 * x**11 + 
a_24 * x**12 + a_26 * x**13 + a_28 * x**14 + a_30 * x**15 + 
a_32 * x**16 + a_34 * x**17 + a_36 * x**18 + a_38 * x**19 + 
a_40 * x**20 + a_42 * x**21 + a_44 * x**22 + a_46 * x**23 + 
a_48 * x**24 + a_50 * x**25 + a_52 * x**26 + a_54 * x**27 + 
a_56 * x**28 + a_58 * x**29 + a_60 * x**30 + a_62 * x**31 + 
a_64 * x**32 + a_66 * x**33 + a_68 * x**34 + a_70 * x**35 + 
a_72 * x**36 + a_74 * x**37 + a_76 * x**38 + a_78 * x**39 + 
a_80 * x**40 + a_82 * x**41 + a_84 * x**42 + a_86 * x**43 + 
a_88 * x**44 + a_90 * x**45 + a_92 * x**46 + a_94 * x**47 + 
a_96 * x**48 + a_98 * x**49 + a_100 * x**50 + a_102 * x**51 + 
a_104 * x**52 + a_106 * x**53 + a_108 * x**54 + a_110 * x**55 + 
a_112 * x**56 + a_114 * x**57 + a_116 * x**58 + a_118 * x**59 + 
a_120 * x**60 + a_122 * x**61 + a_124 * x**62 + a_126 * x**63 + 
a_128 * x**64 + a_130 * x**65 + a_132 * x**66 + a_134 * x**67 + 
a_136 * x**68 + a_138 * x**69 + a_140 * x**70 + a_142 * x**71 + 
a_144 * x**72 + a_146 * x**73 + a_148 * x**74 + a_150 * x**75 + 
a_152 * x**76 + a_154 * x**77 + a_156 * x**78 + a_158 * x**79 + 
a_160 * x**80 + a_162 * x**81 + a_164 * x**82 + a_166 * x**83 + 
a_168 * x**84 + a_170 * x**85 + a_172 * x**86 + a_174 * x**87 + 
a_176 * x**88 + a_178 * x**89 + a_180 * x**90 + a_182 * x**91 + 
a_184 * x**92 + a_186 * x**93 + a_188 * x**94 + a_190 * x**95 + 
a_192 * x**96 + a_194 * x**97 + a_196 * x**98 + a_198 * x**99 + 
a_200 * x**100 + a_202 * x**101 + a_204 * x**102 + a_206 * x**103 + 
a_208 * x**104 + a_210 * x**105 + a_212 * x**106 + a_214 * x**107 + 
a_216 * x**108 + a_218 * x**109 + a_220 * x**110 + a_222 * x**111 + 
a_224 * x**112 + a_226 * x**113 + a_228 * x**114 + a_230 * x**115 + 
a_232 * x**116 + a_234 * x**117 + a_236 * x**118 + a_238 * x**119 + 
a_240 * x**120 + a_242 * x**121 + a_244 * x**122 + a_246 * x**123 + 
a_248 * x**124 + a_250 * x**125 + a_252 * x**126 + a_254 * x**127 + 
a_256 * x**128 + a_258 * x**129 + a_260 * x**130 + a_262 * x**131 + 
a_264 * x**132 + a_266 * x**133 + a_268 * x**134 + a_270 * x**135 + 
a_272 * x**136 + a_274 * x**137 + a_276 * x**138 + a_278 * x**139 + 
a_280 * x**140 + a_282 * x**141 + a_284 * x**142 + a_286 * x**143 + 
a_288 * x**144 + a_290 * x**145 + a_292 * x**146 + a_294 * x**147 + 
a_296 * x**148 + a_298 * x**149 + a_300 * x**150 + a_302 * x**151 + 
a_304 * x**152 + a_306 * x**153 + a_308 * x**154 + a_310 * x**155 + 
a_312 * x**156 + a_314 * x**157 + a_316 * x**158 + a_318 * x**159 + 
a_320 * x**160 + a_322 * x**161 + a_324 * x**162 + a_326 * x**163 + 
a_328 * x**164 + a_330 * x**165 + a_332 * x**166 + a_334 * x**167 + 
a_336 * x**168 + a_338 * x**169 + a_340 * x**170 + a_342 * x**171 + 
a_344 * x**172 + a_346 * x**173 + a_348 * x**174 + a_350 * x**175 + 
a_352 * x**176 + a_354 * x**177 + a_356 * x**178 + a_358 * x**179 + 
a_360 * x**180 + a_362 * x**181 + a_364 * x**182 + a_366 * x**183 + 
a_368 * x**184 + a_370 * x**185 + a_372 * x**186 + a_374 * x**187 + 
a_376 * x**188 + a_378 * x**189 + a_380 * x**190 + a_382 * x**191 + 
a_384 * x**192 + a_386 * x**193 + a_388 * x**194 + a_390 * x**195 + 
a_392 * x**196 + a_394 * x**197 + a_396 * x**198 + a_398 * x**199 + 
a_400 * x**200 + a_402 * x**201 + a_404 * x**202 + a_406 * x**203 + 
a_408 * x**204 + a_410 * x**205 + a_412 * x**206 + a_414 * x**207 + 
a_416 * x**208 + a_418 * x**209 + a_420 * x**210 + a_422 * x**211 + 
a_424 * x**212 + a_426 * x**213 + a_428 * x**214 + a_430 * x**215 + 
a_432 * x**216 + a_434 * x**217 + a_436 * x**218 + a_438 * x**219 + 
a_440 * x**220 + a_442 * x**221 + a_444 * x**222 + a_446 * x**223 + 
a_448 * x**224 + a_450 * x**225 + a_452 * x**226 + a_454 * x**227 + 
a_456 * x**228 + a_458 * x**229 + a_460 * x**230 + a_462 * x**231 + 
a_464 * x**232 + a_466 * x**233 + a_468 * x**234 + a_470 * x**235 + 
a_472 * x**236 + a_474 * x**237 + a_476 * x**238 + a_478 * x**239 + 
a_480 * x**240 + a_482 * x**241 + a_484 * x**242 + a_486 * x**243 + 
a_488 * x**244 + a_490 * x**245 + a_492 * x**246 + a_494 * x**247 + 
a_496 * x**248 + a_498 * x**249 + a_500 * x**250 + a_502 * x**251 + 
a_504 * x**252 + a_506 * x**253 + a_508 * x**254 + a_510 * x**255
,
eqmod 
input_polynomial * input_polynomial
(
r_0*(x**0) + r_2*(x**1)
)
[3329, x**2 - 17],
eqmod 
input_polynomial * input_polynomial
(
r_4*(x**0) + r_6*(x**1)
)
[3329, x**2 - 3312],
eqmod 
input_polynomial * input_polynomial
(
r_8*(x**0) + r_10*(x**1)
)
[3329, x**2 - 2761],
eqmod 
input_polynomial * input_polynomial
(
r_12*(x**0) + r_14*(x**1)
)
[3329, x**2 - 568],
eqmod 
input_polynomial * input_polynomial
(
r_16*(x**0) + r_18*(x**1)
)
[3329, x**2 - 583],
eqmod 
input_polynomial * input_polynomial
(
r_20*(x**0) + r_22*(x**1)
)
[3329, x**2 - 2746],
eqmod 
input_polynomial * input_polynomial
(
r_24*(x**0) + r_26*(x**1)
)
[3329, x**2 - 2649],
eqmod 
input_polynomial * input_polynomial
(
r_28*(x**0) + r_30*(x**1)
)
[3329, x**2 - 680],
eqmod 
input_polynomial * input_polynomial
(
r_32*(x**0) + r_34*(x**1)
)
[3329, x**2 - 1637],
eqmod 
input_polynomial * input_polynomial
(
r_36*(x**0) + r_38*(x**1)
)
[3329, x**2 - 1692],
eqmod 
input_polynomial * input_polynomial
(
r_40*(x**0) + r_42*(x**1)
)
[3329, x**2 - 723],
eqmod 
input_polynomial * input_polynomial
(
r_44*(x**0) + r_46*(x**1)
)
[3329, x**2 - 2606],
eqmod 
input_polynomial * input_polynomial
(
r_48*(x**0) + r_50*(x**1)
)
[3329, x**2 - 2288],
eqmod 
input_polynomial * input_polynomial
(
r_52*(x**0) + r_54*(x**1)
)
[3329, x**2 - 1041],
eqmod 
input_polynomial * input_polynomial
(
r_56*(x**0) + r_58*(x**1)
)
[3329, x**2 - 1100],
eqmod 
input_polynomial * input_polynomial
(
r_60*(x**0) + r_62*(x**1)
)
[3329, x**2 - 2229],
eqmod 
input_polynomial * input_polynomial
(
r_64*(x**0) + r_66*(x**1)
)
[3329, x**2 - 1409],
eqmod 
input_polynomial * input_polynomial
(
r_68*(x**0) + r_70*(x**1)
)
[3329, x**2 - 1920],
eqmod 
input_polynomial * input_polynomial
(
r_72*(x**0) + r_74*(x**1)
)
[3329, x**2 - 2662],
eqmod 
input_polynomial * input_polynomial
(
r_76*(x**0) + r_78*(x**1)
)
[3329, x**2 - 667],
eqmod 
input_polynomial * input_polynomial
(
r_80*(x**0) + r_82*(x**1)
)
[3329, x**2 - 3281],
eqmod 
input_polynomial * input_polynomial
(
r_84*(x**0) + r_86*(x**1)
)
[3329, x**2 - 48],
eqmod 
input_polynomial * input_polynomial
(
r_88*(x**0) + r_90*(x**1)
)
[3329, x**2 - 233],
eqmod 
input_polynomial * input_polynomial
(
r_92*(x**0) + r_94*(x**1)
)
[3329, x**2 - 3096],
eqmod 
input_polynomial * input_polynomial
(
r_96*(x**0) + r_98*(x**1)
)
[3329, x**2 - 756],
eqmod 
input_polynomial * input_polynomial
(
r_100*(x**0) + r_102*(x**1)
)
[3329, x**2 - 2573],
eqmod 
input_polynomial * input_polynomial
(
r_104*(x**0) + r_106*(x**1)
)
[3329, x**2 - 2156],
eqmod 
input_polynomial * input_polynomial
(
r_108*(x**0) + r_110*(x**1)
)
[3329, x**2 - 1173],
eqmod 
input_polynomial * input_polynomial
(
r_112*(x**0) + r_114*(x**1)
)
[3329, x**2 - 3015],
eqmod 
input_polynomial * input_polynomial
(
r_116*(x**0) + r_118*(x**1)
)
[3329, x**2 - 314],
eqmod 
input_polynomial * input_polynomial
(
r_120*(x**0) + r_122*(x**1)
)
[3329, x**2 - 3050],
eqmod 
input_polynomial * input_polynomial
(
r_124*(x**0) + r_126*(x**1)
)
[3329, x**2 - 279],
eqmod 
input_polynomial * input_polynomial
(
r_128*(x**0) + r_130*(x**1)
)
[3329, x**2 - 1703],
eqmod 
input_polynomial * input_polynomial
(
r_132*(x**0) + r_134*(x**1)
)
[3329, x**2 - 1626],
eqmod 
input_polynomial * input_polynomial
(
r_136*(x**0) + r_138*(x**1)
)
[3329, x**2 - 1651],
eqmod 
input_polynomial * input_polynomial
(
r_140*(x**0) + r_142*(x**1)
)
[3329, x**2 - 1678],
eqmod 
input_polynomial * input_polynomial
(
r_144*(x**0) + r_146*(x**1)
)
[3329, x**2 - 2789],
eqmod 
input_polynomial * input_polynomial
(
r_148*(x**0) + r_150*(x**1)
)
[3329, x**2 - 540],
eqmod 
input_polynomial * input_polynomial
(
r_152*(x**0) + r_154*(x**1)
)
[3329, x**2 - 1789],
eqmod 
input_polynomial * input_polynomial
(
r_156*(x**0) + r_158*(x**1)
)
[3329, x**2 - 1540],
eqmod 
input_polynomial * input_polynomial
(
r_160*(x**0) + r_162*(x**1)
)
[3329, x**2 - 1847],
eqmod 
input_polynomial * input_polynomial
(
r_164*(x**0) + r_166*(x**1)
)
[3329, x**2 - 1482],
eqmod 
input_polynomial * input_polynomial
(
r_168*(x**0) + r_170*(x**1)
)
[3329, x**2 - 952],
eqmod 
input_polynomial * input_polynomial
(
r_172*(x**0) + r_174*(x**1)
)
[3329, x**2 - 2377],
eqmod 
input_polynomial * input_polynomial
(
r_176*(x**0) + r_178*(x**1)
)
[3329, x**2 - 1461],
eqmod 
input_polynomial * input_polynomial
(
r_180*(x**0) + r_182*(x**1)
)
[3329, x**2 - 1868],
eqmod 
input_polynomial * input_polynomial
(
r_184*(x**0) + r_186*(x**1)
)
[3329, x**2 - 2687],
eqmod 
input_polynomial * input_polynomial
(
r_188*(x**0) + r_190*(x**1)
)
[3329, x**2 - 642],
eqmod 
input_polynomial * input_polynomial
(
r_192*(x**0) + r_194*(x**1)
)
[3329, x**2 - 939],
eqmod 
input_polynomial * input_polynomial
(
r_196*(x**0) + r_198*(x**1)
)
[3329, x**2 - 2390],
eqmod 
input_polynomial * input_polynomial
(
r_200*(x**0) + r_202*(x**1)
)
[3329, x**2 - 2308],
eqmod 
input_polynomial * input_polynomial
(
r_204*(x**0) + r_206*(x**1)
)
[3329, x**2 - 1021],
eqmod 
input_polynomial * input_polynomial
(
r_208*(x**0) + r_210*(x**1)
)
[3329, x**2 - 2437],
eqmod 
input_polynomial * input_polynomial
(
r_212*(x**0) + r_214*(x**1)
)
[3329, x**2 - 892],
eqmod 
input_polynomial * input_polynomial
(
r_216*(x**0) + r_218*(x**1)
)
[3329, x**2 - 2388],
eqmod 
input_polynomial * input_polynomial
(
r_220*(x**0) + r_222*(x**1)
)
[3329, x**2 - 941],
eqmod 
input_polynomial * input_polynomial
(
r_224*(x**0) + r_226*(x**1)
)
[3329, x**2 - 733],
eqmod 
input_polynomial * input_polynomial
(
r_228*(x**0) + r_230*(x**1)
)
[3329, x**2 - 2596],
eqmod 
input_polynomial * input_polynomial
(
r_232*(x**0) + r_234*(x**1)
)
[3329, x**2 - 2337],
eqmod 
input_polynomial * input_polynomial
(
r_236*(x**0) + r_238*(x**1)
)
[3329, x**2 - 992],
eqmod 
input_polynomial * input_polynomial
(
r_240*(x**0) + r_242*(x**1)
)
[3329, x**2 - 268],
eqmod 
input_polynomial * input_polynomial
(
r_244*(x**0) + r_246*(x**1)
)
[3329, x**2 - 3061],
eqmod 
input_polynomial * input_polynomial
(
r_248*(x**0) + r_250*(x**1)
)
[3329, x**2 - 641],
eqmod 
input_polynomial * input_polynomial
(
r_252*(x**0) + r_254*(x**1)
)
[3329, x**2 - 2688],
eqmod 
input_polynomial * input_polynomial
(
r_256*(x**0) + r_258*(x**1)
)
[3329, x**2 - 1584],
eqmod 
input_polynomial * input_polynomial
(
r_260*(x**0) + r_262*(x**1)
)
[3329, x**2 - 1745],
eqmod 
input_polynomial * input_polynomial
(
r_264*(x**0) + r_266*(x**1)
)
[3329, x**2 - 2298],
eqmod 
input_polynomial * input_polynomial
(
r_268*(x**0) + r_270*(x**1)
)
[3329, x**2 - 1031],
eqmod 
input_polynomial * input_polynomial
(
r_272*(x**0) + r_274*(x**1)
)
[3329, x**2 - 2037],
eqmod 
input_polynomial * input_polynomial
(
r_276*(x**0) + r_278*(x**1)
)
[3329, x**2 - 1292],
eqmod 
input_polynomial * input_polynomial
(
r_280*(x**0) + r_282*(x**1)
)
[3329, x**2 - 3220],
eqmod 
input_polynomial * input_polynomial
(
r_284*(x**0) + r_286*(x**1)
)
[3329, x**2 - 109],
eqmod 
input_polynomial * input_polynomial
(
r_288*(x**0) + r_290*(x**1)
)
[3329, x**2 - 375],
eqmod 
input_polynomial * input_polynomial
(
r_292*(x**0) + r_294*(x**1)
)
[3329, x**2 - 2954],
eqmod 
input_polynomial * input_polynomial
(
r_296*(x**0) + r_298*(x**1)
)
[3329, x**2 - 2549],
eqmod 
input_polynomial * input_polynomial
(
r_300*(x**0) + r_302*(x**1)
)
[3329, x**2 - 780],
eqmod 
input_polynomial * input_polynomial
(
r_304*(x**0) + r_306*(x**1)
)
[3329, x**2 - 2090],
eqmod 
input_polynomial * input_polynomial
(
r_308*(x**0) + r_310*(x**1)
)
[3329, x**2 - 1239],
eqmod 
input_polynomial * input_polynomial
(
r_312*(x**0) + r_314*(x**1)
)
[3329, x**2 - 1645],
eqmod 
input_polynomial * input_polynomial
(
r_316*(x**0) + r_318*(x**1)
)
[3329, x**2 - 1684],
eqmod 
input_polynomial * input_polynomial
(
r_320*(x**0) + r_322*(x**1)
)
[3329, x**2 - 1063],
eqmod 
input_polynomial * input_polynomial
(
r_324*(x**0) + r_326*(x**1)
)
[3329, x**2 - 2266],
eqmod 
input_polynomial * input_polynomial
(
r_328*(x**0) + r_330*(x**1)
)
[3329, x**2 - 319],
eqmod 
input_polynomial * input_polynomial
(
r_332*(x**0) + r_334*(x**1)
)
[3329, x**2 - 3010],
eqmod 
input_polynomial * input_polynomial
(
r_336*(x**0) + r_338*(x**1)
)
[3329, x**2 - 2773],
eqmod 
input_polynomial * input_polynomial
(
r_340*(x**0) + r_342*(x**1)
)
[3329, x**2 - 556],
eqmod 
input_polynomial * input_polynomial
(
r_344*(x**0) + r_346*(x**1)
)
[3329, x**2 - 757],
eqmod 
input_polynomial * input_polynomial
(
r_348*(x**0) + r_350*(x**1)
)
[3329, x**2 - 2572],
eqmod 
input_polynomial * input_polynomial
(
r_352*(x**0) + r_354*(x**1)
)
[3329, x**2 - 2099],
eqmod 
input_polynomial * input_polynomial
(
r_356*(x**0) + r_358*(x**1)
)
[3329, x**2 - 1230],
eqmod 
input_polynomial * input_polynomial
(
r_360*(x**0) + r_362*(x**1)
)
[3329, x**2 - 561],
eqmod 
input_polynomial * input_polynomial
(
r_364*(x**0) + r_366*(x**1)
)
[3329, x**2 - 2768],
eqmod 
input_polynomial * input_polynomial
(
r_368*(x**0) + r_370*(x**1)
)
[3329, x**2 - 2466],
eqmod 
input_polynomial * input_polynomial
(
r_372*(x**0) + r_374*(x**1)
)
[3329, x**2 - 863],
eqmod 
input_polynomial * input_polynomial
(
r_376*(x**0) + r_378*(x**1)
)
[3329, x**2 - 2594],
eqmod 
input_polynomial * input_polynomial
(
r_380*(x**0) + r_382*(x**1)
)
[3329, x**2 - 735],
eqmod 
input_polynomial * input_polynomial
(
r_384*(x**0) + r_386*(x**1)
)
[3329, x**2 - 2804],
eqmod 
input_polynomial * input_polynomial
(
r_388*(x**0) + r_390*(x**1)
)
[3329, x**2 - 525],
eqmod 
input_polynomial * input_polynomial
(
r_392*(x**0) + r_394*(x**1)
)
[3329, x**2 - 1092],
eqmod 
input_polynomial * input_polynomial
(
r_396*(x**0) + r_398*(x**1)
)
[3329, x**2 - 2237],
eqmod 
input_polynomial * input_polynomial
(
r_400*(x**0) + r_402*(x**1)
)
[3329, x**2 - 403],
eqmod 
input_polynomial * input_polynomial
(
r_404*(x**0) + r_406*(x**1)
)
[3329, x**2 - 2926],
eqmod 
input_polynomial * input_polynomial
(
r_408*(x**0) + r_410*(x**1)
)
[3329, x**2 - 1026],
eqmod 
input_polynomial * input_polynomial
(
r_412*(x**0) + r_414*(x**1)
)
[3329, x**2 - 2303],
eqmod 
input_polynomial * input_polynomial
(
r_416*(x**0) + r_418*(x**1)
)
[3329, x**2 - 1143],
eqmod 
input_polynomial * input_polynomial
(
r_420*(x**0) + r_422*(x**1)
)
[3329, x**2 - 2186],
eqmod 
input_polynomial * input_polynomial
(
r_424*(x**0) + r_426*(x**1)
)
[3329, x**2 - 2150],
eqmod 
input_polynomial * input_polynomial
(
r_428*(x**0) + r_430*(x**1)
)
[3329, x**2 - 1179],
eqmod 
input_polynomial * input_polynomial
(
r_432*(x**0) + r_434*(x**1)
)
[3329, x**2 - 2775],
eqmod 
input_polynomial * input_polynomial
(
r_436*(x**0) + r_438*(x**1)
)
[3329, x**2 - 554],
eqmod 
input_polynomial * input_polynomial
(
r_440*(x**0) + r_442*(x**1)
)
[3329, x**2 - 886],
eqmod 
input_polynomial * input_polynomial
(
r_444*(x**0) + r_446*(x**1)
)
[3329, x**2 - 2443],
eqmod 
input_polynomial * input_polynomial
(
r_448*(x**0) + r_450*(x**1)
)
[3329, x**2 - 1722],
eqmod 
input_polynomial * input_polynomial
(
r_452*(x**0) + r_454*(x**1)
)
[3329, x**2 - 1607],
eqmod 
input_polynomial * input_polynomial
(
r_456*(x**0) + r_458*(x**1)
)
[3329, x**2 - 1212],
eqmod 
input_polynomial * input_polynomial
(
r_460*(x**0) + r_462*(x**1)
)
[3329, x**2 - 2117],
eqmod 
input_polynomial * input_polynomial
(
r_464*(x**0) + r_466*(x**1)
)
[3329, x**2 - 1874],
eqmod 
input_polynomial * input_polynomial
(
r_468*(x**0) + r_470*(x**1)
)
[3329, x**2 - 1455],
eqmod 
input_polynomial * input_polynomial
(
r_472*(x**0) + r_474*(x**1)
)
[3329, x**2 - 1029],
eqmod 
input_polynomial * input_polynomial
(
r_476*(x**0) + r_478*(x**1)
)
[3329, x**2 - 2300],
eqmod 
input_polynomial * input_polynomial
(
r_480*(x**0) + r_482*(x**1)
)
[3329, x**2 - 2110],
eqmod 
input_polynomial * input_polynomial
(
r_484*(x**0) + r_486*(x**1)
)
[3329, x**2 - 1219],
eqmod 
input_polynomial * input_polynomial
(
r_488*(x**0) + r_490*(x**1)
)
[3329, x**2 - 2935],
eqmod 
input_polynomial * input_polynomial
(
r_492*(x**0) + r_494*(x**1)
)
[3329, x**2 - 394],
eqmod 
input_polynomial * input_polynomial
(
r_496*(x**0) + r_498*(x**1)
)
[3329, x**2 - 885],
eqmod 
input_polynomial * input_polynomial
(
r_500*(x**0) + r_502*(x**1)
)
[3329, x**2 - 2444],
eqmod 
input_polynomial * input_polynomial
(
r_504*(x**0) + r_506*(x**1)
)
[3329, x**2 - 2154],
eqmod 
input_polynomial * input_polynomial
(
r_508*(x**0) + r_510*(x**1)
)
[3329, x**2 - 1175]
] && and [
   (-9)@16 * 3329@16 <s mem0_0, mem0_0 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_2, mem0_2 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_4, mem0_4 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_6, mem0_6 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_8, mem0_8 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_10, mem0_10 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_12, mem0_12 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_14, mem0_14 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_16, mem0_16 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_18, mem0_18 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_20, mem0_20 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_22, mem0_22 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_24, mem0_24 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_26, mem0_26 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_28, mem0_28 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_30, mem0_30 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_32, mem0_32 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_34, mem0_34 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_36, mem0_36 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_38, mem0_38 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_40, mem0_40 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_42, mem0_42 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_44, mem0_44 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_46, mem0_46 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_48, mem0_48 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_50, mem0_50 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_52, mem0_52 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_54, mem0_54 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_56, mem0_56 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_58, mem0_58 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_60, mem0_60 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_62, mem0_62 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_64, mem0_64 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_66, mem0_66 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_68, mem0_68 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_70, mem0_70 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_72, mem0_72 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_74, mem0_74 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_76, mem0_76 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_78, mem0_78 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_80, mem0_80 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_82, mem0_82 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_84, mem0_84 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_86, mem0_86 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_88, mem0_88 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_90, mem0_90 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_92, mem0_92 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_94, mem0_94 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_96, mem0_96 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_98, mem0_98 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_100, mem0_100 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_102, mem0_102 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_104, mem0_104 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_106, mem0_106 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_108, mem0_108 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_110, mem0_110 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_112, mem0_112 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_114, mem0_114 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_116, mem0_116 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_118, mem0_118 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_120, mem0_120 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_122, mem0_122 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_124, mem0_124 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_126, mem0_126 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_128, mem0_128 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_130, mem0_130 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_132, mem0_132 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_134, mem0_134 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_136, mem0_136 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_138, mem0_138 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_140, mem0_140 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_142, mem0_142 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_144, mem0_144 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_146, mem0_146 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_148, mem0_148 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_150, mem0_150 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_152, mem0_152 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_154, mem0_154 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_156, mem0_156 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_158, mem0_158 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_160, mem0_160 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_162, mem0_162 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_164, mem0_164 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_166, mem0_166 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_168, mem0_168 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_170, mem0_170 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_172, mem0_172 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_174, mem0_174 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_176, mem0_176 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_178, mem0_178 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_180, mem0_180 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_182, mem0_182 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_184, mem0_184 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_186, mem0_186 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_188, mem0_188 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_190, mem0_190 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_192, mem0_192 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_194, mem0_194 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_196, mem0_196 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_198, mem0_198 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_200, mem0_200 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_202, mem0_202 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_204, mem0_204 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_206, mem0_206 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_208, mem0_208 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_210, mem0_210 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_212, mem0_212 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_214, mem0_214 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_216, mem0_216 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_218, mem0_218 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_220, mem0_220 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_222, mem0_222 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_224, mem0_224 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_226, mem0_226 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_228, mem0_228 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_230, mem0_230 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_232, mem0_232 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_234, mem0_234 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_236, mem0_236 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_238, mem0_238 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_240, mem0_240 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_242, mem0_242 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_244, mem0_244 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_246, mem0_246 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_248, mem0_248 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_250, mem0_250 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_252, mem0_252 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_254, mem0_254 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_256, mem0_256 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_258, mem0_258 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_260, mem0_260 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_262, mem0_262 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_264, mem0_264 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_266, mem0_266 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_268, mem0_268 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_270, mem0_270 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_272, mem0_272 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_274, mem0_274 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_276, mem0_276 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_278, mem0_278 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_280, mem0_280 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_282, mem0_282 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_284, mem0_284 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_286, mem0_286 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_288, mem0_288 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_290, mem0_290 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_292, mem0_292 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_294, mem0_294 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_296, mem0_296 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_298, mem0_298 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_300, mem0_300 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_302, mem0_302 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_304, mem0_304 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_306, mem0_306 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_308, mem0_308 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_310, mem0_310 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_312, mem0_312 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_314, mem0_314 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_316, mem0_316 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_318, mem0_318 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_320, mem0_320 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_322, mem0_322 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_324, mem0_324 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_326, mem0_326 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_328, mem0_328 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_330, mem0_330 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_332, mem0_332 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_334, mem0_334 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_336, mem0_336 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_338, mem0_338 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_340, mem0_340 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_342, mem0_342 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_344, mem0_344 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_346, mem0_346 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_348, mem0_348 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_350, mem0_350 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_352, mem0_352 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_354, mem0_354 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_356, mem0_356 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_358, mem0_358 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_360, mem0_360 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_362, mem0_362 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_364, mem0_364 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_366, mem0_366 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_368, mem0_368 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_370, mem0_370 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_372, mem0_372 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_374, mem0_374 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_376, mem0_376 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_378, mem0_378 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_380, mem0_380 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_382, mem0_382 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_384, mem0_384 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_386, mem0_386 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_388, mem0_388 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_390, mem0_390 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_392, mem0_392 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_394, mem0_394 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_396, mem0_396 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_398, mem0_398 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_400, mem0_400 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_402, mem0_402 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_404, mem0_404 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_406, mem0_406 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_408, mem0_408 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_410, mem0_410 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_412, mem0_412 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_414, mem0_414 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_416, mem0_416 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_418, mem0_418 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_420, mem0_420 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_422, mem0_422 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_424, mem0_424 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_426, mem0_426 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_428, mem0_428 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_430, mem0_430 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_432, mem0_432 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_434, mem0_434 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_436, mem0_436 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_438, mem0_438 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_440, mem0_440 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_442, mem0_442 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_444, mem0_444 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_446, mem0_446 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_448, mem0_448 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_450, mem0_450 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_452, mem0_452 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_454, mem0_454 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_456, mem0_456 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_458, mem0_458 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_460, mem0_460 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_462, mem0_462 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_464, mem0_464 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_466, mem0_466 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_468, mem0_468 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_470, mem0_470 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_472, mem0_472 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_474, mem0_474 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_476, mem0_476 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_478, mem0_478 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_480, mem0_480 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_482, mem0_482 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_484, mem0_484 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_486, mem0_486 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_488, mem0_488 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_490, mem0_490 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_492, mem0_492 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_494, mem0_494 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_496, mem0_496 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_498, mem0_498 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_500, mem0_500 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_502, mem0_502 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_504, mem0_504 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_506, mem0_506 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_508, mem0_508 <s 9@16 * 3329@16,
   (-9)@16 * 3329@16 <s mem0_510, mem0_510 <s 9@16 * 3329@16
]
}

